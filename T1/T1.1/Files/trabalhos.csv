abstract,anexo,area,autor,biblioteca,data_defesa,idioma,ies,keywords,linha,orientador,paginas,palavras_chave,programa,projeto_pesquisa,resumo,tipo,titulo,trabalho_id,volume
"Spatial concentrations (or spatial clusters) of moving objects, such
as vehicles and humans, is a mobility pattern that is relevant to many
applications. A fast detection of this pattern and its evolution, e.g., if
the cluster is shrinking or growing, is useful in numerous scenarios, such as
detecting the formation of traffic jams or detecting a fast dispersion of people
in a music concert. An on-line detection of this pattern is a challenging
task because it requires algorithms that are capable of continuously and
efficiently processing the high volume of position updates in a timely manner.
Currently, the majority of approaches for spatial cluster detection operate in
batch mode, where moving objects location updates are recorded during time
periods of certain length and then batch-processed by an external routine,
thus delaying the result of the cluster detection until the end of the time
period. Further, they extensively use spatial data structures and operators,
which can be troublesome to maintain or parallelize in on-line scenarios. To
address these issues, in this thesis we propose DG2CEP, an algorithm that
combines the well-known density-based clustering algorithm DBSCAN with
the data stream processing paradigm Complex Event Processing (CEP) to
achieve continuous and timely detection of spatial clusters. Our experiments
with real world data streams indicate that DG2CEP is able to detect the
formation and dispersion of clusters with small latency while having a higher
similarity to DBSCAN than batch-based approaches.",Marcos P. Roriz_2017_Completo.pdf,REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,MARCOS PAULINO RORIZ JUNIOR,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,22/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Spatial Clustering;Stream Clustering;Real-time Clustering;On-line Clustering;Complex Event Processing.',"REDES MÓVEIS, COMPUTAÇÃO UBÍQUA E COMPUTAÇÃO AUTONÔMICA",MARKUS ENDLER,121,Aglomeração Espacial;Aglomeração em Fluxo de Dados;Aglomeração em Tempo Real;Detecção On-line de Aglomerados;Processamento de Fluxo de Dados.,INFORMÁTICA (31005012004P9),-,"Clusters (ou concentrações) de objetos móveis, como veículos e seres humanos, é um padrão de mobilidade relevante para muitas aplicações. Uma detecção rápida deste padrão e de sua evolução, por exemplo, se o cluster está encolhendo ou crescendo, é útil em vários cenários, como detectar a formação de engarrafamentos ou detectar uma rápida dispersão de pessoas em um show de música. A detecção on-line deste padrão é uma tarefa desafiadora porque requer algoritmos que sejam capazes de processar de forma contínua e eficiente o alto volume de dados enviados pelos objetos móveis em tempo hábil. Atualmente, a maioria das abordagens para a detecção destes clusters operam em lote. As localizações dos objetos móveis são armazenadas durante um determinado período e depois processadas em lote por uma rotina externa, atrasando o resultado da detecção do cluster até o final do período ou do próximo lote. Além disso, essas abordagem utilizam extensivamente estruturas de dados e operadores espaciais, o que pode ser problemático em cenários de grande fluxos de dados. Com intuito de abordar estes problemas, propomos nesta tese o DG2CEP, um algoritmo que combina o conhecido algoritmo de aglomeração por densidade (DBSCAN) com o paradigma de processamento de fluxos de dados (Complex Event Processing) para a detecção contínua e rápida dos aglomerados. Nossos experimentos com dados reais indicam que o DG2CEP é capaz de detectar a formação e dispersão de clusters rapidamente, em menos de alguns segundos, para milhares de objetos móveis. Além disso, os resultados obtidos indicam que o DG2CEP possui maior similaridade com DBSCAN do que abordagens baseadas em lote.",TESE,DG2CEP: An On-line Algorithm for Real-time Detection of Spatial Clusters from Large Data Streams through Complex Event Processing,5013040,1
"Quotation Extraction consists of identifying quotations from a text
and associating them to their authors. In this work, we present a Direct and
Indirect Quotation Extraction System for Portuguese. Quotation Extraction
has been previously approached using different techniques and for several
languages. Our proposal differs from previous work, because we build a
Machine Learning model that, besides recognizing direct quotations, it also
recognizes indirect ones in Portuguese. Indirect quotations are hard to be
identified in a text, due to the lack of explicit delimitation. Nevertheless,
they happen more often then the delimited ones and, for this reason, have
an huge importance on information extraction. Due to the fact that we use
a Machine Learning model based, we can easily adapt it to other languages,
needing only a list of verbs of speech for a given language. Few were the
previously proposed systems that tackled the task of indirect quotations
and neither of them for Portuguese using a Machine Learning approach. We
build a Quotation Extractor using a model for the Structured Perceptron
algorithm. In order to train and evaluate the system, we build QuoTrees
1.0 corpus. We annotate it to tackle the indirect quotation problem. The
Structured Perceptron based on weight interval scheduling obtains an F1
score of 66% for QuoTrees 1.0 corpus.",Rafael dos Reis_2017_Completo.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,RAFAEL DOS REIS SILVA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,08/02/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Machine Learning;Natural Language Processing;Information Extraction;Quotation Extraction;Structured Perceptron;Weighted Interval Scheduling',"APRENDIZADO DE MÁQUINA, RACIOCÍNIO HEURÍSTICO E PROBABILÍSTICO",RUY LUIZ MILIDIU,59,Processamento de Linguagem Natural;Extração de Informação;Extração de Citação;Perceptron Estruturado;Agendamento de Tarefas Ponderado,INFORMÁTICA (31005012004P9),-,"A Extração de Citações consiste na identificação de citações de um
texto e na associação destas com seus autores. Neste trabalho, apresentamos
um Extrator de Citações Diretas e Indiretas para o Português. A tarefa
de Extração de Citações já foi abordada usando diversas técnicas em
diversos idiomas. Nossa proposta difere das anteriores, pois construímos um
modelo de Aprendizado de Máquina que, além de indetificar citações diretas,
também identifica as citações indiretas. Citações indiretas são difíceis de
serem identificadas num texto por não conter delimitações explícitas. Porém,
são mais frequentes do que as delimitadas e, por essa razão, possuem
grande importância na extração de informação. Por utilizarmos um modelo
baseado em Aprendizado de Máquina, podemos facilmente adaptá-lo para
outras línguas, bastando apenas uma lista de verbos do dizer num dado
idioma. Poucos foram os sistemas propostos anteriormente que atacaram o
problema das citações indiretas e nenhum deles para o Português usando
Aprendizado de Máquina. Nós construímos um Extrator de Citações usando
um modelo para o algoritmo do Perceptron Estruturado. Com o objetivo
de treinar e avaliar o sistema, construímos o corpus QuoTrees 1.0. Nós
anotamos este corpus a fim de atacar o problema das citações indiretas. O
Perceptron Estruturado baseado no agendamento de tarefas ponderado tem
desempenho F1 igual a 66% para o corpus QuoTrees 1.0.",DISSERTAÇÃO,Direct and Indirect Quotation Extraction for Portuguese,5013130,1
"The classification of objects in real contexts is the technological apex
of object recognition. This type of classification is complex, containing
diverse computer vision problems in abundance. This project proposes
to solve that type of classification through the use of machine learning
knowledge applied to the MS COCO dataset. The implemented algorithm
in this project consists of a Convolutional Neural Network model that
is able to learn characteristics of the objects and predict their classes.
Some experiments are made that compare different results of predictions
using different techniques of learning. There is also a comparison of the
results from the implementation with state of art in contextual objects
segmentation.",Luis Marcelo Vital_2017_Completo.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,LUIS MARCELO VITAL ABREU FONSECA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,07/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Machine Learning;Image Processing;Convolutional Neural Networks;Object Classification;Data Mining',"APRENDIZADO DE MÁQUINA, RACIOCÍNIO HEURÍSTICO E PROBABILÍSTICO",RUY LUIZ MILIDIU,96,Aprendizado de Máquina;Processamento de Imagens;Redes Neurais Convolutivas;Classificação de Objetos;Mineração de Dados.,INFORMÁTICA (31005012004P9),-,"A classificação de imagens em contexto real é o ápice tecnológico do reconhecimento de objetos. Esse tipo de classificação é o mais complexo possível, contendo diversos problemas de visão computacional em abundância. Este projeto propõe solucionar esse tipo de classificação através do uso do conhecimento no aprendizado de máquina aplicado ao dataset do MS COCO. O algoritmo implementado neste projeto consiste de um modelo de Rede Neural Convolutiva que consegue aprender características dos objetos e realizar predições sobre suas classes. São elaborados alguns experimentos que comparam diferentes resultados de predições a partir de diferentes técnicas de aprendizado. É também realizada uma comparação dos resultados da implementação com o estado da arte na segmentação de objetos em contexto.",DISSERTAÇÃO,Classificação de Objetos em Contexto Real por Redes Neurais Convolutivas,5013137,1
"Quotation Extraction and Attribution is the task of identifying quotations
from a given text and associating them to their authors. In this work,
we present a Quotation Extraction and Attribution system for the Portuguese
language. The Quotation Extraction and Attribution task has been
previously approached using various techniques and for a variety of languages
and datasets. Traditional models to this task consist of extracting a rich
set of hand-designed features and using them to feed a shallow classifier. In
this work, unlike the traditional approach, we avoid using hand-designed features
using unsupervised learning techniques and deep neural networks to
automatically learn relevant features to solve the task. By avoiding design
features by hand, our machine learning model became easily adaptable to
other languages and domains. Our model is trained and evaluated at the
GloboQuotes corpus, and its F1 performance metric is equal to 89.43%.",Luis Felipe Miller 2017_Completo.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,LUIS FELIPE MULLER DE OLIVEIRA HENRIQUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,08/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Machine learning;Natural language processing;Quotation extraction;Neural networks;Deep learning.',-,RUY LUIZ MILIDIU,68,Aprendizado de máquina;Processamento de linguagem natural;Extração de citações;Redes neurais;Deep learning,INFORMÁTICA (31005012004P9),-,"A Extração e Atribuição de Citações é a tarefa de identificar citações de um texto e associá-las a seus autores. Neste trabalho, apresentamos um sistema de Extração e Atribuição de Citações para a língua portuguesa. A tarefa de Extração e Atribuição de Citações foi abordada anteriormente utilizando diversas técnicas e para uma variedade de linguagens e datasets. Os modelos tradicionais para a tarefa, consistem em extrair manualmente um rico conjunto de atributos e usá-los para alimentar um classificador raso. Neste trabalho, ao contrário da abordagem tradicional, evitamos usar attributos projetados à mão, usando técnicas de aprendizagem não supervisionadas e redes neurais profundas para automaticamente aprender atributos relevantes para resolver a tarefa. Ao evitar a criação manual de atributos, nosso modelo de aprendizagem de máquina, tornou-se facilmente adaptável a outros domínios e línguagens. Nosso modelo foi treinado e avaliado no corpus GloboQuotes e sua métrica de desempenho F1 é igual a 89.43%.",DISSERTAÇÃO,Deep Architecture for Quotation Extraction,5017265,1
"Driving is a daily task that allows individuals to travel faster and more comfortably, however, more than half of fatal crashes are related to recklessness driving behaviors. Reckless maneuvers can be detected with accuracy by analyzing data related to driver-vehicle interactions, abrupt turns, acceleration, and deceleration, for instance. Although there are algorithms for online anomaly detection, they are usually designed to run on computers with high computational power. In addition, they typically target scale through parallel computing, grid computing, or cloud computing. This thesis presents an online anomaly detection approach based on complex event processing to enable driving behavior classification. In addition, we investigate if mobile devices with limited computational power, such as smartphones, can be used for online detection of driving behavior. To do so, we first model and evaluate three online anomaly detection algorithms in the data stream processing paradigm, which receive data from the smartphone and the in-vehicle embedded sensors as input. The advantages that stream processing provides lies in the fact that reduce the amount of data transmitted from the mobile device to servers/the cloud, as well as reduce the energy/battery usage due to transmission of sensor data and possibility to operate even if the mobile device is disconnected. To classify the drivers, a statistical mechanism used in document mining that evaluates the importance of a word in a collection of documents, called inverse document frequency, has been adapted to identify the importance of an anomaly in a data stream, and then quantitatively evaluate how cautious or reckless drivers' maneuvers are. Finally, an evaluation of the approach (using the algorithm that achieved better result in the first step) was carried out through a case study of the 25 drivers’ driving
behavior. The results show an accuracy of 84% and an average processing time of 100 milliseconds.",Igor Oliveira_2017_Completo.pdf,REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,IGOR OLIVEIRA VASCONCELOS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,31/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Online anomaly detection;complex event processing;in-vehicle sensing;online driving behavior detection;mobile devices',PROJETO E IMPLEMENTAÇÃO DE MIDDLEWARE,MARKUS ENDLER,102,Detecção online de anomalia;processamento de eventos complexos;sensoriamento à bordo do veículo;detecção online do comportamento de condução;dispositivos móveis.,INFORMÁTICA (31005012004P9),-,"Dirigir é uma tarefa diária que permite uma locomoção mais rápida e mais confortável, no entanto, mais da metade dos acidentes fatais estão relacionados à imprudência. Manobras imprudentes podem ser detectadas com boa precisão, analisando dados relativos à interação motorista-veículo, por exemplo, curvas, aceleração e desaceleração abruptas. Embora existam algoritmos para detecção online de anomalias, estes normalmente são projetados para serem executados em computadores com grande poder computacional. Além disso, geralmente visam escala através da computação paralela, computação em grid ou computação em nuvem. Esta tese apresenta uma abordagem baseada em complex event processing para a detecção online de anomalias e classificação do comportamento de condução. Além disso, objetivamos identificar se dispositivos móveis com poder computacional limitado, como os smartphones, podem ser usados para uma detecção online do comportamento de condução. Para isso, modelamos e avaliamos três algoritmos de detecção online de anomalia no paradigma de processamento de fluxos de dados, que recebem os dados dos sensores do smartphone e dos sensores à bordo do veículo como entrada. As vantagens que o processamento de fluxos de dados proporciona reside no fato de que este reduz a quantidade de dados transmitidos do dispositivo móvel para servidores/nuvem, bem como se reduz o consumo de energia/bateria devido à transmissão de dados dos sensores e possibilidade de operação mesmo se o dispositivo móvel estiver desconectado. Para classificar os motoristas, um mecanismo estatístico utilizado na mineração de documentos que avalia a importância de uma palavra em uma coleção de documentos, denominada frequência de documento inversa, foi adaptado para identificar a importância de uma anomalia em um fluxo de dados, e
avaliar quantitativamente o grau de prudência ou imprudência das manobras dos motoristas. Finalmente, uma avaliação da abordagem (usando o algoritmo que obteve melhor resultado na primeira etapa) foi realizada através de um estudo de caso do comportamento de condução de 25 motoristas em cenário real. Os resultados mostram uma acurácia de classificação de 84% e um tempo médio de processamento de 100 milissegundos.",TESE,Detecção móvel e online de anomalia em múltiplos fluxos de dados: Uma abordagem baseada em processamento de eventos complexos para detecção de comportamento de condução.,5017316,1
"This master's dissertation presents a solution for the automatic generation of
examples of use from the textual description of use cases. Use cases describe
specifications in a sufficiently formal way that is enough to automatically generate
usage examples. A generated example is a text in a natural language which is the
paraphrase of one possible manner to use the software, extracted from the use case
and applied to a real context where actors are converted into fictitious personas and
attributes are valued according to the business rules specified in the use case. The
proposed format to present the example aims to allow clients to read, to understand
and to judge whether the expressed behavior is in fact what he wants. With this
approach, it is expected that the customer himself can approve the specifications
and when defects are found, so the specification can quickly be corrected and
reflected in the examples. At the same time, the formalized specification in the form
of a use case will help developers create solutions that are by construction closer to
the correct one when compared to conventional textual specifications.",Fernando Alberto_2017_Completo-1.pdf,ENGENHARIA DE SOFTWARE,FERNANDO ALBERTO CORREIA DOS SANTOS JUNIOR,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,24/04/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Software engineering;Examples of use;Use cases;Behavior generation;Automatic generation of examples of use',-,ARNDT VON STAA,128,Engenharia de software;Exemplos de uso;Casos de uso;Geração de comportamentos;Geração automática de exemplos de uso.,INFORMÁTICA (31005012004P9),-,"Esta dissertação apresenta uma solução que permite a geração automática de exemplos de uso a partir da descrição textual de casos de uso. Os casos de uso descrevem Especificações em um nível de formalização suficiente para a geração dos exemplos. Um exemplo gerado é um texto em linguagem natural que é o resultado da paráfrase de um possível comportamento do software, extraído de um caso de uso e aplicado a um contexto real, em que atores são convertidos em personagens fictícios e os atributos são valorados de acordo com as regras de negócios especificadas no caso de uso. O formato proposto para a construção de exemplos tem como objetivo permitir que clientes possam ler, entender e julgar se o comportamento que está sendo proposto é o desejado. Com isso é esperado que o próprio cliente possa validar as especificações e que, quando defeitos forem encontrados, a especificação possa logo ser corrigida e refletida de volta nos exemplos. Ao mesmo tempo a especificação formalizada na forma de um caso de uso auxiliará desenvolvedores a criar soluções mais próximas do correto por construção, quando comparado com especificações textuais convencionais.",DISSERTAÇÃO,GERAÇÃO AUTOMÁTICA DE EXEMPLOS DE USO A PARTIR DA DESCRIÇÃO TEXTUAL DE CASOS DE USO.,5017330,1
"One oftheproblemsfoundinnaturallanguageprocessingsystems,is
the difficultytoidentifytextualelementsreferringtothesameentity,this
task iscalledcoreference.Solvingthisproblemisanintegralpartofdis-
course comprehensionsinceitallowslanguageuserstoconnectthepieces
of speechinformationconcerningtothesameentity.Consequently,coref-
erence resolutionisakeytaskinnaturallanguageprocessing.Despitethe
large effortsofexistingresearch,thecurrentperformanceofcoreference
resolution systemshasnotreachedasatisfactorylevelyet.Inthiswork,
wedescribeastructurelearningsystemforunrestrictedcoreferencereso-
lution thatexplorestwotechniques:latentcoreferencetreesandautomatic
entropy-guidedfeatureinduction.Thelatenttreemodelingmakesthelearn-
ing problemcomputationallyfeasible,sinceitincorporatesarelevanthidden
structure. Additionally,usinganautomaticfeatureinductionmethod,we
can efcientlybuildenhancednon-linearmodelsusinglinearmodellearning
algorithms, namely,thestructuredandsparseperceptronalgorithm.We
evaluatethesystemontheCoNLL-2012SharedTaskclosedtrackdataset,
for theEnglishportion.Theproposedsystemobtainsa 62:24% valueon
the competition’sofficialscore.Thisresultisbelowthe 65:73%, thestate-
of-the-art performanceforthistask.Nevertheless,oursolutionsignificantly
reduces thetimetoobtaintheclustersofadocument,since,oursystem
takes 0:35 seconds perdocumentinthetestingset,whileinthestate-of-
the-art, ittakes 5 seconds foreachone.",Adriel Garcia_2017_Completo.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,ADRIEL GARCIA HERNANDEZ,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,26/04/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'machinelearning;naturallanguageprocessing;coreferenceresolution;sparse linearmodel;featureinduction;supervisedmachinelearning.',-,RUY LUIZ MILIDIU,62,machine learning;natural language processing;coreference resolution;sparse linear model;feature induction;supervised machine learning.,INFORMÁTICA (31005012004P9),-,"Um dos problemas encontrados nos sistemas de processamento de linguagem natural é a dificuldade em identificar elementos textuais que se referem à mesma entidade. Este fenômeno é chamado de correferência. Resolver esse problema é parte integrante da compreensão do discurso, permitindo que os usuários da linguagem conectem as partes da informação de fala relativas à mesma entidade. Por conseguinte, a resolução de correferência é um importante foco de atenção no processamento da linguagem natural. Apesar da riqueza das pesquisas existentes, o desempenho atual dos sistemas de resolução de correferência ainda não atingiu um nível satisfatório. Neste trabalho, descrevemos um sistema de aprendizado estruturado para resolução de correferência sem restrições que explora duas técnicas: árvores de correferência latente e indução automática de atributos guiadas por entropia. A modelagem de árvore latente torna o problema de aprendizagem computacionalmente viável porque incorpora uma estrutura escondida relevante. Além disso, utilizando um método automático de indução de recursos, podemos construir eficientemente modelos não-lineares, usando algoritmos de aprendizado de modelo linear como, por exemplo, o algoritmo de perceptron estruturado e esparso. Nós avaliamos o sistema para textos em inglês, utilizando o conjunto de dados da CoNLL-2012 Shared Task. Para a língua inglesa, nosso sistema obteve um valor de 62.24% no score oficial dessa competição. Este resultado está abaixo do desempenho no estado da arte para esta tarefa que es de 65.73%. No entanto, nossa solução reduz significativamente o tempo de obtenção dos clusters dos documentos, pois, nosso sistema leva 0.35 segundos por documento no conjunto de testes, enquanto no estado da arte, leva 5 segundos para cada um.",DISSERTAÇÃO,Coreference resolution for the English language,5018684,1
"The main fine tuning strategies used by relational database administrators
are the construction of access structures, such as indexes, partial indexes and
materialized views, and techniques such as denormalization and query rewriting.
These techniques and access structures, together or separately, can improve the
performance of queries submitted to the database. Database partitioning, a
technique traditionally used for data distribution, has also the potential for fine
tuning, since it allows the scanning of tables to be performed only on partitions
that satisfy query predicates. Even in queries with high selectivity predicates,
whose execution plans often use indexes, partitioning can offer even greater
benefit. This dissertation proposes to evaluate the partitioning as a fine tuning
action of relational databases and, for that, develops heuristics for selection of
partitioning strategies and evaluation of its benefit. An evaluation of the quality
of the results obtained is carried out through experiments with a standard
benchmark for this type of research and we have shown that, in certain cases, it is
advantageous to partition data.",Antony Seabra_2017_Completo.pdf,BANCOS DE DADOS,ANTONY SEABRA DE MEDEIROS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,26/04/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'relational databases;DBMS;fine tuning;partitioning',-,SERGIO LIFSCHITZ,156,bancos de dados relacionais;SGBD;sintonia fina;particionamento,INFORMÁTICA (31005012004P9),-,"As principais estratégias de sintonia fina utilizadas por administradores de
bancos de dados relacionais são a construção de estruturas de acesso, como
índices, índices parciais e visões materializadas, e técnicas como
desnormalização e reescrita de consultas. Estas técnicas e estruturas de acesso,juntas ou separadas, podem melhorar o desempenho das consultas submetidas ao banco de dados. O particionamento de tabelas do banco de dados, técnica tradicionalmente utilizada para distribuição de dados, também possui potencial para sintonia fina, pois permite que a varredura das tabelas seja realizada somente nas partições que satisfazem os predicados das consultas. Mesmo em consultas com predicados de seletividade alta, cujos planos de execução frequentemente utilizam índices, o particionamento pode oferecer um benefício ainda maior. Esta dissertação de mestrado propõe avaliar o particionamento como ação de sintonia fina de bancos de dados relacionais e, para tanto,desenvolve heurísticas para seleção de estratégias de particionamento e avaliação do seu benefício. Uma avaliação da qualidade dos resultados obtidos é realizada através de experimentos com um benchmark padrão para este tipo de pesquisa e mostramos que, em certos casos, é vantajoso particionar dados.",DISSERTAÇÃO,Particionamento como ação de sintonia fina de Bancos de Dados Relacionais,5018782,1
"Many applications are made programmable for advanced end-users by
adding facilities such as scripting and macros. Other applications take a
programming language to the center stage of its UI. That is the case, for
example, of the spreadsheet formula language. While scripting has benefited
from the advances of programming language research, producing mature
and reusable languages, the state of UI-level languages lags behind. We
claim that a better understanding of such languages is necessary. In this
work, we model the semantics of existing end-user programming languages
in three different domains: multimedia, spreadsheets and engineering. Our
focus is on dataflow languages, a representative paradigm for end-user
programmable applications. Based on this analysis, we aim to provide
a better understanding of dataflow semantics as used in the context of
end-user programming and propose guidelines for the design of UI-level
languages for end-user programmable applications.",Hisham Hashem_2017.pdf,LINGUAGENS DE PROGRAMAÇÃO,HISHAM HASHEM MUHAMMAD,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,28/04/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'programming languages;semantics;dataflow;end-user programming',PROJETO E IMPLEMENTAÇÃO DE LINGUAGENS DE PROGRAMAÇÃO,ROBERTO IERUSALIMSCHY,184,"Linguagens de programação;semântica, dataflow;programação de usuários finais",INFORMÁTICA (31005012004P9),-,"Muitas aplicações são tornadas programáveis para usuários finais avançados adicionando recursos como scripting e macros. Outras aplicações dão a uma linguagem de programação um papel central na sua interface com o usuário. Esse é o caso, por exemplo, da linguagem de fórmulas de planilhas de cálculo. Enquanto a área de scripting se beneficiou dos avanços das pesquisas em linguagens de programação, produzindo linguagens maduras e reusáveis, o estado das linguagens em nível de interface não teve o mesmo grau de desenvolvimento. Argumentamos que um melhor entendimento desta classe de linguagens se faz necessário. Neste trabalho, modelamos semânticas de linguagens de usuário final existentes, em três diferentes domínios: multimídia, planilhas e engenharia. Nosso foco é em linguagens de dataflow, um paradigma representativo em aplicações programáveis por usuários finais. Com base nessa análise, temos como objetivo prover um melhor entendimento do design de linguagens de dataflow no contexto de programação de usuários finais e propor linhas-guia para o projeto de linguagens de nível de interface baseadas neste paradigma para aplicações programáveis.",TESE,Dataflow Semantics for End-User Programmable Applications,5018829,1
"With the increasing popularity and accessibility of high-quality Virtual
Reality (VR) systems, concerns have been raised about the propensity of VR to
induce balance loss. Balance is essential for safe use of VR experience and its loss
can result in severe injury. This project is set to create a system able to measure the
impact of VR in the human balance system. In this work, we design and conduct an
experiment making use of the Oculus Rift VR headset and MS Kinect Sensor. In
this experiment, we are able to visualize, quantify, and compare the effect of
different VR scenes on the balance of the experiment subjects as well as the effect
of visual and auditory warnings of balance loss.",Armando Henrique2017.pdf,COMPUTAÇÃO GRÁFICA,ARMANDO ENRIQUE MARTINEZ GONZALEZ,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,17/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Virtual Reality;Fall Risk Analysis;Center of Gravity;Center of Mass.',ANIMAÇÃO E REALIDADE VIRTUAL,ALBERTO BARBOSA RAPOSO,56,Realidade Virtual;Análise do Risco de Queda;Centro de Gravidade;Centro de Massa.,INFORMÁTICA (31005012004P9),-,"Com o aumento da popularidade e acessibilidade de sistemas de realidade virtual (RV) de alta qualidade, tem-se levantado preocupações com relação a tendência dos sistemas de realidade virtual em provocar perda de equilíbrio. O equilíbrio é essencial para o uso seguro da realidade virtual e a perda do mesmo pode causar lesões graves. O objetivo deste projeto é criar um sistema para avaliar o impacto da realidade virtual no equilíbrio humano. Neste trabalho, propomos e conduzimos um experimento usando o Oculus Rift e o MS Kinect Sensor. Nesse experimento, foi possível observar, quantificar e comparar o efeito de diferentes cenas de RV no equilíbrio dos usuários, bem como o efeito de avisos visuais e sonoros sobre perda de equilíbrio.",DISSERTAÇÃO,Fall risk analysis during VR interaction.,5018859,1
"Knowledge bases are a powerful tool for supporting a large spectrum of applications such as exploratory search, ranking, and recommendation. Knowledge bases can be viewed as graphs whose nodes represent entities and whose edges represent relationships. Currently, search engines take advantage of knowledge bases to improve their recommendations. However, search engines are single entity-centric and face difficulties when trying to explain why and how two entities are related, a problem known as entity relatedness. This thesis explores the use of knowledge bases in RDF format to address the entity relatedness problem, in two directions. In one direction, it defines the concept of connectivity profiles for entity pairs, which are concise explanations about how the entities are related. The thesis introduces a strategy to generate a connectivity profile for an entity pair that combines semantic annotations and similarity metrics to summarize a set of relationship paths between the given entity pair. The thesis then describes the DBpedia profiler tool, which implements the strategy for DBpedia, and whose effectiveness was evaluated through user experiments. In another direction, motivated by the challenges of exploring large online knowledge bases, the thesis introduces a generic search strategy, based on the backward search heuristic, to prioritize certain paths over others. The strategy combines similarity and ranking measures to create different alternatives. Finally, the thesis evaluates and compares the different alternatives in two domains, music and movies, based on specialized path rankings taken as ground truth.",José E. Talavera_2017.pdf,BANCOS DE DADOS,JOSE EDUARDO TALAVERA HERRERA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,19/05/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Pathfinding;similarity measure;path ranking;RDF graph;SPARQL query.',-,MARCO ANTONIO CASANOVA,102,Busca de caminhos;medidas de similaridade;ranqueamento de caminhos;grafos RDF;consultas SPARQL.,INFORMÁTICA (31005012004P9),-,"Bases de conhecimento são ferramentas poderosas que fornecem suporte a um amplo espectro de aplicações, por exemplo, busca exploratória, ranqueamento e recomendação. Essas bases de conhecimento podem ser vistas como grafos, onde os nós representam entidades e as arestas seus relacionamentos. Atualmente, motores de busca usam bases de conhecimento para melhorar suas recomendações. No entanto, estes motores de busca são orientados a uma única entidade e enfrentam dificuldades ao tentar explicar porque e como duas entidades estão relacionadas, um problema conhecido como relacionamento entre entidades. Esta tese explora o uso de bases de conhecimento em formato RDF para endereçar o problema de relacionamento entre entidades, em duas direções. Em uma direção, a tese define o conceito de perfis de conectividade para pares de entidades, que são explicações concisas sobre como as entidades se relacionam. A tese introduz uma estratégia para gerar um perfil de conectividade entre um par de entidades, que combina anotações semânticas e métricas de similaridade para resumir um conjunto de caminhos entre as duas entidades. Em seguida, introduz a ferramenta DBpedia profiler, que implementa a estratégia proposta, e cuja efetividade foi medida através de experimentos com usuários. Em outra direção, considerando os desafios para explorar grandes bases de conhecimento online, a tese apresenta uma estratégia genérica de busca baseada na heurística backward, a qual prioriza alguns caminhos sobre outros. A estratégia combina medidas de similaridade e de ranqueamento, criando diferentes alternativas. Por último, a tese avalia e compara as diferentes alternativas em dois domínios, música e filmes, adotando como ground truth rankings especializados de caminhos especialmente desenvolvidos para os experimentos.",TESE,On the Connectivity of Entity Pairs in Knowledge Bases,5018910,1
"The popularization of the Internet of Things sparked a growing
opportunity for the creation of applications in various areas, by combining
the use of sensors and/or actuators. In IoT environments, the role of elements
called gateways is to provide an intermediate communication layer between
IoT devices and cloud services. A crucial factor for the construction of
large-scale applications is to allow the use of IoT devices in a transparent
manner, in a service-oriented paradigm, where details of communication and
configuration are handled by the gateways. In service model, applications
must discover the high-level interfaces of the devices and do not have to
deal with underlying details that are handled by gateways. In scenarios
of high dynamism and mobility (with connections and disconnections of
devices occuring all the time), this discovery and configuration must occur
continuously. Traditional service discovery protocols, such as Universal
Plug and Play (UPnP) or Service Location Protocol (SLP), have not been
developed taking into consideration the high dinamicity of IoT environments.
In this sense, we introduce complex event processing (CEP), which is a
technology for real-time processing of heterogeneous event flows, which
allows the use of CQL (Continuous Query Language for the search of events
of interest. In a model where events related to sensor discovery are sent to a
CEP flow, expressive queries are written for an application to continuously
discover services of interest. This work presents the extension of MHub /
CDDL to support continuous service discovery in IoT, using CEP. The MHub
/ CDDL (Mobile Hub / Context Data Distribution Layer) is a middleware
for service discovery and quality context management in IoT, developed in a
partnership between the Laboratory for Advanced Collaboration (LAC) from
PUC-Rio and the Laboratório de Sistemas Distribuídos Inteligentes (LSDi)
from Universidade Federal do Maranhão (UFMA). The implementation of
this work is done in Android (Java) platform and a case study in the domain
of smart parking is conducted and implemented, elucidating the use of the
continuous discovery mechanism.",Felipe O Carvalho_2017.pdf,REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,FELIPE OLIVEIRA CARVALHO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,26/04/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Distributed Systems;Internet of Things;Middleware;Service Discovery;Complex Event Processing.',"REDES MÓVEIS, COMPUTAÇÃO UBÍQUA E COMPUTAÇÃO AUTONÔMICA",MARKUS ENDLER,65,Sistemas Distribuídos;Internet das Coisas;Middleware;Descoberta de Serviços;Processamento de Eventos Complexos.,INFORMÁTICA (31005012004P9),-,"A popularização da Internet das Coisas (IoT, Internet of Things)
provocou uma crescente oportunidade para a criação de aplicações em
diversas áreas, através da combinação do uso de sensores e/ou atuadores.
Em ambientes de IoT, o papel de elementos chamados de gateways consiste
em fornecer uma camada de comunicação intermediária entre os dispositivos
de IoT e serviços de nuvem. Um fator crucial para a construção de aplicações
em larga escala é que os dispositivos de IoT possam ser utilizados de
maneira transparente, num paradigma orientado a serviços, onde detalhes de
comunicação e configuração destes objetos são tratados pelos gateways. No
modelo de serviços, as aplicações devem descobrir as interfaces de alto-nível
dos dispositivos e não precisam lidar com detalhes subjacentes, que são
tratados pelos gateways. Em cenários de grande dinamismo e mobilidade
(com conexões e desconexões de dispositivos acontecendo a todo momento),
a descoberta e configuração de objetos deve ocorrer de forma contínua.
Os protocolos de descoberta de serviços tradicional, como o Universal
Plug and Play (UPnP) ou o Service Location Protocol (SLP), não foram
desenvolvidos levando em consideração o alto dinamismo de ambientes
IoT. Nesse sentido, introduzimos o processamento de eventos complexos
(CEP), que é uma tecnologia para processamento em tempo real de fluxos
de eventos heterogêneos, que permite a utilização de consultas em linguagem
CQL (Continuous Query Language) para a busca de eventos de interesse.
Em um modelo onde os eventos relacionados à descoberta de sensores são enviados para um fluxo CEP, consultas expressivas são escritas para que uma aplicação descubra continuamente serviços de interesse. Este trabalho
apresenta a extensão do MHub/CDDL para o suporte à descoberta contínua
de serviços em IoT, utilizando CEP. O MHub/CDDL (Mobile Hub / Context
Data Distribution Layer) é um middleware para descoberta de serviços e
gerenciamento de qualidade de contexto em IoT, desenvolvido numa parceria
entre o Laboratory for Advanced Collaboration (LAC) da PUC-Rio e o
Laboratório de Sistemas Distribuídos Inteligentes (LSDi) da Universidade
Federal do Maranhão (UFMA). A implementação deste trabalho é feita
para a plataforma Android (Java) e um estudo de caso no domínio de
estacionamentos inteligentes é conduzido e implementado, elucidando o uso
do mecanismo de descoberta contínuo.",DISSERTAÇÃO,Descoberta Contínua de Serviços em IoT,5028634,1
"Many techniques for the structured information extraction from natural
language data have been developed and have demonstrated their potentials
yielding satisfactory results. Nevertheless, to obtain such results, they
require some activities that are usually done separately, such as text annotation
to generate corpora, Part-Of- Speech tagging, features engineering and
extraction, machine learning models’ training etc., making the information
extraction task a costly activity due to the effort and time spent on this.
The present work proposes and develops a web based platform called LER
(Learning Entities and Relations), that integrates the needed workflow for
these activities, with an interface that aims the ease of use. The work also
shows the platform implementation and its use.",Jonatas_2017_Completo.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,JONATAS DOS SANTOS GROSMAN,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,20/04/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Natural language processing;Automatic learning;Information extraction;Ontologies;Data curation.',"APRENDIZADO DE MÁQUINA, RACIOCÍNIO HEURÍSTICO E PROBABILÍSTICO",HELIO CORTES VIEIRA LOPES,196,Processamento de linguagem natural;Aprendizado automático;Extração de informação;Ontologias;Curadoria de dados,INFORMÁTICA (31005012004P9),-,"Diversas técnicas para extração de informações estruturadas de dados em linguagem natural foram desenvolvidas e demonstraram resultados muito satisfatórios. Entretanto, para obterem tais resultados, requerem uma série de atividades que geralmente são feitas de modo isolado, como a anotação de textos para geração de corpora, etiquetamento morfossintático, engenharia e extração de atributos, treinamento de modelos de aprendizado de máquina etc., o que torna onerosa a extração dessas informações, dado o esforço e tempo a serem investidos. O presente trabalho propõe e desenvolve uma plataforma em ambiente web, chamada LER (Learning Entities and Relations) que integra todo o fluxo necessário para essas atividades, com uma interface amigável e que visa à máxima facilitação. Outrossim, o trabalho mostra os resultados da implementação e uso da plataforma proposta.",DISSERTAÇÃO,LER: Anotação e Classificação Automática de Entidades e Relações,5034746,1
"Ultrasound exams are a popular tool for image acquisition in day-to-day medicine, since it is a noninvasive, safe and cheap procedure. However, speckle noise is intrinsic to any ultrasound exam, and it is responsible for image quality degradation and for hindering its interpretation by doctors and patients alike, while also impairing the accuracy of post processing computational methods, such as classification, reconstruction, tissue characterization and segmentation, among others. Hence, smoothing or denoising methods that preserves the observed content core attributes are essential for those processes. Defined as a multiplicative noise, following non-Gaussian statistics and as strongly correlated, its solution today is still a matter of debates and research. In this work, several 2D filters that aim to smooth or remove speckle noise along with qualitative methods to evaluate their performances and means of choosing their best parameters are presented.",,COMPUTAÇÃO GRÁFICA,THIAGO RIBEIRO DA MOTTA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,12/04/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Speckle noise;ultrasound exams;edge preservation;2D filters evaluation;parameters estimation;qualitative image metrics;k-means.',"PROCESSAMENTO DE IMAGEM, VISÃO COMPUTACIONAL E REALIDADE AUMENTADA",ALBERTO BARBOSA RAPOSO,127,Ruído speckle;exame de ultrassom;preservação de arestas;avaliação de filtros 2D;escolha de parâmetros;métricas qualitativas de imagem;k-means,INFORMÁTICA (31005012004P9),-,"Exames de ultrassom são uma ferramenta popular de aquisição de imagens na medicina atual por ser um procedimento não-invasivo, seguro e barato. Entretanto, inerente a qualquer exame de ultrassom encontra-se o ruído speckle, responsável pela degradação da imagem e dificultando tanto sua interpretação por parte de médicos e pacientes, quanto prejudicando a acurácia de métodos computacionais de pós processamento, como classificação, reconstrução, caracterização de tecidos e segmentação, entre outros. Portanto, métodos de remoção ou suavização deste ruído que preservem as principais características do conteúdo observado se fazem fundamentais para um avanço nestes processos. Definido como um ruído multiplicativo, que segue estatísticas não-Gaussianas e como fortemente correlacionado, sua solução ainda hoje é tema de debates e estudos. Neste trabalho apresentaremos diversos métodos de filtragem 2D que se propõem a reduzir ou solucionar o ruído speckle bem como métodos qualitativos para avaliar suas performances e técnicas para escolher os melhores parâmetros de cada filtro a fim de eleger quais métodos melhor solucionam este ruído.",DISSERTAÇÃO,Uma Avaliação de Filtros 2D para Remoção de Ruído Speckle em Exames de Ultrassom.,5035228,1
"The operational units at the Exploration and Production (E&P) area
at PETROBRAS make use of daily reports to register situations and events
from their Stationary Production Units (SPUs), the well-known petroleum
production platforms. One of these reports, called SITOP (the Portuguese
acronym for Offshore Unities’ Operational Situation), is a daily document
in free text format that presents numerical information and, mainly, textual
information about operational situation of offshore units. The textual
section, although unstructured, stores a valuable database with historical
events in the production environment, such as: valve breakages, failures in
processing equipment, beginning and end of maintenance activities, actions
executed, responsibilities, etc. The value of these data is high, as well as the
costs of searching relevant information, consuming many hours of attention
from technicians and engineers to read the large number of documents. The
goal of this dissertation is to develop a model of natural language processing
to recognize named entities and extract relations among them, described
formally as a domain ontology applied to events in offshore oil and gas
processing units. After all, there will be a method for automatic structuring
of the information from these operational reports. Our results show that
this methodology is useful in SITOP’s case, also indicating some possible
enhancements. Relation extraction showed better results than named entity
recognition, what can be explained by the difference in the amount of classes
in these tasks. We also verified that the increase in the amount of data was
one of the most important factors for the improvement in learning and
methodology efficiency as a whole.",Pedro Henrique T Furtado_2017_Completo.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,PEDRO HENRIQUE THOMPSON FURTADO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,20/04/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Natural language processing;automatic learning;ontologies;oil and gas.',-,HELIO CORTES VIEIRA LOPES,130,Processamento de Linguagem Natural;Aprendizado Automático;Ontologias;Petróleo e Gás Natural,INFORMÁTICA (31005012004P9),-,"As unidades operacionais da área de Exploração e Produção (E&P)
da PETROBRAS utilizam relatórios diários para o registro de situações
e eventos em Unidades Estacionárias de Produção (UEPs), as conhecidas
plataformas de produção de petróleo. Um destes relatórios, o SITOP
(Situação Operacional das Unidades Marítimas), é um documento diário
em texto livre que apresenta informações numéricas (índices de produção,
algumas vazões, etc.) e, principalmente, informações textuais. A parte
textual, apesar de não estruturada, encerra uma valiosíssima base de
dados de histórico de eventos no ambiente de produção, tais como:
quebras de válvulas, falhas em equipamentos de processo, início e término
de manutenções, manobras executadas, responsabilidades etc. O valor
destes dados é alto, mas o custo da busca de informações também o é,
pois se demanda a atenção de técnicos da empresa na leitura de uma
enorme quantidade de documentos. O objetivo do presente trabalho é o
desenvolvimento de um modelo de processamento de linguagem natural para
a identificação, nos textos dos SITOPs, de entidades nomeadas e extração
de relações entre estas entidades, descritas formalmente em uma ontologia
de domínio aplicada a eventos em unidades de processamento de petróleo e
gás em ambiente offshore. Ter-se-á, portanto, um método de estruturação
automática da informação presente nestes relatórios operacionais. Os
resultados obtidos demonstram que a metodologia é útil para este caso,
ainda que passível de melhorias em diferentes frentes. A extração de relações
apresenta melhores resultados que a identificação de entidades, o que pode
ser explicado pela diferença entre o número de classes das duas tarefas.
Verifica-se também que o aumento na quantidade de dados é um dos
fatores mais importantes para a melhoria do aprendizado e da eficiência
da metodologia como um todo.",DISSERTAÇÃO,Interpretação Automática de Relatórios de Operação de Equipamentos,5046409,1
"Structural topology optimization methods are used to find the optimal
material distribution within a given domain, subject to loading, boundary
conditions and design constraints, in order to minimize some specified
measure. Structural topology optimization can be divided into two types:
continuum and discrete, with the discrete type being the research focus
of this dissertation. The goal of this work is the creation of a system to
achieve all the steps of this optimization process, aiming problems with
large dimensions. In order to perform the optimization, it is necessary create
a ground structure, defined as a set of nodes covering the entire domain,
connected by bars, with the supports and the applied loads. This work
proposes a new method for the ground structure generation, using as input
only the domain boundary, in contrast with methods that require a domain
already discretized, such as a polyhedron mesh. With the generated mesh,
this work has implemented the topological optimization, needing to solve
a linear programming problem. All the optimization part was performed
within the TopSim framework, implementing the interior point method for
the linear programming resolution. The results presented have good quality,
both in generation and optimization, for 2D and 3D cases, considering cases
with more than 68 million bars.",Vinicius Gama_2017_Completo.pdf,COMPUTAÇÃO GRÁFICA,VINICIUS GAMA TAVARES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,31/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Ground structure method;Topology optimization of trusses;Linear programming.',-,WALDEMAR CELES FILHO,68,Malha densa de barras;Otimização topológica de barras;Programação linear,INFORMÁTICA (31005012004P9),-,"Métodos de otimização topológica estrutural visam obter a melhor
distribuição de material dentro de um dado domínio, sujeito a carga,
condições de contorno e restrições de projeto, de forma a minimizar alguma
medida especificada. A otimização topológica estrutural pode ser dividida
em dois tipos: contínua e discreta, sendo a forma discreta o foco da
pesquisa desta dissertação. O objetivo deste trabalho é a criação de um
sistema para realizar todos os passos dessa otimização, visando a
resolução de problemas com grandes dimensões. Para realizar esse tipo de
otimização, é necessária a criação de uma malha densa de barras, esta
definida como conjunto de nós cobrindo todo o domínio, conectados através
de barras, além da especificação dos apoios e das forças aplicadas. Este
trabalho propõe um novo método para geração da malha densa de barras,
utilizando como entrada somente o contorno do domínio que se deseja
otimizar, contrapondo com métodos que necessitam de um domínio já
discretizado, como uma malha de poliedros. Com a malha gerada, este
trabalho implementou a otimização topológica, sendo necessário resolver um
problema de programação linear. Toda a parte de otimização foi realizada
dentro do framework TopSim, tendo implementado o método dos pontos
interiores para a resolução da programação linear. Os resultados
apresentados possuem boa qualidade, tanto na geração quanto na otimização,
para casos 2D e 3D, tratando casos com mais de 68 milhões de barras.",DISSERTAÇÃO,Sistema eficiente de otimização topológica estrutural utilizando o método de malha densa de barras,5046482,1
"The Resource Description Framework (RDF) was adopted as a W3C recommendation in 1999 and today is a standard for exchanging data in the Web. Indeed, a large amount of data has been converted to RDF, often as multiple datasets physically distributed over different locations. The SPARQL Protocol and RDF Query Language (SPARQL) was officially introduced in 2008 to retrieve RDF datasets and provide endpoints to query distributed sources. An alternative way to access RDF datasets is to use keyword-based queries, an area that has been extensively researched, with a recent focus on Web content. This dissertation describes a strategy to compile keyword-based queries into federated SPARQL queries over distributed RDF datasets, under the assumption that each RDF dataset has a schema and that the federation has a mediated schema. The compilation process of the federated SPARQL query is explained in detail, including how to compute a set of external joins between the local subqueries, how to combine, with the help of the UNION clauses, the results of local queries which have no external joins between them, and how to construct the TARGET clause, according to the structure of the WHERE clause. Finally, the dissertation covers experiments with real-world data to validate the implementation.",Yenier Torres_2017_Completo.pdf,BANCOS DE DADOS,YENIER TORRES IZQUIERDO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,31/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Busca por palavras-chave;dados conectados;SPARQL;RDF;consultas federadas;esquema mediado',-,MARCO ANTONIO CASANOVA,66,Keyword Search;Linked Data;SPARQL;RDF;Federated Query;Mediated Schema.,INFORMÁTICA (31005012004P9),-,"O Resource Description Framework (RDF) foi adotado como uma recomendação do W3C em 1999 e hoje é um padrão para troca de dados na Web. De fato, uma grande quantidade de dados foi convertida em RDF, muitas vezes em vários conjuntos de dados fisicamente distribuídos ao longo de diferentes localizações. A linguagem de consulta SPARQL (sigla do inglês de SPARQL Protocol and RDF Query Language) foi oficialmente introduzido em 2008 para recuperar dados RDF e fornecer endpoints para consultar fontes distribuídas. Uma maneira alternativa de acessar conjuntos de dados RDF é usar consultas baseadas em palavras-chave, uma área que tem sido extensivamente pesquisada, com foco recente no conteúdo da Web. Esta dissertação descreve uma estratégia para compilar consultas baseadas em palavras-chave em consultas SPARQL federadas sobre conjuntos de dados RDF distribuídos, assumindo que cada conjunto de dados RDF tem um esquema e que a federação tem um esquema mediado. O processo de compilação da consulta SPARQL federada é explicado em detalhe, incluindo como computar o conjunto de joins externos entre as subconsultas locais geradas, como combinar, com a ajuda de cláusulas UNION, os resultados de consultas locais que não têm joins entre elas, e como construir a cláusula TARGET, de acordo com a composição da cláusula WHERE. Finalmente, a dissertação cobre experimentos com dados do mundo real para validar a implementação.",DISSERTAÇÃO,Keyword Search over Federated RDF Graphs by Exploring their Schemas,5046558,1
"Clustering plays an important role in data mining, being useful in
many fields that deal with exploratory data analysis, such as information
retrieval, document extraction, and image segmentation. Although they are
essential in data mining applications, most clustering algorithms are adhoc
methods. They have a lack of guarantee on the solution quality, which
in many cases is related to a premature convergence to a local minimum
of the search space. In this research, we address the problem of data
clustering from an optimization perspective, where we propose a hybrid
genetic algorithm to solve the Minimum Sum-of-Squares Clustering (MSSC)
problem. This meta-heuristic is capable of escaping from local minima and
generating near-optimal solutions to the MSSC problem. Results show that
the proposed method outperformed the best current literature results –
in terms of solution quality – for almost all considered sets of benchmark
instances for the MSSC objective.",Daniel L. Gribel_2017_Completo.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,DANIEL LEMES GRIBEL,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,20/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Clustering;Meta-heuristic;Unsupervised learning;Minimum sum-of-squares;Data mining.',ENGENHARIA DE ALGORITMOS,THIBAUT VICTOR GASTON VIDAL,54,Clusterização;Meta-heurística;Aprendizado não-supervisionado;Mínima soma dos quadrados;Mineração de dados.,INFORMÁTICA (31005012004P9),-,"Clusterização desempenha um papel importante em aplicações de data mining, sendo útil em muitos campos que lidam com a análise exploratória de dados, tais como recuperação de informações, extração de documentos e segmentação de imagens. Embora sejam essenciais em aplicações de mineração de dados, a maioria dos algoritmos de clusterização são métodos ad-hoc. Eles apresentam falta de garantia na qualidade da solução, que em muitos casos está relacionada a uma convergência prematura para um mínimo local no espaço de busca. Neste trabalho, abordamos o problema de clusterização a partir da perspectiva de otimização, onde propomos uma meta-heurística adaptativa para resolver o problema Minimum Sum-of-Squares Clustering (MSSC, em inglês). A meta-heurística proposta é capaz de escapar de mínimos locais e gerar soluções quase ótimas para o problema MSSC. Os resultados mostram que o método proposto superou os resultados atuais da literatura – em termos de qualidade da solução -- para quase todos os conjuntos de instâncias considerados para o problema MSSC.",DISSERTAÇÃO,Hybrid Genetic Algorithm for the Minimum Sum-of-Squares Clustering Problem,5046615,1
"Flexibility and freedom are always desired in virtual reality environments.
Traditional inputs, like mouse or keyboard, hamper the interactions between the user
and the virtual environment. To improve the interaction in qualitative terms in a
virtual environment, the interaction must be as natural as possible, and because of
that, hand gestures have become a popular means to the human-computer interaction.
The development of immersion devices like head-mounted displays brought the need
for a new way of interaction and a challenge to developers. Hand gestures recognition
using electromyography signals (EMG) has increased the attention because the rise of
cheaper wearable devices that can record accurate EMG data. One of the outstanding
devices in this area is Myo armband, equipped with eight EMG sensors and a nineaxis
inertial measurement unit (IMU). The objective of this work is to evaluate the
usability of the Myo armband as a device for selection and manipulation of 3D
objects in virtual reality environments, aiming to improve the user experience, taking
advantage of the possibility to measure the force applied to a gesture and to use Myo
vibrations as a feedback system. This study aims to answer the following question:
Has Myo armband high grade of usability for selection/manipulation of 3D objects in
Virtual Reality Environments? And to achieve that purpose, four sub-questions were
proposed to guide this research: I) Which resources of Myo can be used in Virtual
Reality Environments (VRE)? II) What are the limitations of the Myo armband? III)
Can selection and manipulation tasks be performed using Myo armband? IV) How
can Myo armband enrich the selection and manipulation tasks? To answer to the first
two sub-questions, we conducted a literature review that covers Myo technology, its
advantages and limitations, and related works. Also, it includes basic concepts about
Interactions in VRE. To answer to the last two sub-questions, we proposed two
selection/manipulation techniques using Myo, which were tested with users and the
results were compared, evaluating their usability.",Yadira Garnica Bonome_2017.pdf,INTERAÇÃO HUMANO-COMPUTADOR,YADIRA GARNICA BONOME,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,17/05/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Myo armband;3D manipulation and selection;3D interaction;Virtual reality;Gesture-based control;Human-computer interaction;Wearable devices.',-,ALBERTO BARBOSA RAPOSO,73,Bracelete Myo;Manipulação e seleção 3D;Interação 3D;Realidade virtual;Controle baseado em gestos;Interação homem-computador;Dispositivos Wearable.,INFORMÁTICA (31005012004P9),-,"Flexibilidade e liberdade são sempre desejados em ambientes de realidade virtual. Dispositivos de entrada tradicionais, como mouse ou teclado, dificultam as interações entre o usuário e o ambiente virtual. Para melhorar a interação em termos qualitativos em um ambiente virtual, a interação deve ser tão natural quanto possível, por isso, gestos com a mão se tornaram um meio popular para a interação humano-computador. O desenvolvimento de dispositivos de imersão como os capacetes trouxeram a necessidade de uma nova forma de interação e um desafio para os desenvolvedores. O reconhecimento de gestos da mão usando sinais eletromiográficos (EMG) aumentou a atenção devido ao surgimento de dispositivos mais baratos que conseguem gravar dados EMG precisos. Um dos dispositivos mais destacados nessa área é o bracelete Myo, equipado com oito sensores EMG e uma unidade de medição inercial (IMU). O objetivo deste trabalho é avaliar a usabilidade do bracelete Myo como um dispositivo de seleção e manipulação de objetos 3D em ambientes de realidade virtual, visando melhorar a experiência do usuário, aproveitando a possibilidade de medir a força aplicada a um gesto assim como de usar as vibrações do Myo como sistema de feedback. Este estudo pretende responder à seguinte pergunta: O bracelete Myo tem alto grau de usabilidade para a seleção/manipulação de objetos 3D em Ambientes de Realidade Virtual? Para atingir esse objetivo, foram propostas quatro subquestões para orientar essa pesquisa: i) Quais vantagens do Myo podem ser usadas em Ambientes de Realidade Virtual (VRE)? Ii) Quais são as limitações do bracelete Myo? Iii) É possível realizar tarefas de seleção e manipulação de usando o bracelete Myo? Iv) Como o uso do bracelete Myo pode enriquecer as tarefas de seleção e manipulação? Para responder às duas primeiras subquestões foi realizada uma revisão da literatura que compreende a tecnologia do Myo, vantagens e limitações, e os trabalhos relacionados. Além disso, inclui conceitos básicos sobre Interações em VRE. Para responder às duas últimas subquestões foram propostas duas técnicas de seleção/manipulação usando o Myo e foram testadas com os usuários e os resultados foram comparados, avaliando sua usabilidade.",DISSERTAÇÃO,Proposing two new handling interaction techniques for 3D virtual objects using the Myo armband.,5046624,1
"This thesis presents a study on the problem space in the context of development of technologies for Autism Spectrum Disorder's (ASD) therapists and it aims to contribute to the design of technologies in this area. Although there is a large number of technologies to the audience with ASD, such technologies usually allow a limited use, given the extent of the Autism Spectrum and the variety of disorders that people with ASD diagnosis can have. Thus, it is essential to develop more flexible technologies that meet the needs of each individual with ASD. Moreover, the literature presents specific reports (of success or failure) about the developed technologies, but there is no organization of the domain. Therefore, we consider that there is a gap regarding the problem characterization in this area, and it is important to explore the problem space before proposing new solutions. There are several challenges related to the adoption of technologies for generating adaptable software by ASD therapists, such as the lack or little knowledge of the therapists about this kind of technology, the limited time for learning and using these technologies and even the lack of interest. We conducted studies in order to explore the problem space in the context of the End User Development (EUD) and to raise the main meanings, questions and difficulties of the therapists related to adaptable and/or extensible technologies and they showed us that the use of the design probes concept was essential for the approximation of the therapists with such technology type and a deepening understanding of their own difficulties and needs, and the challenges placed in this context. Moreover, these studies have allowed us to raise a set of possible changes and adaptations that a technology targeted for this audience could do in order to address the needs of therapists and their patients. From this achieved set, we have analyzed these possible changes based on the Semiotic Engineering theory for EUD context and we have investigated how existing technologies enable or not these kinds of changes. Thus, this research has enabled us to identify the ASD domain as an EUD application area with several challenges, to analyze them in greater depth and to identify possible ways to overcome them,
contributing to the design of technologies for the ASD audience.",Priscilla Fonseca 2017.pdf,INTERAÇÃO HUMANO-COMPUTADOR,PRISCILLA FONSECA DE ABREU BRAZ,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,11/01/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'End User Development;Autism Spectrum Disorder;Problem Space;Semiotic Engineering',-,ALBERTO BARBOSA RAPOSO,160,End User Development;Transtorno do Espectro do Autismo;Espaço de problema;Engenharia Semiótica,INFORMÁTICA (31005012004P9),-,"Esta tese apresenta um estudo sobre o espaço de problema no contexto do desenvolvimento de tecnologias para terapeutas do Transtorno do Espectro do Autismo (TEA) e visa contribuir para o design de tecnologias nessa área. Apesar de haver uma grande quantidade de tecnologias para o público com TEA, tais tecnologias possibilitam em geral um uso restrito, tendo em vista a amplitude do chamado Espectro do Autismo e a variedade de Transtornos que as pessoas com um diagnóstico de TEA podem manifestar. Desse modo, torna-se essencial o desenvolvimento de tecnologias mais flexíveis que atendam às necessidades de cada indivíduo com TEA. Além disso, a literatura apresenta relatos pontuais (de sucesso ou insucesso) sobre as tecnologias desenvolvidas, mas não há uma estruturação maior do domínio. Diante disso, consideramos que há uma lacuna com relação à caracterização do problema nessa área, sendo importante explorar o espaço de problema antes de propor novas soluções. Diversos são os desafios envolvidos na adoção de tecnologias para geração de software adaptável por terapeutas de TEA, tais como a falta ou o pouco conhecimento dos terapeutas sobre esse tipo de tecnologia, o tempo escasso para o aprendizado e uso dessas tecnologias e até mesmo a falta de interesse nelas. Realizamos estudos para explorar o espaço de problema no contexto de End User Development (EUD) e levantar os principais significados, questionamentos e dificuldades dos terapeutas relacionados às tecnologias adaptáveis e/ou extensíveis e eles nos mostraram que o uso do conceito de sondas de design foi essencial para a aproximação dos terapeutas com esse tipo de tecnologia e para um aprofundamento, por parte do designer, das dificuldades e necessidades deles e os desafios inseridos nesse contexto. Além disso, esses estudos nos permitiram levantar um conjunto de possíveis alterações e adaptações que uma tecnologia voltada para esse público poderia fazer de modo a atender as necessidades dos terapeutas com seus atendidos. A partir desse conjunto levantado, analisamos essas possíveis alterações com base na teoria da Engenharia Semiótica para o contexto de EUD e investigamos como tecnologias existentes possibilitam ou não fazer esses tipos de alterações. Desse modo, essa pesquisa nos permitiu
identificar o domínio de TEA como uma área de aplicação de EUD com diversos desafios, analisá-los com maior profundidade e apontar possíveis caminhos para superá-los, contribuindo para o Design de tecnologias para o público de TEA.",TESE,Uma análise do espaço de problema de End User Development no domínio de tecnologias para terapeutas do Transtorno do Espectro do Autismo,5047218,1
"The volume of RDF data published on the Web increased considerably,
which stressed the importance of following the Linked Data principles to foster
interoperability. One of the principles requires that a new dataset should be
interlinked with other datasets published on the Web. This thesis contributes to
addressing this principle in two ways. First, it uses community detection
algorithms and profiling techniques for the automatic creation and analysis of a
Linked Open Data (LOD) diagram, which facilitates locating datasets in the LOD
cloud. Second, it describes three approaches, backed up by fully implemented
tools, to recommend datasets to be interlinked with a new dataset, a problem
known as the dataset interlinking recommendation problem. The first approach
uses link prediction measures to provide a list of datasets recommendations for
interlinking. The second approach employs supervised learning algorithms, jointly
with link prediction measures. The third approach uses clustering algorithms and
profiling techniques to produce dataset interlinking recommendations. These
approaches are backed up, respectively, by the TRT, TRTML and DRX
tools. Finally, the thesis extensively evaluates these tools, using real-world
datasets, reporting results that show that they facilitate the process of creating
links between disparate datasets.",Alexander Arturo_2017_Completo.pdf,BANCOS DE DADOS,ALEXANDER ARTURO MERA CARABALLO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,17/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Dataset Interlinking Recommendation;Linked Data;Semantic Web.',-,MARCO ANTONIO CASANOVA,89,Recomendação de conjuntos de dados para interligação;Dados Interligados;Web Semântica,INFORMÁTICA (31005012004P9),-,"O volume de dados RDF publicados na Web aumentou consideravelmente, o que ressaltou a importância de seguir os princípios de dados interligados para promover a interoperabilidade. Um dos princípios afirma que todo novo conjunto de dados deve ser interligado com outros conjuntos de dados publicados na Web. Esta tese contribui para abordar este princípio de duas maneiras. Em primeiro lugar, utiliza algoritmos de detecção de comunidades e técnicas de criação de perfis para a criação e análise automática de um diagrama da nuvem da LOD (Linked Open Data), o qual facilita a localização de conjuntos de dados na nuvem da LOD. Em segundo lugar, descreve três abordagens, apoiadas por ferramentas totalmente implementadas, para recomendar conjuntos de dados a serem interligados com um novo conjunto de dados, um problema conhecido como problema de recomendação de interligação de conjunto de dados. A primeira abordagem utiliza medidas de previsão de links para produzir recomendações de interconexão. A segunda abordagem emprega algoritmos de aprendizagem supervisionado, juntamente com medidas de previsão de links. A terceira abordagem usa algoritmos de agrupamento e técnicas de criação de perfil para produzir recomendações de interconexão. Essas abordagens são implementadas, respectivamente, pelas ferramentas TRT, TRTML e DRX. Por fim, a tese avalia extensivamente essas ferramentas, usando conjuntos de dados do mundo real. Os resultados mostram que estas ferramentas facilitam o processo de criação de links entre diferentes conjuntos de dados.",TESE,Clustering and Dataset Interlinking Recommendation in the Linked Open Data Cloud,5047259,1
"While many data stream systems have to provide continuous (24x7) services with no acceptable downtime, they also have to cope with changes in their execution environments and in the requirements that they must comply (e.g., moving from on-premises architecture to a cloud system, changing the network technology, adding new functionality or modifying existing parts). On one hand, dynamic software reconfiguration (i.e., the capability of evolving on the fly) is a desirable feature. On the other hand, stream systems may suffer from the disruption and overhead caused by the reconfiguration. Due to the necessity of reconfiguring (i.e., evolving) the system whilst the system must not be disrupted (i.e., blocked), consistent and non-disruptive reconfiguration is still considered an open problem. This thesis presents and validates a non-quiescent approach for dynamic software reconfiguration that preserves the consistency of distributed data stream processing systems. Unlike many works that require the system to reach a safe state (e.g., quiescence) before performing a reconfiguration, the proposed approach enables the system to smoothly evolve (i.e., be reconfigured) in a non-disruptive way without reaching quiescence. The evaluation indicates that the proposed approach supports consistent distributed reconfiguration and has negligible impact on availability and performance. Furthermore, the implementation of the proposed approach showed better performance results in all experiments than the quiescent approach and Upstart.",Rafael Oliveira _2017.pdf,REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,RAFAEL OLIVEIRA VASCONCELOS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,07/04/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Dynamic reconfiguration;Adaptability;Software adaptation;Mobile communication;Data Stream Processing',PROJETO E IMPLEMENTAÇÃO DE MIDDLEWARE,MARKUS ENDLER,74,Reconfiguração dinâmica;Adaptabilidade;Adaptação de software;Comunicação móvel;Processamento de fluxo de dados,INFORMÁTICA (31005012004P9),-,"Ao mesmo tempo em que sistemas de processamento de fluxo de dados devem prover serviços de análise e manipulação de dados ininterruptamente (disponibilidade 24x7), eles comumente também precisam lidar com mudanças em seus ambientes de execução (e.g., alterar a topologia da rede) e nos requisitos que eles devem cumprir (e.g., adição de novas funções de processamento dos fluxos de dados). Por um lado, reconfiguração dinâmica de software (i.e., a capacidade de substituir parte do software em tempo de execução) é uma característica desejável. Por outro lado, sistemas de fluxo de dados podem sofrer com a interrupção e sobrecarga causada pela reconfiguração. Por conta da necessidade de reconfigurar (i.e., evoluir) o sistema ao mesmo tempo em que o sistema não pode ser interrompido (i.e., bloqueado), reconfiguração consistente e não bloqueante é ainda considerada um problema em aberto na literatura. Esta tese apresenta e valida uma abordagem não quiescente para reconfiguração dinâmica de software que preserva a consistência de sistemas de fluxo de dados distribuídos. A abordagem proposta permite que o sistema seja reconfigurado gradual e suavemente, sem precisar interromper o processamento do fluxo de dados ou atingir a quiescência. A avaliação indica que a abordagem proposta realiza reconfiguração distribuída consistentemente e tem um impacto desprezível sobre a diminuição na disponibilidade e no desempenho do sistema. Além disto, a implementação da abordagem proposta teve um desempenho melhor em todos os testes comparativos.",TESE,Uma Abordagem Eficiente para Reconfiguração Coordenada em Sistemas Distribuídos de Processamento de Data Streams,5047306,1
"This thesis presents a new sequent calculus called LMT! that has
the properties to be terminating, sound and complete for Propositional
Implicational Minimal Logic (M!). LMT! is aimed to be used for proof
search in M!, in a bottom-up approach. Termination of the calculus is
guaranteed by a strategy of rule application that forces an ordered way to
search for proofs such that all possible combinations are stressed. For an
initial formula , proofs in LMT! has an upper bound of jj  2jj+1+2log2jj,
which together with the system strategy ensure decidability. System rules are
conceived to deal with the necessity of hypothesis repetition and the contextsplitting
nature of !-left, avoiding the occurrence of loops and the usage of
backtracking. Therefore, LMT! steers the proof search always in a forward,
deterministic manner. LMT! has the property to allow extractability of
counter-models from failed proof searches (bicompleteness), i.e., the attempt
proof tree of an expanded branch produces a Kripke model that falsies the
initial formula. Counter-model generation (using Kripke semantics) is achieved
as a consequence of the completeness of the system. LMT! is implemented
as an interactive theorem prover based on the calculus proposed here. We
compare our calculus with other known deductive systems for M!, especially
with Fitting's Tableaux, a method that also has the bicompleteness property.
We also proposed here a translation of LMT! to the Dedukti proof checker as
a way to evaluate the correctness of the implementation regarding the system
specication and to make our system easier to compare to others.",Jefferson_2017.pdf,TEORIA DA COMPUTAÇÃO,JEFFERSON DE BARROS SANTOS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,18/05/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Logic;Propositional Minimal Implicational Logic;Sequent Calculus;Proof Search;Counter-model Generation',TEORIA DA PROVA E PROVA AUTOMÁTICA DE TEOREMAS,EDWARD HERMANN HAEUSLER,82,Lógica;Lógica Proposicional Minimal Implicacional;Calculo de Sequentes;Busca de Provas;Geracão de Contra-modelo,INFORMÁTICA (31005012004P9),-,"Esta tese apresenta um novo cálculo de sequente, correto e completo
para a Lógica Proposicional Minimal Implicacional (M!). LMT! destina-se
a ser usado para a busca de provas em M!, em uma abordagem bottom-up. A
Terminação do cálculo _e garantida por uma estratégia de aplicação de regras
que força uma maneira ordenada no procedimento de busca de provas de tal
forma que todas as combinações possíveis são exploradas. Para uma fórmula
inicial _, as provas em LMT→têm um limite superior de jαj _ 2jαj+1+2_log2jαj,
que juntamente com a estratégia do sistema, garantem a decidibilidade do
mesmo. As regras do sistema são concebidas para lidar com a necessidade
de repetição de hipóteses e a natureza de perda de contexto da regra →-
esquerda , evitando a ocorrência de loops e o uso de backtracking. Portanto,
a busca de prova em LMT→ _e deterministica, sempre executando buscas no
sentido forward. LMT→ tem a propriedade de permitir a extração de contramodelos a partir de buscas de prova que falharam (bicompletude), isto é, a árvore de tentativa de prova de um ramo totalmente expandido produz um
modelo de Kripke que falsifica a fórmula inicial. A geração de contra-modelo
(usando a semântica Kripke) _e obtida como consequência da completude do
sistema. LMT! _e implementado como um provador de teoremas interativo
baseado no cálculo proposto aqui. Comparamos nosso cálculo com outros
sistemas dedutivos conhecidos paraM!, especialmente com Tableaux no estilo
Fitting, um método que também tem a propriedade de ser bicompleto. Também
propomos aqui uma tradução de LMT! para o verificador de prova Dedukti
como uma forma de avaliar a correção da implementação que desenvolvemos,
no que diz respeito _a especificação do sistema, além de torná-lo mais fácil de
comparar com outros sistemas existentes.",TESE,Systems for Provability and Countermodel Generation in Propositional Minimal Implicational Logic,5047363,1
"Buses, equipped with active GPS devices that continuously transmit their position, can be understood as mobile traffic sensors. Indeed, bus trajectories provide a useful data source for analyzing traffic in the bus network of a city, if the city traffic authority makes the bus trajectories available openly, timely and in a continuous way. In this context, this thesis proposes a bus GPS data-driven approach for analyzing and monitoring the bus network of a city. It combines graph algorithms, geospatial data mining techniques and statistical methods. The major contribution of this thesis is a detailed discussion of key operations and algorithms for modeling, analyzing and monitoring bus network traffic, specifically: (1) modelling, analyzing, and segmentation of the bus network; (2) mining the bus trajectory dataset to uncover traffic patterns; (3) detecting traffic anomalies, classifying them according to their severity, and estimating their impact; (4) maintaining and comparing different versions of the bus network and traffic patterns to help urban planners assess changes. Another contribution is the description of experiments conducted for the bus network of the City of Rio de Janeiro, using bus trajectories obtained from June 2014 to February 2017, which have been made available by the City Hall of Rio de Janeiro. The results obtained corroborate the usefulness of the proposed approach for analyzing and monitoring the bus network of a city, which may help traffic managers and city authorities improve traffic control and urban mobility plans.",Kathrin Rodriguez 2017.pdf,BANCOS DE DADOS,KATHRIN RODRIGUEZ LLANES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,26/05/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'detection of traffic anomalies;travel time estimation;trajectory data mining;bus networks.',-,MARCO ANTONIO CASANOVA,137,detecção de anomalias no trânsito;estimativa do tempo de viagem;mineração de dados de trajetórias;redes de ônibus.,INFORMÁTICA (31005012004P9),-,"Ônibus, equipados com dispositivos GPS ativos que transmitem continuamente a sua posição, podem ser entendidos como sensores móveis de trânsito. De fato, as trajetórias dos ônibus fornecem uma fonte de dados útil para analisar o trânsito na rede de ônibus de uma cidade, dado que as autoridades de trânsito da cidade disponibilizem as trajetórias de forma aberta, oportuna e contínua. Neste contexto, esta tese propõe uma abordagem que usa os dados de GPS dos ônibus para analisar e monitorar a rede de ônibus de uma cidade. Ela combina algoritmos de grafos, técnicas de mineração de dados geoespaciais e métodos estatísticos. A principal contribuição desta tese é uma definição detalhada de operações e algoritmos para analisar e monitorar o tráfego na rede de ônibus, especificamente: (1) modelagem, segmentação e análise da rede de ônibus; (2) mineração do conjunto de dados de trajetória de ônibus para descobrir padrões de tráfego; (3) detecção de anomalias de trânsito, classificação de acordo com sua gravidade, e avaliação do seu impacto ; (4) manutenção e comparação de diferentes versões da rede de ônibus e dos seus padrões de tráfego para ajudar os planejadores urbanos a avaliar as mudanças. Uma segunda contribuição é a descrição de experimentos realizados para a rede de ônibus da Cidade do Rio de Janeiro, utilizando trajetórias de ônibus correspondentes ao período de junho de 2014 até dezembro de 2016, disponibilizadas pela Prefeitura do Rio de Janeiro. Os resultados obtidos corroboram a utilidade da abordagem proposta para analisar e monitorar a rede de ônibus de uma cidade, o que pode ajudar os gestores do trânsito e as autoridades municipais a melhorar os planos de controle de trânsito e de mobilidade urbana.",TESE,Bus Network Analysis and Monitoring,5047438,1
"Norms are applied in multiagent systems as mechanisms capable of
restricting the behavior of software agents in order to achieve a desirable social
order. However, norms eventually can be conflicting — for example, when there is
a norm that prohibits an agent to perform a particular action and another norm that
obligates the same agent to perform the same action in the same period of time. The
agent’s decision about which norms to fulfill can be defined based on rewards,
punishments and agent goals. Sometimes, this balance will not be enough to allow
the agent to make the best decision. In this context, this proposal introduces an
approach that considers the agent’s personality traits in order to improve the plan
decision-making process and resolving normative conflicts. Our approach’s
applicability and validation is demonstrated by an experiment that reinforces the
importance of considering the norms both in the agent’ and society’s points of view.",Paulo H Cardoso 2017_Completo.pdf,ENGENHARIA DE SOFTWARE,PAULO HENRIQUE CARDOSO ALVES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,02/08/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Solving Normative Conflicts;Normative Agents;Multiagent Systems;Personality Traits.',ENGENHARIA DE SOFTWARE PARA SISTEMAS MULTI-AGENTES,ALBERTO BARBOSA RAPOSO,57,Resolução de Conflitos Normativos;Sistemas Multiagentes;Agentes Normativos;Traços de Personalidade,INFORMÁTICA (31005012004P9),-,"Normas são aplicadas em sistemas multiagentes como mecanismos capazes de restringir o comportamento dos agentes de software com o objetivo de alcançar uma ordem social desejável. Entretanto, essas normas podem entrar em conflito, como por exemplo, uma norma que proíbe um agente de realizar uma dada ação e outra norma que obriga o mesmo agente a realizar a mesma ação no mesmo intervalo de tempo. A decisão do agente sobre quais normas serão cumpridas pode ser definida com base nas recompensas, punições e objetivos do agente. No entanto, em determinadas situações a avaliação desses atributos pode não ser o suficiente para permitir que o agente efetue uma tomada de decisão satisfatória. Nesse contexto, foi elaborada uma abordagem que considera traços de personalidade em agentes de software para aprimorar o processo de resolução de conflitos normativos e a escolha dos planos para tomada decisões, além de realizar a comparação da abordagem proposta entre diferentes abordagens encontradas na literatura.",DISSERTAÇÃO,Agentes de Software com Traços de Personalidade baseados na arquitetura BDI para Tomada de Decisões Normativas,5047479,1
"Recent advances in recognition technologies, such as speech, touch
and gesture, have given rise to a new class of user interfaces that does
not only explore multiple modalities but also allows for multiple interacting
users. The development of applications with both multimodal and multiuser
interactions arise new specification and execution issues. The specification
of multimodal application is commonly the focus of multimodal interaction
research, while the specification of the synchronization of audiovisual media
is usually the focus of multimedia research. In this thesis, aiming to assist
the specification of such applications, we propose to integrate concepts from
those two research areas and to extend multimedia languages with first-class
entities to support multiuser and multimodal features. Those entities were
instantiated in NCL and HTML. To evaluate our approach, we performed
an evaluation with NCL and HTML developers to capture evidences of their
acceptance of the proposed entities and instantiations in those languages.",Álan Lívio.pdf,HIPERTEXTO E MULTIMÍDIA,ALAN LIVIO VASCONCELOS GUEDES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,29/09/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Multimedia Languages;Multimodal User Interactions;MUI;Multiuser User Interactions;Nested Context Language;NCL;HTML',-,SIMONE DINIZ JUNQUEIRA BARBOSA,111,Linguagens Multimídia;Interações Multimodais;MUI;Interações Multiusuário;Nested Context Language;NCL;HTML,INFORMÁTICA (31005012004P9),-,"Os recentes avanços em tecnologias de reconhecimento, como fala, toque e gesto,deram origem a uma nova classe de interfaces de usuário que não apenas explora múltiplas modalidades de interação, mas também permite múltiplos usuários interagindo. O desenvolvimento de aplicativos com interações multimodais e multiusuários trazem desaﬁos para a sua especiﬁcação e execução. A especiﬁcação de uma aplicação multimodal é comumente o foco das pesquisas em interação multimodal, enquanto a especiﬁcação de sincronismos audiovisuais geralmente é o foco das pesquisas em multimídia. Nesta tese, com o objetivo de auxiliar a especiﬁcação de tais aplicações, buscamos integrar conceitos dessas duas pesquisas e propomos estender linguagens multimídia com entidades de primeira classe para suportar recursos multiusuário e multimodais. Essas entidades foram instanciadas nas linguagens NCL e HTML. Para avaliar nossa abordagem, realizamos uma avaliação com desenvolvedores NCL e HTML para capturar indícios de aceitação das entidades propostas e suas sintaxes nessas linguagens.",TESE,Extending multimedia languages to support multimodal user interactions,5135624,1
"Reservoir modeling is a very important task that permits the
representation of a geological region of interest. Given the uncertainty involved
in the process, one wants to generate a considerable number of possible
scenarios so as to nd those which best represent this region. Then, there is
a strong demand for quickly generating each simulation. Since its inception,
many methodologies have been proposed for this purpose and, in the last two
decades, multiple-point geostatistics (MPS) has been the dominant one. This
methodology is strongly based on the concept of training image (TI) and the
use of its characteristics, which are called patterns. In this work, we propose a
new MPS method that combines the application of a technique called Locality
Sensitive Hashing (LSH), which permits to accelerate the search for patterns
similar to a target one, with a Run-Length Encoding (RLE) compression
technique that speeds up the calculation of the Hamming similarity. We
have performed experiments with both categorical and continuous images
which showed that LSHSIM is computationally ecient and produce good
quality realizations, while achieving a reasonable space of uncertainty. In
particular, for categorical data, the results suggest that LSHSIM is faster than
MS-CCSIM, one of the state-of-the-art methods.",Pedro Nuno_2017.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,PEDRO NUNO DE SOUZA MOURA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,21/09/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Multiple-Point Geostatistics;Locality Sensitive Hashing;Run-Length Encoding;Pattern Modeling;Training Image',ENGENHARIA DE ALGORITMOS,MARCUS VINICIUS SOLEDADE POGGI DE ARAGAO,93,Geoestatística Multiponto;Locality Sensitive Hashing;Run-Length Encoding;Modelagem de Padrões;Imagem de Treinamento,INFORMÁTICA (31005012004P9),-,"A modelagem de reservatórios consiste em uma tarefa de muita relevância na medida em que permite a representação de uma dada região geológica de interesse. Dada a incerteza envolvida no processo, deseja-se gerar uma grande quantidade de cenários possíveis para se determinar aquele que melhor representa essa região. Há, então, uma forte demanda de se gerar rapidamente cada simulação. Desde a sua origem, diversas metodologias foram propostas para esse propósito e, nas últimas duas décadas, Multiple-Point Geostatistics (MPS) passou a ser a dominante. Essa metodologia é fortemente baseada no conceito de imagem de treinamento (TI) e no uso de suas características, que são denominadas de padrões. No presente trabalho, é proposto um novo método de MPS que combina a aplicação de dois conceitos-chave: a técnica denominada Locality Sensitive Hashing (LSH), que permite a aceleração da busca por padrões similares a um dado objetivo; e a técnica de compressão Run-Length Encoding (RLE), utilizada para acelerar o cálculo da similaridade de Hamming. Foram realizados experimentos com imagens de treinamento tanto categóricas quanto contínuas que evidenciaram que o LSHSIM é computacionalmente eficiente e produz realizações de boa qualidade, enquanto gera um espaço de incerteza de tamanho razoável. Em particular, para dados categóricos, os resultados sugerem que o LSHSIM é mais rápido do que o MS-CCSIM, que corresponde a um dos métodos componentes do estado-da-arte.",TESE,LSHSIM: A Locality Sensitive Hashing Based Method for Multiple-Point Geostatistics,5135855,1
"Chávez López, Alexander; Garcia, Alessandro Fabricio (Advisor). How does refactoring aﬀect internal quality attributes? A multi-project study. Rio de Janeiro, 2017. 80p. Dissertação de mestrado – Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro. Developers often apply code refactoring to improve the internal quality attributes of a program, such as coupling and size. Given the structural decay of certain program elements, developers may need to apply multiple refactorings to these elements to achieve quality attribute improvements. We call re-refactoring when developers refactor again a previously refactored element in a program, such as a method or a class. There is limited empirical knowledge on to what extent developers successfully improve internal quality attributes through (re-)refactoring in their actual software projects. This dissertation addresses this limitation by investigating the impact of (re-)refactoring on ﬁve well-known internal quality attributes: cohesion, complexity, coupling, inheritance, and size. We also rely on the version history of 23 open source projects, which have 29,303 refactoring operations and 49.55% of re-refactoring operations. Our analysis revealed relevant ﬁndings. First, developers apply more than 93.45% of refactoring and re-refactoring operations to code elements with at least one critical internal quality attribute, as oppositely found in previous work. Second, 65% of the operations actually improve the relevant attributes, i.e. those attributes that are actually related to the refactoring type being applied; the remaining 35% operations keep the relevant quality attributes unaﬀected. Third, whenever refactoring operations are applied without additional changes,whichwecallroot-canal refactoring,theinternalqualityattributes are either frequently improved or at least not worsened. Contrarily, 55% of the refactoring operations with additional changes, such as bug ﬁxes, surprisingly improve internal quality attributes, with only 10% of the quality decline. This ﬁnding is also valid for re-refactoring. Finally, we also summarizeourﬁndingsasconcreterecommendationsforbothpractitionersand researchers.",Alexander Chávez _Completo.pdf,-,ALEXANDER CHAVEZ LOPEZ,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,01/09/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Refatorao;Re-refatorao;Qualidade Estrutural de Cdigo-Fonte;Atributos Internos de Qualidade;Mtricas de Software.',-,ALESSANDRO FABRICIO GARCIA,80,Refactoring;Re-refactoring;Code Structural Quality;Internal Quality Attributes;Software Metrics.,INFORMÁTICA (31005012004P9),-,"Desenvolvedores frequentemente aplicam refatoração para melhorar os atributos internos de qualidade em projetos de software, tais como acoplamento e tamanho. Chamamos de rerrefatoração quando desenvolvedores refatoram um elemento de código-fonte previamente refatorado. O conhecimento empírico é limitado acerca de até que ponto refatoração e rerrefatoração de fato melhoram os atributos internos de qualidade. Nesta dissertação, nós investigamos a limitação supracitada com base em cinco atributos internos de qualidade conhecidos: acoplamento, coesão, complexidade, herança e tamanho. Também nos baseamos no histórico de versionamento de 23 projetos de software de código-fonte aberto, os quais possuem 29,303 operações de refatoração e 49.55% de rerrefatorações. Nossa análise revelou descobertas interessantes apresentadas como segue. Primeiro, desenvolvedores aplicam mais de 93.45% de operações de refatoração e rerrefatoração sobre elementos de código-fonte com ao menos um atributo interno de qualidade crítico, contrariando trabalhos anteriores. Segundo, para 65% das operações, os atributos internos de qualidade relacionados melhoram, enquanto que os demais 35% permanecem não-afetados. Terceiro, sempre que operações de refatoração são aplicadas sem mudanças adicionais no código fonte, o que chamamos de operação de refatoração root-canal, os atributos internos de qualidade frequentemente melhoram, ou ao menos, não pioram. Ao contrário, 55% das operações de refatoração aplicadas com mudanças adicionais, tais como correção de bugs, surpreendentemente melhoram os atributos internos de qualidade, com somente 10% de piora, o que também é válido para rerrefatoração. Nós sumarizamos nossas descobertas na forma de recomendações para desenvolvedores e pesquisadores.",DISSERTAÇÃO,How does refactoring affect internal quality attributes? A multi-project study,5839595,1
"Herrera de Figueiredo, Alice; Celes Filho, Waldemar (Advisor). Directionality ﬁelds in generation and evaluation of quadrilateralmeshes.RiodeJaneiro,2017.59p.DissertaçãodeMestrado – Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.
One of the main challenges in quadrilateral mesh generation is to ensure the alignment of the elements with respect to domain constraints. Unaligned meshes insert numerical problems in simulations that use these meshes as a domain discretization. However, there is no alignment metric for evaluating the quality of quadrilateral meshes. A directionality ﬁeld represents the diﬀusion of the constraints orientation to the interior of the domain. Kowalski et al. use a directionality ﬁeld for domain partitioning into quadrilateral regions. In this work, we reproduce their partitioning method with some modiﬁcations, aiming to reduce the ﬁnal number of partitions. We also propose a metric to evaluate the quality of a quadrilateral mesh with respect to the alignment with domain constraints.",Alice Herrera.pdf,COMPUTAÇÃO GRÁFICA,ALICE HERRERA DE FIGUEIREDO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,29/09/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Domain Partitioning;Mesh;Quadrilateral',-,WALDEMAR CELES FILHO,59,Partição de Domínio;Malha;Quadrilátero.,INFORMÁTICA (31005012004P9),-,"Herrera de Figueiredo, Alice; Celes Filho, Waldemar. Campos de direcionalidade na geração e avaliação de malhas de quadriláteros. Rio de Janeiro, 2017. 59p. Dissertação de Mestrado – Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.
Um dos principais desaﬁos para a geração de malhas de quadriláteros é garantir o alinhamento dos elementos em relação às restrições do domínio. Malhas não alinhadas introduzem problemas numéricos em simulações que usam essas malhas como subdivisão do domínio. No entanto, não existe uma métrica de alinhamento para a avaliação de qualidade de malhas de quadriláteros. Um campo de direcionalidade representa a difusão das orientações das restrições no interior do domínio. Kowalski et al. usam um campo de direcionalidade para particionar o domínio em regiões quadrilaterais. Neste trabalho, reproduzimos o método de particionamento proposto por Kowalski et al. com algumas alterações, visando reduzir o número ﬁnal de partições. Em seguida, propomos uma métrica para avaliar a qualidade de malhas de quadriláteros em relação ao alinhamento com as restrições do domínio.",DISSERTAÇÃO,Campos de direcionalidade na geração e avaliação de malhas de quadriláteros,5845769,1
"Aline Medeiros Saettler; Eduardo Sany Laber (advisor). Approximation Algorithms for Decision Trees. Rio de Janeiro, 2017. 90p. Tese de Doutorado — Departamento de Inform´atica, Pontif´ıcia Universidade Cato´lica do Rio de Janeiro.
Decision tree construction is a central problem in several areas of computer science, for example, data base theory and computational learning. This problem can be viewed as the problem of evaluating a discrete function, where to check the value of each variable of the function we have to pay a cost, and the points where the function is deﬁned are associated with a probability distribution. The goal of the problem is to evaluate the function minimizing the cost spent (in the worst case or in expectation). In this Thesis, we present four contributions related to this problem. The ﬁrst one is an algorithm that achieves an O(log(n)) approximation with respect to both the expected and the worst costs. The second one is a procedure that combines two trees, one with worst cost W and another with expected cost E, and produces a tree with worst cost at most (1+ρ)W and expected cost at most (1/(1−e−ρ))E, where ρ is a given parameter. We also prove that this is a sharp characterization of the best possible trade-oﬀ attainable, showing that there are inﬁnitely many instances for which we cannot obtain a decision tree with both worst cost smaller than (1+ρ)OPTW(I) and expected cost smaller than (1/(1−e−ρ))OPTE(I), where OPTW(I) (resp. OPTE(I)) denotes the cost of the decision tree that minimizes the worst cost (resp. expected cost) for an instance I of the problem. The third contribution is an O(log(n)) approximation algorithm for the minimization of the worst cost for a variant of the problem where the cost of reading a variable depends on its value. Our ﬁnal contribution is a randomized rounding algorithm that, given an instance of the problem (with an additional integer k ≥ 0) and a parameter 0 <  < 1/2, builds an oblivious decision tree with cost at most (3/(1 − 2))ln(n)OPT(I) and produces at most (k/) errors, where OPT(I) denotes the cost of the oblivious decision tree with minimum cost among all oblivious decision trees for instance I that make at most k classiﬁcation errors.",Aline  Medeiros 2017.pdf,OTIMIZAÇÃO E RACIOCÍNIO AUTOMÁTICO,ALINE MEDEIROS SAETTLER,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,05/09/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Decision Trees;Approximation Algorithms;Combinatorial Optimiza tion.',"APRENDIZADO DE MÁQUINA, RACIOCÍNIO HEURÍSTICO E PROBABILÍSTICO",EDUARDO SANY LABER,90,Arvores de Decisa˜o;Algoritmos de Aproxima¸ca˜o;Optimizac¸˜ao Combinato´ria.,INFORMÁTICA (31005012004P9),-,"Aline Medeiros Saettler; Eduardo Sany Laber. Algoritmos de Aproxima¸c˜ao para ´Arvores de Decis˜ao. Rio de Janeiro, 2017. 90p. Tese de Doutorado — Departamento de Informa´tica, Pontif´ıcia Universidade Cato´lica do Rio de Janeiro.
A constru¸c˜ao de a´rvores de decisa˜o ´e um problema central em diversas a´reas da ciˆencia da computac¸˜ao, por exemplo, teoria de banco de dados e aprendizado computacional. Este problema pode ser visto como o problema de avaliar uma fun¸ca˜o discreta, onde para veriﬁcar o valor de cada varia´vel da fun¸ca˜o temos que pagar um custo, e os pontos onde a func¸˜ao esta´ deﬁnida esta˜o associados a uma distribui¸ca˜o de probabilidade. O objetivo do problema ´e avaliar a fun¸ca˜o minimizando o custo gasto (no pior caso ou no caso m´edio). Nesta tese, apresentamos quatro contribui¸c˜oes relacionadas a esse problema. A primeira´e um algoritmo que alcan¸ca uma aproxima¸c˜ao de O(log(n)) em relac¸˜ao a tanto o custo esperado quanto ao pior custo. A segunda ´e um m´etodo que combina duas a´rvores, uma com pior custo W e outra com custo esperado E, e produz uma ´arvore com pior custo de no ma´ximo (1+ρ)W e custo esperado no ma´ximo (1/(1−e−ρ))E, onde ρ ´e um paraˆmetro dado. No´s tamb´em provamos que esta ´e uma caracteriza¸ca˜o justa do melhor trade-oﬀ alcan¸c´avel, mostrando que existe um nu´mero inﬁnito de instaˆncias para as quais na˜o podemos obter uma a´rvore de decis˜ao com tanto o pior custo menor que (1 + ρ)OPTW(I) quanto o custo esperado menor que (1/(1 − e−ρ))OPTE(I), onde OPTW(I) (resp. OPTE(I)) denota o pior custo da a´rvore de decisa˜o que minimiza o pior custo (resp. custo esperado) para uma instaˆncia I do problema. A terceira contribuic¸˜ao ´e um algoritmo de aproxima¸ca˜o de O(log(n)) para a minimiza¸ca˜o do pior custo para uma variante do problema onde o custo de ler uma vari´avel depende do seu valor. Nossa u´ltima contribui¸ca˜o ´e um algoritmo randomized rounding que, dada uma instaˆncia do problema (com um inteiro adicional k ≥ 0) e um paraˆmetro 0 <  < 1/2, produz uma a´rvore de decisa˜o oblivious com custo no ma´ximo (3/(1 − 2))ln(n)OPT(I) e que produz no m´aximo (k/) erros, onde OPT(I) denota o custo da ´arvore de decis˜ao oblivious com o menor custo entre todas as a´rvores oblivious para a instaˆncia I que produzem no m´aximo k erros de classiﬁcac¸˜ao.",TESE,Approximation Algorithms for Decision Trees,5846998,1
"Cunha, Marcio Luiz Coelho; Fuks, Hugo(Advisor). Software of Places: Toward a Self-Learning Closed Plant Production System. Rio de Janeiro, 2017. 111p. Tese de Doutorado - Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.  
As the population grows, more food will need to be produced in the next four decades than has been in the past 10,000 years. However, the modern world still depends on high yield monoculture production which is increasingly threatened by unusual weather, water shortages, and insufficient land. In order to overcome these problems and feed the world, a practical path to provide quality fresh healthy food at scale with minimal weather dependency, water usage and reduced carbon footprint is necessary. One reasonable approach is to build vertical farms inside the cities in a close environment full of sensors and artificial lighting controlled by software for efficient production of food crops. This thesis proposes a model, entitled Software of Places Cycle (SoPC), that should be able to answer to environmental stimuli in a closed plant production system using artificial lighting in order to create a self-learning environment. This thesis describes the SoPC, the approaches and processes of implementing a mini Plant Factory using Artificial Lighting based on the discussion on five action-research cycles. The thesis main contribution is a conceptual model to guide the development and maintenance of a mini-PFAL (m-PFAL), a minor contribution is the deployment of the SoP, i.e., the very notion of having software dedicated to a specific place.",Marcio Cunha_2017_Completo.pdf,ENGENHARIA DE SOFTWARE,MARCIO LUIZ COELHO CUNHA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,20/12/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Plant Factories with Artificial Lighting;Internet of Things;Vertical Farming;Controlled Environment;Analytics of Things;Ambient Intelligence.',-,HUGO FUKS,111,Plant Factories with Artificial Lighting;Internet of Things;Vertical Farming;Controlled Environment;Analytics of Things;Ambient Intelligence.,INFORMÁTICA (31005012004P9),-,"À medida que a população cresce, mais alimentos precisarão ser produzidos nas próximas quatro décadas do que nos últimos 10.000 anos. No entanto, o mundo moderno ainda depende da produção de monoculturas de alto rendimento, cada vez mais ameaçada por condições climáticas incomuns, escassez de água e terra insuficiente. A fim de superar esses problemas e alimentar o mundo, é necessário um caminho prático para fornecer alimentos frescos, com qualidade e em escala, com mínima dependência do clima e com uso de água e pegada de carbono reduzidos. Uma abordagem razoável é construir fazendas verticais dentro das cidades em um ambiente fechado repleto de sensores e iluminação artificial controlada por software para uma produção e gestão eficiente do plantio de alimentos. Esta tese propõe a instanciação de um modelo, chamado Ciclo do Software dos Lugares (SoPC), que é capaz de responder a estímulos ambientais em um sistema fechado de produção de plantas com iluminação artificial que possibilite a criação de ambientes com auto-aprendizagem. Esta tese descreve o SoPC, as abordagens e processos de implementação de uma mini fábrica de plantas com iluminação artificial com base na discussão em cinco ciclos de pesquisa-ação.",TESE,Software of Places: Toward a Self-Learning Closed Plant Production System,5890242,1
"Jimenéz, Haydée Guillot; Furtado, Antonio Luz (Advisor). Applying Process Mining to the Academic Administration Domain. Rio de Janeiro, 2017. 67p. Dissertação de Mestrado – Departamento de Informática, Pontifícia Universidade Católica do Rio de Janeiro.
Higher Education Institutions keep a sizable amount of data, including student records and the structure of degree curricula. This work, adopting a process mining approach, focuses on the problem of identifying how closely students follow the recommended order of the courses in a degree curriculum, and to what extent their performance is affected by the order they actually adopt. It addresses this problem by applying to student records two already existing techniques: process discovery and conformance checking, and frequent itemsets. Finally, the dissertation covers experiments performed by applying these techniques to a case study involving over 60,000 student records from PUC-Rio. The experiments show that the frequent itemsets technique performs better than the process discovery and conformance checking techniques. They equally confirm the relevance of analyses based on the process mining approach to help academic coordinators in their quest for better degree curricula.
Keywords",Haydée Guillot 2017.pdf,BANCOS DE DADOS,HAYDEE GUILLOT JIMENEZ,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,21/09/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Anlise acadmica;frequncia de conjuntos;minerao de processos;organizao curricular',-,ANTONIO LUZ FURTADO,0,Academic analytics;frequent itemsets;process mining;degree curriculum structure,INFORMÁTICA (31005012004P9),-,"As instituições de ensino superior mantêm uma quantidade considerável de dados que incluem tanto os registros dos alunos como a estrutura dos currículos dos cursos de graduação. Este trabalho, adotando uma abordagem de mineração de processos, centra-se no problema de identificar quão próximo os alunos seguem a ordem recomendada das disciplinas em um currículo de graduação, e até que ponto o desempenho de cada aluno é afetado pela ordem que eles realmente adotam. O problema é abordado aplicando-se duas técnicas já existentes aos registros dos alunos: descoberta de processos e verificação de conformidade; e frequência de conjuntos de itens. Finalmente, a dissertação cobre experimentos realizados aplicando-se essas técnicas a um estudo de caso com mais de 60.000 registros de alunos da PUC-Rio. Os experimentos indicam que a técnica de frequência de conjuntos de itens produz melhores resultados do que as técnicas de descoberta de processos e verificação de conformidade. E confirmam igualmente a relevância de análises baseadas na abordagem de mineração de processos para ajudar coordenadores acadêmicos na busca de melhores currículos universitários.",DISSERTAÇÃO,Applying Process Mining to the Academic Administration Domain,5964506,1
"Nowadays, we often hear about the importance of Information and Communication Technologies (ICT) by various social actors. The influence of ICT crosses the various areas of society as agriculture, services, trade, industry, research, among others. If we do an inverse reasoning it will be difficult to name social fields that are not directly or indirectly influenced by ICTs. In addition, the demand for workers in Computer Science and areas related to the STEM (Science, Technology, Engineering, and Mathematics) is on the rise. That is why it is important to make the young person interested in technology and participate in it in a fun and playful way. The present work proposes the creation of a Virtual Reality tool that allows students to learn basic concepts of programming and computational thinking with the purpose that they enjoy the technology and feel motivated to learn more. The tool is a Visual Programming Language, the algorithms are formed by block-assembly, thereby solving one of the students' main problems which are ""syntax errors"". In addition, the tool brings with it a set of level-ordered challenges aimed at teaching students basic principles of programming and logic (sequential programming, repetitive and conditional data structure), where at each level the student will learn the different concepts and behaviors of computational thinking. For the evaluations with the users we counted on the participation of 18 students between 12 and 15 years old coming from two public institutions of Rio de Janeiro.",,COMPUTAÇÃO GRÁFICA,HERMINIO PAUCAR CURASMA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,24/10/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Computational thinking;virtual reality;programming in education;introduction to programming;learning programming through games;learning programming with virtual reality.',ANIMAÇÃO E REALIDADE VIRTUAL,ALBERTO BARBOSA RAPOSO,0,Pensamento computacional;realidade virtual;programação na educação;introdução à programação;aprender programação através de jogos;aprende programação com realidade virtual.,INFORMÁTICA (31005012004P9),-,"No nosso quotidiano ouvimos com frequência falar da importância das Tecnologias de Informação e Comunicação (TIC) pelos diversos atores sociais. A influência das TIC atravessa as diversas áreas da sociedade como: agricultura, serviços, comércio, indústria, investigação, entre outros. Se fizermos um raciocínio inverso será difícil nomearmos campos sociais que não sejam influenciados direta ou indiretamente pelas TIC. Além disso a demanda de trabalhadores em Computer Science e áreas relacionadas a STEM (Science, Technology, Engineering and Mathematics) está em aumento. Por isso, é importante fazer com que as crianças desde tenra idade se interessem pela tecnologia e participem dela de uma forma divertida e lúdica. O presente trabalho propõe a criação de uma ferramenta de Realidade Virtual que permite que os estudantes aprendam conceitos básicos da programação e pensamento computacional tendo como finalidade que eles desfrutem da tecnologia e se sintam motivados em aprender mais. A ferramenta é uma Linguagem Visual de Programação. Os algoritmos são formados mediante a montagem de blocos-, resolvendo com isso um dos principais problemas dos estudantes que são os “erros de sintaxe”. Além disso a ferramenta traz consigo um conjunto de desafios ordenados por níveis, que têm como finalidade ensinar aos estudantes princípios básicos da programação e a lógica (programação sequencial, estrutura de dados repetitiva e condicional), onde em cada nível o aluno aprenderá as diferentes conceitos e comportamentos do pensamento computacional. Para as avaliações com os usuários se contou com a participação de 18 alunos com idades entre 12 e 15 anos provenientes de duas instituições públicas do Rio de Janeiro.",DISSERTAÇÃO,Uma ferramenta de realidade virtual para a introdução à programação e pensamento computacional para jovens,5964992,1
"The last years witnessed a growing number of devices that track objects moving in geographical space with GPS or other positioning devices, producing large volumes of mobility data – or trajectory data. Also, thanks to the emergence of the Linked Data initiative, another unprecedented global space is also growing at a fast pace: the Web of Data. This thesis contributes to mitigate some of the challenges that arise in this scenario. First, it investigates how trajectory data may benefit from the Linked Data Initiative, by guiding the whole trajectory enrichment process with the use of external datasets. Then, it addresses how to compute the similarity between Linked Data entities, such as the entities that represent trajectory POIs (Points Of Interests), by considering their relevant features, also extracted from Linked Data. Finally, it targets the computation of the similarity between trajectories taking advantage of the categories of its POIs, also available as Linked Data.",Lívia Couto.PDF,BANCOS DE DADOS,LIVIA COUTO RUBACK RODRIGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,15/12/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Linked Data;Semantic Web;Movement data;Semantic trajectories;Semantic similarity.',-,MARCO ANTONIO CASANOVA,104,Dados Interligados;Web Semântica;Dados de movimento;Trajetórias semânticas;Similaridade semântica,INFORMÁTICA (31005012004P9),-,"Os últimos anos testemunharam o uso crescente de dispositivos que rastreiam objetos em movimento no espaço geográfico e que utilizam tecnologias como GPS. Como consequência, um grande volume de dados de mobilidade – ou dados de trajetórias – tem sido produzido. Além disso, graças à iniciativa de dados interligados, um outro espaço de dados global sem precedentes tem crescido rapidamente: a Web de Dados. Esta tese contribui para mitigar alguns dos desafios que surgem neste cenário. Em primeiro lugar, a tese investiga como dados de trajetória podem se beneficiar da iniciativa de dados interligados, guiando todo o processo de enriquecimento semântico utilizando fontes de dados externas. Em segundo lugar, aborda como computar a similaridade entre entidades em dados interligados, como por exemplo entidades que representem PDIs (Pontos de Interesse) de trajetórias, considerando seus atributos relevantes, também extraídos de dados interligados. Por fim, trata da computação da similaridade entre trajetórias, aproveitando categorias dos seus PDIs, também disponíveis como dados interligados.",TESE,Enriching and analyzing Semantic Trajectories with Linked Open Data,5969550,1
"Code smells are anomalous code structures which often indicate maintenance
problems in software systems. The identification of code smells is
required to reveal code elements, such as classes and methods, that are
poorly structured. Some examples of code smell types perceived as critical
by developers include God Classes and Feature Envy. However, the individual
smell identification, which is performed by a single developer, may
be ineffective. Several studies have reported limitations of individual smell
identification. For instance, the smell identification usually requires an indepth
understanding of multiple elements scattered in a program, and each
of these elements is better understood by a different developer. As a consequence,
a single developer often struggles and to find to confirm or refute
a code smell suspect. Collaborative smell identification, which is performed
together by two or more collaborators, has the potential to address this
problem. However, there is little empirical evidence on the effectiveness of
collaborative smell identification. In this thesis, we addressed the aforementioned
limitations as follows. First, we conducted empirical studies aimed at
understanding the effectiveness of both collaborative and individual smell
identification. We computed and compared the effectiveness of collaborators
and single developers based on the number of correctly identified code
smells. We conducted these studies in both industry’s companies and research
laboratories with 67 developers, including novice and professional
developers. Second, we defined some influential factors on the effectiveness
of collaborative smell identification, such as the smell granularity. Third, we
revealed and characterized some collaborative activities which improve the
developers’ effectiveness for identifying code smells. Fourth, we also characterized
opportunities for further improving the effectiveness of certain
collaborative activities. Our results suggest that collaborators are more effective
than single developers in: (i) both professional and academic settings,
and (ii) identifying a wide range of code smell types.",Roberto Felício_2017.pdf,ENGENHARIA DE SOFTWARE,ROBERTO FELICIO DE OLIVEIRA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,21/08/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'Code Smell;Identification of Code Smell;Collaborative Smell Identification;Individual Smell Identification;Experimental Software Engineering',-,CARLOS JOSE PEREIRA DE LUCENA,143,Anomalia de Código;Identificação de Anomalia de Código;Identificação Colaborativa de Anomalia;Identificação Individual de Anomalia;Engenharia de Software Experimental,INFORMÁTICA (31005012004P9),-,"Anomalias de código são estruturas anômalas de código que podem indicar problemas de manutenção. A identificação de anomalias é necessária para revelar elementos de código mal estruturados, tais como classes e métodos. Porém, a identificação individual de anomalias, realizada por um único desenvolvedor, pode ser ineficaz. Estudos reportam limitações da identificação individual de anomalias. Por exemplo, a identificação de anomalias requer uma compreensão profunda de múltiplos elementos de um programa, e cada elemento é melhor entendido por um desenvolvedor diferente. Logo, um desenvolvedor isolado frequentemente tem dificuldades para encontrar, confirmar e refutar uma suspeita de anomalia. Identificação colaborativa de anomalias, que é realizada em conjunto por dois ou mais colaboradores, tem o potencial para resolver esse problema. Porém, há pouca evidência empírica sobre a eficácia da identificação colaborativa de anomalias. Nesta tese, nós conduzimos estudos empíricos para entender a eficácia da identificação individual e colaborativa de anomalias. Computamos e comparamos a eficácia de colaboradores e desenvolvedores isolados com base no número de anomalias identificadas corretamente. Conduzimos tais estudos em empresas e laboratórios de pesquisa, totalizando 67 desenvolvedores, incluindo desenvolvedores novatos e experientes. Também definimos alguns fatores de influência sobre a eficácia da identificação colaborativa de anomalias, tais como a granularidade da anomalia. Revelamos e caracterizamos algumas atividades colaborativas que melhoram a eficácia dos desenvolvedores na identificação de anomalias. Finalmente, identificamos oportunidades para melhorar certas atividades colaborativas. Nossos resultados sugerem que colaboradores são significativamente mais eficazes que desenvolvedores isolados, tanto desenvolvedores novatos quanto experientes. Concluímos que colaborar é vantajoso para melhorar a identificação de uma vasta gama de tipos de anomalia.",TESE,To collaborate or not to collaborate? Improving the identification of code smells,5970963,1
"Information exploration processes are usually recognized by their inherent
complexity, lack of knowledge and uncertainty, concerning both the domain and
the solution strategies. Even though there has been much work on the
development of computational systems supporting exploration tasks, such as
faceted search and set-oriented interfaces, the lack of a formal understanding of
the exploration process and the absence of a proper separation of concerns
approach in the design phase is the cause of many expressivity issues and serious
limitations. This work proposes a novel design approach of exploration tools
based on a formal framework for representing exploration actions and processes.
Moreover, we present a new exploration system that generalizes the majority of
the state-of-the art exploration tools. The evaluation of the proposed framework is
guided by case studies and comparisons with state-of-the-art tools. The results
show the relevance of our approach both for the design of new exploration tools
with higher expressiveness, and formal assessments and comparisons between
different tools.",1313520_2017_Pretextual.pdf,HIPERTEXTO E MULTIMÍDIA,THIAGO RIBEIRO NUNES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,06/10/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO DE JANEIRO,b'exploration;formal model;framework;semi-structured data',-,DANIEL SCHWABE,135,exploração;modelo formal;framework;dados semiestrutrados,INFORMÁTICA (31005012004P9),-,"Tarefas de exploração de informação são reconhecidas por possuir características tais como alta complexidade, falta de conhecimento do usuário sobre o domínio da tarefa e incertezas sobre as estratégias de
solução. O estado-da-arte em exploração de dados inclui uma variedade
de modelos e ferramentas baseadas em diferentes paradigmas de interação, como por exemplo, busca por palavras-chave, busca facetada e orientação-a-conjuntos. Não obstante os muitos avanços das últimas décadas, a falta de uma abordagem formal do processo de exploração,juntamente com a falta de uma adoção mais pragmática do princípio de
separação-de-responsabilidades no design dessas ferramentas são a causa de muitas limitações. Dentre as limitações, essa tese aborda a falta de expressividade, caracterizada por restrições na gama de estratégias de solução possíveis, e dificuldades de análise e comparação entre as ferramentas propostas. A partir desta observação, o presente trabalho
propõe um modelo formal de ações e processos de exploração, uma nova
abordagem para o projeto de ferramentas de exploração e uma ferramenta que generaliza o estado-da-arte em exploração de informação.
As avaliações do modelo, realizadas por meio de estudos de caso,análises e comparações o estado-da-arte, corroboram a utilidade da abordagem.",TESE,A Model for Exploration of Semi-Structured Datasets,5972097,1
"The production of learning objects allied the areas of technology and education, being a key factor to bring new possibilities of learning. Due to the increase in computational resources, the production of this type of educational material grew and began to incorporate practices of software production in general, highlighting some peculiarities due to the educational objectives proposed in the objects. Thereat, the introduction of Software Engineering practices is increasingly present in the production of learning objects, and, as in software production, the qualification of this production process has become a necessity. In software production, a consolidated solution for production qualification is in the evaluation frameworks, such as CMMI and MPS.BR. In order to think this type of solution for the production of learning objects, this research presents a framework to evaluate the practices performed by the centers of production of learning objects. This framework is a result of the triangulation of pertinent practices related to the theme obtained from a literature review (including a systematic review and systematic mapping) and from interviews with members of nine Brazilian learning object centers. As a result obtained from its application stands out its capacity to promote the reflection of the production process, as well as well as to encourage them to carry out new practices.",[DISSERTAÇÃO] JOÃO PEDRO DEWES GUTERRES.pdf,CIÊNCIA DA COMPUTAÇÃO,JOAO PEDRO DEWES GUTERRES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,08/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'SEM PALAVRAS CHAVE',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",MILENE SELBACH SILVEIRA,187,SEM PALAVRAS CHAVE,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),Qualificação do Processo de Produção de Objetos de Aprendizagem,"A produção de objetos de aprendizagem aliou as áreas da tecnologia e da educação, sendo um fator chave para trazer novas possibilidades de aprendizagem. Devido ao aumento dos recursos computacionais, a produção desse tipo de material educacional cresceu e passou a incorporar práticas da produção de software em geral, destacando-se algumas peculiaridades por conta dos objetivos educacionais propostos nos objetos. Com isso, a introdução de práticas de Engenharia de Software está cada vez mais presente na produção dos objetos de aprendizagem, e, assim como na produção de software, a qualificação deste processo de produção se tornou uma necessidade. Na produção de software, uma solução consolidada para qualificação da produção está nos frameworks de avaliação, como o CMMI e MPS.BR. De modo a pensar esse tipo de solução para a produção de objetos de aprendizagem, esta pesquisa apresenta um framework para avaliar as práticas realizadas pelos centros de produção de objetos de aprendizagem. Esse framework é resultado da triangulação de práticas pertinentes relacionadas ao tema obtidas a partir de uma revisão de literatura (incluindo uma revisão e um mapeamento sistemático) e de entrevistas com integrantes de nove centros brasileiros de produção de objetos de aprendizagem. Como resultado obtido de sua aplicação destaca-se a capacidade de promover a reflexão do processo de produção e das práticas dos centros de produção de objetos de aprendizagem, bem como de incentivá-los à realização de novas práticas.",DISSERTAÇÃO,QPPOA: FRAMEWORK PARA QUALIFICAÇÃO DO PROCESSO DE PRODUÇÃO DE OBJETOS DE APRENDIZAGEM,4992242,1
"The next generation of MultiProcessor Systems-on-Chip (MPSoC) will encompass
hundreds of integrated processing elements into a single chip, with the promise of highthroughput,
low latency and, preferably, low energy utilization. Due to the high communication
parallelism required by several applications targeting MPSoC architectures, the
Network-on-Chip (NoC) has been widely adopted as a reliable and scalable interconnection
mechanism.
The NoC design space should be explored to meet the demanding requirements
of current applications. Among the parameters that define a NoC configuration, the routing
algorithm has been employed to provide services such as fault tolerance, deadlock and
livelock freedom, as well as Quality of Service (QoS). As the adoption and complexity of
System-on-Chip (SoC) increases for embedded systems, the concern for data protection
appears as a new design requirement.
Currently, MPSoCs can be attacked by exploiting either hardware or software vulnerabilities,
with the later responsible for 80% of the security incidents in embedded systems.
Protection against software vulnerabilities can occur at (i) Application Level, by using
techniques such as data encryption to avoid plain data transmissions between Intellectual
Property (IP) modules; or (ii) Communication Level, inspecting or filtering elements at the
interconnect fabric with communication monitors or firewalls, respectively. As such, a routing
algorithm aware of security requirements could also offer protection utilizing trusted communication
paths in the NoC, avoiding potential malicious elements in otherwise unsafe communication
paths.
The main contribution of this work is a NoC protection technique at communication
level by adapting Segment-based Routing (SBR) and Region-based Routing (RBR) algorithms
to consider system security requirements, characterized by security zones which are
defined on the NoC according to the mapping of applications on IP modules. Evaluation of
the proposed routing technique considers aspects such as the scalability of routing tables,
the number of secure communication paths, and the impact of this technique on applications
of the NASA Numerical Aerodynamic Simulation (NAS) Parallel Benchmark (NPB).",[DISSERTAÇÃO] RAMON COSTI FERNANDES.pdf,CIÊNCIA DA COMPUTAÇÃO,RAMON COSTI FERNANDES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,13/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Networks-on-Chip;NoCs;Intrachip Routing',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,CESAR AUGUSTO MISSIO MARCON,89,Redes Intrachip;NoCs;Roteamento Intrachip;Segurança em NoC,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"A próxima geração de sistemas multiprocessados intra-chip, do inglês MultiProcessor
Systems-on-Chip (MPSoC), comportará centenas de elementos de processamento
num único chip, com a promessa de alta vazão de comunicação, baixa latência e, preferencialmente,
baixo consumo de energia. Devido à elevada demanda de comunicação paralela
de aplicações para MPSoCs, a rede intra-chip, do inglês Network-on-Chip (NoC), tem sido
amplamente adotada como um meio de comunicação confiável e escalável para MPSoCs.
O espaço de projeto para NoCs deve ser explorado para atender à demanda das
aplicações atuais. Dentre os parâmetros que definem uma NoC, o algoritmo de roteamento
tem sido utilizado para prover serviços como tolerância à falhas, liberdade de deadlocks e
de livelocks, assim como Quality of Service (QoS). Conforme a adoção e complexidade de
Systems-on-Chip (SoC) aumenta para sistemas embarcados, a preocupação com a proteção
de dados também torna-se um requisito para o projeto de MPSoCs.
Atualmente, MPSoCs podem ser atacados explorando vulnerabilidades em hardware
ou software, sendo o último responsável por 80% dos incidentes de segurança em
sistemas embarcados. A proteção contra vulnerabilidades de software pode acontecer em:
(i) Nível de Aplicação, utilizando técnicas como a criptografia, para evitar a transmissão de
dados desprotegidos entre os elementos de um MPSoC, conhecidos como módulos de propriedade
intelectual, do inglês Intellectual Property (IP); ou (ii) Nível de Comunicação, inspecionando
ou filtrando elementos na arquitetura de interconexão através de monitores de
comunicação ou firewalls, respectivamente. Portanto, um algoritmo de roteamento, ciente
dos requisitos de segurança do sistema, deve oferecer proteção ao utilizar rotas confiáveis
na NoC, evitando elementos potencialmente maliciosos em rotas porventura inseguras.
A principal contribuição deste trabalho é uma técnica de proteção para NoCs que
atua em nível de comunicação, adaptando os algoritmos Segment-based Routing (SBR) e
Region-based Routing (RBR) para que estes considerem aspectos de segurança do sistema,
estes caracterizados por zonas de segurança definidas na NoC de acordo com o mapeamento
de aplicações nos IPs. A avaliação da técnica de roteamento considera aspectos
como a escalabilidade das tabelas de roteamento, a quantidade de rotas seguras definidas
entre os IPs, e o impacto desta técnica de roteamento em aplicações do benchmark NASA
Numerical Aerodynamic Simulation (NAS) Parallel Bencharm (NPB).",DISSERTAÇÃO,A SECURITY-AWARE ROUTING APPROACH FOR NETWORKS-ON-CHIP,4993235,1
"n the last three decades of research in Distributed Systems (DSs), one core aspect discussed
is the one of synchrony. With an asynchronous system, we make no assumptions
about process execution speeds and/or message delivery delays; with a synchronous system,
we do make assumptions about these parameters [1]. Synchrony in DSs impacts directly
the complexity and functionality of fault-tolerant algorithms. Although a synchronous infrastructure
contributes towards the development of simpler and reliable systems, yet such
an infrastructure is too expensive and sometimes even not feasible to implement. On the
other hand, a fully asynchronous infrastructure is more realistic, but some problems were
shown to be unsolvable in such an environment through the impossibility result by Fischer,
Lynch and Paterson [2]. The limitations in both fully synchronous or fully asynchronous
environments have led to the development of partial synchronous distributed systems [3, 4].
In a study of partial synchronous distributed systems functionality, and of Virtual Networks
(VNs) properties, we found that there are several challenges for this kind of systems
that can be solved with VNs due to the properties that virtualization brings. For example a)
resources sharing provided by VNs allows decreasing the cost when sharing the synchronous
portion of the physical infrastructure, b) isolation provided by the VNs nature can benet
the coexistent DSs on same physical infrastructure that demand certain level of isolation, c)
resilience guaranteed through the Virtual Networks Embedding (VNE) process that allows
allocating spare resources beside the primary ones for virtual networks that require availability
guarantees, for example fault tolerant DSs. In our work, we argue that VNs and
a suitable VN embedding process oer suitable environment for running distributed applications
with partial synchrony. This has led to the abstraction of new type of VNs: The
Hybrid Synchrony Virtual Networks (HSVNs).
In this thesis, we introduce the general idea of Hybrid Synchrony Virtual Networks
(HSVNs) motivated by the hybrid synchronous DSs, and we branch our work into
two branches: a) Space_HSVNs addressed to spatial hybrid synchronous DSs, and b)
Time_HSVNs addressed to the time hybrid synchronous DSs. In spatial hybrid synchronous
DSs, the hybrid synchronous physical infrastructure is composed of subsets of synchronous and asynchronous components, and each of these subsets maintains its synchrony status
through time (i.e., synchronous subsets remain synchronous and asynchronous ones remain
asynchronous). In time hybrid synchronous DSs, the hybrid synchronous physical infrastructure
is composed of subsets of nodes and links that can alternate their synchrony status
through time (i.e., the components behave synchronously during time intervals, and asynchronously
during other time intervals).
The main contributions of this thesis are: a) characterize the HSVNs in its two types
Space_HSVNs and Time_HSVNs to reect both the synchrony space-variant and timevariant
nature of DSs; b) propose a suitable embedding framework for both Space_HSVNs
and Time_HSVNs, and c) provide an evaluation of the embedding models addressed to the
HSVNs.",Rasha_HSVN_thesis.pdf,CIÊNCIA DA COMPUTAÇÃO,RASHA HASAN,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,16/01/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Distributed Systems;Synchrony;Virtual Networks;Embedding',PROCESSAMENTO PARALELO E DISTRIBUÍDO,FERNANDO LUIS DOTTI,144,Distributed Systems;Synchrony;Virtual Networks;Embedding,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Nas ultimas decados de pesquisa em Sistemas Distribuídos (SDs), um aspecto central discutido é o de sincronia. Com um sistema assíncrono, não fazemos suposiçoẽs sobre velocidades de execução de processos e / ou atrasos de entrega de mensagens; Com um sistema síncrono, fazemos suposiçoẽs sobre esses parâmetros. Sincronismo em SDs impacta diretamente a complexidade e funcionalidade de algoritmos tolerantes a falhas. Uma infra-estrutura síncrona contribui para o desenvolvimento de sistemas mais simples e fiàveis, mas tal infra-estrutura é muito cara e às vezes nem sequer viàvel de implementar. Uma infra-estrutura totalmente assíncrona é mais realista, mas alguns problemas foram mostrados como insolúveis em tal ambiente atravès do resultado de impossibilidade por Fischer, Lynch e Paterson. As limitaçoẽs tanto em ambientes totalmente síncronos como totalmente assíncronos levaram ao desenvolvimento de sistemas distribuídos como síncronia parcial.

Em um estudo de funcionalidade de sistemas distribuídos síncronos parciais e de propriedades de Redes Virtuais (RVs), descobrimos que existem vàrios desafios para este tipo de sistemas que podem ser resolvidos com RVs devido as propriedades que a virtualização traz. Por exemplo (a) partilha de recursos fornecida por RVs permite diminuir o custo ao partilhar a parte síncrona da infra-estrutura física, (b) isolamento fornecido por a natureza da RVs, isso pode beneficiar os SDs coexistentes na mesma infra-estrutura física que exigem certo nível de isolamento, (c) resiliência garantido atravès do processo de alocação de recursos de Redes Virtuais, isso permite alocar recursos de reposição ao lado dos primàrios para redes virtuais que exigem garantias de disponibilidade, por exemplo, SDs tolerantes a falhas. Em nosso trabalho, argumentamos que as RVs e um adequado processo de alocação de recursos das RVs oferecem um ambiente adequado para executar aplicativos distribuídos com sincronia parcial. Isto levou à abstração de um novo tipo de RVs: As Redes Virtuais com sincronia híbrida (RVSHs). 

Nesta tese, apresentamos a idéia geral das Redes Virtuais com sincronia híbrida motivado pelos SDs com síncronia híbrida, e dividimos nosso trabalho em duas partes: (a) Espaço-RVSHs propostos pelo SDs com sincronia híbrida em espaço, e (b) Tempo-RVSHs propostos pelo SDs com sincronia híbrida em tempo. No SDs com síncronia híbrida em espaço, a infra-estrutura é composta de subconjuntos de componentes síncronos e assíncronos, e cada um desses subconjuntos mantém seu status de sincronia através do tempo (i.e., os subconjuntos síncronos permanecem síncronos e os assíncronos permanecem assíncronos). No SDs com síncronia híbrida em tempo, a infra-estrutura é composta de subconjuntos de nòs e laços que podem alternar seu status de sincronia atravès do tempo (i.e., os componentes se comportam de forma síncrona durante os intervalos de tempo e de forma assíncrona durante outros intervalos de tempo). 

As principais contribuiçoẽs desta tese são: (a) caracterizam os RVSHs em seus dois tipos Espaço-RVSHs e Tempo-RVSHs para refletir tanto a natureza de sincronia em espaço e em tempo; (b) propor uma estrutura adequada para o processo de alocação de recursos para ambos Espaço-RVSHs e Tempo-RVSHs, e (c) fornecer uma avaliação dos modelos propostos para RVSHs.",TESE,Hybrid Synchrony Virtual Networks,5008999,1
"Reinforcement learning algorithms are often used to compute agents capable of acting
in environments without any prior knowledge. However, these algorithms struggle to converge in
environments with large branching factors and their large resulting state-spaces. In this dissertation,
we develop an approach to compress the number of entries in a Q-value table using a deep autoencoder.
We develop a set of techniques to mitigate the large branching factor problem. We present
the application of such techniques in the scenario of a Real-Time Strategy (RTS) game, where both
state space and branching factor are a problem. We empirically evaluate an implementation of the
technique to control agents in an RTS game scenario where classical reinforcement learning fails
and point towards future work.",[DISSERTAÇÃO] LEONARDO ROSA AMADO.pdf,CIÊNCIA DA COMPUTAÇÃO,LEONARDO ROSA AMADO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,13/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Reinforcement Learning;State space;Auto-encoders',INTELIGÊNCIA COMPUTACIONAL,FELIPE RECH MENEGUZZI,65,Aprendizado por reforço;Espaço de estados;Auto-encoders,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),Mobile Data Analytics v2,"Algoritmos de aprendizado por reforço são usados para computar agentes capazes de agir
em ambientes onde não se tem conhecimento prévio. Porém, esses algoritmos não são capazes de
convergir em ambientes onde o espaço de estados é muito grande com possível grande fator de
ramificação. Nessa dissertação, desenvolvemos técnicas para comprimir o espaço de estados usando
um auto-encoder. Para lidar com grande fator de ramificação, apresentamos técnicas capazes de
reduzir este fator. Como aplicação, usamos um jogo de estratégia em tempo real, onde se tem
um grande espaço de estados e um grande fator de ramificação. Nós mostramos que algoritmos
tradicionais de aprendizado por reforço não são capazes de competir nesse tipo de ambiente. Nós
detalhamos como implementar as técnicas desenvolvidas e discutimos trabalhos futuros.",DISSERTAÇÃO,Q-TABLE COMPRESSION FOR REINFORCEMENT LEARNING,5010086,1
"The evolution of integrated circuit manufacturing process allowed the SoC (System-on-
Chip) design in the 90’s, and currently the design of multiprocessors systems on chip – MPSoCs
(Multiprocessor System-on-Chip). Embedded systems use these devices, due to the offered
computational power. The MPSoC design is a challenging task. Specify the MPSoC characteristics,
define the components that compose the system and analyze their features are decisions that may
change over the product development. Traditional design methods do not favor the design space
exploration, leading to expensive products due to required hardware simulation at the gate level,
which is only available at the end of the design flow. To solve the design problems of traditional
methods, Platform Based Design (PBD) techniques is a design choice. The basis of PBD is a virtual
platform model, enabling fast simulations, software debugging and reusability of hardware
components. This Thesis comprises the study and development in two research axes: (1) modeling
of virtual platforms; (2) analytical methods for software heuristics targeting embedded real-time
applications. Virtual platforms are modeled by using ADLs (Architecture Description Languages).
This work presents the modeling of several virtual platforms, using different abstraction levels
(from RTL to untimed models) and memory architectures (shared and distributed). Based on the
evaluations performed in each architecture, the HeMPS platform was adapted to execute real-time
applications. The results showed that using the proposed scheduling mechanism and RTA mapping,
the results meet the constraints defined by the applications. Comparing platforms with mapping
and schedule heuristics on literature, the proposed platform met 100% of the restrictions resulting
from the test cases.",HOMOLOGAÇÃO - VOLUME.pdf,CIÊNCIA DA COMPUTAÇÃO,GUILHERME AFONSO MADALOZZO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,12/01/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'many-core systems;many-core systems modeling;real-time applications;scheduling;mapping',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,FERNANDO GEHM MORAES,111,sistemas many-core;modelagem de sistemas many-core;aplicações tempo-real;escalonamento;mapeamento,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"A evolução no processo de fabricação de circuitos integrados permitiu o projeto de SoCs na
década de 1990, e atualmente o projeto de sistemas multiprocessados em um único chip - MPSoCs
(Multiprocessor System-on-Chip). Estes dispositivos são amplamente utilizados em sistemas
embarcados, dado o poder computacional oferecido pelos mesmos. Aplicações com restrições de
tempo-real vêm sendo utilizadas constantemente, sendo um desafio para o projeto de SoCs. O
projeto de MPSoCs é altamente complexo. Especificar as características do MPSoC, definir os
componentes que compõe o sistema e analisar suas funcionalidades são decisões que podem
apresentar alterações ao longo do desenvolvimento do produto. Métodos tradicionais de projeto
não favorecem as tomadas de decisões e encarecem o produto, pois requerem simulação em nível
de hardware, estando disponível apenas no final do fluxo de projeto. Para solucionar os problemas
apresentados pelos métodos tradicionais de projeto, adotou-se a técnica de projeto baseado em
plataforma (PBD – Platform Based Design). O método de projeto PBD adota a modelagem de
plataformas virtuais em nível de sistema possibilitando rápidas simulações, depuração de software
e reuso de componentes de hardware. Esta Tese tem por objetivo realizar estudos e
desenvolvimentos em 2 eixos de pesquisa: (1) modelagem de plataformas virtuais com diferentes
organizações de memória; (2) estudo de métodos analíticos para mecanismos de software em
sistemas com restrições de tempo-real. Para a modelagem de plataformas virtuais usa-se as ADLs
(Architecture Description Language) OVP e ArchC. Neste tema de trabalho, diversas plataformas
foram modeladas em diferentes níveis de abstração (de RTL a modelos sem temporização) e com
diferentes arquiteturas de memória (compartilhada e distribuída). Com base nas avaliações
realizadas em cada arquitetura, adequou-se a plataforma HeMPS para executar aplicações com
restrições de tempo-real. Os resultados apresentaram que, com a utilização do mecanismo de
escalonamento e do mapeamento RTA propostos, os dados resultantes das aplicações com
restrições de tempo-real aconteceram dentro do período de tempo definido pela aplicação.
Comparando plataformas com heurísticas de mapeamento e escalonamento presentes na
literatura, a plataforma desenvolvida na presente Tese atende as restrições de aplicações Hard-RT,
garantindo 100% das restrições resultantes dos casos de testes.",TESE,Adequação de Modelos Arquiteturais para Aplicações Tempo Real em  Sistemas Many-Core,5010157,1
"Among the available EUD environments, there are those whose end-user development
activity occurs in steps, tiers or levels, depending on the specific goals of the one that
adapts it. Thus, with respect to VLEs, it is common to have at least two levels of end-user
development: the one corresponding to the activities of the technical team that adapts it to
the institutional context, and that corresponding to the teacher’s activities when designing
and creating an interface in the VLE for their class. After successive adaptations by several
actors, it is understood that the interface, as a communicative artifact in all its explicit and
implicit messages, may lose its unity as a metacommunication message, impacting on the
student’s perception of their teacher’s self-expression. In this way, the purpose of this work
was to study: the forms of self-expression practiced by teachers when customizing their interfaces
in VLEs, their perception by the students and the impact of the customizations made
by others in the teacher’s expression. To do so, we developed a qualitative and descriptive
research, comprising four studies and based on questionnaires and interviews with teachers,
students and technicians, reaching our results through content analysis. We observed
that the teachers who participated in the studies practice self-expression on the interface
through verbal expression, customization and content elaboration. Voluntary students in
this research, in its majority, perceive the teacher’s self-expression, which is not impaired
by the EUD activities in other layers. As contributions we provide a set of epistemic tools
that we have been developing in the course of research and applying in our studies. These
tools allow us to increase knowledge about the EUD activity in VLEs, focusing on aspects of
teacher’s self-expression, thus allowing us to expand our knowledge about teacher-student
relationships in VLEs. We also offer as contribution our more specific findings, in the case
someone prefers to apply them instead of making use of the epistemic tools provided by us.",Tese_LucianaEspindola.pdf,CIÊNCIA DA COMPUTAÇÃO,LUCIANA DA SILVEIRA ESPINDOLA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,18/01/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Human-Computer Interaction;End-User Development;Virtual Learning Environments;Semiotic Engineering;Self-Expression',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",MILENE SELBACH SILVEIRA,205,Interação Humano-Computador;Desenvolvimento por Usuário Final;Ambientes Virtuais de Aprendizagem;Engenharia Semiótica;Autoexpressão,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Dentre os ambientes de EUD disponíveis, há aqueles cuja atividade de desenvolvimento
por usuário final ocorre em etapas, camadas ou níveis, dependendo dos objetivos
específicos daquele que o adapta. Assim, com relação a AVAs, é comum existir ao menos
dois níveis de desenvolvimento por usuário final: aquele correspondente às atividades da
equipe técnica que o adapta para o contexto institucional e aquele correspondente às atividades
do professor ao projetar e criar uma interface no AVA para a sua disciplina. Após
sucessivas adaptações por diversos atores, entende-se que a interface, como artefato comunicativo
no conjunto de suas mensagens explícitas e implícitas, talvez perca a unidade
como mensagem de metacomunicação, impactando na percepção do aluno sobre a autoexpressão
de seu professor. Desta forma, este trabalho teve como propósito estudar: as
formas de autoexpressão praticadas por professores ao customizarem suas interfaces em
AVAS, sua percepção pelos alunos e o impacto das customizações feitas por outros na
expressão do professor. Para tanto, desenvolvemos uma pesquisa qualitativa e descritiva,
compreendendo quatro estudos e baseada em questionários e entrevistas com professores,
alunos e técnicos, chegando aos nossos resultados por meio de análise de conteúdo.
Observamos que os professores que participaram dos estudos praticam autoexpressão na
interface por meio da expressão verbal, customização e elaboração de conteúdo. Os alunos
voluntários nesta pesquisa, em sua maioria, percebem a autoexpressão do professor,
que não é prejudicada pelas atividades de EUD em outras camadas. Como contribuições
fornecemos um conjunto de ferramentas epistêmicas que fomos desenvolvendo no decorrer
da pesquisa e aplicando em nossos estudos. Essas ferramentas permitem ampliar o
conhecimento sobre a atividade de EUD em AVAs, dando foco aos aspectos de autoexpressão
do professor, permitindo assim expandir nosso conhecimento a respeito das relações
professor-aluno em AVAs. Também oferecemos como contribuição nossos achados
mais específicos, para o caso de alguém que prefira aplicá-los ao invés de fazer uso das
ferramentas epistêmicas por nós fornecidas.",TESE,Autoexpressão e sua Percepção em Ambientes de EUD Multinível: Aplicação em Ambientes Virtuais de Aprendizagem,5010168,1
"In recent years, researchers of the High Performance Computing (HPC) community have
proposed solutions for the problem of high energy consumption in large scale distributed platforms,
as the energy consumption increases proportionally to the computational power. Besides the cost of
electric bills, the main problem of the high energy consumption of HPC platforms are the emissions
of greenhouse gases. For this purpose, several energy-aware heuristics have been developed for
scheduling applications in clusters, grids and clouds. However, performance is still required for
such an application. The trade-off between energy consumption and execution time is not easy to
achieve. Furthermore, resources heterogeneity is the key to improve the efficiency of energy efficient
schedulers. To solve the problem of energy consumption in computational grids, we developed a
new power-aware scheduling heuristic that tries to find a solutions with better trade-off between
energy consumption and execution time of Bag-of-Tasks (BoT) applications in computational grids.
Additionally, we evaluate and discuss the impact of resources heterogeneity over energy efficient
schedulers. This evaluation helps to understand how Resource Management Systems (RMS) could
contribute with energy consumption reduction by being aware of local schedulers and thus provide
better resources.",[TESE] ANDRIELE BUSATTO DO CARMO.pdf,CIÊNCIA DA COMPUTAÇÃO,ANDRIELE BUSATTO DO CARMO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,18/01/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Energy-efficient Scheduling;Computational Grids;High Performance Computing',PROCESSAMENTO PARALELO E DISTRIBUÍDO,LUIZ GUSTAVO LEAO FERNANDES,94,Escalonamento Energeticamente Eficiente;Grades Computacionais;Computação de Alto Desempenho;Heterogeneidade de Recursos,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Nos últimos anos, pesquisadores da comunidade de Computação de Alto Desempenho têm
proposto soluções para o problema do alto consumo de energia em plataformas distribuídas de larga
escala, visto que o aumento da energia consumida é proporcional ao poder computacional. Além do
custo de energia elétrica, o principal problema do elevado consumo de energia de plataformas de alto
desempenho é a emissão de gases de efeito estufa. Para isso, diversas heurísticas de escalonamento
conscientes de energia têm sido desenvolvidas para o escalonamento de aplicações em aglomerados
de computadores, grades e nuvens. No entanto, desempenho é ainda requisito para esse tipo de
aplicação. O equilíbrio entre consumo de energia e tempo de execução não é fácil de atingir. Além
disso, a heterogeneidade dos recursos é a chave para melhorar a eficiência de escalonadores energeticamente
eficientes. Para resolver o problema do consumo de energia em grades computacionais,
este trabalho propõe uma nova política de escalonamento consciente de energia que tenta equilibrar
da melhor maneira o consumo de energia e o tempo de execução de aplicações do tipo saco de
tarefas em grades computacionais. Além disso, o presente trabalho avalia e discute o impacto da
heterogeneidade dos recursos sobre escalonadores energeticamente eficientes. Essa avaliação permitiu
o entendimento de como o Sistema de Gerenciamento de Recursos pode contribuir com o
algoritmo se ele está consciente da proposta dos escalonadores.",TESE,Exploiting Resource Heterogeneity on Computational Grids for Energy-Efficient Scheduling,5010170,1
"Extracting meaningful information from data is not an easy task. Data can come in batches or through
a continuous stream, and can be incomplete or complete, duplicated, or noisy. Moreover, there are
several algorithms to perform data mining tasks, and the no-free lunch theorem states that there
is not a single best algorithm for all problems. As a final obstacle, algorithms usually require hyperparameters
to be set in order to operate, which not surprisingly often demand a minimum knowledge
of the application domain to be fine-tuned. Since many traditional data mining algorithms employ a
greedy local search strategy, fine-tuning is a crucial step towards achieving better predictive models.
On the other hand, Estimation of Distribution Algorithms perform a global search, which often is
more efficient than performing a wide search through the set of possible parameters. By using a
quality function, estimation of distribution algorithms will iteratively seek better solutions throughout
its evolutionary process. Based on the benefits that estimation of distribution algorithms may offer
to clustering and decision tree-induction, two data mining tasks considered to be NP-hard and NPhard/
complete, respectively, this works aims at developing novel algorithms in order to obtain better
results than traditional, greedy algorithms and baseline evolutionary approaches.",dissertação_final_ (HOMOLOGAÇÃO).pdf,CIÊNCIA DA COMPUTAÇÃO,HENRY EMANUEL LEAL CAGNINI,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,20/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'estimation of distribution algorithm;decision-tree induction;clustering;optimization',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,RODRIGO COELHO BARROS,79,algoritmos de estimativa de distribuição;indução de árvores de decisão;agrupamento;otimização,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),Estimation of Distribution Algorithms for Decision-Tree Induction,"Extrair informações relevantes a partir de dados não é uma tarefa fácil. Tais dados podem vir a partir
de lotes ou em fluxos contínuos, podem ser completos ou possuir partes faltantes, podem ser duplicados,
e também podem ser ruidosos. Ademais, existem diversos algoritmos que realizam tarefas de
mineração de dados e, segundo o teorema do ""Almoço Grátis"", não existe apenas um algoritmo que
venha a solucionar satisfatoriamente todos os possíveis problemas. Como um obstáculo final, algoritmos
geralmente necessitam que hiper-parâmetros sejam definidos, o que não surpreendentemente
demanda um mínimo de conhecimento sobre o domínio da aplicação para que tais parâmetros sejam
corretamente definidos. Já que vários algoritmos tradicionais empregam estratégias de busca local
gulosas, realizar um ajuste fino sobre estes hiper-parâmetros se torna uma etapa crucial a fim de obter
modelos preditivos de qualidade superior. Por outro lado, Algoritmos de Estimativa de Distribuição
realizam uma busca global, geralmente mais eficiente que realizar uma buscam exaustiva sobre todas
as possíveis soluções para um determinado problema. Valendo-se de uma função de aptidão, algoritmos
de estimativa de distribuição irão iterativamente procurar por melhores soluções durante seu
processo evolutivo. Baseado nos benefícios que o emprego de algoritmos de estimativa de distribuição
podem oferecer para as tarefas de agrupamento e indução de árvores de decisão, duas tarefas
de mineração de dados consideradas NP-difícil e NP-difícil/completo respectivamente, este trabalho
visa desenvolver novos algoritmos de estimativa de distribuição a fim de obter melhores resultados
em relação a métodos tradicionais que empregam estratégias de busca local gulosas, e também sobre
outros algoritmos evolutivos.",DISSERTAÇÃO,Estimation Of Distribution Algorithms For Clustering And Classification,5010171,1
"The advent of the Internet of Things (IoT) and the popularization of mobile devices with non-volatile memory brings new challenges regarding the removal of files. Techniques traditionally employed in magnetic media are not effective when applied to non-volatile memories, such as flash memory. Because of the peculiar characteristics of this type of memory, notably the existence of a Flash Translation Layer (FTL), operating systems only manage logical blocks, and no longer have direct control of the physical blocks of a flash memory. Consequently, new methods of safe removal have been developed, which employ Zero Override, Block Erase, and Cryptographic Erase techniques.
This work analyzes these methods, compares their operations and proposes a new method, with better performance than those described in the literature. The proposed method is a hybrid method, which combines overwriting and deletion operations to obtain a balanced use of these operations, avoid unnecessary deletion of unused blocks and reduce premature memory wear. To verify the efficiency of the proposed method and of the other methods, a simulator was developed to exercise the removal of files in several experiments.",[DISSERTAÇÃO] JULIA SILVA WEBER.pdf,CIÊNCIA DA COMPUTAÇÃO,JULIA SILVA WEBER,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,21/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Flash Memory;Secure File Removal;Overwriting and Erasing',PROCESSAMENTO PARALELO E DISTRIBUÍDO,AVELINO FRANCISCO ZORZO,108,Memória flash;Remoção segura de arquivos;Sobrescrita e Apagamento,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"O advento da Internet das Coisas (IoT) e a popularização de dispositivos móveis com memória não-volátil traz novos desafios quanto a remoção de arquivos. Técnicas tradicionalmente empregadas em meios magnéticos não são efetivas quando aplicadas para memórias não voláteis, como a memória flash. Devido às características peculiares este tipo de memória, notadamente a existência de uma Camada de Tradução da Flash (FTL), sistemas operacionais somente gerenciam blocos lógicos, e não tem mais controle direto dos blocos físicos de uma memória flash. Consequentemente, novos métodos de remoção segura foram desenvolvidos, que empregam operações de Sobrescrita com zeros, de Apagamento de blocos e técnicas de Apagamento Criptográfico.
Este trabalho analisa estes métodos, compara suas operações e propõe um novo método, com melhor desempenho que os descritos na literatura. O método proposto é um método híbrido, que combina operações de sobrescrita e apagamento de forma a obter um uso equilibrado destas operações, evitar o apagamento desnecessário de blocos ainda não utilizados e reduzir o desgaste prematuro da memória. Para verificar a eficiência do método proposto e dos demais métodos, foi desenvolvido um simulador para exercitar a remoção de arquivos em diversos experimentos.",DISSERTAÇÃO,Eliminação Segura de Arquivos em Memória Não-Volátil,5010174,1
"The growth in performance when comparing processors with memory devices is alarming.
Processors’ performance have grown up to 60% per year against only 10% of memories up to 2010.
This growth variation has reduced lately, but there is still a enormous performance gap. For years,
researchers have focused their study on speed in case of processors, and mostly capacity in case
of memories. Due to this fact, implementing memory controllers with advanced schedulers is an
interesting approach to maximize the memory performance and try to balance with the needs of processors.
Therefore, this work proposes a memory controlling technique composed of two hierarchical
levels: application and memory. The Application-level Controller is responsible for supporting the
application by scheduling memory requests according to task priorities. The Memory-level Controller
is responsible for reorganizing incoming requests according to the FR-FCFS algorithm and bank interleaving
policy. The implementation of two controlling levels aims to reduce memory bottleneck
and increase access efficiency with lower latencies and higher bandwidths.",[DISSERTAÇÃO] MARCELO MELO LINCK.pdf,CIÊNCIA DA COMPUTAÇÃO,MARCELO MELO LINCK,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,22/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'memory;DRAM;memory controller;microprocessor;DDR',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,CESAR AUGUSTO MISSIO MARCON,80,memory;DRAM;memory controller;microprocessor;DDR,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"The growth in performance when comparing processors with memory devices is alarming.
Processors’ performance have grown up to 60% per year against only 10% of memories up to 2010.
This growth variation has reduced lately, but there is still a enormous performance gap. For years,
researchers have focused their study on speed in case of processors, and mostly capacity in case
of memories. Due to this fact, implementing memory controllers with advanced schedulers is an
interesting approach to maximize the memory performance and try to balance with the needs of processors.
Therefore, this work proposes a memory controlling technique composed of two hierarchical
levels: application and memory. The Application-level Controller is responsible for supporting the
application by scheduling memory requests according to task priorities. The Memory-level Controller
is responsible for reorganizing incoming requests according to the FR-FCFS algorithm and bank interleaving
policy. The implementation of two controlling levels aims to reduce memory bottleneck
and increase access efficiency with lower latencies and higher bandwidths.",DISSERTAÇÃO,INCREASING MEMORY ACCESS EFFICIENY THROUGH A TWO-LEVEL MEMORY CONTROLLER,5010339,1
"Brain-Machine Interface (ICM) or Brain-Computer Interface (BCI) is a computer system capable of establishing communication between human neurophysiological activity and a computer. A hybrid BCI (hBCI) consists of a combination of two or more types of BCI, two or more signal acquisition techniques, or a combination of a BCI with other non-BCI based interaction techniques. A collaborative ICC (cBCI) integrates the brain activity of a group of individuals, with the main purposes of improving the classification of signs or increasing human capacity. Currently, low-cost equipment is available in the market, which facilitates access to BCIs. One of these devices is the Emotiv EEG, a portable electroencephalogram with 14 electrodes, which in addition to registering the neurophysiological signals, processes and makes them available as affective measures or neural measurements. Yet, the speed with which a subject responds to a challenge may suggest how right he is about this decision-making, thus becoming a behavioral measure. This work has as the main objective ""To verify if neural and behavioral measures have relation with the correct and incorrect decision-making, in order to be able to estimate how correct or incorrect the given answer"". As a tool for signal acquisition, the Emotiv EEG was used. In addition, this work developed an RSVP-based decision-making task, in addition to storing in a structured and synchronized way the data collected by the emotiv EEG and the proposed task. An experiment was carried out with 10 participants, in which each participant performed 112 trials. For data analysis, statistical techniques were applied such as descriptive analysis, including data summarization and boxplots charts, and multivariate analysis using logistic regression to estimate the relationship between neural and behavioral measures with the decisions made. The proposed task proved to be efficient, revealing in the results that the difficulty was effective. The developed database proved to be efficient in synchronizing the task data and the recorded measurements. After several approaches, we did not find a regression model that could explain with high explanatory power the data sampled. The study did not allow us to confirm whether there are relations between neural and behavioral measures and decision-making, whether correct or incorrect.",[DISSERTAÇÃO] ÂNDERSON RODRIGO SCHUH.pdf,CIÊNCIA DA COMPUTAÇÃO,ANDERSON RODRIGO SCHUH,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,30/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Colaborative Brain-Computer Interface;Decision-making;Rapid Serial Visual Presentarion;EEG',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",MARCIA BORBA CAMPOS,137,Interface Cérebro-Computador Colaborativa;Tomada de Decisão;Rapid Serial Visual Presentarion;EEG,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Interface Cérebro-Máquina (ICM) ou Interface Cérebro-Computador (ICC) é um sistema computacional capaz de estabelecer a comunicação entre a atividade neurofisiológica humana e um computador. Uma ICC híbrida (ICCh) consiste na combinação de dois ou mais tipos de ICC, duas ou mais técnicas de aquisição de sinal, ou, ainda, da combinação de uma ICC com outras técnicas de interação não baseadas em ICC. Já uma ICC Colaborativa (ICCc), integra a atividade cerebral de um grupo de indivíduos, com as principais finalidades de melhorar a classificação dos sinais ou aumentar a capacidade humana. Atualmente, no mercado, estão disponíveis equipamentos de baixo custo, que facilitam o acesso as ICCs. Um destes equipamentos é o Emotiv EEG, um Eletroencefalograma portátil, com 14 eletrodos, que, além e registrar os sinais neurofisiológicos, os processa e disponibiliza em forma de medidas afetivas, ou ainda, medidas neurais. Ainda, a velocidade com que um sujeito responde a um desafio, pode sugerir o quão certo ele está sobre esta tomada de decisão, tornando-se assim, uma medida comportamental. Este trabalho tem como principal objetivo “Verificar se medidas neurais e comportamentais possuem relação com as tomadas de decisão corretas e incorretas, afim de poder estimar o quão correta ou incorreta é a resposta dada”. Como ferramenta de aquisição de sinal foi utilizado o Emotiv EEG. Além disso, este trabalho desenvolveu uma tarefa de tomada de decisão baseada em RSVP, além de armazenar de forma estruturada e sincronizada os dados coletados pelo emotiv EEG e a tarefa proposta. Foi executado um experimento com 10 participantes, no qual cada participante executou 112 ensaios. Para a análise dos dados, foram aplicadas técnicas de estatística foram empregadas tais como, análise descritiva, incluindo sumarização dos dados e gráficos de boxplots, e análise multivariada, utilizando regressão logística para estimar a relação entre medidas neurais e comportamentais com as decisões tomadas. A tarefa proposta mostrou-se eficiente, revelando nos resultados que a dificuldade empregada se mostrou efetiva. O banco de dados desenvolvido mostrou-se eficiente na sincronização dos dados da tarefa e as medidas registradas. Após diversas abordagens, não foi encontrado um modelo de regressão que pudesse explicar com alto poder explicativo os dados amostrados. O estudo realizado não permite confirmar se há relações entre medidas neurais e comportamentais e as tomadas de decisão, sejam elas corretas ou incorretas.",DISSERTAÇÃO,Interface Cérebro-Computador Híbrida e Colaborativa no Processo de Tomada de Decisão,5010363,1
"Structure-based virtual screening (SBVS) on compounds databases has been widely applied
in early drug discovery when 3D structure of drug target is known. In SBVS, computational
approaches usually ’dock’ small molecules into binding site of drug target and ’score’ their binding
affinity. However, the costs involved on applying docking algorithms into huge compounds databases
is prohibitive, due to the computational resources required by this operation. In this context, different
machine learning strategies can be applied to rank ligands, based on binding affinity, and to
reduce the number of compounds to be tested. In this work, we propose a deep learning energybased
model using siamese neural networks to rank ligands. This model takes as inputs grids of
biochemical properties of ligands and receptors and calculates their compatibilidade. We show that
the model can learn to identify important biochemical interactions between ligands and receptors.
Besides, we demonstrate that the compatibility score is computed based only on conformation of
small molecule independent of its position and orientation in relation to the receptor. The model
were trained using known ligands and decoys in a Fully Flexible Receptor model of InhA-NADH
complex (PDB ID: 1ENY), achieving outstanding results.",[DISSERTAÇÃO] ALAN DIEGO DOS SANTOS.pdf,CIÊNCIA DA COMPUTAÇÃO,ALAN DIEGO DOS SANTOS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,29/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Virtual Screening;Siamese Neural;Network;Scoring Function;Molecular Docking',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,DUNCAN DUBUGRAS ALCOBA RUIZ,59,Triagem Virtual;Redes Neurais Siameses;Funções de Escore;Docagem Molecular,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Triagem virtual de bancos de dados de ligantes é amplamente utilizada nos estágios iniciais
do processo de descoberta de fármacos. Abordagens computacionais ’docam’ uma pequena
molécula dentro do sítio ativo de um estrutura biológica alvo e avaliam a afinidade das interações
entre a molécula e a estrutura. Todavia, os custos envolvidos ao aplicar algoritmos de docagem
molecular em grandes bancos de ligantes é proibitivo, dado a quantidade de recursos computacionais
necessários para essa execução. Nesse contexto, estratégias de aprendizagem de máquina podem ser
aplicadas para ranquear ligands baseadas na afinidade com determinada estrutura biológica e, dessa
forma, reduzir o número de compostos químicos a serem testados. Nesse trabalho, propomos um
modelo para ranquear ligantes based na arquitetura de redes neurais siamesas. Esse model calcula
a compatibilidade entre receptor e ligante usando grades de propriedades bioquímicas. Nós também
mostramos que esse modelo pode aprender a identificar interações molecular importantes entre
ligante e receptor. Além disso, a compatibilidade é calculada baseada em relação a conformação
do ligante, independente de sua posição e orientação em relação ao receptor. O modelo proposto
foi treinado usando ligantes ativos previamente conhecidos e moléculas chamarizes (decoys) em um
modelo de receptor totalmente flexível (Fully Flexible Receptor - FFR) do complexo InhA-NADH da
Mycobacterium tuberculosis, encontrando ótimos resultados.",DISSERTAÇÃO,RANKING LIGANDS IN STRUCTURE-BASED VIRTUAL SCREENING USING SIAMESE NEURAL NETWORKS,5010405,1
"Internet of Things (IoT) is considered a computational evolution that advocates the existence
of a large number of physical objects embedded with sensors and actuators, connected by
wireless networks and communicating through the Internet. From the beginning of the concept to
the present day, IoT is widely used in the various sectors of industry and also in academia. One of
the needs encountered in these areas was to be connected to IoT devices or subsystems throughout
the world.
Thus, cloud computing gains space in these scenarios where there is a need to be connected
and communicating with a middleware to perform the data processing of the devices. The
concept of cloud computing refers to the use of memory, storage and processing of shared resources,
interconnected by the Internet. However, IoT applications sensitive to communication latency, such
as medical emergency applications, military applications, critical security applications, among others,
are not feasible with the use of cloud computing, since for the execution of all calculations and
actions messaging between devices and the cloud is required.
Solving this limitation found in the use of cloud computing, the concept of fog computing
arises and whose main idea is to create a federated processing layer, still in the local network of
the computing devices of the ends of the network. In addition to fog computing, there is also edge
computing operating directly on the devices layer, performing some kind of processing, even with
little computational complexity, in order to further decrease the volume of communication, besides
collaborating to provide autonomy in decision making yet in the Things layer. A major challenge for
both fog and edge computing within the IoT scenario is the definition of a system architecture that
can be used in different application domains, such as health, smart cities and others.
This work presents a system architecture for IoT devices capable of enabling data processing
in the devices themselves or the closest to them, creating the edge computing layer and fog computing
layer that can be applied in different domains, improving Quality of Services (QoS) and autonomy
in decision making, even if the devices are temporarily disconnected from the network (offline). The validation of this architecture was done within two application scenarios, one of public lighting in
smart city environment and another simulating an intelligent agricultural greenhouse. The main
objectives of the tests were to verify if the use of the concepts of edge and fog computing improve
system efficiency compared to traditional IoT architectures. The tests revealed satisfactory results,
improving connection times, processing and delivery of information to applications, reducing the
volume of communication between devices and core middleware, and improving communications
security. It also presents a review of related work in both academia and industry.",[DISSERTAÇÃO] MATHEUS CRESPI SCHENFELD (nova versão).pdf,CIÊNCIA DA COMPUTAÇÃO,MATHEUS CRESPI SCHENFELD,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,23/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Internet of Things;Fog Computing;Edge Computing;Middleware;Cloud Computing',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,FABIANO PASSUELO HESSEL,91,Internet of Things;Fog Computing;Edge Computing;Middleware;Cloud Computing,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Internet das Coisas (IoT) é considerada uma evolução computacional que preconiza a
existência de uma grande quantidade de objetos físicos embarcados com sensores e atuadores,
conectados por redes sem fio e que se comunicam através da Internet. Desde o surgimento do
conceito até os dias atuais, a IoT é amplamente utilizada nos diversos setores da indústria e também
no meio acadêmico. Uma das necessidades encontradas nessas áreas foi a de estar conectado com
dispositivos ou subsistemas de IoT espalhados por todo o mundo.
Assim, cloud computing ganha espaço nesses cenários onde existe a necessidade de estar
conectado e se comunicando com um middleware para realizar o processamento dos dados dos
dispositivos. O conceito de cloud computing refere-se ao uso de memória, armazenamento e processamento
de recursos compartilhados, interligados pela Internet. No entanto, aplicações IoT sensíveis
à latência de comunicação, tais como, aplicações médico-emergenciais, aplicações militares, aplicações
de segurança crítica, entre outras, são inviáveis com o uso de cloud computing, visto que
para a execução de todos os cálculos e ações é necessária a troca de mensagens entre dispositivos
e nuvem.
Solucionando essa limitação encontrada na utilização de cloud computing, surge o conceito
de fog computing, cuja ideia principal é criar uma camada federada de processamento ainda na rede
local dos dispositivos de computação das extremidades da rede. Além de fog computing também
surge edge computing operando diretamente na camada dos dispositivos, realizando algum tipo de
processamento, mesmo que de pouca complexidade computacional, a fim de diminuir ainda mais o
volume de comunicação, além de colaborar para prover autonomia na tomada de decisões ainda na
camada das coisas. Um grande desafio tanto para fog quanto para edge computing dentro do cenário
de IoT é a definição de uma arquitetura de sistema que possa ser usada em diferentes domínios de
aplicação, como saúde, cidades inteligentes entre outros.
Esse trabalho apresenta uma arquitetura de sistema para dispositivos IoT capaz de habilitar
o processamento de dados nos próprios dispositivos ou o mais próximo deles, criando a camada de edge e fog computing que podem ser aplicadas em diferentes domínios, melhorando a Qualidade
dos Serviços (QoS) e autonomia na tomada de decisão, mesmo se os dispositivos estiverem
temporariamente desconectados da rede (offline). A validação dessa arquitetura foi feita dentro de
dois cenários de aplicação, um de iluminação pública em ambiente de IoT e outro simulando uma
estufa agrícola inteligente. Os principais objetivos das execuções dos testes foram verificar se a
utilização dos conceitos de edge e fog computing melhoram a eficiência do sistema em comparação
com arquiteturas tradicionais de IoT. Os testes revelaram resultados satisfatórios, melhorando os
tempos de conexão, processamento e entrega das informações às aplicações, redução do volume de
comunicação entre dispositivos e core middleware, além de melhorar a segurança nas comunicações.
Também é apresentada uma revisão de trabalhos relacionados tanto no meio acadêmico como no
da indústria.",DISSERTAÇÃO,FOG E EDGE COMPUTING: UMA ARQUITETURA HÍBRIDA EM UM AMBIENTE DE INTERNET DAS COISAS,5010429,1
"Even though agile actively seeks collaboration with all its stakeholders, most agile
projects did not extend themselves toward the operations people. Communication problems
are a recurring problem in agile teams which is also eminent in the relationship between
developers and operations. DevOps came to try to solve these problems. This study aims to
understand how communication happens in DevOps from the perceptions of practitioners.
The same went through a Literature Review on DevOps and Communication and a Field
Study with qualitative data being collected through interviews. The results indicate that
today there are at least three different DevOps configurations being applied in the industry,
being: dev and ops professionals allotted to the same team; A team of devops with a shared
skill set; And a separate team of dev and ops working together. Despite this, professionals
still reported similar results, such as: co-allocated and multi-functional team communicates
better; It is important to work together and share technical knowledge; The power of decision
will vary according to the situation; among others. DevOps is all about collaboration to better
serve the customer, with this in mind, this paper aims to help narrow the gap pointed out by
Erich, Amrit and Daneva between dev and ops, as well as helping practitioners to improve
their communication practices in their daily work.",[Dissertation]- Elisa Costa Diel.pdf,CIÊNCIA DA COMPUTAÇÃO,ELISA COSTA DIEL,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,16/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'DevOps;communication;communication challenges;communication strategy',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,SABRINA DOS SANTOS MARCZAK,76,DevOps;comunicação;desafios de comunicação;estratégias de comunicação,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Apesar de o Ágil buscar colaboração com todos as partes envolvidas, a maioria
dos projetos ágeis não extende essa colaboração para o pessoal de operações. Problemas
de comunicação são um problema recorrente em equipes ágeis que também é eminente
na relação entre desenvolvedores e operações. DevOps veio para tentar resolver esses
problemas. Este estudo tem como objetivo compreender como a comunicação acontece
em DevOps a partir das percepções dos praticantes. O mesmo passou por uma a Revisão
de Literatura sobre DevOps e Comunicação e um Estudo de Campo com dados qualitativos
sendo coletados através de entrevistas. Os resultados indicam que hoje existem pelo
menos três configurações diferentes de DevOps sendo aplicadas na indústria, sendo elas:
profissionais devs e ops alocados na mesma equipe; Uma equipe de devops com um conjunto
de habilidades compartilhadas; E uma equipe separada de dev e ops que trabalha
em conjunto. Apesar disso, os profissionais relataram resultados semelhantes, tais como:
equipe co-alocada e multi-funcional se comunica melhor; É importante trabalhar em conjunto
e compartilhar conhecimentos técnicos; O poder de decisão variará de acordo com a
situação; entre outros. DevOps é tudo sobre a colaboração para melhor servir o cliente, com
isso em mente, este trabalho visa contribuir a diminuir a lacuna apontada por Erich, Amrit e
Daneva entre dev e ops, bem como ajudar os profissionais a melhorar as suas práticas de
comunicação no trabalho.",DISSERTAÇÃO,COMMUNICATION IN DEVOPS,5010452,1
"The growth of data generated through the Internet increases the difficulty of gather
relevant information for users. To minimize this, it is necessary to presented this information
to the users in such a way that they can analyze it and extract useful information. In recent
years, the use of narratives for the presentation of large volumes of data has been explored,
considering its benefits. In this context, the present work aims to analyze and propose ways
of presenting data, in narrative form, in the context of social networks, in order to support
user interaction and data analysis. The work presents a model for the representation of
narratives through visualizations, with data extracted from social networks, conceived from
studies related to the areas of narratives and information visualization. The model was
instantiated to enable its analysis through user studies. The studies indicated that the model
has several applications, allowing the analysis of different types of data as well as its use
by users with and without prior specific interests. We believe that the present work can
support the construction of interactive narratives, through visualizations with data extracted
from social networks, contemplating, not only the construction of the interface itself but, also,
the mapping of the data to its visual representation.",[DISSERTAÇÃO] EDUARDO GHIDINI.pdf,CIÊNCIA DA COMPUTAÇÃO,EDUARDO GHIDINI,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,29/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'HCI;storytelling;information visualization;social network',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",MILENE SELBACH SILVEIRA,119,IHC;storytelling;visualização de informação;redes sociais,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"O crescimento de dados gerados com o uso da internet, faz com que, também,
aumente a dificuldade de fazer com que informações relevantes cheguem aos usuários.
Para que isso aconteça, além do tratamento da informação, é necessário que esta informação
seja apresentada de tal forma que o usuário consiga analisá-la e extrair informações
úteis. Nos últimos anos, vem se explorando o uso de narrativas para a apresentação de
grandes volumes de dados, considerando-se os benefícios desta forma de apresentação.
Neste contexto, o presente trabalho objetiva analisar e propor formas de apresentar dados,
em forma de narrativas, no contexto de redes sociais, a fim de apoiar a interação do usuário
e sua análise dos dados. O trabalho apresenta um modelo para representação de narrativas
por meio de visualizações, com dados extraídos de redes sociais, concebido a partir de
estudos relacionados às áreas de narrativas e de visualização de informações. O modelo
foi instanciado para que viabilizasse a sua análise por meio de estudos com usuários. Os
estudos indicam que o modelo tem variadas aplicações, tanto no sentido de possibilitar a
análise de diferentes tipos de dados, como, também, no que se refere ao uso por usuários
com e sem interesse prévio em algum assunto específico. Através do modelo proposto e do
resultado de sua análise, acredita-se que o presente trabalho possa apoiar a construção de
narrativas interativas neste contexto, contemplando, não somente a construção da interface
em si, mas, também, o mapeamento dos dados para a sua representação visual.",DISSERTAÇÃO,REPRESENTAÇÃO DE NARRATIVAS INTERATIVAS POR MEIO DE VISUALIZAÇÕES COM DADOS EXTRAÍDOS DE REDES SOCIAIS,5010487,1
"New technology deployment projects should understand the immediate impact this change has on the professionals involved in the work process. In the first months of a technological change, it is possible to identify a certain resistance and even some conflicts that affect the levels of productivity of the teams. The objective of this work was to understand the relation of productivity with the changes in the work characteristics of teams that begin to adopt the SCRUM agile method for software development and to propose practices that help to reduce the learning curve and to improve productivity. Through the understanding of the Tuckman model, which describes the five stages of a team development: formation, confusion / conflicts, normalization, performance, and disintegration, a case study was carried out with a team that started to adopt Scrum and identified the relationship between productivity and the phases described by Tuckman. In this sense, a field research was necessary to find practices that could help teams to increase their productivity during the confusion phase described by Tuckman. Through the preparation of a preliminary proposal, and an it’s preliminary evaluation in a Focus Group activity, this research presents a proposal to use the OKR framework (Objectives and Key-Results) along with SCRUM with the objective of assisting team development, to concentration in the phase of confusion / conflicts.",[DISSERTAÇÃO] MARINA BELLENZIER.pdf,CIÊNCIA DA COMPUTAÇÃO,MARINA BELLENZIER,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,29/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Agile method;productivity;OKR;SCRUM',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,RAFAEL PRIKLADNICKI,91,Método ágil;produtividade;OKR;SCRUM,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Projetos de implantação de novas tecnologias devem compreender o impacto imediato que esta mudança causa nos profissionais envolvidos em relação ao processo de trabalho. Nos primeiros meses de uma mudança tecnológica é possível identificar uma certa resistência e até alguns conflitos que afetam os níveis de produtividade das equipes. O objetivo do trabalho foi compreender a relação da produtividade com as mudanças nas características de trabalho de equipes que passam a adotar o método ágil SCRUM para desenvolvimento de software e propor técnicas que auxiliem na redução da curva de aprendizado e melhora da produtividade. Através da compreensão do modelo de Tuckman, que descreve que no desenvolvimento de equipes existem cinco estágios: formação, confusão/conflitos, normatização, desempenho, e desintegração, foi realizado um estudo de caso com uma equipe que passou a adotar Scrum e identificou-se a relação entre a produtividade e as fases descritas por Tuckman. Neste sentido, se fez necessário um estudo de campo para encontrar técnicas que pudessem auxiliar as equipes a aumentarem sua produtividade na fase de confusão, descrita por Tuckman. Através da elaboração de uma proposta preliminar e sua avaliação preliminar através de um Focus Group, essa pesquisa apresenta uma proposta de um da utilização do framework OKR (Objectives and Key-Results) juntamente com SCRUM com objetivo de auxiliar o desenvolvimento das equipes, sob a ótica da produtividade e com foco na fase de confusão/conflitos.",DISSERTAÇÃO,UM ESTUDO SOBRE A RELAÇÃO DA ADOÇÃO DO MÉTODO ÁGIL SCRUM COM A PRODUTIVIDADE EM EQUIPES DE DESENVOLVIMENTO DE SOFTWARE,5011513,1
"Service Function Chaining (SFC) is an important research field in networking area with
many encapsulation and forwarding mechanisms being proposed. The non-standard forwarding
methods used to implement SFC breaks the mechanism of regular network troubleshooting tools,
what challenges the detection of SFC misconfiguration or performance degradation. This work
presents the SFC Path Tracer, a tool for troubleshooting SFC in NFV/SDN environments. This
tool enables the identification of problems in the SFC domain by generating packet trace and
computing intra-hop delays from a specific SFC path. SFC Path Tracer is agnostic regarding the
SFC encapsulation and forwarding mechanisms being effective to detect most problems in an SFC
environment.",[VOLUME] - Rafael Anton Eichelberger.pdf,CIÊNCIA DA COMPUTAÇÃO,RAFAEL ANTON EICHELBERGER,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,15/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Service Function Chaining;SFC;NFV;SDN;Network',PROCESSAMENTO PARALELO E DISTRIBUÍDO,TIAGO COELHO FERRETO,74,Service Function Chaining;SFC;NFV;SDN;Network,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Service Function Chaining (SFC) is an important research field in networking area with
many encapsulation and forwarding mechanisms being proposed. The non-standard forwarding
methods used to implement SFC breaks the mechanism of regular network troubleshooting tools,
what challenges the detection of SFC misconfiguration or performance degradation. This work
presents the SFC Path Tracer, a tool for troubleshooting SFC in NFV/SDN environments. This
tool enables the identification of problems in the SFC domain by generating packet trace and
computing intra-hop delays from a specific SFC path. SFC Path Tracer is agnostic regarding the
SFC encapsulation and forwarding mechanisms being effective to detect most problems in an SFC
environment.",DISSERTAÇÃO,SFC PATH TRACER: A TROUBLESHOOTING TOOL FOR SERVICE FUNCTION  CHAINING,5011516,1
"In the logistics domain, there is a problem in supply and demand which concerns the accurate
and punctual distribution of supplies in a network of locations. This problem stems from the
uncertainty of future demand, and often causes the supplier to waste resources on unneeded supplies
and demanding parties to suffer from under supply. Prediction models to forecast demand can minimise
the impact of this situation by using predictions to coordinate both production and distribution
strategies. Making accurate forecasts, however, is difficult because it depends on the quality of the
data available to train predictive models, specially when considering multiple demanding locations
and a variety of products to choose from. In this dissertation, we develop an architecture that uses
agents and machine learning models to improve demand forecasting in such domains. We evaluate
two datasets, one with limited data and the other with richer data, and experiment with Machine
Learning algorithms to solve the demand forecasting problem on these datasets. We then compare
the results obtained from these experiments and those obtained with our architecture.",[DISSERTAÇÃO] STEPHAN CHANG.pdf,CIÊNCIA DA COMPUTAÇÃO,STEPHAN CHANG,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,24/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'demand forecasting;prediction models;multi-agent systems;artificial intelligence;machine learning',INTELIGÊNCIA COMPUTACIONAL,FELIPE RECH MENEGUZZI,61,previsão de demanda;modelos de predição;sistemas multiagente;inteligência artificial;aprendizado de máquina,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),Prediction model for dispatch behavior,"Há um problema no domínio da logística que está relacionado à distribuição precisa e
pontual de suprimentos em uma rede de localizações. Este problem surge da incerteza da demanda
futura e causa o desperdício de recursos na produção de suprimentos que acabam inutilizados,
enquanto locais de demanda sofrem com baixos estoques. Uma das formas de reduzir o impacto
desse problema é utilizar modelos de predição para prever a demanda que está por vir, de forma a
utilizar essa informação para coordenar o processo de produção e distribuição de suprimentos. Fazer
previsões acuradas, no entanto, é uma tarefa desafiadora pois para treinar modelos de predição
depende-se não apenas de uma grande quantidade de dados, mas também da qualidade dos exemplos
de treino, especialmente quando consideramos que há muitos locais de demanda e uma grande
variedade de produtos envolvidos nas demandas. Nesta dissertação, construímos uma arquitetura
que utiliza conceitos de agentes e modelos de aprendizado de máquina para melhorar a qualidade
das previsões de demanda feitas neste tipo de cenário. Para a experimentação, analisamos duas
bases de dados: uma que possui dados limitados, e outra que possui dados com melhor qualidade,
para que possamos estudar a performance dos algoritmos em cada caso. Ao final, comparamos os
resultados da experimentação dos modelos avulsos com os resultados obtidos ao utilizar a nossa
arquitetura.",DISSERTAÇÃO,AN ARCHITECTURE FOR IMPROVED DEMAND FORECASTING,5011517,1
"Face Detection is one of the most studied subjects in the Computer Vision field. Given
an arbitrary image or video frame, the goal of face detection is to determine whether there are any
faces in the image and, if present, return the image location and the extent of each face. Such a
detection is easily done by humans, but it is still a challenge within Computer Vision. The high
degree of variability and the dynamicity of the human face makes it an object very difficult to
detect, mainly in complex environments. Recently, Deep Learning approaches started to be applied
for Computer Vision tasks with great results. They opened new research possibilities in different
applications, including Face Detection. Even though Deep Learning has been successfully applied for
such a task, most of the state-of-the-art implementations make use of off-the-shelf face detectors
and do not evaluate differences among them. In other cases, the face detectors are trained in a
multitask manner that includes face landmark detection, age detection, and so on. Hence, our goal
is threefold. First, we summarize and explain many advances of deep learning, detailing how each
different architecture and implementation work. Second, we focus on the face detection problem
itself, performing a rigorous analysis of some of the existing face detectors as well as implementations
of our own. We experiment and evaluate variations of hyper-parameters for each of the detectors
and their impact in different datasets. We explore both traditional and more recent approaches,
as well as implementing our own face detectors. Finally, we implement, test, and compare a meta
learning approach for face detection, which aims to learn the best face detector for a given image.
Our experiments contribute in understanding the role of deep learning in face detection as well as
the subtleties of changing hyper-parameters of the face detectors and their impact in face detection.
We also show how well features obtained with deep neural networks trained on a general-purpose
dataset perform on a meta learning approach for face detection. Our experiments and conclusions
show that deep learning has indeed a notable role in face detection.",[DISSERTAÇÃO] THOMAS DA SILVA PAULA.pdf,CIÊNCIA DA COMPUTAÇÃO,THOMAS DA SILVA PAULA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,28/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Deep Learning;Face Detection;Neural Networks;Machine Learning;Computer Vision',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,RODRIGO COELHO BARROS,143,Aprendizado Profundo;Reconhecimento Facial;Redes Neurais;Aprendizado de Máquina;Visão Computacional,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Reconhecimento facial é um dos assuntos mais estudos no campo de Visão Computacional.
Dada uma imagem arbitrária ou um frame arbitrário, o objetivo do reconhecimento facial é
determinar se existem faces na imagem e, se existirem, obter a localização e a extensão de cada
face encontrada. Tal detecção é facilmente feita por seres humanos, porém continua sendo um
desafio em Visão Computacional. O alto grau de variabilidade e a dinamicidade da face humana
tornam-a difícil de detectar, principalmente em ambientes complexos. Recentementemente, abordagens
de Aprendizado Profundo começaram a ser utilizadas em tarefas de Visão Computacional
com bons resultados. Tais resultados abriram novas possibilidades de pesquisa em diferentes aplicações,
incluindo Reconhecimento Facial. Embora abordagens de Aprendizado Profundo tenham
sido aplicadas com sucesso para tal tarefa, a maior parte das implementações estado da arte utilizam
detectores faciais off-the-shelf e não avaliam as diferenças entre eles. Em outros casos, os
detectores faciais são treinados para múltiplas tarefas, como detecção de pontos fiduciais, detecção
de idade, entre outros. Portanto, nós temos três principais objetivos. Primeiramente, nós resumimos
e explicamos alguns avanços do Aprendizado Profundo, detalhando como cada arquitetura e
implementação funcionam. Depois, focamos no problema de detecção facial em si, realizando uma
rigorosa análise de alguns dos detectores existentes assim como algumas implementações nossas.
Nós experimentamos e avaliamos variações de alguns hiper-parâmetros para cada um dos detectores
e seu impacto em diferentes bases de dados. Nós exploramos tanto implementações tradicionais
quanto mais recentes, além de implementarmos nosso próprio detector facial. Por fim, nós implementamos,
testamos e comparamos uma abordagem de meta-aprendizado para detecção facial, que
visa aprender qual o melhor detector facial para uma determinada imagem. Nossos experimentos
contribuem para o entendimento do papel do Aprendizado Profundo em detecção facial, assim como
os detalhes relacionados a mudança de hiper-parâmetros dos detectores faciais e seu impacto no resultado
da detecção facial. Nós também mostramos o quão bem features obtidas com redes neurais
profundas — treinadas em bases de dados de propósito geral – combinadas com uma abordagem de
meta-aprendizado, se aplicam a detecção facial. Nossos experimentos e conclusões mostram que o
aprendizado profundo possui de fato um papel notável em detecção facial.",DISSERTAÇÃO,Contributions in Face Detection with Deep Neural Networks,5011522,1
"Upcoming non-volatile memory technologies are a big promise in computer architecture
and are expected to be powerful tools to address today’s issues regarding efficient data manipulation.
They provide high performance and byte granularity while also having the distinct advantage of being
persistent. However in order to explore these technologies to their full potential, existing systems
and architecture must adapt to this new way of working with data and workaround the challenges
that come with it.
Existing work in the area already proposes methods to adapt existing architecture to NVM
as well as innovative ways to employ these memories in future applications. However operating
system support to such NVM-enabled solutions, although existent, still very limited. In this work,
we present two variations of the existing mmap system call, designed to both explore NVM characteristics
and provide user data consistency. Both are very simple solutions that allow users to control
the persistence and define checkpoints to their files while using the common mapped file syntax.
We have implemented and tested these methods over Linux using a NVM file system as our base.
Our results show that these mechanisms can ensure file integrity in the presence of system failures
while also providing a reasonable performance.",[DISSERTAÇÃO] GIANLUCCA OLIVEIRA PUGLIA.pdf,CIÊNCIA DA COMPUTAÇÃO,GIANLUCCA OLIVEIRA PUGLIA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,21/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Non-Volatile Memory (NVM);Operating Systems (OS);Systematic Mapping Study (SMS);File Systems',PROCESSAMENTO PARALELO E DISTRIBUÍDO,AVELINO FRANCISCO ZORZO,93,Memórias Não-Voláteis;Sistemas Operacionais;Mapeamento Sistemático;Sistemas de Arquivos,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"As tecnologias de memórias não-voláteis são uma grande promessa na área de arquitetura
de computadores e é esperado que sejam poderosas ferramentas para solucionar os problemas referentes
a manipulação eficiente de dados dos dias de hoje. Estas tecnologias provêm alta performance
e acesso em granularidade de bytes com a distinta vantagem de serem persistentes. Porém, afim de
explorar estas tecnologias em todo seu potencial, os sistemas e arquiteturas de hoje precisam buscar
meios de se adaptar a esta nova forma de acessar dados e de superar os desafios que vêm com ela.
Trabalhos existentes na área já propõem métodos para adaptar as arquiteturas existentes
para o uso de NVM bem como formas inovadoras de empregar estas memórias em futuras aplicações.
No entanto, o suporte dos sistemas operacionais a estas soluções, ainda que existente, ainda
é muito limitado. Neste trabalho, nós apresentamos duas variações da chamada de sistema msync,
modeladas para explorar as características das tecnologias de NVM e garantir consistência para os
dados dos usuários. Ambas são soluções simples que permitem aos usuários definirem checkpoints
de seus arquivos usando a sintaxe comum de sistemas de arquivos. Nós implementamos e testamos
estes métodos sobre o sistema operacional Linux utilizando como base um sistema de arquivo
nativamente voltado a NVM. Nossos resultados mostram que estes mecanismos são capazes de
garantir a integridade dos arquivos mesmo na presença de falhas no sistema enquanto mantém uma
performance razoável.",DISSERTAÇÃO,EXPLORING ATOMICITY ON MEMORY MAPPED FILES BASED ON NON-VOLATILE MEMORY FILE SYSTEMS,5011524,1
"The increase in the volume of unstructured web data in recent decades has been driven by the arising of new media, devices and technologies. In this context, the Semantic Web was developed, whose objective is to provide a layer of knowledge representation to that data, facilitating the treatment by automated processes. Ontologies are key elements of the Semantic Web, providing a description of the concepts and relationships between them, for a specific domain. However, ontologies of the same domain may differ in structure, granularity or terminology, requiring a process of matching between them to be performed, producing a set of correspondences between semantically related entities (alignment). A growing number of matching approaches have emerged in the literature, and the need to evaluate and qualitatively compare the produced alignments is presented. Tasks that make use of alignments started to demand better graphical representations for it. In this context, this work presents an adaptatve approach for alignment visualization, that allows users to choose how e what to visualize, ccording to own preferences or to the task being performed at that moment (creation, manipulation, evaluation, etc.). To support this model, a survey was conducted with alignment specialists to validate the most important aspects in an alignment visualization. Finally, a prototype was built with the purpose of validating the solution. The results obtained from the prototype validation with users show that the model handles the problems it proposes to solve very well, with a margin for future work on the possible visualization configurations.",[TESE] BERNARDO SEVERO DE SOUZA.pdf,CIÊNCIA DA COMPUTAÇÃO,BERNARDO SEVERO DE SOUZA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,20/02/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'ontology matching;ontology alignment visualization;ontology alignment evaluation',INTELIGÊNCIA COMPUTACIONAL,RENATA VIEIRA,106,alinhamento de ontologias;visualização de alinhamentos de ontologias;avaliação de alinhamentos de ontologias,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),"Linguagem natural e ontologias: construção, população e alinhamento","O aumento do volume de dados não estruturados na Web nas últimas décadas tem sido impulsionado pelo surgimento de novos meios de comunicação, dispositivos e tecnologias. Neste contexto se desenvolve a Web Semântica, cujo objetivo é o de atribuir uma camada de representação de conhecimento a esses dados, facilitando o tratamento por processos automatizados. Ontologias são elementos chave da Web Semântica, oferecendo uma descrição dos conceitos e dos relacionamentos entre os mesmos para um domínio específico. Entretanto, ontologias de um mesmo domínio podem divergir em sua estrutura, granularidade ou terminologia, necessitando que um processo de mapeamento entre as mesmas seja realizado, produzindo um conjunto de correspondências entre entidades semanticamente relacionadas (alinhamento). Um número crescente de abordagens de mapeamento tem surgido na literatura e a necessidade de avaliar e comparar qualitativamente os alinhamentos produzidos se faz presente. Tarefas que fazem uso de alinhamentos passaram a demandar melhores representações gráficas dos mesmos. Neste contexto, este trabalho apresenta uma abordagem adaptativa de visualização para alinhamentos, que permite ao usuário escolher como e o que visualizar, de acordo com preferências próprias ou para uma atividade sendo realizada no momento (criação, manipulação, avaliação, etc.). Para da suporte a esse modelo, uma pesquisa foi realizada com especialistas em alinhamentos para validar os aspectos mais importantes em uma visualização de alinhamento. Por fim, um protótipo foi construído com o intuito de validar a solução. Os resultados obtidos da avaliação com usuário no protótipo mostram que o modelo lida com os problemas que se propões a resolver muito bem, com uma margem para trabalhos futuros em configurações de visualização possíveis.",TESE,AN ADAPTIVE APPROACH FOR ONTOLOGY ALIGNMENT VISUALIZATION,5011531,1
"BDI (Beliefs, Desires, Intentions) architecture is the preferable strategy concerning
the development of systems of agents sited in complex and dynamic environments. BDI architecture,
which is founded in the symbolic model, represents a consolidated model that counts
upon a substantial theoretical-practical contribution. However, according to Gärdenforns, despite
its strength, there are some aspects of the cognitive phenomenon which give rise to the
need of a conceptual model that has to establish itself between the perceptual and the symbolic
levels. Taking into account the principle in which the recognition of objects can be set as a
process of the concept formation, this thesis proposal comprises the design and implementation
of knowledge representation model for BDI agents based on the assumptions of the paradigm
of conceptual spaces. In this sense, the actual proposal seeks to establish the necessary mechanisms
for the development of this model through Jason platform and the programming language
CSML. From the issues which are inherent to the development of an application directed to the
aid of visually impaired people, this proposal aims to evaluate the implications of the building
processes of the conceptual inference model for BDI agents.",[Volume ] Joao Mário Lopes Brezolin.pdf,CIÊNCIA DA COMPUTAÇÃO,JOAO MARIO LOPES BREZOLIN,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,15/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'BDI architecture;Conceptual spaces;CSML;Jason',INTELIGÊNCIA COMPUTACIONAL,RAFAEL HEITOR BORDINI,97,Arquitetura BDI;Espaços conceituais;CSML;Jason,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"A arquitetura BDI (Beliefs, Desires, Intentions) é a estratégia preferencial no que
tange ao desenvolvimento de sistemas de agentes situados em ambientes complexos e dinâmicos.
Alicerçada no modelo simbólico, a arquitetura BDI representa um modelo consolidado que conta
com um substancial aporte pratico-teórico. Entretanto, conforme assinala Gärdenforns, apesar
da robustez desse modelo, há aspectos do fenômeno cognitivo que suscitam a necessidade de um
modelo conceitual que deve estabelecer-se entre os níveis simbólico e perceptual. Partindo do
princípio que o reconhecimento de objetos pode ser assinalado como um processo de formação de
conceitos, a presente proposta de tese abrange a concepção e implementação de um modelo de
representação do conhecimento para agentes BDI com base nos pressupostos do paradigma dos
espaços conceituais. Nesse sentido, a presente proposta busca estabelecer através da plataforma
Jason e da linguagem de programação CSML os mecanismos necessários para o desenvolvimento
desse modelo. A partir da problemática inerente ao desenvolvimento de uma aplicação voltada
ao auxilio de deficientes visuais, esta proposta busca avaliar as implicações do processo de
construção do modelo de inferência conceitual para agentes BDI.",TESE,ESPAÇOS CONCEITUAIS: UMA PROPOSTA DE USO DE REPRESENTAÇÕES CONCEITUAIS APLICADA MULTI-AGENTES SISTEMAS,5011550,1
"Crowd simulation models have been playing an important role in computer sciences for a
few decades now, since pioneer works. At the beginning, agents simulated on crowds behaved
all the same way, such behaviour being controlled by the same set of rules. In time, simulation
models evolved and began to incorporate greater variety of behaviours. Crowd simulation
models that implement different agent behaviours are so-called Heterogeneous Crowd models,
opposing to former Homogeneous Crowd models. Advances in crowd simulation models
that attempt to make agents with more realistic human-like behaviours explore heterogeneity
of agent behaviours in order to achieve overall simulation realism. In general, human behavioural
and psychological studies are used as base of knowledge to simulate observed human
behaviours within virtual agents. Toward this direction, later crowd simulation works explore
personality traits and emotion models. Some other work in the field of emotional virtual
agents, researchers are attempting to recreate emotion contagion phenomena in small groups of
agents, and even studying emotion contagion impact between virtual agents and human participants.
Under the belief that emotion contagion in virtual agents might lead to more realistic
behaviours on crowds, this work is focused on recreating emotion contagion computational
models designed for small groups of agents, and adapting it for crowd simulation context.",Volume_Thesis - Amyr Borges F. Neto.pdf,CIÊNCIA DA COMPUTAÇÃO,AMYR BORGES FORTES NETO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,09/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'crowd simulation models;personality models;emotion models;behavioural models',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",SORAIA RAUPP MUSSE,93,modelos de simulação de multidões;modelos de personalidade;modelos de emoção;modelos comportamentais,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),Cultural Crowds,"Modelos de simulação de multidões têm tido um papel importante em ciências da computação
já há algumas décadas desde os trabalhos pioneiros. No início, agentes simulados em
multidões comportavam-se todos da mesma maneira, e tal comportamento era controlado pelas
mesmas regras em todos os agentes. Com o tempo, os modelos de simulação evoluiram,
e começaram a agregar uma maior variedade de comportamentos nos agentes. Modelos de simulação
de multidões que implementam diferentes comportamentos nos agentes são chamados
modelos de Multidões Heterogêneas, em oposição aos modelos de Multidões Homogêneas precedentes.
Avanços nos modelos de simulação de multidões que buscam criar agentes com comportamentos
humanos realistas, na tentativa de atingir tal realismo, exploram heterogeneidade
nos comportamentos dos agentes. Em geral, estudos em psicologia e comportamento humano
são usados como conhecimento de base, e os comportamentos observados nestes estudos são
simulados em agentes virtuais. Nesta direção, trabalhos recentes em simulação de multidões
exploram características de personalidade e modelos de emoções. No campo de emoções em
agentes virtuais, pesquisadores estão tentando recriar fenômenos de contágio de emoções em
pequenos grupos de agentes, ou mesmo estudar o impacto de contágio de emoção entre agentes
virtuais e participantes humanos. Sob a crença de que contágio de emoção em agentes virtuais
possa levar a comportamentos mais realistas em multiões, este trabalho foca em recriar modelos
computacionais de contágio de emoções destinados a pequenos grupos de agentes, adaptando
estes modelos para um contexto de simulação de multidões.",TESE,GIVING EMOTION CONTAGION ABILITY TO VIRTUAL AGENTS IN CROWDS,5011577,1
"Software development has become increasingly an activity that requires a social and
collaborative effort. In this context, there is not only a need to develop technical skills, but
also to know how to develop teamwork skills. Collaborative programming practices are
important tools for collaborative learning and training among software developers. The
Coding Randori Dojo (CRD) is a collaborative coding practice that has been increasingly
adopted in the software industry, and its main purpose is to provide collaborative learning.
The goal of this dissertation is to evaluate collaborative learning in CRD. The methodology
is structured in three stages: Exploratory, Evaluation and, Consolidation. In the first stage,
a characterization of collaborative coding practices was performed, as well as an
execution of two feasibility studies. In the second stage, we conducted observational
studies and case studies, where collaborative learning has been evaluate with mor
emphasis. The last stage consolidated the results by the conception of a set of patterns.
The research methodology of this dissertation lead to an extensive evaluation of CRD, and
the identification of approaches that can be used to evaluate collaborative learning during
the practice. The findings present benefits perceived from the participants regarding CRD,
as well an understanding of collaboration levels that can contribute to the collaborative
learning.",[TESE] BERNARDO JOSÉ DA SILVA ESTÁCIO.pdf,CIÊNCIA DA COMPUTAÇÃO,BERNARDO JOSE DA SILVA ESTACIO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,28/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Software Engineering;Collaborative Programming Practices;Coding Dojo Randori;Pair Programming;Collaborative Learning',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,RAFAEL PRIKLADNICKI,172,Engenharia de Software;Práticas de Codificação Colaborativa;Código Doo Randori;Programação em Par;Aprendizagem Colaborativa.,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"O desenvolvimento de software tem se tornado cada vez mais uma atividade que
necessita de um esforço social e colaborativo. Neste cenário, não há apenas a
necessidade de desenvolver competências técnicas, mas também de se saber trabalhar
em equipe. As práticas de codificação colaborativa representam uma importante
ferramenta de aprendizagem colaborativa e treinamento entre desenvolvedores. O Coding
Dojo Randori (CDR) é uma prática de codificação colaborativa que tem sido adotado de
forma crescente na indústria de software, e seu principal propósito é a aprendizagem
colaborativa. Esta tese tem por objetivo avaliar a aprendizagem colaborativa em CDR. A
metodologia de pesquisa foi estruturada em três etapas: Exploratória, Avaliação e
Consolidação. Na primeira etapa, realizou-se a caracterização do estado da arte de
práticas de codificação colaborativa, assim como a execução de dois estudos de
viabilidade de CDR. Na segunda etapa foram conduzidos estudos observacionais e
estudos de caso, onde se pode avaliar com mais ênfase a aprendizagem colaborativa em
CDR. A última etapa consolidou os resultados por meio da concepção de um conjunto de
padrões para apoiar a adoção de CDR com foco na aprendizagem colaborativa. A
metodologia de pesquisa deste trabalho possibilitou uma avaliação extensiva de CDR, e a
identificação de abordagens que podem ser utilizadas para avaliar a aprendizagem
colaborativa em CDR. Os resultados apresentaram uma percepção positiva dos
participantes em relação a prática, assim como um entendimento dos níveis de
colaboração que podem contribuir para a aprendizagem colaborativa.",TESE,UMA AVALIAÇÃO EMPÍRICA SOBRE A APRENDIZAGEM COLABORATIVA EM CODING DOJO RANDORI NO CONTEXTO DE DESENVOLVIMENTO DE SOFTWARE,5011590,1
"Among the main computational techniques currently applied to study proteins, classical
molecular dynamics plays a great hole, specially its variation called replica exchange
molecular dynamics or REMD, which provides efficient conformational sampling. Regular
secondary structures elements of proteins are formed and maintained via stabilization by
hydrogen bonds within helices and between strands of a -sheet. Packing of these structural
elements, allowed by flexible turns and loops connecting them, leads to the formation of
a structure that, in the successful cases, represents the native, functional state of a protein.
Ionic, dipole, van derWaals, hydrophobic interactions, and hydrogen bonding are fundamental
to these events. Most of these forces are strongest up to a distance of 4.0 Å. Hence, these
are the distances involved in the formation of local structural nubs that can further propagate
and form whole elements of secondary structure. The common practice while simulating is,
however, to keep fixed the cutoff at values higher or equal to 8.0 Å. Here a novel replica
exchange molecular dynamics approach based on running cutoffs (varying from 4.0 Å to 8.0
Å) to enhance protein structure prediction is presented. We first proved the method as a
reproducible one, as well as following a Boltzmann distribution and sampling different structures
of conventional REMD. The human villin headpiece protein (PDB ID: 1UNC) was used
as case study. We tested 9 different simulation protocols, in triplicate, and proved the use
of incremental cutoff as an effective approach to enhance the quality and speed of protein
structure predictions via replica exchange molecular dynamics. Applying the method to the
protein test set, although of limited size, CuT-REMD showed good performance against the
ab initio methods, most of the time being either as the best prediction method or with close
results to the best ones. This made it possible to compare CuT-REMD also with de novo
methods and besides more difficulty, CuT-REMD maintained a good performance even surpassing
certain servers for all tested proteins. The results obtained are encouraging, with
the emergence of new questions to be addressed in the future.",[TESE] THIAGO LIPINSKI PAES.pdf,CIÊNCIA DA COMPUTAÇÃO,THIAGO LIPINSKI PAES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,24/03/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,"b'Replica Exchange Molecular Dynamics;Running Cutoff,;Protein Structure Prediction;Sampling'",BIOINFORMÁTICA E COMPUTAÇÃO BIOINSPIRADA,OSMAR NORBERTO DE SOUZA,184,Replica Exchange Molecular Dynamics;Raio de Corte Incremental;Predição de Estruturas de Proteínas;Amostragem,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),"Métodos Biofísicos Moleculares Computacionais e Bioinformática Estrutural no Estudo de Doenças Negligenciadas: A Via de Síntese de Ácidos Graxos do Mycobacterium tuberculosis, Parte III","Dentre os principais métodos computacionais aplicados atualmente ao estudo de
proteínas, a dinâmica molecular clássica realiza importante papel, especialmente sua variação
intitulada Replica Exchange Molecular Dynamics ou REMD, a qual provê amostragem
conformacional eficiente. Elementos de Estruturas Secundárias (EES) regulares de proteínas
são formados e mantidos através de estabilização por ligações de hidrogênio dentro de
hélices e entre fitas de uma folha . O empacotamento desses elementos estruturais, permitido
por voltas e laços flexíveis conectando-os, leva à formação de uma estrutura que, nos
casos bem sucedidos, representa o estado nativo, funcional de uma proteína. Interações
iônicas, dipolo-dipolo, de van der Waals e hidrofóbicas, além de ligações de hidrogênio, são
fundamentais para esses eventos. A maioria dessas forças é mais forte até uma distância
de 4,0 Å. Assim, essas (de 0,0 Å a 4,0 Å) são as distâncias envolvidas na formação de
estruturas locais, que podem ainda se propagar e formar elementos inteiros de estrutura
secundária. A prática comum ao se executar simulações por DM é, no entanto, manter um
raio de corte fixo em valores maiores ou iguais a 8,0 Å. Esta tese apresenta o método CuTREMD,
uma nova abordagem de REMD com base em raio de corte incremental (variando
de 4,0 Å a 8,0 Å) testando a hipótese de que tal abordagem pode otimizar a predição de
estruturas terciárias de proteínas. Primeiramente, foi utilizada a proteína villin headpiece humana
(código PDB 1UNC), como estudo de caso, e nove diferentes protocolos de simulação
foram testados, todos em triplicata. Posteriormente, com base nos resultados obtidos, um
protocolo-padrão foi escolhido como protocolo CuT-REMD, e um conjunto de nove proteínas
adicionais foi testado, sendo os resultados comparados com o método REMD convencional.
A utilização de raio de corte incremental provou-se uma abordagem eficaz para melhorar
a qualidade e velocidade das predições de estruturas de proteínas via REMD. Aplicando o
método ao conjunto teste de proteínas, embora de tamanho limitado, CuT-REMD mostrou
bom desempenho em relação aos métodos ab initio, colocando-se na grande maioria das
vezes ou como o melhor método de predição ou com resultados próximos aos melhores
métodos. Isso possibilitou compará-lo também com métodos de novo; e, embora com mais
dificuldade, CuT-REMD manteve bom desempenho, inclusive superando certos servidores
em todas as ocasiões. Os resultados obtidos, em suma, mostram-se encorajadores, com o
surgimento de novos questionamentos a serem abordados futuramente.",TESE,CUT-REMD: UM NOVO MÉTODO PARA PREDIÇÃO DE ESTRUTURAS TERCIÁRIAS DE PROTEÍNAS BASEADO EM RAIO DE CORTE INCREMENTAL,5011883,1
"Current computer systems separate main memory from storage. Programming languages typically
reflect this distinction using different representations for data in memory (e.g. data structures,
objects) and storage (e.g. files, databases). Moving data back and forth between these different
layers and representations compromise both programming and execution efficiency. Recent nonvolatile
memory technologies, such as Phase-Change Memory, Resistive RAM, and Magnetoresistive
RAM make it possible to collapse main memory and storage into a single layer of persistent memory,
opening the way for simpler and more efficient programming abstractions for handling persistence.
This Ph.D. thesis introduces a design for the runtime environment for languages with automatic
memory management, based on an original combination of orthogonal persistence, persistent memory
programming, persistence by reachability, and lock-based failure-atomic transactions. Such design
can significantly increase programming and execution efficiency, as in-memory data structures are
transparently persistent, without the need for programmatic persistence handling, and removing the
need for crossing semantic boundaries.
In order to validate and demonstrate the proposed concepts, this work also presents JaphaVM,
the first Java Virtual Machine specifically designed for persistent memory. In experimental results
using benchmarks and real-world applications, JaphaVM in most cases executed the same operations
between one and two orders of magnitude faster than database- and file-based implementations,
while requiring significantly less lines of code.",[Volume_Thesis _ Taciano Dreckmann Perez].pdf,CIÊNCIA DA COMPUTAÇÃO,TACIANO DRECKMANN PEREZ,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,03/05/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Non-Volatile Memory;Storage-Class Memory;Persistent Memory;Orthogonal Persistence;Java;Java Virtual Machine',PROCESSAMENTO PARALELO E DISTRIBUÍDO,CESAR AUGUSTO FONTICIELHA DE ROSE,152,Memória Não-Volátil;Memória Persistente;Persistência Ortogonal;Java;Máquina Virtual Java,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),MOS - Modern Operating Systems,"Sistemas computacionais da atualidade tradicionalmente separam memória e armazenamento. Linguagens
de programação tipicamente refletem essa distinção usando diferentes representacões para
dados em memória (ex. estruturas de dados, objetos) e armazenamento (ex. arquivos, bancos
de dados). A movimentação de dados entre esses dois níveis e representações, bidirecionalmente,
compromete tanto a eficiência do programador quanto de execução dos programas. Tecnologias
recentes de memoria não-volátil, tais como memória de transição de fase, resistiva e magnetoresistiva,
possibilitam combinar memória principal e armazenamento em uma única entidade de memória
persistente, abrindo caminho para abstrações mais eficientes para lidar com persistência de dados.
Essa tese de doutorado introduz uma abordagem de projeto para o ambiente de execução de
linguagens com gerência automática de memória, baseado numa combinação original de persistência
ortogonal, programação para memória persistente, persistência por alcance, e transações com
atomicidade em caso de falha. Esta abordagem pode melhorar significativamente a produtividade do
programador e a eficiência de execução dos programas, uma vez que estruturas de dados em memória
passam a ser persistentes de forma transparente, sem a necessidade de programar explicitamente o
armazenamento, e removendo a necessidade de cruzar fronteiras semânticas.
De forma a validar e demonstrar a abordagem proposta, esse trabalho também apresenta
JaphaVM, a primeira Máquina Virtual Java especificamente projetada para memória persistente.
Resultados experimentais usando benchmarks e aplicações reais demonstram que a JaphaVM, na
maioria dos casos, executa as mesmas operações entre uma e duas ordens de magnitude mais rapidamente
do que implementações equivalentes usando bancos de dados ou arquivos, e, ao mesmo
tempo, requer significativamente menos linhas de código.",TESE,PERSISTENT MEMORY AND ORTHOGONAL PERSISTENCE:A PERSISTENT HEAP DESIGN AND ITS IMPLEMENTATION FOR THE JAVA VIRTUAL MACHINE,5011897,1
"Design trends for next-generation Multi-Processor Systems point to the integration of a
large number of processing cores, requiring high-performance interconnects. One solution
being applied to improve the communication infrastructure in such systems is the usage of
Networks-on-Chip as they present considerable improvement in bandwidth and scaleability.
Still,as the number of integrated cores continues to increase and the system scales, the metallic
interconnects in Networks-on-Chip can become a performance bottleneck. As a result, a new
strategy must be adopted in order for those issues to be remedied.
Optical Integrated Networks (OINs) are currently considered to be one of the most promising
paradigm in this design context: they present higher bandwidth, lower power consumption
and lower latency to broadcast information. Also, the latest work demonstrates the feasibility
of OINs with their fabrication technologies being available and CMOS compatible.
However, OINs’ designers face several challenges:
— Currently, controllers represent the main communication bottleneck and are one of
the factors limiting the usage of OINs. Therefore, new controlling solutions with low
latency are required.
— Designers lack tools to model and validate OINs. Most research nowadays is focused
on designing devices and improving basic components performance, leaving system
unattended.
In this context, in order to ease the deployment of OIN-based systems, this PhD project
focuses on three main contributions: (1) the development of accurate system-level modelling
methods to realize a system-level simulation platform; (2) the definition and development of
an efficient control approach for OIN-based systems, and; (3) the system-level evaluation of
the proposed control approach using the defined modelling methods.",[TESE] Felipe Gohring Magalhaes.pdf,CIÊNCIA DA COMPUTAÇÃO,FELIPE GOHRING DE MAGALHAES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,25/05/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'simulation platform;OIN-based;system-level;modelling methods',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,FABIANO PASSUELO HESSEL,114,system-level modelling;system-level simulation platform;OIN-based;modelling methods,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"La tendance du marché dans la conception des architectures multiprocesseurs de la prochaine
génération consiste à intégrer de plus en plus de coeurs dans la même puce. Cette concentration
des coeurs dans la même puce exige l’amélioration des politiques d’intercommunication.
L’une des solutions proposées dans ce contexte consiste à utiliser les réseaux sur puce vu
qu’ils présentent une amélioration considérable en termes de la bande passante, l’évolutivité
et de l’extensibilité. Néanmoins, vu la croissance exponentielle en nombres de coeurs sur
puce, les interconnexions électriques dans les réseaux sur puce peuvent devenir un goulet
d’étranglement dans la performance du système. Par conséquent, des nouvelles techniques et
technologies doivent être adoptées pour remédier à ces problèmes.
Les réseaux optiques intégrés (OIN venant de l’anglais Optical Integrated Networks) sont
actuellement considérés comme l’un des paradigmes les plus prometteurs dans ce contexte.
Les OINs offrent une plus grande bande passante, une plus faible consommation d’énergie
et moins de latence lors de l’échange des données. Plusieurs travaux récents démontrent la
faisabilité des OIN avec les technologies de fabrication disponibles et compatibles avec CMOS.
Cependant, les concepteurs des OINs font face à plusieurs défis :
— Actuellement, les contrôleurs représentent le principal goulot d’étranglement de la
communication et présentent l’un des facteurs minimisant l’efficacité des OINs. Alors,
la proposition des nouvelles solutions de contrôle à faible latence est de plus en plus
primordiale pour en tirer profit.
— Le manque d’outils de modélisation et de validation des OINs. La plupart des travaux
se concentrent sur la conception des dispositifs et l’amélioration des performances des
composants de base, tout en laissant le système sans assistance.
Dans ce contexte, afin de faciliter le déploiement de systèmes basés sur les OINs, cette thèse
se focalise sur les trois contributions majeures suivantes : (1) le développement d’un ensemble
de méthodes précises de modélisation qui va permettre par la suite de réaliser une plateforme
de simulation au niveau du système ; (2) la définition et le développement d’une approche
de contrôle efficace pour les systèmes basés sur les OINs ; (3) l’évaluation de l’approche de
contrôle proposée.",TESE,HIGH-LEVEL MODELLING OF OPTICAL INTEGRATED NETWORKS-BASED SYSTEMS WITH THE PROVISION OF A LOW LATENCY CONTROLLER,5012810,1
"The general objective of this work was to define and evaluate a programming
language to stimulate orientation and mobility (O&M) skills by people with visual impairment
(VI). It is believed that by using a programming language to guide a robot in a virtual
setting, the person with VI can better understand O&M skills. In this sense, it was defined
the programming language, which was called GoDonnie, and was based on the Logo language.
A GoDonnie is run in a programming environment called Donnie, which provides
soundbacks about running the robot. In addition, it has a 2D graphic simulator with a virtual
robot, in which one can visualize the execution of the language commands, that is, the
virtual robot sedeslocando in the scenario. This simulator allows the use by people with
low vision, as well as serve as a resource to better visualize how the person who is blind is
interacting with the robot in the environment. A GoDonnie has been evaluated to verify its
usability. Evaluations were carried out which included visually impaired people and teachers
of programming, without visual impairment. The results pointed out that GoDonnie has good
usability, as well as helps the development of O&M.",[Volume] Juliana Damasio.pdf,CIÊNCIA DA COMPUTAÇÃO,JULIANA DAMASIO OLIVEIRA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,04/08/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'person with visual impairment;orientation and mobility;programming language;educational robotics',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,MARCIA BORBA CAMPOS,190,pessoa com deficiência visual;linguagem de programação;orientação e mobilidade;robótica educacional;ensino de programação,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"O objetivo geral desse trabalho foi definir e avaliar uma linguagem de programação
para estimular as habilidades de orientação e mobilidade (O&M) por pessoas com deficiência
visual. Acredita-se que ao utilizar uma linguagem de programação para guiar um
robô em um cenário virtual, a pessoa com deficiência visual (DV) possa melhor compreender
habilidades de O&M. Neste sentido, foi definida a linguagem de programação, que foi
denominada GoDonnie, e foi baseada na linguagem Logo. A GoDonnie é executada em
um ambiente de programação chamado Donnie, que fornece feedbacks sonoros sobre a
execução do robô. Além disso, possui um simulador gráfico 2D com um robô virtual, no
qual se pode visualizar a execução dos comandos da linguagem, ou seja, o robô virtual se
deslocando no cenário. Esse simulador permite o uso por pessoas com baixa visão, além
de servir como recurso para melhor visualizar como a pessoa que é cega está interagindo
com o robô no ambiente. A GoDonnie foi avaliada para verificar a sua usabilidade. Foram
realizadas avaliações que incluíram pessoas com deficiência visual e professores de programação,
sem deficiência visual. Os resultados apontaram que a GoDonnie possui uma
boa usabilidade, assim como auxilia o desenvolvimento de O&M.",DISSERTAÇÃO,GODONNIE: Definição e Avaliação de Uma Linguagem de Programação para Comandar Robô por Programadores Iniciantes com Deficiência Visual,5035374,1
"The growing need to extend IT (Information Technology) resources to meet business
needs has raised concerns about how to increase capacity with lower cost and greater
use of data center. Therefore, in order to avoid underutilization of infrastructure resources
virtualization is a trend towards cost reduction and consolidation of the server infrastructure,
thus taking advantage of existing assets. However, with virtualization growth, there
is a problem related to resources concurrence in consolidated environments, where diskintensive
applications such as databases can be impacted in this type of environment, if
they do not have their resources managed properly, can generate performance degradation
and increasing execution time respectively.
In order to optimize performance and reduce I/O contention, Kassiano J.M. [16]
presented a study on the acceleration of Hadoop applications through manual adjustment of
disk resource allocation, showing that it is possible to get performance gains. Therefore, proposed
work follows this line of study, however, with objective of optimizing the execution of
database applications in virtualized environments with shared resources, applying a dynamic
adjustment policy of disk resources allocation. It aims to distribute disk resources optimally
through an algorithm, avoiding that one or more processes consume all disk resources,
while others wait to be executed or are being executed without minimum of appropriate disk
resources, thus, taking more time to complete their execution. In order to demonstrate this
scenario, workloads of OLTP (Online Transaction Processing) and DW (Data Warehouse)
databases have been evaluated using the Orion data load simulator [23] and real captured
data from a loading test provided by a large IT company in partnership with PUCRS University
(Pontifical Catholic University of Rio Grande do Sul), through the Oracle RAT (Real
Application Testing) [26].
Laboratory tests have been performed using the following test scenarios: without
adjustment of disk resources, with static adjustment of disk resources and through a dynamic
adjustment policy of disk resources based on performance metrics. In this case, it
can be observed that dynamic policy obtained the best result among the other test groups,
generating a gain of 23% for OLTP database workloads, 21% for DW database workloads
and 18% for environments with different types of workloads in concurrency like DW and
OLTP.",[DISSERTAÇÃO] FABIO MIGUEL BLASAK DA FONSECA.pdf,CIÊNCIA DA COMPUTAÇÃO,FABIO MIGUEL BLASAK DA FONSECA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,11/08/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'resource allocation;virtualization;disk contention;databases',PROCESSAMENTO PARALELO E DISTRIBUÍDO,CESAR AUGUSTO FONTICIELHA DE ROSE,133,alocação de recursos;virtualização;contenção de disco;banco de dados,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),Projeto PDTI - PUCRS/Dell,"A crescente necessidade de extensão dos recursos de TI (Tecnologia da Informação)
para atender as demandas do negócio, geraram uma preocupação de como aumentar
a capacidade com menor custo e maior aproveitamento do data center. Portanto, a fim de
evitar a subutilização de recursos de infraestrutura a virtualização é uma tendência para
redução de custos e consolidar a infraestrutura de servidores, aproveitando assim os ativos
existentes. Entretanto, com o crescimento da virtualização, surge um problema relacionado
a concorrência por recursos em ambientes consolidados, onde aplicações com uso intensivo
de disco, como bancos de dados, podem ser impactados neste tipo de ambiente, caso
não tenham os seus recursos gerenciados apropriadamente, podendo gerar degradação no
desempenho e consequentemente aumentando o tempo de execução.
A fim de otimizar performance e reduzir a contenção de E/S (Entrada/Saída), Kassiano
J. M. [16] apresentou um estudo sobre a aceleração de aplicações Hadoop através de
ajuste manual na alocação de recursos de disco, mostrando que é possível obter ganhos de
performance. Logo, o trabalho proposto, segue esta linha de estudo, entretanto, com o objetivo
de otimizar a execução de aplicações de banco de dados em ambientes virtualizados
com recursos compartilhados, aplicando uma política de ajuste dinâmico de alocação de
recursos de disco, a qual visa acelerar ainda mais os ganhos de performance. Essa política
tem por objetivo distribuir os recursos de disco de forma otimizada, conforme algoritmo aplicado,
evitando que um ou mais processos consumam todos os recursos de disco, enquanto
outros aguardam para serem executados ou executam com o mínimo de recursos de disco
apropriados, por isso, levando maior tempo para concluir o processamento. Para evidenciar
esta situação, foram avaliados workloads de banco de dados do tipo OLTP (Online Transaction
Processing) e DW (Data Warehouse), utilizando o simulador de cargas de dados Orion
[23] e com dados reais capturados de um teste de carga cedidos por uma empresa de TI
de grande porte, em parceria com a universidade PUCRS (Pontifícia Universidade Católica
do Rio Grande do Sul), através do recurso Oracle RAT (Real Application Testing) [26].
Foram realizados testes em laboratório utilizando os seguintes cenários de teste:
sem ajuste de recursos de disco, com ajuste estático de recursos de disco e através de uma
política de ajuste dinâmico de recursos de disco com base em métricas de performance. A
partir disso, pode-se observar que a política dinâmica obteve o melhor resultado entre os
demais grupos de teste, gerando um ganho de 23% para a execução de workloads de banco
de dados OLTP, 21% para workloads de banco de dados DW e 18% durante a execução de
ambientes com workloads de tipos diferentes em concorrência, exemplo: DW e OLTP.",DISSERTAÇÃO,OTIMIZANDO A EXECUÇÃO DE APLICAÇÕES DE BANCO DE DADOS ATRAVÉS DE UMA MELHOR ALOCAÇÃO DE RECURSOS DE DISCO EM AMBIENTES VIRTUALIZADOS,5052326,1
"Virtual reality is a technology that allows users to view, and interact with a 3D Virtual Environment (VE) in real time. A Collaborative Virtual Environment (CVE) is type of VE that allows two or more users to be in the same virtual environment together. Collaborative virtual environments have some issues that VE does not have. For example, different techniques are requered in order to allow two users to manipulate (move or rotate) a virtual object together. Some of these techniques can lead users to do unnatural movements.
This study evaluates haptic feedback to let users aware of wrong movements during a cooperative object manipulation. It was used the SkeweR technique as a testbed. This technique is based on the use of crushing points, where the users grab the object for the first time, to simultaneously move/rotate an object. Once the user keeps his hand positioned on the crushing point, during the object manipulation, the interaction becomes more natural, in the sense that it is more similar to the real process. However, due to the lack of any physical constraint to the users’ movements, it is often noticed that the user’s hand moves apart from the crushing point during the interaction.
To solve this problem, this work proposes the usage of tactile feedback to inform the user about the distance between his hand and the crushing point. The tactile feedback is provided by a vibration micromotor attached to the users’ thumb. To validate our method, we ran a user study based on the 3D manipulation of a virtual object, which had to be translated and rotated through a virtual path along a virtual wire, from the beginning to the end of it.
During the interaction, users manipulated a 3DOF position tracker and should keep this tracker at the same position of the crushing point. During the trials, the participants used three modalities of interaction: without any feedback, with a visual feedback and with tactile feedback. Results showed that the users do more natural manipulations when using tactile feedback.",[Dissertacão]Thomas Volpato de Oliveira.pdf,CIÊNCIA DA COMPUTAÇÃO,THOMAS VOLPATO DE OLIVEIRA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,22/08/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Awareness Feedback;Haptic Feedback;3D Manipulations;Virtual Reality',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",MARCIO SARROGLIA PINHO,65,Awareness Feedback;Retorno Háptico;Manipulação 3D;Realidade Virtual,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Realidade Virtual é uma tecnologia que permite usuarios visualizar e interagir com ambientes virtuais (AV) 3D em tempo real. Um ambiente virtual colaborative (AVC) é um tipo de AV que permite dois ou mais usuarios estarem juntos no mesmo ambiente virtual. Ambientes virtuais colaborativos têm algumas dificuldades que AV não tem. Por exemplo, diferentes técnicas são necessaria a fim de permitir dois usuarios a manipularem (mover ou girar) juntos um objeto virtual. Algumas dessas tecnicas podem levar os usuarios a realizarem movimentos não naturais.
Este trabalho avalia retorno haptico para deixar os usuarios cientes de movimentos errados durante a manipulação colaborativa de objetos. Foi utilizado a técnica SkeweR como teste. Esta técnica é baseade em Crushing Points, onde os usuarios pegam o objeto pela primeira vez, para simultaneamente mover e girar o objeto. Uma vez que os usuários mantem a posição da mão sobre o Crushing Point, durante a manipulação do objeto, a interação se torna mais natural, no sentido que é mais simular ao processo real de segurar um objeto. Entretanto, devido a falta de restrições fisicas de movimento, frequentemente, durante a interação, a mão do usuário se move para fora do Crushing Point.
Para solucionar este problema, este trabalho propoe o uso de rotorno tatil para informar os usuários sobre a distância entre a posição da mão e o Crushing Point. O retorno tátil é fornecido por um minimotor de vibração preso no polegar do usuário. Para validar nosso método, nos rodamos um estudo com usuários, no qual deveriam realizar a manipulação 3D de um objeto virtual. Este objeto precisava ser transladado e girado através de um caminho virtual ao longo de um fio virtual, do inicio deste, até o fim.
Durante a interação, os usuários manipularam um rastreador de posição com 3 Graus de Liberdade e deveriam mandar a posição do rastreador na mesma posição do Crushing Point. Durante as rodadas do experimento, participantes testaram três modalidades de interação: sem nenhum retorno, com retorno visual, com retorno tátil. Resultado dos testes mostrou que usuários realizaram manipulações mais naturais quando estavam usando o retorno tátil.",DISSERTAÇÃO,USAGE OF TACTILE FEEDBACK TO AID COOPERATIVE OBJECT MANIPULATION IN VIRTUAL ENVIRONMENTS,5052344,1
"Recommender systems are software used to generate personalized lists according to users
profiles. The area is new and is growing since the internet popularization having its roots in information
retrieval. Collaborative filtering is the most common approach of recommender systems used in
both academy and industry because content-based filtering has problems such as lack of semantic
information and poor content extraction techniques from items. Nowadays there are more content
available in the form of multimedia such as video, images and text. Also, there are advances in pattern
recognition though techniques like convolutional neural networks. In this work a convolutional
neural network is used to extract features from movie trailers frames to further use these features
to create a content-based recommender system with the goal of assess whether the success of such
networks on tasks like image classification and object detection also occur in the recommendation
context. To evaluate that, the proposed method was compared with a media aesthetic detection
method, two methods of feature extraction from text using TF-IDF and the traditional user and
item collaborative filtering methods. Our results indicate that the purposed method is superior to
the other content-based methods and is competitive to the collaborative filtering methods, being
superior to the item-collaborative method regarding classification accuracy, and being superior to
all other methods regarding execution time. In conclusion, we can state that the method using
convolutional neural networks to represent items is promising for the recommender systems context.",[DISSERTAÇÃO] Ralph José Rassweiler Filho.pdf,CIÊNCIA DA COMPUTAÇÃO,RALPH JOSE RASSWEILER FILHO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,22/08/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'recommender systems;convolutional neural networks;pattern recognition',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,RODRIGO COELHO BARROS,65,sistemas de recomendação;redes neurais convolucionais;reconhecimento de padrões;filtragem baseada em conteúdo,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),PRÓ-FCSI - PROGRAMA DE FORMAÇÃO E CAPACITAÇÃO EM SISTEMAS DE INFORMAÇÃO,"Sistemas de recomendação são softwares cujo propósito é gerar listas personalizadas, de
acordo com as preferências de usuários. A área é bastante recente e está em expansão desde a popularização
da internet tendo suas raízes em recuperação de informação. Dos dois tipos tradicionais
de sistemas de recomendação, a filtragem colaborativa é a mais utilizada na academia e na indústria
por trazer melhores resultados que o segundo tipo, a filtragem baseada em conteúdo. Este último
sofre de problemas tais como a falta de informação semântica e a dificuldade em extrair conteúdo
dos itens. Atualmente há uma maior disponibilidade de conteúdo de itens na forma de recursos multimídia
tais como vídeos, imagens e texto. Também houve avanços no reconhecimento de padrões
em imagens através de técnicas como as redes neurais convolucionais. Neste trabalho, propõe-se
utilizar uma rede neural convolucional como extratora de atributos dos quadros que compõe trailers
de filmes que servem como base para um sistema de recomendação baseado em conteúdo com o
objetivo de avaliar se o sucesso destas redes em tarefas como classificação de imagens e detecção de
objetos também ocorre no contexto de recomendações. Para esta avaliação, comparou-se o método
proposto com um método de detecção de estética de mídia, dois métodos de extração de conteúdo
de texto usando TF-IDF e os tradicionais métodos colaborativos entre usuários e itens. Os resultados
obtidos mostram que o método proposto neste trabalho é superior aos demais métodos baseados
em conteúdo e é competitivo com os métodos colaborativos, superando o método colaborativo entre
itens na métrica que representa acurácia de classificação e também, superando todos os outros
métodos com relação ao tempo de execução. Concluiu-se que o método que utiliza redes neurais
convolucionais para representar itens é promissor para o contexto de sistemas de recomendação.",DISSERTAÇÃO,APRENDIZADO NEURAL DE REPRESENTAÇÃO DE CONTEÚDO PARA SISTEMA DE RECOMENDAÇÃO DE FILMES,5052396,1
"Proteins perform a vital row in all living beings, mediating a series of
processes necessary to life. Although we have ways to determine the
composition of such molecules, we lack sufficient knowledge regarding the
determination of their 3D structure in a cheap and fast manner, which plays an
important role in their functions. One of the main computational methods
applied to the study of proteins and their folding process, which determine its
structure, is Molecular Dynamics. An enhancement of this method, known as
Replica-Exchange Molecular Dynamics (or REMD) is capable of producing
much better results, at the expense of a significant increase in computational
costs and raw volume of data generated. This dissertation presents a novel
optimization for this method, titled Analytical Data Filtering, which aims to
optimize post-simulation analysis by filtering unsatisfactory predicted
structures via the use of different absolute quality metrics. The proposed
methodology has the potential of working together with other optimization
approaches as well as covering an area still untouched at large by them to the
best of the author knowledge. Further on, the SnapFi tool is presented, a tool
designed specially for the purpose of filtering unsatisfactory structure
predictions and also being able to work with the different optimization
approaches of the Replica-Exchange Molecular Dynamics method. A study
was then conducted on a test dataset of REMD protein structure prediction
simulations aiming to elucidate a series of formulated hypothesis regarding
the impact of the different temperatures of the REMD process in the final
quality of the predicted structures, the efficiency of the different absolute
quality metrics and a possible filtering configuration that take advantage of
such metrics. It was observed that high temperatures may be safely
discarded from post-simulation analysis of REMD protein structure prediction
simulations, that absolute quality metrics posses a high variance of efficiency
(regarding quality terms) between different protein structure prediction
simulations and that different filtering configurations composed of such
quality metrics carry on this inconvenient variance.",[Dissertação] Rafael Cauduro Oliveira Macedo.pdf,CIÊNCIA DA COMPUTAÇÃO,RAFAEL CAUDURO OLIVEIRA MACEDO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,30/08/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Replica-Exchange Molecular Dynamics;Protein Structure Prediction;Filtering;Quality Metric',BIOINFORMÁTICA E COMPUTAÇÃO BIOINSPIRADA,OSMAR NORBERTO DE SOUZA,149,Replica Exchange Molecular Dynamics;Predição de Estruturas de Proteínas;Filtragem;Métrica de Qualidade,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),"Métodos Biofísicos Moleculares Computacionais e Bioinformática Estrutural no Estudo de Doenças Negligenciadas: A Via de Síntese de Ácidos Graxos do Mycobacterium tuberculosis, Parte III","Proteínas executam um papel vital em todos os seres vivos, mediando
uma série de processos necessários para a vida. Apesar de que existam
maneiras de determinar a composição dessas moléculas, ainda falta-nos
conhecimentos suficiente para determinar de uma maneira rápida e barata a
sua estrutura 3D, que desempenha um papel importante na suas funções.
Um dos principais métodos computacionais aplicados ao estudo das
proteínas e o seu processo de enovelamento, o qual determina a sua
estrutura, é Dinâmica Molecular. Um aprimoramento deste método,
conhecido como Replica Exchange Molecular Dynamics (ou REMD), é capaz
de produzir resultados muito melhores, com o revés de significativamente
aumentar o seu custo computacional e gerar um volume muito maior de
dados. Esta dissertação apresenta um novo método de otimização deste
método, intitulado Filtragem de Dados Analíticos, que tem como objetivo
otimizar a análise pós-simulação filtrando as estruturas preditas
insatisfatórias através do uso de métricas de qualidade absolutas. A
metodologia proposta tem o potencial de operar em conjunto com outras
abordagens de otimização e também cobrir uma área ainda não abordada
por elas. Adiante, a ferramenta SnapFi é apresentada, a qual foi designada
especialmente para o propósito de filtrar estruturas preditas insatisfatórias e
ainda operar em conjunto com as diferentes abordagens de otimização do
método REMD. Um estudo foi então conduzido sobre um conjunto teste de
simulações REMD de predição de estruturas de proteínas afim de elucidar
uma séries de hipóteses formuladas sobre o impacto das diferentes
temperaturas na qualidade final do conjunto de estruturas preditas do
processo REMD, a eficiência das diferentes métricas de qualidade absolutas
e uma possível configuração de filtragem que utiliza essas métricas. Foi
observado que as temperaturas mais altas do método REMD para predição
de estruturas de proteínas podem ser descartadas de forma segura da
análise posterior ao seu término e também que as métricas de qualidade
absolutas possuem uma alta variância (em termos de qualidade) entre
diferentes simulações de predições de estruturas de proteínas. Além disso,
foi observado que diferentes configurações de filtragem que utilize tais
métricas carrega consigo esta variância.",DISSERTAÇÃO,ON THE ANALYSIS OF REMD PROTEIN STRUCTURE PREDICTION SIMULATIONS FOR REDUCING VOLUME OF ANALYTICAL DATA,5052404,1
"Historically embedded systems (ES) were designed to perform a single task throughout
their lifetime. However, this view has changed with the new paradigm of computing called
the Internet of Things or IoT. An example of environment where IoT can be applied are smart
cities by creating products such as smart poles. Thus, smart poles can be responsible not
only for city lighting, but also for the control of security cameras, in addition to temperature
and noise sensors. In this scenario, the virtualization technique in ES appears to contribute
to the development of IoT devices since it allows a better use of the available resources in the
ES besides contributing to the increase of the security. ES security has been neglected and
IoT oriented ES have attracted malicious attacks as they play a central role in the operation
of essential services for individuals and enterprises. Therefore, the objective of this work is to
identify a set of security mechanisms that use cryptography techniques that, combined with
the virtualization technique, can establish a security architecture for IoT oriented virtualized
ES (VES). Thus, establishing a minimum level of confidence between the users and the SEV.
Two security mechanisms have been implemented in prplHypervisor: integrity checking and
introspection of guest system hypercalls. The results show that for a guest system with a
size of 256kB the integrity check mechanism imposed a 150.33ms initialization delay time
while the introspection engine imposed 10.57ms of initialization delay. 2,029 lines of code
have been added to the prplHypervisor to perform the integrity check and 120 lines of code
to implement the introspection engine. The final size of the prplHypervisor has 32kB which
represents a 53% increase over the original code. However, growth does not prevent the use
of security mechanisms since the storage capacity available on the platform is 2MB.",[Dissertação] Matheus Duarte Vasconcelos.pdf,CIÊNCIA DA COMPUTAÇÃO,MATHEUS DUARTE VASCONCELOS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,31/08/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Secutiry;Embedded System;Virtualization;IoT',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,FABIANO PASSUELO HESSEL,79,Segurança;Sistemas Embarcados;Virtualização;IoT,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Historicamente os sistemas embarcados (SE) eram desenvolvidos para realizar
uma única tarefa em toda a sua vida. Entretanto, esta visão mudou com o novo paradigma da
computação chamado Internet das Coisas ou IoT. Um ambiente onde a IoT pode ser aplicada
são as cidades inteligentes através da criação de produtos como, por exemplo, os postes
inteligentes. Assim, os postes inteligentes podem ser responsáveis não só pela iluminação
da cidade, mas encarregados também pelo controle de câmeras de segurança, além de
sensores de temperatura e ruído. Neste cenário, a técnica de virtualização em SE surge para
contribuir no desenvolvimento de dispositivos IoT, pois, permite uma melhor utilização dos
recursos disponíveis nos SE além de auxiliar para o aumento da segurança. A segurança
dos SE tem sido negligenciada e os SE voltados para IoT têm atraído ataques maliciosos,
visto que, desempenham um papel central no funcionamento de serviços essenciais para as
pessoas e empresas. O objetivo deste trabalho é identificar um conjunto de mecanismos
de segurança que utilizam técnicas de criptografia que, combinados com a técnica de
virtualização, possam estabelecer uma arquitetura de segurança para os SE virtualizados
(SEV) voltados para IoT. Assim, estabelecendo um nível de confiança mínimo entre os
usuários e os SEV. Foram implementados dois mecanismos de segurança no prplHypervisor:
a verificação de integridade e a introspecção das hypercalls do sistema convidado. Os
resultados mostram que para um sistema convidado com tamanho de 256kB o mecanismo de
verificação de integridade impôs um tempo de atraso na inicialização de 150,33ms enquanto
o mecanismo de introspecção impôs 10,57ms de atraso na inicialização. Foram adicionados
2.029 linhas de código ao prplHypervisor para realizar a verificação de integridade e 120
linhas de código para implementar o mecanismo de introspecção. O tamanho final do
prplHypervisor possui 32kB o que representa um aumento de 53% em relação ao código
original. Todavia, o crescimento não inviabiliza o uso dos mecanismos de segurança, dado
que, a capacidade de armazenamento disponível na plataforma utilizada é de 2MB.",DISSERTAÇÃO,UMA ARQUITETURA DE SEGURANÇA PARA SISTEMAS EMBARCADOS VIRTUALIZADOS,5052407,1
"Institutes, companies, and governments have increased the adoption of security
principles when using cloud computing environments. From sophisticated authentication
methods to strong cryptography algorithms, confidentiality has gained attention fromboth
cloud users and cloud providers for preventing data leakage, but demanding extra computational
resources. Due to the nature of the on-demand billing process applied for public
cloud providers, considering the pay-as-you-go fashion, to add security mechanisms may
impact in the rented resources increasing the overall costs and minimizing the feasibility
for some applications. To understand confidentiality usage in a computational environment,
one may consider adopting cryptography in at least three main axes: (a) the communication
on shared networks; (b) the data storage on shared services; and (c) the data
processing in a virtual environment. A full-stack secure solution, considering those three
axes, allows users to have the cloud computing benefits, even if they have strict Privacy
Level Agreements (PLA). However, the costs of adding such privacy for assets in a cloud
environment should be estimated giving support to the manager making decisions about
application’s availability and performance. This Ph.D. research presents (i) an architecture
of full-stack confidentiality for cloud computing; and (ii) amodel to estimate confidentiality
costs for communicating, storing, and processing in cloud computing environments. The
axes can be combined to estimate user’s overheads according to its security needs. The
predicted values could be used for resizing cloud resources or even recalculating rental
costs of cloud services. The model’s evaluation presented an accuracy close to 95%. In
the evaluation, we used a database-based benchmark in a cloud environment including
standard cryptography algorithms, such as AES, and Querying over Encrypted Databases.",[TESE] MAURO STRELOW STORCH.pdf,CIÊNCIA DA COMPUTAÇÃO,MAURO STRELOW STORCH,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,10/08/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Security;Cryptography;Cost Modeling;Cloud Computing',PROCESSAMENTO PARALELO E DISTRIBUÍDO,CESAR AUGUSTO FONTICIELHA DE ROSE,107,Segurança da Informação;Criptografia;Modelagem de Custos;Computação em Nuvem,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),Gerência de Recursos em Grades e Nuvens Computacionais (GR/GNC-PQ),"A adoção de princípios de segurança em sistemas computacionais é uma demanda
crescente para diversas instituições, incluindo empresas e mesmo agências governamentais.
A confidencialidade se destaca dentre os princípios de segurança por evitar
vazamento de dados, porém, sua adoção demanda recursos computacionais extras. Esse
princípio utiliza complexos métodos de autenticação e poderosos algoritmos de criptografia,
tanto por usuários quanto por provedores de nuvem. Pela natureza de precificação
sob demanda dos provedores de nuvem, adicionar mecanismos de segurança para aplicações
pode impactar nos custos totais dos serviços podendo até inviabilizar a adoção de
computação em nuvem por algumas aplicações. Na intenção de mapear a adoção de confidencialidade
em ambientes computacionais, deve-se considerar pesquisar a utilização
de criptografia nos seus três principais eixos: (a) na comunicação em redes compartilhadas;
(b) no armazenamento de dados emserviços compartilhados; e (c) no processamento
de dados em ambientes virtualizados. Quando aplicado nesses três eixos, o princípio de
confidencialidade promove um ambiente completamente seguro permitindo que usuários
usufruam dos benefícios da computação em nuvem, mesmo que possuam estritos níveis
de privacidade (PLA). No entanto, o custo de se adicionar tal privacidade nos serviços dos
usuários em um ambiente de computação em nuvem deve ser estimado para dar suporte
aos administradores para tomarem decisões sobre suas aplicações. Considerando esse
cenário, este trabalho propõe (i) um projeto de ""confidencialidade total"" para computação
em nuvem; e (ii) uma modelagem do custo do princípio de confidencialidade para os
eixos comunicação, armazenamento e processamento. Considera-se também que esses
eixos podem ser combinados para aumentar os níveis de segurança da aplicação do usuário.
Os resultados preditos pela modelagem podem ser utilizados para redimensionar os
recursos na nuvem, recalcular os custos da aplicação ou mesmo ajudar na tomada de decisão
na escolha de provedores. Na avaliação do modelo feita neste trabalho, utilizou-se
um benchmark para simulação de um ambiente de e-commerce onde foi possível predizer
a sobrecarga dos mecanismos de segurança, por exemplo criptografia AES e busca em
banco de dados encriptados, com uma precisão próxima a 95%.",TESE,FULL-STACK CONFIDENTIALITY COST MODELING FOR CLOUD COMPUTING,5052665,1
"Thermal cycles and high temperatures can have a major impact on the systems performance,
power consumption and reliability, which is a major and increasingly critical design metric in
emerging multi-processor embedded systems. Existing thermal management techniques rely on
physical sensors to provide them temperature values to regulate the system’s operating
temperature and thermal variation at runtime. However, on-chip thermal sensors present
limitations (e.g. extra power and area cost), which may restrict their use in large scale systems. In
this context, this Thesis proposes a lightweight software-based runtime temperature model,
enabling to capture detailed temperature distribution information of multiprocessor systems with
a negligible overhead in the execution time. The temperature model is embedded in a distributedmemory
MPSoC platform, described at the RTL level. Results show that the average absolute
temperature error estimation, compared to the HotSpot tool is smaller than 4% in systems with up
to 36 processing elements.
Task mapping is the process selected to act in the system, using the temperature information
generated by the proposed model. Task mapping is the process of assigning a processing element
to execute a given task. The number of cores in many-core systems increases the complexity of the
task mapping. The main concerns of task mapping for large systems include (i) scalability; (ii)
dynamic workload; and (iii) reliability. It is necessary to distribute the mapping decisions across the
system to ensure scalability. The workload of emerging many-core systems may be dynamic, i.e.,
new applications may start at any moment, leading to different mapping scenarios. Therefore, it is
necessary to execute the mapping process at runtime to support dynamic workload.
The workload assignment plays a major role in the many-core system reliability. Load
imbalance may generate hotspots zones and consequently thermal implications. Recently, task
mapping techniques aiming at improving system reliability have been proposed in the literature.
However, such approaches rely on centralized mapping decisions, which are not scalable. To address
these challenges, the main goal of this Thesis is to propose a hierarchical runtime mapping heuristic,
which provides scalability and a fair thermal distribution. Thermal distribution inside the system
increases the system reliability in long-term, due to the reduction of hotspot regions. The proposed
mapping heuristic considers the PE temperature as a cost function. The proposal adopts a
hierarchical thermal monitoring scheme, able to estimate at runtime the instantaneous
temperature at each processing element. The mapping uses the temperature estimated by the
monitoring scheme to guide the mapping decision. Results compare the proposal against a mapping
heuristic whose main cost function minimizes the communication energy. Results obtained in large
systems, show a decrease in the maximum temperature (best case, 8%) and improvement in the
thermal distribution (best case, 50% lower standard deviation of processor temperatures). Such
results demonstrate the effectiveness of the proposal. Also, a 45% increase in the lifetime of the
system was achieved in the best case, using the proposed mapping.",[TESE] Guilherme Machado de Castilhos.pdf,CIÊNCIA DA COMPUTAÇÃO,GUILHERME MACHADO DE CASTILHOS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,10/08/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'MPSoC;Thermal Management;Energy Management;reliability',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,FERNANDO GEHM MORAES,87,MPSoC;Gerência Térmica;Gerência Energética;confiabilidade,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"As altas variações térmicas e de temperatura de operação podem ter um impacto significativo
no desempenho do sistema, no consumo de energia e na confiabilidade, uma métrica cada vez mais
crítica em sistema multiprocessados. As técnicas de gerenciamento térmico existentes dependem
de sensores físicos para fornecer os valores de temperatura para regular a temperatura de operação
e a variação térmica do sistema em tempo de execução. No entanto, os sensores térmicos em um
chip apresentam limitações (por exemplo, custo extra de potência e de área), o que pode restringir
seu uso em sistemas com uma grande quantidade de processadores. Neste contexto, este Tese
propõe um modelo de temperatura baseado em software, realizado em tempo de execução,
permitindo capturar informações detalhadas da distribuição de temperatura de sistemas
multiprocessados com custo mínimo no desempenho das aplicações. Para validar a proposta, o
modelo foi incluído em uma plataforma MPSoC com memória distribuída, descrita no nível RTL.
Além disso, os resultados mostram que o erro absoluto médio da estimativa de temperatura, em
comparação com a ferramenta HotSpot, é menor do que 4% em sistemas com até 36 elementos de
processamento.
O mapeamento de tarefas foi o processo escolhido para atuar no sistema, utilizando as
informações de temperatura geradas pelo modelo proposto. O mapeamento de tarefas é o
processo de selecionar um elemento de processamento para executar uma determinada tarefa. O
número de núcleos em sistemas multiprocessados, aumenta a complexidade do mapeamento de
tarefas. As principais preocupações no mapeamento de tarefas em sistemas de grande porte
incluem: (i) escalabilidade; (Ii) carga de trabalho dinâmica; e (iii) confiabilidade. É necessário
distribuir a decisão de mapeamento em todo o sistema para assegurar a escalabilidade. A carga de
trabalho de sistemas multiprocessados pode ser dinâmica, ou seja, novas aplicações podem
começar a qualquer momento, levando a diferentes cenários de mapeamento. Portanto, é
necessário executar o processo de mapeamento em tempo de execução para suportar carga
dinâmica de trabalho.
A atribuição de carga de trabalho desempenha um papel importante na confiabilidade do
sistema. O desequilíbrio de carga pode gerar zonas de hotspot e consequentemente implicações
térmicas. Recentemente, técnicas de mapeamento de tarefas com o objetivo de melhorar a
confiabilidade do sistema foram propostas na literatura. No entanto, tais abordagens dependem de
decisões de mapeamento centralizado, que não são escaláveis. Para enfrentar esses desafios, esta
Tese propõe uma heurística de mapeamento hierárquico realizado em tempo de execução, que
ofereça escalabilidade e uma melhor distribuição térmica. A melhor distribuição térmica dentro do
sistema aumenta a confiabilidade do sistema a longo prazo, devido à redução das variações térmicas
e redução de zonas de hotspot. A heurística de mapeamento proposta considera a temperatura do
PE como uma função custo. A proposta adota um esquema hierárquico de monitoramento de
temperatura, capaz de estimar em tempo de execução a temperatura instantânea de cada elemento
de processamento. O mapeamento usa a temperatura estimada pelo método de monitoramento
para orientar a decisão de mapeamento. Os resultados comparam a proposta com uma heurística
de mapeamento cuja principal função de custo minimiza a energia de comunicação. Os resultados
obtidos mostram diminuição da temperatura máxima (melhor caso, 8%) e melhora na distribuição
térmica (melhor caso, valor 50% menor do desvio padrão das temperaturas dos processadores).
Além disso, alcançou-se, no melhor caso, um aumento de 45% no tempo de vida do sistema
utilizando o mapeamento proposto.",TESE,GERENCIAMENTO TÉRMICO E ENERGÉTICO EM MPSOCS,5052669,1
"Software Product Lines (SPL) aim to develop systems based on reuse of software components.
Through this concept it is possible to create a set of similar systems, thus reducing time
to market and cost and thus obtaining greater productivity and improve software quality. Although
reuse is the basis for developing systems from SPLs, the testing activity does not yet fully benefit
from this concept. This is due to an important aspect inherent to SPLs, i.e., variability. The variability
refers to how the members/components that compose the products of an SPL are different
from each other. It represents different types of variation on different levels with different types
of dependencies. The problem of dealing with variability in the test context is not a trivial task,
since when variability in SPLs grows, the amount of tests needed to assess the product quality can
increase exponentially. This thesis presents a method called SPLiT-MBt to generate functional test
cases and scripts to test products derived from SPLs. Thus, test cases to test products common
functionalities are generated based on the reuse inherent to SPLs. In order to provide this reuse,
SPLiT-MBt is applied in two steps. In the first step, variability and test information annotated in
system models are used to generate test sequences using different methods, e.g., HSI, UIO, DS
or TT. These methods are applied to formal models, e.g., Finite State Machines (FSMs) that are
extended to deal with variability information. In the second step, test models and sequences are
reused to generate test scripts, which could be executed by different functional testing tools with
the aim of evaluating the quality of products. Finally, in order to demonstrate the applicability of
this work, we apply our method to test products of two SPLs, i.e., an actual SPL named PLeTs and
an academic SPL named AGM. Moreover, we also performed an experimental study to evaluate the
effort to generate test cases for SPL products. The main goal was to make a comparison between
our SPLiT-MBt and two other methodologies/approaches. Thus, the results point out that the
effort to generate test cases using our method was reduced considerably when compared to the
other methodologies.",[TESE] Leandro Teodoro Costa.pdf,CIÊNCIA DA COMPUTAÇÃO,LEANDRO TEODORO COSTA,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,22/08/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Software Product Line;Software Product Line Testing;Functional Testing;Model-based Testing;Test Case Generation Method',PROCESSAMENTO PARALELO E DISTRIBUÍDO,AVELINO FRANCISCO ZORZO,145,Linha de Produto de software;Teste de Linha de Produto de Software;Teste Funcional;Teste Baseado em Modelos;Métodos de Geração de Casos de Teste,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"com base na reutilização de componentes de software. Através deste conceito, é possível
criar um conjunto de sistemas similares, reduzindo assim o tempo de comercialização e custo e,
consequentemente, obter maior produtividade e melhorias na qualidade do software. Embora o reuso
seja a base para o desenvolvimento de sistemas para LPS, a atividade de teste ainda não se beneficia
totalmente desse conceito. Isto se deve a um importante fator inerente a LPS, i.e., Variabilidade. A
variabilidade diz respeito a como os membros/componentes que compõem os produtos de uma LPS
diferem entre si. Além disso, a variabilidade representa diferentes tipos de variação sob diferentes
níveis com diferentes tipos de dependência. O problema de lidar com a variabilidade no contexto do
teste não é uma tarefa trivial, uma vez que quando a variabilidade em LPSs cresce, a quantidade de
testes necessários para avaliar a qualidade do produto pode aumentar exponencialmente. Portanto,
esta tese apresenta um método chamado SPLiT-MBt para gerar casos de teste funcional e scripts
para testar produtos derivados de LPSs. Assim, os casos de teste para testar funcionalidades comuns
entre os produtos são gerados com base nesse reuso inerente às LPSs. Para fornecer esse reuso, o
SPLiT-MBt é aplicado em duas etapas. Na primeira, as informações de variabilidade e teste anotadas
em modelos de sistema são utilizadas para gerar seqüências de teste usando diferentes métodos, e.g.,
HSI, UIO, DS ou TT. Esses métodos são aplicados a modelos formais, e.g., Máquinas de Estado
Finitos (FSMs), as quais são estendidas para lidar com informações de variabilidade. Na segunda
etapa, os modelos de teste e as seqüências geradas são reutilizados para gerar scripts de teste, os
quais podem ser executados por diferentes ferramentas de teste funcional com o objetivo de avaliar
a qualidade dos produtos. Finalmente, para demonstrar a aplicabilidade deste trabalho, utilizamos
nosso método para testar produtos de duas LPSs, i.e., uma LPS real chamada PLeTs e uma LPS
acadêmica chamada AGM. Além disso, realizamos um estudo experimental com o intuito de avaliar
o esforço de gerar casos de teste para produtos de uma LPS. O objetivo foi comparar o nosso SPLiTMBt
com outras duas metodologias/abordagens de teste de LPSs. Ao final, os resultados apontam
que o esforço para gerar casos de teste usando nosso método foi reduzido consideravelmente quando
comparado com as outras metodologias.",TESE,SPLIT-MBT: A Model-Based Testing Method for Software Product Lines,5052715,1
"Model-driven engineering provides abstractions and notations for improving the understanding
and for supporting the modelling, coding, and verification of applications for specific
domains. Ontologies, on the other hand, provide formal and explicit definitions of shared
conceptualisations and enable the use of semantic reasoning. Although these areas have
been developed by different communities, important synergies can be achieved when both
are combined. These advantages can be explored in the development of multi-agent systems,
given their complexity and the need for integrating several components that are often
addressed from different angles. This work investigates how to apply ontologies for agentoriented
software engineering. Initially, we present a new modelling approach where multiagent
systems are designed using the proposed OntoMAS ontology. Then, we describe
techniques, implemented in a tool, to help programmers bring their concepts into code and
also generate code automatically from instantiated ontology models. Several advantages
can be obtained from these new approaches to model and code multi-agent systems, such
as semantic reasoning to carry out inferences and verification mechanisms. But the main
advantage is the unified high (knowledge) level specification language that allows modelling
the three dimensions that are united in the JaCaMo framework so that systems specifications
can be better communicated across developing teams. The evaluations of these
proposals indicate that they contribute with the different aspects of agent-oriented software
engineering, such as the specification, verification, and programming of these systems.",[TESE] Artur Luiz Silva da Cunha Freitas.pdf,CIÊNCIA DA COMPUTAÇÃO,ARTUR LUIZ SILVA DA CUNHA FREITAS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,31/08/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Ontology;Multi-Agent System;Model-Driven Engineering;Agent-Oriented Software Engineering;JaCaMo;OntoMAS;OntoJaCaMo',INTELIGÊNCIA COMPUTACIONAL,RENATA VIEIRA,135,Ontologia;Sistema Multiagente;Engenharia Dirigida a Modelos;Engenharia de Software Orientada a Agentes;JaCaMo;OntoMAS;OntoJaCaMo,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"A engenharia orientada a modelos fornece abstrações e notações para melhorar a
compreensão e para apoiar a modelagem, codificação e verificação de aplicações em domínios
específicos. As ontologias, por outro lado, fornecem definições formais e explícitas
de conceitualizações compartilhadas e permitem o uso de raciocínio semântico. Embora
essas áreas tenham sido desenvolvidas por diferentes comunidades, sinergias importantes
podem ser alcançadas quando ambas são combinadas. Essas vantagens podem ser exploradas
no desenvolvimento de sistemas multi-agentes, dada a sua complexidade e a necessidade
de integrar vários componentes que são frequentemente abordados de diferentes
ângulos. Este trabalho investiga como aplicar ontologias para engenharia de software orientada
a agentes. Inicialmente, apresentamos uma nova abordagem de modelagem onde
os sistemas multi-agentes são projetados usando a ontologia OntoMAS proposta. Então,
descrevemos técnicas, implementadas em uma ferramenta, para ajudar os programadores
a trazer seus conceitos em código e também gerar código automaticamente a partir de modelos
instanciados da ontologia. Várias vantagens podem ser obtidas a partir dessas novas
abordagens para modelar e codificar sistemas multi-agentes, como o raciocínio semântico
para realizar inferências e mecanismos de verificação. Mas a principal vantagem é a linguagem
de especificação unificada de alto nível (conhecimento) que permite modelar as três
dimensões que estão unidas em JaCaMo para que as especificações dos sistemas possam
ser melhor comunicadas entre equipes em desenvolvimento. As avaliações dessas propostas
indicam que elas contribuem com os diferentes aspectos da engenharia de software
orientada a agentes, como a especificação, verificação e programação desses sistemas.",TESE,Model-Driven Engineering of Multi-Agent Systems Based on Ontology,5052720,1
"The treatment of textual information has been increasingly relevant in many domains.
One of the first tasks for extracting information from texts is the Named Entities Recognition
(NER), which consists of identifying references to certain entities and finding out their classification.
There are many NER domains, among them the most usual are medicine and biology. One of the
challenging domains in the recognition of Named Entities (NE) is the Geology domain, which is an
area lacking computational linguistic resources. This thesis proposes a method for the recognition
of relevant NE in the field of Geology, specifically to the subarea of Brazilian Sedimentary Basin,
in Portuguese texts. Generic and geological features were defined for the generation of a machine
learning model. Among the automatic approaches to NE classification, the most prominent is the
Conditional Random Fields (CRF) probabilistic model. CRF has been effectively used for word
processing in natural language. To generate our model, we created GeoCorpus, a reference corpus
for Geological NER, annotated by specialists. Experimental evaluations were performed to compare
the proposed method with other classifiers. The best results were achieved by CRF, which shows
76,78% of Precision and 54,33% of F-Measure.",[Thesis]Daniela Oliveira Ferreira do Amaral.pdf,CIÊNCIA DA COMPUTAÇÃO,DANIELA OLIVEIRA FERREIRA DO AMARAL,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,14/09/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Named Entity Recognition;Conditional Random Fields;Corpus;Geology;Brazilian Sedimentary Basin',INTELIGÊNCIA COMPUTACIONAL,RENATA VIEIRA,115,Reconhecimento de Entidades Nomeadas;Conditional Random Fields;Corpus;Geologia;Bacia Sedimentar Brasileira,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),"Linguagem natural e ontologias: construção, população e alinhamento","O tratamento da informação textual torna-se cada vez mais relevante para muitos domínios.
Nesse sentido, uma das primeira tarefas para Extração de Informações a partir de textos é o
Reconhecimento de Entidades Nomeadas (REN), que consiste na identificação de referências feitas
a determinadas entidades e sua classificação. REN compreende muitos domínios, entre eles os mais
usuais são medicina e biologia. Um dos domínios desafiadores no reconhecimento de EN é o de
Geologia, sendo essa uma área carente de recursos linguísticos computacionais.
A presente tese propõe um método para o reconhecimento de EN relevantes no domínio
da Geologia, subárea Bacia Sedimentar Brasileira, em textos da língua portuguesa. Definiram-se
features genéricas e geológicas para a geração do modelo de aprendizado. Entre as abordagens
automáticas para classificação de EN, a mais proeminente é o modelo probabilístico Conditional
Random Fields (CRF). O CRF tem sido utilizado eficazmente no processamento de textos em
linguagem natural. A fim de gerar um modelo de aprendizado foi criado o GeoCorpus, um corpus
de referência para REN Geológicas, anotado por especialistas. Avaliações experimentais foram
realizadas com o objetivo de comparar o método proposto com outros classificadores. Destacam-se
os melhores resultados para o CRF, o qual alcançou 76,78% e 54,33% em Precisão e Medida-F.",TESE,Reconhecimento de Entidades Nomeadas na Área da Geologia: Bacias Sedimentares Brasileiras,5052743,1
"The process of inferring agent’s plans/goals from their observed actions is known
as plan recognition. Predicting human intentions is one of the ultimate goals of Artificial
Intelligence; plan recognition contributes to this goal by analysing how low-level
observations about agents and environment can be associated with a high-level plan
description. Most approaches to plan recognition, in realistic environments, are based
on manually constructed rules, where the knowledge base is represented as a plan library
for recognising activities and plans. Besides, these approaches do not usually
have the ability to incorporate complex temporal dependencies, and they take the unrealistic
assumption that an agent carries out only one activity at a time and the sequence
of actions is all coherently executed towards a single goal. Moreover, the incomplete
knowledge about the agent’s behaviour and the similarity among several plan execution
generate multiple hypotheses about the agent’s plan(s) that are consistent with the
observations. This work addresses the problems of recognising multiple plans in realistic
environments, learning activity duration, and detecting anomalies in plan execution.
We deal with problems related to disambiguation of multiple hypotheses and detecting
anomalies in plan sequence by exploiting both the inherent hierarchical organisation of
activities and their expected time and duration, developing an efficient algorithm to filter
the hypotheses by applying temporal and path length constraints. We present a number
of experimental results showing that, besides addressing those limitations of traditional
plan recognition algorithms, our filtering approach can significantly improve the accuracy
of the underlying plan recognition algorithm. The experiments include a number of
synthetically generated plan libraries as well as plan libraries and observations obtained
from a real-world dataset useful in the context of ambient assisted living.",[TESE] GIOVANI PARENTE FARIAS.pdf,CIÊNCIA DA COMPUTAÇÃO,GIOVANI PARENTE FARIAS,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,31/08/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'activity recognition;plan recognition;multiple-goal recognition;failure prediction',INTELIGÊNCIA COMPUTACIONAL,RAFAEL HEITOR BORDINI,182,reconhecimento de atividade;reconhecimento de plano;reconhecimento de múltiplos objetivos;predição de falha,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"O processo de inferir os planos/objetivos de um agente com base na observação
de suas ações é conhecido como reconhecimento de plano. Prever as intenções humanas
é um dos objetivos atuais da Inteligência Artificial; o reconhecimento de plano contribui
para esse objetivo ao analisar como as observações em baixo nível sobre agentes e
meio ambiente podem ser associadas a uma descrição em alto nível do plano. A maioria
das abordagens para reconhecimento de plano, em ambientes reais, baseiam-se em
regras construídas manualmente, onde a base de conhecimento é representada como
uma biblioteca de planos para reconhecimento de atividades e planos. Além disso, essas
abordagens geralmente não têm capacidade de incorporar dependências temporais
complexas, assumindo a hipótese irrealista de que um agente executa apenas uma atividade
por vez e que a sequência de ações é executada de forma coerente para alcançar
um único objetivo. Além disso, o conhecimento incompleto sobre o comportamento do
agente e a similaridade entre a execução de vários planos geram múltiplas hipóteses
sobre o(s) plano(s) que são consistentes com as observações. Este trabalho aborda os
problemas para reconher múltiplos planos em ambientes reais, aprender a duração de
uma atividade e detectar anomalias na execução de um plano. Tratamos o problema
de desambiguar múltiplas hipóteses e detectamos anomalias na sequência de execução
do plano explorando tanto a organização hierárquica inerente das atividades quanto horário
e duração esperados, desenvolvendo um algoritmo eficiente para filtrar hipóteses
aplicando restrições temporais e no comprimento do caminho. Apresentamos uma série
de experimentos mostrando que, além de abordar limitações dos algoritmos de reconhecimento
de planos tradicionais, nossa abordagem de filtrar hipóteses pode melhorar
significativamente a precisão do algoritmo de reconhecimento. Os experimentos incluem
bibliotecas de planos geradas sinteticamente, bem como bibliotecas e observações obtidas
a partir de conjuntos de dados do mundo real, útil no contexto de ambiente assistido.",TESE,Plan Recognition and Failure Prediction for Ambient Assisted Living,5052756,1
"Asynchronous quasi-delay-insensitive (QDI) circuits are a promising solution to
cope with aggressive process variations faced by circuit design in modern technologies,
as these can gracefully accommodate a wide range of gate and wire delay variations. Moreover,
with the trend for mobile devices and the Internet of Things (IoT), QDI design presents
itself as an interesting circuit implementation alternative for use in conjunction with aggressive
low-power techniques such as deep voltage scaling (VS). The main drawback of QDI
design is often cited as the fact that it leads to significant area and power overheads, which
can compromise its adoption. This work investigates the compatibility of QDI design with
voltage scaling, by exploring and analyzing several available QDI templates. Among these,
this work selects the SCL template as an interesting option to reduce area and power overheads.
However, as this Dissertation demonstrates, SCL current implementation presents
serious timing issues. Because of these, an alternative asynchronous design template is
proposed. Using the ASCEnD Design Flow, standard cell libraries for the studied templates
are generated, to evaluate the benefits and overheads of the newly proposed template.",Dissertacao_Guazzelli_PUBLICAVEL.pdf,CIÊNCIA DA COMPUTAÇÃO,RICARDO AQUINO GUAZZELLI,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,31/03/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Asynchronous circuits;asynchronous design;quasi delay insensitive;QDI design;voltage scaling;threshold voltage;near-threshold voltage;sub-threshold voltage',SISTEMAS EMBARCADOS E SISTEMAS DIGITAIS,NEY LAERT VILAR CALAZANS,88,Circuitos assíncronos;projeto assíncrono;quasi-delay insensitive;projeto QDI;escalamento de tensão;tensão de limiar;tensão quase-limiar;tensão sub-limiar,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Circuitos quasi-delay insensitive (QDI) são uma solução promissora para lidar com
variações significativas de processo em tecnologias modernas, já que suas características
acomodam variações significativas de atraso de fios e portas lógicas. Além disso, com
as novas tendências de dispositivos móveis e a Internet das Coisas (IoT), o projeto QDI
apresenta-se como um alternativa de implementação para aplicações operando em tensão
baixa ou muito baixa. Como principal desvantagem, o projeto QDI conduz a um aumento
significativo no consumo de área e potência, o que pode comprometer seu emprego. Este
trabalho investiga a compatibilidade de projeto QDI sob escalamento de tensão, explorando
e analisando templates QDI disponíveis na literatura. Entre estes templates, seleciona-se
o template SCL como um opção interessante para amenizar consumo de área e potência.
Contudo, a proposta original deste template, demonstra-se aqui, apresenta problemas
temporais. Devido a estes problemas, propõe-se aqui um template alternativo. Usando o
fluxo de projeto ASCEnD, bibliotecas de standard cells para os templates em questão foram
geradas a fim de avaliar os benefícios e desvantagens destes.",DISSERTAÇÃO,QDI Asynchronous Design and Voltage Scaling,5054163,1
Não Consta,[Volume] Willian Eduardo Becker.pdf,CIÊNCIA DA COMPUTAÇÃO,WILLIAN EDUARDO BECKER,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,24/11/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'No Consta',ENGENHARIA DE SOFTWARE E BANCO DE DADOS,RODRIGO COELHO BARROS,81,Não Consta,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"A UTILIZAÇÃO DE REDES SOCIAIS TORNOU-SE UMA ATIVIDADE COTIDIANA NA SOCIEDADE ATUAL. COM O ENORME, E ININTERRUPTO, FLUXO DE INFORMAÇÕES GERADAS NESTES ESPAÇOS, ABRE-SE A POSSIBILIDADE DE EXPLORAR ESTES DADOS DE DIVERSAS FORMAS. A ANÁLISE DE SENTIMENTO (AS) É UMA TAREFA QUE VISA OBTER CONHECIMENTO SOBRE A POLARIDADE DAS MENSAGENS POSTADAS, ATRAVÉS DE DIVERSAS TÉCNICAS DE PROCESSAMENTO DE LINGUAGEM NATURAL, ONDE A MAIORIA DAS SOLUÇÕES LIDA COM SOMENTE UM IDIOMA DE CADA VEZ. ENTRETANTO, ABORDAGENS QUE NÃO RESTRINGEM-SE A EXPLORAR SOMENTE UMA LÍNGUA, ESTÃO MAIS PRÓXIMAS DE EXTRAÍREM TODO O CONHECIMENTO E POSSIBILIDADES DESTES DADOS. ABORDAGENS RECENTES BASEADAS EM APRENDIZADO DE MÁQUINA PROPÕEM-SE A RESOLVER A AS APOIANDO-SE PRINCIPALMENTE NAS REDES NEURAIS PROFUNDAS (DEEP LEARNING), AS QUAIS OBTIVERAM BONS RESULTADOS RECENTEMENTE NESTA TAREFA. NESTE TRABALHO SÃO PROPOSTAS TRÊS ARQUITETURAS DE REDES NEURAIS CONVOLUCIONAIS QUE LIDAM COM DADOS MULTI-LINGUAIS EXTRAÍDOS DO TWITTER CONTENDO QUATRO LÍNGUAS. OS DOIS PRIMEIROS MODELOS PROPOSTOS CARACTERIZAM-SE PELO FATO DE POSSUÍREM UM TOTAL DE PARÂMETROS MUITO MENOR QUE OS DEMAIS BASELINES CONSIDERADOS, E AINDA ASSIM, OBTÊM RESULTADOS SUPERIORES COM UMA BOA MARGEM DE DIFERENÇA. O ÚLTIMO MODELO PROPOSTO É CAPAZ DE REALIZAR UMA CLASSIFICAÇÃO MULTITAREFA, IDENTIFICANDO A POLARIDADE DAS SENTENÇAS E TAMBÉM A LÍNGUA. COM ESTE ÚLTIMO MODELO OBTÉM-SE UMA ACURÁCIA DE 74.43% PARA AS E 98.40% PARA IDENTIFICAÇÃO DA LÍNGUA EM UM DATASET COM QUATRO LÍNGUAS, MOSTRANDO-SE A MELHOR ESCOLHA ENTRE TODOS OS BASELINES ANALISADOS.",DISSERTAÇÃO,UMA ABORDAGEM DE REDES NEURAIS CONVOLUCIONAIS PARA ANÁLISE DE SENTIMENTO MULTI-LINGUAL,5197485,1
"The accelerated growth of data repositories, in the different areas of activity, opens
space for research in the area of data mining, in particular, with the methods of classification
and combination of classifiers. The Boosting method is one of them, which combines
the results of several classifiers in order to obtain better results. The main purpose of this
dissertation was the experimentation of alternatives to increase the effectiveness and performance
of the algorithm AdaBoost.M1, which is the implementation often employed by the
Boosting method. An empirical study was made taking into account stochastic aspects trying
to shed some light on an obscure internal parameter, in which algorithm creators and
other researchers assumed that the training error threshold should be correlated with the
number of classes in the target data set and logically, most data sets should use a value of
0.5. In this paper, we present empirical evidence that this is not a fact, but probably a myth
originated by the mistaken application of the theoretical assumption of the joint effect. To
achieve this goal, adaptations were proposed for the algorithm, focusing on finding a better
suggestion to define this threshold in a general case.",[Dissertação]Volume.pdf,CIÊNCIA DA COMPUTAÇÃO,ANTONIO DO NASCIMENTO LEAES NETO,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,20/11/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Data Mining;Ensemble Methods;Classification;Boosting;AdaBoost;AdaBoost.M1',INTELIGÊNCIA COMPUTACIONAL,RAFAEL HEITOR BORDINI,83,Mineração de dados;Classificação;Combinação de classificadores;Boosting;AdaBoost;AdaBoost.M1,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"O crescimento acelerado dos repositórios de dados, nas diversas áreas de atuação,
abre espaço para pesquisas na área da mineração de dados, em específico, com os
métodos de classificação e de combinação de classificadores. O Boosting é um desses
métodos, e combina os resultados de diversos classificadores com intuito de obter melhores
resultados. O propósito central desta dissertação foi a experimentação de alternativas
para aumentar a eficácia e o desempenho do algoritmo AdaBoost.M1 que é a implementação
frequentemente empregada pelo Boosting, de forma que foi feito um estudo empírico
levando em consideração aspectos estocásticos tentando lançar alguma luz sobre um parâmetro
interno obscuro, em que criadores do algoritmo e outros pesquisadores assumiram
que o limiar de erro de treinamento deve ser correlacionado com o número de classes no
conjunto de dados de destino e, logicamente, a maioria dos conjuntos de dados deve usar
um valor de 0.5. Neste trabalho, apresentamos evidências empíricas de que isso não é
um fato, mas provavelmente um mito originado pela aplicação equivocada do pressuposto
teórico do efeito conjunto. Para alcançar esse objetivo, foram propostas adaptações para o
algoritmo, focando em encontrar uma sugestão melhor para definir esse limiar em um caso
geral.",DISSERTAÇÃO,CLASSIFICAÇÃO COM ALGORITMO ADABOOST.M1: O MITO DO LIMIAR DE ERRO DE TREINAMENTO,5197543,1
"During the process of software development, the reuse of materials is often performed in several ways and for different reasons. Among these ways, we highlight the reuse of source code developed by other programmer, which are embedded in a new source code and adapted in order to fit into this new context. However, even though programmers are able to achieve their goals by doing so, many times they reuse a source code without properly understand it, and, consequently, they do not understand their own source-code, generated through reuse. Based on Semiotic Engineering theory under the Human-Centered Computing perspective, we can see this source code as an interface which delivers to its user (in our case, the programmer who is reusing a source code) an encoded message from its designer (in our case, the programmer who developed the source code being reused), and, thus, we observe that there may have been several communicative implications resulting from the lack of the designer's understanding about the message he is delivering. Those implications are even more perceived when it comes to novice programmers, who are still building their computational thinking. In this thesis we investigated on how and why novice programmers reuse source codes and how they understand their own source codes while they build them by reusing other programmers' source codes. Furthermore, we offer a conceptual tool based on Semiotic Engineering's metacommunication template to support those programmers understanding and appropriating source codes. Besides that, we observe, through the studies conducted in order to analyze our tool, that not only can it support programmers understanding a source code, but also support them reflecting upon questions not usually addressed by them, related, for instance, to communicative aspects of their source codes. We hope this reflection makes programmers aware of the importance of those aspects, considering a scenario where their source codes may be reused by other programmers, in a continuous cycle of development and communication.",[TESE] Luana Müller.pdf,CIÊNCIA DA COMPUTAÇÃO,LUANA MULLER,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,08/12/2017,PORTUGUES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'source code reuse;novice programmers;appropriation;Semiotic Engineering',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",MILENE SELBACH SILVEIRA,108,reúso de código-fonte;programadores iniciantes;apropriação;Engenharia Semiótica,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Durante o processo de desenvolvimento de software, a prática de reúso é comumente utilizada por diversos motivos e de diversas formas. Dentre estas formas, destaca-se a cópia de trechos de códigos-fonte, muitas vezes desenvolvidos por outros programadores, que são incorporados a um novo código-fonte e adaptados para se adequar ao novo contexto. No entanto, apesar de o programador conseguir atingir desta forma seus objetivos, muitas vezes ele realiza este reúso sem compreender o código-fonte reusado, e, por consequência, sem compreender seu próprio código-fonte, criado com reúso. Embasando-nos na teoria da Engenharia Semiótica, sob a perspectiva da área de Human-Centered Computing, observamos este código-fonte como uma interface que entrega ao seu usuário (neste caso, o programador que está reusando um código-fonte) uma mensagem implícita do designer (neste caso, o programador que escreveu o código-fonte sendo reusado) e observamos que podem haver diversas implicações comunicativas decorrentes da falta de compreensão do designer sobre a mensagem que está sendo emitida. Essas implicações fazem-se ainda mais presentes no caso dos programadores iniciantes, que ainda estão construindo seu pensamento computacional. Nesta tese, conduzimos uma investigação sobre como e porque programadores iniciantes reusam códigos-fonte e como interpretam seus códigos-fonte quando estes foram construídos reusando outros códigos-fonte. Além disso, oferecemos uma ferramenta conceitual baseada no template de metacomunicação da Engenharia Semiótica para apoiar estes programadores na compreensão e apropriação de trechos de códigos-fonte para reúso. Observamos, através dos estudos para análise da ferramenta, que ela pode não apenas apoiar os programadores na compreensão dos códigos-fonte, mas, também, em sua reflexão sobre questões não comumente pensadas, relacionadas a aspectos comunicativos destes códigos. Esperamos que esta reflexão os conscientize da importância destes aspectos, em um cenário em que seus códigos podem ser, em algum momento, reusados por outros programadores, em um ciclo contínuo de desenvolvimento e comunicação.",TESE,UMA ABORDAGEM SEMIÓTICA PARA APOIAR PROGRAMADORES INICIANTES DURANTE O PROCESSO DE REÚSO E DE APROPRIAÇÃO DE CÓDIGOS-FONTE,5197573,1
"Tags work as a tool to support search engines on the task of finding resources by their subject
matter and the content they bring. However, users have expanded tag functionality with the intent
of expressing opinion, personal categorization, and even as a tool for spamming propagation. Users
can provide a great deal of qualitative data about their motivations for tagging, but little has been
explored regarding tagging behaviour and patterns from a quantitative point of view. As tags are
basically keywords that users resort as a tool for describing content, we analyzed their use from a
linguistics perspective. The results we found during a set of user studies we conducted supported us
on the task of designing a language-based approach that rely on tagging patterns as quantitative data
for the identification of tagging behaviour. During a case study to analyze our approach, we used
real datasets to compute the features we defined in our model in combination with clustering tools
applied to datasets from Flickr and Instagram, resulting in personas that explain users’ motivation
for tagging. We were able to point to the differences among tagging behaviour and how the choice
of structure or language for tagging could be used as source to identify users’ motivation for tagging
when sharing content online. We found that the patterns and motivation we have modeled in our
approach replicate in real datasets. This could benefit those who wish to use tags as source for
a variety of projects, such as modeling users’ behaviour through tags available online, choosing a
recommendation approach based on users’ motivations for tagging, preselecting data and tags as
source for recommendation according to system goals and user needs, identifying users opened to
receive contextual content, among others.",[TESE] Angelina de Carvalho Ziesemer.pdf,CIÊNCIA DA COMPUTAÇÃO,ANGELINA DE CARVALHO ALVAREZ ZIESEMER,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,14/12/2017,INGLES,PONTIFÍCIA UNIVERSIDADE CATÓLICA DO RIO GRANDE DO SUL,b'Tagging;Behaviour;Semiotic',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS, REALIDADE VIRTUAL E INTERAÇÃO HUMANO COMPUTADOR",MILENE SELBACH SILVEIRA,123,Tagging;Behaviour;Semiotic,CIÊNCIA DA COMPUTAÇÃO (42005019016P8),-,"Tags work as a tool to support search engines on the task of finding resources by their subject
matter and the content they bring. However, users have expanded tag functionality with the intent
of expressing opinion, personal categorization, and even as a tool for spamming propagation. Users
can provide a great deal of qualitative data about their motivations for tagging, but little has been
explored regarding tagging behaviour and patterns from a quantitative point of view. As tags are
basically keywords that users resort as a tool for describing content, we analyzed their use from a
linguistics perspective. The results we found during a set of user studies we conducted supported us
on the task of designing a language-based approach that rely on tagging patterns as quantitative data
for the identification of tagging behaviour. During a case study to analyze our approach, we used
real datasets to compute the features we defined in our model in combination with clustering tools
applied to datasets from Flickr and Instagram, resulting in personas that explain users’ motivation
for tagging. We were able to point to the differences among tagging behaviour and how the choice
of structure or language for tagging could be used as source to identify users’ motivation for tagging
when sharing content online. We found that the patterns and motivation we have modeled in our
approach replicate in real datasets. This could benefit those who wish to use tags as source for
a variety of projects, such as modeling users’ behaviour through tags available online, choosing a
recommendation approach based on users’ motivations for tagging, preselecting data and tags as
source for recommendation according to system goals and user needs, identifying users opened to
receive contextual content, among others.",TESE,A LANGUAGE-BASED APPROACH TO SUPPORT THE IDENTIFICATION OF TAGGING BEHAVIOUR,5495903,1
"Epidural nerve block is a blind medical procedure, where the correct place for
inserting the needle is associated with a feeling of loss of resistance (LOR) when
penetrating the epidural space. It aims at injecting the anesthetic fluid, avoiding the
puncturing of dura-mater membrane. Since the risk of failure is high (6 to 25%), with
potential damages to the patient, much training is required to minimize it. However, in
some hospitals in Brazil this practice is performed directly on the patients.
This work implements an epidural nerve block simulator that unifies three main
aspects as a strategy for the reduction of the failure rate on epidural procedures: The
development and use of computational models, the integration of a haptic device and the
gamification of user interactions.
The main contribution of this work is the development of a computational model
to dimension the thickness of tissues on epidural lumbar region (skin, fat, muscle,
interpinous ligament, ligamentum flavum, epidural space and dura-mater), based on the
input of values for age, height and weight. This model design is based on experimental
data from epidural procedures on 2000 real parturient, and the possibility to configure
these values enables the creation of a wide diversity of virtual patients for the practice.
The second relevant contribution is the creation of an experiment-based needle
model for epidural tissues reactions, considering the representation of its biomechanical
properties (thickness, stiffness, dampening, viscosity, static and dynamic friction), the
needle depth and its displacement. This model converts movements into forces,
allowing real-time force feedback reactions to the needle penetration movements.
A third contribution includes a set of developed models, considering the
interaction between the tissue and the needle, and taking into account its different tip
types, diameters, insertion angles and direction changes on penetration. These models
represent the haptic device response, in real time, to changes on the needle inclination,
movement restrictions and the simulation of internal restrictions on the continuum, as
well, where the needle tip corresponds to the metal edge from the haptic device.
The integrated haptic device has six degrees of freedom (DOF): three for
displacements and three for rotations on orthogonal directions. This device reacts to the
user interactions and reproduces the physical sensations of the needle insertions and its
restrictions to movements caused by the internal tissues of the epidural lumbar region,
based on the developed computational models.
The gamification strategy subdivides the epidural block procedure into tasks and
includes, based on the observation of real procedures, the following main steps: the
application of a local anesthesia, the Tuohy needle insertion and the saline pressure
tracking. It facilitates the tracking and evaluation of the user performance and
progression. The interface displays the current player score and his achievements, to
motivate the practice and the student improvement.
The implemented features on the visual interface include: (1) pre-scaling and
tridimensional representation of the epidural tissues and the virtual patient body in use,
(2) real-time adjustment of the biomechanical tissue properties, (3) record and
reproduction of needle movements, (4) computation and display of needle insertion
angles, (5) display of current needle depth and applied forces, (6) simulation of syringe
fit and saline fluid injection by pressing of the haptic device buttons. The
implementation uses the Unity engine and C# programming language.
The gamification has not been employed yet in any currently known epidural
nerve block simulators. Compared to other works, this is the only that automatically
adjusts the thickness of all epidural tissues based on the patient input data. Other
systems display only uni-axial answers to needle penetration, do not visually display all
the epidural internal tissues or the needle angles, or even include a tutorial mode.
Almost none of other works allows a free choice of the needle insertion point, a
requirement to train the midline location on the patient for epidural nerve block
procedures.",EpiduralNerveBlockSimulator-AndreLuizBrazil__VERSÃO FINAL.pdf,COMPUTAÇÃO VISUAL,ANDRE LUIZ BRAZIL,UNIVERSIDADE FEDERAL FLUMINENSE,24/01/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Tissue modeling;haptic device interaction;gamified interface;medical Procedures;serious games;loss of resistance (LOR);force model',-,AURA CONCI,0,Modelagem de tecidos;perda de resistência (LOR);interface gamificada;interação com dispositivo háptico;procedimentos médicos;jogos para treinamento;modelo de forças,COMPUTAÇÃO (31003010046P4),-,"O bloqueio do nervo epidural é um procedimento médico cego onde a inserção correta da agulha está associada à sensação de perda de resistência (LOR) à penetração dos tecidos, objetivando alcançar o espaço epidural sem perfurar a membrana dura-mater. O risco de falhas é alto (6 a 25%), com a possibilidade de sérios danos ao paciente. Para reduzi-lo, é necessário muito treinamento, sendo esse feito diretamente nos pacientes, em alguns hospitais no Brasil. 
	Esse trabalho implementa um simulador para a prática de procedimentos de bloqueio do nervo epidural que unifica três aspectos principais como uma estratégia para a redução da taxa de falha em bloqueios do nervo epidural: o desenvolvimento e uso de modelos computacionais, a integração de um dispositivo háptico e a gamificação das interações do usuário.
A principal contribuição do trabalho consiste no desenvolvimento de um modelo para dimensionar e representar a espessura dos diversos tecidos da região lombar epidural (pele, gordura, músculo, ligamento interspinhoso, ligamento flavum, espaço epidural e dura-mater), de acordo com valores de idade, peso e altura fornecidos. Esse modelo baseia-se em dados de 2000 experimentos em parturientes reais, e a possibilidade de configuração desses valores permite gerar uma grande diversidade de pacientes virtuais de forma realista para a prática epidural.
A segunda contribuição importante é a criação de um modelo para a reação dos tecidos epidurais, baseado em experimentos de inserção de agulhas, que considera suas propriedades biomecânicas (espessura, rigidez, amortecimento, viscosidade, fricção estática e dinâmica), bem como a profundidade e o deslocamento da agulha. Esse modelo relaciona movimentos às forças e permite uma resposta em termos de reação do háptico, em tempo real, aos movimentos de penetração realizados pelo usuário.
A terceira contribuição inclui um conjunto de modelos que considera a interação entre a agulha e os tecidos, de acordo com os possíveis tipos de pontas, espessuras, ângulos de introdução e desvios da direção de inserção, algo não modelado antes em outros trabalhos. Esses modelos permitem uma resposta do dispositivo háptico, em tempo real, a mudanças de inclinação da agulha e restrições de movimento, bem como a simulação de restrições internas no contínuo, onde a ponta da agulha corresponde a ponta de metal do dispositivo háptico.
 	O dispositivo háptico integrado possui seis graus de liberdade (DOF): três deslocamentos e três rotações em direções ortogonais. Reage a interações do usuário e reproduz sensações físicas das penetrações com agulhas e suas restrições a movimentos, provocadas por tecidos internos da região lombar epidural, baseando-se nos modelos computacionais desenvolvidos.
A estratégia de gamificação utilizada subdivide o procedimento de bloqueio epidural em tarefas e inclui, a partir da observação de procedimentos reais, as seguintes etapas principais: a aplicação de uma anestesia local, a inserção da agulha Tuohy e o monitoramento da pressão salina. Isto facilita o acompanhamento e a avaliação do progresso e dos resultados atingidos pelo usuário. A interface exibe a pontuação atual do jogador e suas conquistas, motivando a prática e o aprimoramento do médico.
	As funcionalidades implementadas na interface de visualização incluem: (1) pré-dimensionamento e representação tridimensional dos tecidos epidurais e do corpo da paciente virtual em uso, (2) Ajuste das propriedades biomecânicas dos tecidos em tempo real, (3) gravação e reprodução dos movimentos realizados, (4) cálculo e exibição dos ângulos de introdução da agulha, (5) exibição da profundidade atual da agulha e das forças aplicadas, (6) simulação do encaixe da seringa e da injeção do líquido salino, a partir pressionamento dos botões do dispositivo háptico. A implementação utiliza a engine Unity e a linguagem de programação C#. 
	A gamificação não havia sido aplicada ainda em nenhum dos simuladores de bloqueio de nervo epidural atualmente conhecidos. Comparado a outros trabalhos, esse é o único que realiza o ajuste automático da espessura dos tecidos epidurais, com base nos dados do paciente. Outros sistemas exibem apenas respostas uni-axiais à penetração, não apresentam uma visualização tridimensional de todos os tecidos epidurais internos, nem dos ângulos de inclinação da agulha, ou dispõem de um modo tutorial. Quase nenhum dos outros trabalhos permite a livre escolha do ponto de inserção da agulha, necessária para treinar a localização da linha mediana do paciente no procedimento epidural.",TESE,An Epidural Nerve Block Simulator Using Haptics and Gamification,4367108,
"Nowadays, the Internet is mostly used for content generation, sharing and access. Users are no longer interested at connecting their computers and mobiles to an end system located at somewhere in the network border, but at obtaining pieces of content. This paradigm shift in the Internet usage has motivated the proposal of information-centric networking (ICN) architectures. ICN architectures replaces the current connection oriented architecture to a general data oriented one. To establish this new paradigm, contents are identified by their names and the architecture uses those names for routing and retrieving contents. The routers, in addition to establishing routes towards contents, also temporarily store copies of the requested content. Therefore, ICN introduces a largely distributed in-networking caching infrastructure. When copies of contents are highly spread across the network routers, end-to-end routing algorithms between client and servers are no longer the most effective way of retrieving contents. In this work, we propose DIVER, a routing algorithm that explores the network in order to search and retrieve router copies that are closer to the clients compared at retrieving from the server. DIVER uses probe packets with the goal of finding the routers copies and the routers, upon receiving the probe packets, answer them by inserting their availability information in a space efficient data structure. DIVER keeps the most diversified as possible the acquired availability information in the explorations in order to raise the probability of satisfying future requests for the same content. We compare the proposed routing algorithm against other two literature proposals and the results show that DIVER is capable of retrieving 266% more content from the routers storage and, in one of the evaluated scenarios, DIVER retrieved around 96,8% of the content without forwarding requests to the server.",Dissertação___VERSÃO FINAL.pdf,SISTEMAS DE COMPUTAÇÃO,IAN VILAR BASTOS,UNIVERSIDADE FEDERAL FLUMINENSE,07/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Information-Centric Networking;Diversity-Based-Search-and-Routing;Bloom Filters',-,IGOR MONTEIRO MORAES,0,Redes Orientedas a Conteúdo;Busca e Roteamento Baseados na Diversidade;Filtro de Bloom,COMPUTAÇÃO (31003010046P4),-,"Atualmente a Internet é utilizada principalmente para geração, compartilhamento e acesso a conteúdos. Os usuários deixaram de estar interessados em conectar seu computador ou dispositivo móvel a um sistema final localizado em algum ponto da borda da Internet e passaram a estar interessados em obter pedaços de conteúdo. Essa mudança de paradigma no uso da Internet motivou a proposta das redes orientadas a conteúdo. As redes orientadas a conteúdo visam substituir a atual arquitetura centrada na conexão entre sistemas finais por uma arquitetura de propósito geral centrada nos dados. Para estabelecer esse novo paradigma, os conteúdos são identificados através de seus nomes, os quais são utilizados para o roteamento das requisições. Os roteadores, além de estabelecer rotas para os conteúdos e encaminhar as requisições, também armazenam temporariamente cópias dos conteúdos requisitados. Portanto, as redes orientadas a conteúdo introduzem uma infraestrutura de rede altamente distribuída de caching. Com as cópias dos conteúdos disseminadas nos roteadores da rede, algoritmos de roteamento fim-a-fim entre clientes e servidores deixam de ser a maneira mais eficiente de recuperar os conteúdos. Neste trabalho é proposto o DIVER, um algoritmo de roteamento para explorar os roteadores da rede com o objetivo de buscar e recuperar as cópias dos conteúdos que estejam mais próximas dos clientes em relação ao servidor. O DIVER utiliza pacotes de sondagem para localizar as cópias armazenadas. Os roteadores, ao receberem as sondagens, inserem as suas informações de disponibilidade em uma estrutura de conteúdos compacta para respondê-las. Para atender o máximo de futuras requisições para um mesmo conteúdo o DIVER mantém mais diversificadas possível as informações de disponibilidade adquiridas durante as explorações. O algoritmo de roteamento proposto é comparado via simulações com dois outros algoritmos de roteamento da literatura e os resultados mostram que o DIVER é capaz de ter um desempenho 266% superior em termos de aproveitamento do armazenamento temporário dos roteadores e, em um dos cenários analisados, recupera cerca de 96,8% do conteúdo sem a necessidade de as requisições chegarem até o servidor.",DISSERTAÇÃO,Roteamento em Redes Orientadas a Conteúdo,4948416,
"A growing number of data- and compute-intensive experiments have been modeled as scientific workflows in the last decade. Meanwhile, clouds have emerged as a prominent environment to execute this type of applications.  In this scenario, the investigation of workflow scheduling strategies, aiming at reducing its execution times, became a top priority and a very popular research field. However, few works consider the problem of data file assignment when solving the task scheduling problem.  Usually, a workflow is represented by a graph where nodes represent tasks and the scheduling problem consists in allocating tasks to machines to be executed at a predefined time aiming at reducing the makespan of the whole workflow. In this work, we show that the scheduling  of scientific workflows can be improved when both task scheduling and the data file assignment problems are treated together. Thus, we propose a new workflow representation, where nodes of the workflow graph represent either tasks or data files, and define the Task Scheduling and Data Allocation Problem, considering this new model. We formulated this problem as an integer programming problem. Moreover, a hybrid evolutionary algorithm for solving it is also introduced.",Dissertação___Versão final.pdf,SISTEMAS DE COMPUTAÇÃO,LUAN TEYLO GOUVEIA LIMA,UNIVERSIDADE FEDERAL FLUMINENSE,17/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Scheduling Problem;Data Allocation;Scientific Workflow;Metaheuristic',-,LUCIA MARIA DE ASSUMPCAO DRUMMOND,0,Problema de escalonamento;Alocação de Dados;Workflow Científico;Metaheurística,COMPUTAÇÃO (31003010046P4),-,"Na última década, um número crescente de experimentos computacionalmente intensivos envolvendo grandes volumes de dados têm sido modelados na forma de workflows científicos. Ao mesmo tempo, as nuvens computacionais surgem como um ambiente promissor para executar esse tipo de aplicação. Neste cenário, a investigação de estratégias de escalonamentos se tornaram essenciais, sendo este um campo de pesquisa extremamente popular. No entanto, poucos trabalhos consideram o problema da alocação de dados durante a resolução do problema de escalonamento de tarefas. Um workflow é geralmente representado como um grafo, no qual os nós equivalem às tarefas e, neste caso, o problema de escalonamento consiste em alocar essas tarefas a máquinas que as executarão em um tempo pré definido.  O objetivo é reduzir o tempo total de execução de todo o workflow. Neste trabalho é mostrado que o escalonamento de workflows científicos pode ser melhorado quando os problemas de escalonamento de tarefa e alocação de dados são tratados de forma conjunta. Para isso, uma nova representação, na qual os nós do grafo representam tanto tarefas como dados, é proposta. Além disso, o problema de Escalonamento de Tarefas e Alocação de Dados é definido, considerando esse novo modelo. Esse problema foi formulado como um problema de programação inteira. Por fim, um algoritmo evolucionário híbrido capaz de escalonar tarefas e alocar os dados em ambientes de nuvens computacionais também é apresentado.",DISSERTAÇÃO,Escalonamento de Tarefas e Alocação de Arquivos de Dados de Workflows Científicos em Nuvens Computacionais,4988696,
"In open multi-agent systems, norms are being used to regulate the behavior of the autonomous, heterogeneous and independently designed agents. Norms describe the behavior that can be performed, must be performed, and cannot be performed in the system. One of the main challenges on developing normative systems is that norms may conflict with each other. Norms are in conflict when the fulfillment of one norm violates the other and vice-versa. In previous works, the Conflict Checkers consider that conflicts can be detected by simply analyzing pairs of norms. However, there may be conflicts that can only be detected when we analyze several norms together. This work presents a Conflict Checker, program that detect conflicts, able to detect conflicts between two or more norms at the same time. The Conflict Checker strategy is divided into three steps: (1) applying filters to smooth the computational cost in most cases, since the problem is intrinsically exponential (SHOHAM; TENNENHOLTZ, 1995), (2) deontic transformation is used to analyze various norms at the same time and (3) intersection among the norms for verification of conflicts. A new normative language more expressive, represented by a BNF grammar, was developed to define norms and a Conflict Checker was implemented in tool format. Two validation principles were applied: software testing and formal verification. Software testing proposed numerous test cases to test the ability to capture conflicts and the benefits of using filters to soften the complexity of the problem. The formal verification used the DBC (Design-by-Contract) technique implemented by JML (Java Modeling Language) in KeY platform to prove the correctness of the methods. The application of the validation techniques confirmed that Conflict Checker was able to detect conflicts not found in other approaches. The strategy developed in this thesis emerges as a new syntax for definition and verification of conflicts in multi-agent systems.",Tese___VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,EDUARDO AUGUSTO SILVESTRE,UNIVERSIDADE FEDERAL FLUMINENSE,08/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'multi-agent systems;normative multi-agent system;normative conflict;norm',-,VIVIANE TORRES DA SILVA,0,sistemas multiagentes;sistemas multiagentes normativos;conflitos normativos;norma,COMPUTAÇÃO (31003010046P4),-,"Em sistemas multiagentes abertos, normas são utilizadas para regular o comportamento dos agentes autônomos, heterogêneos e concebidos de forma independente. Normas descrevem o comportamento que pode ser executado, deve ser executado e não pode ser executado no sistema. Um dos principais desafios no desenvolvimento de sistemas normativos é que as normas podem entrar em conflito umas com as outras. Normas estão em conflito quando o cumprimento de uma norma viola a outra e vice-versa. Em trabalhos anteriores, os Conflict Checkers consideravam que conflitos podem ser detectados simplesmente analisando as normas em pares. Entretanto, existem conflitos que só podem ser detectados apenas quando várias normas são analisadas em conjunto. Este trabalho apresenta um Conflict Checker, programa que detecta conflitos, capaz de verificar conflitos entre duas ou mais normas ao mesmo tempo. A estratégia do Conflict Checker é dividida em três passos: (1) aplicação de filtros para suavizar o custo computacional na maioria dos casos, pois, o problema é intrinsicamente exponencial (SHOHAM; TENNENHOLTZ, 1995), (2) transformação deôntica é utilizada para análise de várias normas ao mesmo tempo e (3) interseção entre as normas para verificação dos conflitos. Uma nova linguagem normativa mais expressiva, representada por uma gramática BNF, foi desenvolvida para definição de normas e um Conflict Checker foi implementado em formato de ferramenta. Dois princípios de validação foram aplicados: teste de software e verificação formal. O teste de software propôs inúmeros casos de teste para testar a capacidade de capturar conflitos e os benefícios da utilização dos filtros para suavizar a complexidade do problema. A verificação formal utilizou a técnica de DBC (Design-by-Contract - Design por Contrato) implementada pela JML (Java Modeling Language) na plataforma KeY para provar a corretude dos métodos. A aplicação das técnicas de validação confirmou que o Conflict Checker foi capaz de detectar conflitos não encontrados em outras abordagens. A estratégia desenvolvida nesta tese surge como uma nova sintaxe para definição e verificação de conflitos em sistemas multiagentes.",TESE,Verificação de Conflitos entre Múltiplas Normas em Sistemas Multiagentes,4991505,
"The present work deals with the Car Renter Salesman Problem (CaRS), which is a Traveling Salesman Problem variant. The CaRS concerns tourists intending to travel through a set of cities using rented vehicles at minimum cost. The aim of the current research is to establish an optimal route and renting vehicle type to each trip. Since CaRS is a NP-Hard problem, this work presents both exact and heuristic approaches. To be more especific, two mathematical formulations are presented. The first one was based on a quadractic formulation from literature, while the second one was a newly developed linear formulation. The heuristic approach was based on a Multi-Start Iterated Local Search metaheuristic. Its local search step was based on the Random Variable Neighborhood Descent methodology. Computational experiments showed that the exact approaches were able to optimally solve 35 out of 47 instances, 17 of these were first proved herein. As for the heuristic approach, tests in Euclidean instances showed that the proposed method outperformed current state-of-art results.",Dissertação___VERSÃO FINAL.pdf,ALGORITMOS E OTIMIZAÇÃO,SAVIO SOARES DIAS,UNIVERSIDADE FEDERAL FLUMINENSE,10/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Car Renter Salesman;Heuristics;Integer Programming;Transport and Tourism',-,LUIZ SATORU OCHI,0,Problema do Caixeiro Alugador;Heurísticas;Programação Inteira;Turismo e Transporte,COMPUTAÇÃO (31003010046P4),-,"Este trabalho aborda o Problema do Caixeiro Alugador (PCA), uma variante do Problema do Caixeiro Viajante. No PCA um turista deseja viajar por um conjunto de cidades alugando diferentes tipos de veículo a um custo mínimo. O objetivo é determinar a rota ótima e o tipo de veículo que será alugado para cumprir cada viagem. Por se tratar de um problema NP-Difícil, neste trabalho são apresentadas duas abordagens exatas e uma heurística para sua resolução. Dentre as abordagens exatas, a primeira é baseada em uma formulação quadrática inteira da literatura e a segunda, uma nova formulação linear inteira. A abordagem heurística é baseada na meta-heurística Multi-Start Iterated Local Search com uma etapa de busca local inspirada na metodologia de Random Variable Neighborhood Descent. Os resultados computacionais obtidos nas abordagens exatas foram capazes de encontrar o ótimo de 35 das 47 instâncias testadas, sendo que 17 destas tiveram o ótimo provado pela primeira vez. A abordagem heurística foi aplicada às instâncias euclidianas e os resultados obtidos são comparadas com o estado da arte, mostrando que a abordagem proposta possui um melhor desempenho.",DISSERTAÇÃO,Abordagens para Resolução do Problema do Caixeiro Alugador,4991612,
"The computation of geodesic distances is an important research topic in geometry processing
and shape analysis on meshes as it is a basic component of many methods used in
these areas. Different approaches have been proposed for computing geodesic distances
including exact methods and approximate ones. Approximate methods are in many cases
the best choice as they are more efficient than the exact methods and yield results that
are not far from the optimum distance. Many methods have been proposed for approximate
geodesic distance computation, for instance, variations of the Fast Marching method
and more recently by spectral and diffusion flow-based methods. These approaches are
very efficient for distance query but usually depend on a pre-processing step which can
be computationally intensive. In this work, we present an iterative parallel algorithm
for computing approximate geodesic distances on meshes that do not require any preprocessing
step. The convergence of our iterative algorithm depends on the number of
rings around the source points from which distance information propagates. Hence, our
method is particularly efficient for multisource geodesic distance computation. In the experiments,
we show how our method scales with the size of the problem and compare its
mean error and processing times with such measures computed with other methods found
in the literature. We also demonstrate its use for solving two common geometry processing
problems: the regular sampling problem and the Voronoi tessellation on meshes.",Dissertação___VERSÃO FINAL.pdf,COMPUTAÇÃO VISUAL,LUCIANO ARNALDO ROMERO CALLA,UNIVERSIDADE FEDERAL FLUMINENSE,02/02/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'fast marching;geodesic distance;triangular meshes;tridimensional models',-,ANSELMO ANTUNES MONTENEGRO,0,marcha rápida;distância geodésica;malhas triangulares;modelos tridimensionais,COMPUTAÇÃO (31003010046P4),-,"O cálculo de distâncias geodésicas é um importante tópico de pesquisa no processamento
de geometria e análise de formas em malhas, pois é um componente básico de muitos
métodos usados nessas áreas. Diferentes abordagens têm sido propostas para calcular
distâncias geodésicas incluindo métodos exatos e aproximados. Métodos aproximados são
em muitos casos a melhor escolha, pois são mais eficientes do que os métodos exatos e,
ao mesmo tempo, produzem resultados próximos da distância exata. Muitos métodos
têm sido propostos para o cálculo de distâncias geodésicas aproximadas, por exemplo,
variações do método Fast Marching e mais recentemente por métodos espectrais e de
fluxo de difusão. Estas abordagens são muito eficientes para a consulta da distância,
mas geralmente dependem de uma etapa de pré-processamento que pode ser computacionalmente
intensa. Neste trabalho, apresentamos um algoritmo paralelo iterativo para
calcular distâncias geodésicas aproximadas em malhas que não requer nenhum passo de
pré-processamento. A convergência do algoritmo iterativo proposto depende do número
de anéis em torno dos pontos de origem, a partir dos quais a informação da distância se
propaga. Assim, nosso método é particularmente eficiente para a computação de distâncias
geodésicas de múltiplas fontes. Nos experimentos, mostramos como nosso método
escala com o tamanho do problema e comparamos seu erro médio e os tempos de processamento
com os de outros métodos encontrados na literatura. Também demonstramos
seu uso para resolver dois problemas comuns de processamento de geometria: o problema
de amostragem regular e a tesselação de Voronoi em malhas.",DISSERTAÇÃO,An Iterative Parallel Algorithm for Computing Geodesic Distances on Triangular Meshes,4992030,
"The problem of inpainting consists of filling missing or damaged regions in images and videos in such a way that the filling pattern does not produce artifacts that deviate from the original data. In addition to restoring the missing data, the inpainting technique can also be used to remove undesired objects. The search for surface inpainting solutions became more relevant when the 3D geometry acquisition devices came into mainstream. Meshes are typically generated from point clouds that are obtained from 3D shape acquisition devices. After the scanning and triangulation processes, the meshes often have holes due to defects in the original cloud. One of the most common causes is occlusion.

In this dissertation, we address the problem of inpainting on surfaces through a new method based on dictionary learning and sparse coding. The use of the dictionary learning techniques to solve the inpainting problem in images has obtained successful results; on the other hand, its application on surfaces described by triangle meshes is still a challenge.

Our method learns the dictionary through the subdivision of the mesh into patches and rebuilds the mesh via a method of reconstruction inspired by the Non-local Means method on the computed sparse codes. One of the advantages of our method is that it is capable of filling the missing regions and simultaneously removes noise and enhances important features of the mesh. Moreover, the inpainting result is globally coherent as the representation based on the dictionaries captures all the geometric information in the transformed domain. We present two variations of the method: a direct one, in which the model is reconstructed and restored directly from the representation in the transformed domain and a second one, adaptive, in which the missing regions are recreated iteratively through the successive propagation of the sparse code computed in the hole boundaries, which guides the local reconstructions. The second method produces better results for large regions because the sparse codes of the patches are adapted according to the sparse codes of the boundary patches. Finally, we present and analyze experimental results that demonstrate the performance of our method compared to the literature.",Dissertação___VERSÃO FINAL.pdf,COMPUTAÇÃO VISUAL,LIZETH JOSELINE FUENTES PEREZ,UNIVERSIDADE FEDERAL FLUMINENSE,03/02/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Surface Inpainting;Dictionary learning;Sparse coding;Non-local means;Poison equation;Hole-filling methods;Triangular meshes',-,ANSELMO ANTUNES MONTENEGRO,0,Inpainting em superfícies;aprendizagem de dicionários;codificação esparsa;Non-local means;equação de Poisson;métodos preenchimento de buracos;malhas triangulares,COMPUTAÇÃO (31003010046P4),-,"O problema de inpainting consiste no preenchimento de regiões perdidas ou deterioradas em imagens e vídeos, de tal forma que o padrão de preenchimento não produza artefatos que destoem do dado original. Além da restauração de dados perdidos, a técnica de inpainting também pode ser usada para remoção de objetos indesejados. A possibilidade de aquisição de dados geométricos a partir da popularização de dispositivos de aquisição de geometria 3D tornou relevante a questão de inpainting em malhas. Malhas são geradas a partir de nuvens de pontos que são obtidas dos dispositivos de aquisição de forma 3D. Após os processos de digitalização e triangulação, as malhas apresentam frequentemente orifícios devido a defeitos na nuvem original. Uma das causas mais comuns é a oclusão.

Na presente dissertação, abordamos o problema de inpainting em superfícies através de um novo método baseado em aprendizado de dicionários e codificação esparsa. O uso da técnica de aprendizado de dicionários para resolver o problema de inpainting em imagens têm obtido resultados bastante satisfatórios; por outro lado, sua aplicação em superfícies descritas por malhas de triângulos ainda é um desafio. Nosso método aprende o dicionário através da subdivisão da malha em retalhos (patches) e reconstrói a malha através de um método de reconstrução inspirado no método Non-local Means sobre os códigos esparsos. Nosso método tem como vantagens a capacidade de preencher as regiões perdidas ao mesmo tempo em que remove ruído e realça características importantes da malha. Além disso, é globalmente coerente, uma vez que a representação esparsa baseada nos dicionários captura toda a informação geométrica no domínio transformado. Apresentamos duas variações do método: uma direta na qual o modelo é reconstruído e restaurado diretamente da representação no domínio transformado e uma segunda, adaptativa, na qual as regiões perdidas são recriadas iterativamente através da propagação sucessiva do código esparso, computado no bordo dos furos, que guia reconstruções locais. O segundo método produz melhores resultados para regiões extensas porque os códigos esparsos dos novos retalhos são adaptados de acordo com os códigos esparsos dos retalhos do bordo. Finalmente, apresentamos e analisamos resultados experimentais que demonstram o desempenho de nosso método comparado com os da literatura.",DISSERTAÇÃO,Dictionary Learning-based Inpainting on Triangular Meshes,4992070,
"Digital Games and other interactive applications have been allowing users to interact with games in many levels. Nowadays games and applications are using many different intuitive interfaces, such as touch screens, motion controllers and body movements from the user. Several researchers have developed ways to process human motion by either classifying the action or reconstructing body poses based on accelerometers, gyroscopes, heat sensors and cameras. This work presents a novel strategy for whole-body motion classification using Long Short-Term Memory Recurrent Neural Networks (LSTM), by only using sparse data from two separated Inertial Measurements Units (IMU) sensors. Our model, Peek, shows efficiency by achieving an overall accuracy of 96% during tests.",Dissertação___VERSÃO FINAL.pdf,COMPUTAÇÃO VISUAL,RAFAEL REGO DRUMOND,UNIVERSIDADE FEDERAL FLUMINENSE,14/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Motion Classifier;IMU Device;Deep Learning;Recurrent Neural Networks;Sparse Data;LSTM;Machine Learning',-,ESTEBAN WALTER GONZALEZ CLUA,0,Classificador de Movimento;Sensor IMU;Aprendizado Profundo;Redes Neurais Recorrentes;Dados Esparsos;LSTM;Aprendizado de Máquina,COMPUTAÇÃO (31003010046P4),-,"Jogos digitais e outras aplicações interativas têm permitido que usuários interajam com o jogo em diferentes formas. Atualmente jogos e aplicações estão usando muitas interfaces intuitivas diferentes, como telas sensíveis ao toque, controles com acelerômetros e com o movimento natural do corpo do usuário. Vários pesquisadores desenvolveram maneiras de processar o movimento humano, classificando a ação ou reconstruindo poses corporais com base em acelerômetros, giroscópios, sensores de calor e câmeras. Este trabalho apresenta uma nova estratégia para a classificação de movimentos de todo o corpo usando Redes Neurais Recorrentes de Memória de Curto Prazo (LSTM), usando apenas dados esparsos de dois sensores de Unidades de Medição Inerciais (IMU). Este modelo, Peek, demonstra eficiência obtendo uma precisão global de acertos de 96 % durante os testes.",DISSERTAÇÃO,Peek: Classificação de Movimento Utilizando Dados Esparsos de Membros Superiores em Redes com LSTM,4993247,
"Developing sofware is a complex activity with risks involved. Many of the risks have an association with volatile requirements. Such volatility motivated the creation and adoption of agile and hybrid methodologies, which are now a reality in organizations. However, the volatility of requirements can also be found in scientific research, which increasingly depends on complex computational simulations. One of the problems that occurred in the last century was the ability to collect data for experiments, with the advancement of computing this problem was in its large part solved. However, the resolution of this problem leads to a change in the way of working of scientists, requiring larger teams to process and analyze this data. This change has not been followed by the way scientists manage the process of experimentation, creating a chaotic environment. Requirements are unstable and change constantly during execution of the project and its complexity requires a lot of effort to understand the research domain. This dissertation presents an approach, called SciAgile, which applies a series of different agile methodologies in the life cycle of the simulation-based scientific experiment. SciAgile focuses on the management of a chaotic environment, where it aims to respond to the rapid changes requirements may come to suffer and to broaden the communication of the team to disseminate the changes easily. In this way, the life cycle of the scientific experiment becomes oriented to a set of practices that provide the basis for improving the process of scientific experimentation. To evaluate the proposed approach, we conducted an experimental study with two reference scientific workflows of the area of in-silico experimentation. Results provide indications that the adoption of agile methodologies practices are capable of improving the process of scientific experimentation.",Dissertação___VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,AUGUSTO CONSULMAGNOS ROMEIRO,UNIVERSIDADE FEDERAL FLUMINENSE,03/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Agile Methods;Cientific Experiment;Cientific Workflows',-,DANIEL CARDOSO MORAES DE OLIVEIRA,0,Metodologias Ágeis;Experimento Científico;Workflows Científicos,COMPUTAÇÃO (31003010046P4),-,"Desenvolver software é uma atividade complexa e com riscos envolvidos. Muitos dos riscos existentes tem associação com requisitos voláteis. Tal volatilidade motivou a criação e a adoção de metodologias ágeis e híbridas, que hoje são uma realidade nas organizações. Porém, a volatilidade dos requisitos
também pode ser encontrada na pesquisa científica, que cada vez mais depende de complexas simulações computacionais. Um dos problemas que ocorriam no século passado era a capacidade de coletar dados para os experimentos, com o avanço da computação este problema foi em sua grande parte resolvido. Porém a resolução deste problema ocasiona na mudança da forma de trabalho dos cientistas, necessitando de equipes maiores para o processamento e a análise destes dados. Esta mudança não foi acompanhado
na forma como os cientistas gerenciam o processo de experimentação científico tornando um ambiente caótico. Requisitos são instáveis e mudam constantemente durante a execução do projeto e sua complexidade exige muito esforço para a compreensão do domínio da pesquisa. Esta dissertação apresenta uma abordagem, denominada SciAgile, que aplica uma série de práticas de diferentes metodologias ágeis no ciclo de vida do experimento científico baseado em simulação. SciAgile foca justamente na gerência de um ambiente caótico, onde visa responder às mudanças rápidas que os requisitos possam vir a sofrer e ampliar a comunicação da equipe para disseminar o conhecimento mais facilmente. Dessa forma, o ciclo de vida do experimento científico se torna orientado a um conjunto de práticas que fornecem a base para melhoria do processo de experimentação científica. Para avaliar a proposta, foi realizado um estudo experimental com dois workflows científicos de referência da área de experimentos in silico. Resultados fornecem indícios que a adoção de práticas de metodologias ágeis é capaz de melhorar o processo de experimentação científica.",DISSERTAÇÃO,SciAgile: Uma Abordagem Ágil em Processos de Experimentação Científica,4998520,
"This thesis deals with two one-to-one pickup and delivery problems: the Multi-vehicle one-to-one Pickup and Delivery Problem with Split Loads (MPDPSL) and the Multi-Vehicle Profitable Pickup and Delivery Problem (MVPPDP). Both are NP-hard problems and important for various practical applications, including bulk product transportation by ships and bike-sharing systems. Three challenging vehicle routing attributes are combined in this study, one-to-one pickup and delivery, split loads, and selection of customers, which are known to require advanced local-search neighborhoods and exploration procedures. Initially, we developed a heuristic algorithm based on Iterated Local Search (ILS) and Random Variable Neighborhood Descent (RVND) to solve the MPDPSL. The core of this algorithm consists of a new large-neighborhood search that efficiently deals with pickup and delivery locations and split loads reducing the problem of finding the best insertion combination of a pickup and delivery pair into a route to a resource-constrained shortest path problem, which is solved via dynamic programming. Next, non-dominated labels are obtained for each separate route and a knapsack problem is solved to find the best insertion plan considering the whole solution. Computational experiments on benchmark instances showed the great performance of the proposed algorithm, finding new best solutions on 92 out of 93 instances and also outperforming previous methods in average. New instances for the MPDPSL and also new results are reported, producing consistently good solution on several runs. The good results found on the MPDPSL motivated us to adapt this algorithm for solving the MVPPDP. Then, again, an ILS combined with a RVND was used, but now the idea was to allow the algorithm to accept infeasible solutions during its search process. In order to reduce the search space of infeasible solutions, a granular local search concept was also incorporated into the algorithm. The proposed algorithm had a great performance on instances from the literature, finding 20 better or equal results on 36 instances than previous methods.",Tese___VERSÃO FINAL.pdf,ALGORITMOS E OTIMIZAÇÃO,MATHEUS NOHRA HADDAD,UNIVERSIDADE FEDERAL FLUMINENSE,04/04/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,"b'vehicle routing, one-to-one pickup and delivery;split loads;customer selection;metaheuristics;large neighborhoods'",-,LUIZ SATORU OCHI,0,roteamento de veículos;coleta e entrega um-para-um;coleta e entrega fracionária;seleção de clientes;metaheurísticas;grandes vizinhanças,COMPUTAÇÃO (31003010046P4),-,"Esta tese trata dois problemas de coleta e entrega um-para-um: o Problema de Roteamento de Veículos com Coleta e Entrega Fracionária Um-para-um (PRVCEFU) e o Problema de Roteamento de Veículos com Coleta e Entrega Premiada (PRVCEP). Ambos são problemas NP-difíceis e importantes para várias aplicações práticas, incluindo o transporte de produtos a granel por navios e sistemas de compartilhamento de bicicletas. Três atributos desafiadores de roteamento de veículos são combinados neste estudo, coleta e entrega um-para-um, coleta e entrega fracionária e seleção de clientes, que são conhecidos por exigirem vizinhanças avançadas de buscas locais e procedimentos de exploração eficientes. Inicialmente, desenvolvemos um algoritmo heurístico baseado na metaheurística Iterated Local Search (ILS) e no método Random Variable Neighborhood Descent (RVND) para resolver o PRVCEFU. O núcleo deste algoritmo consiste em uma busca em grande vizinhança que lida eficientemente com localizações de coleta e entrega e cargas fracionárias, reduzindo o problema de encontrar a melhor combinação de inserção de um par de coleta e entrega em uma rota para um problema de caminho mínimo com recursos limitados, que é resolvido via programação dinâmica. Em seguida, os rótulos não dominados são obtidos para cada rota separada e um problema da mochila é resolvido para encontrar o melhor plano de inserção, considerando a solução completa. Experimentos computacionais em instâncias da literatura mostraram o desempenho competitivo do algoritmo proposto, encontrando novas melhores soluções em 92 de 93 casos e também superando os métodos anteriores na média. Novas instâncias para o PRVCEFU e também novos resultados são relatados, produzindo consistentemente boas soluções em várias execuções. Os bons resultados encontrados no PRVCEFU nos motivaram a adaptar este algoritmo para resolver o PRVCEP. Então, novamente, um ILS combinado com o RVND foi usado, porém agora a ideia foi permitir que o algoritmo aceitasse soluções inviáveis durante seu processo de busca. A fim de reduzir o espaço de busca de soluções inviáveis, o conceito de busca local granular também foi incorporado ao algoritmo. O algoritmo proposto teve um desempenho competitivo em instâncias da literatura, encontrando 20 resultados melhores ou iguais em 36 instâncias do que os métodos anteriores.",TESE,A Large Neighborhood-Based Heuristic for One-to-One Pickup and Delivery Problems,4998703,
"The election process is one of the most important events for a country, especially for the executive position such as mayor, governor, and president. The destiny of a nation may be drastically changed by the election results. Effects on economy and politics are felt even during the electoral campaign. Prediction pools are hired to post predictions so as to minimize surprises. Making predictions is part of human nature to anticipate events and thus get better prepared for possible future scenarios. The culture of prediction is rooted in the most diverse areas such as financial market, sports and sports betting. The forecasting process is an indispensable tool so as to develop and refine our decisions more assertively to obtain better results. Traditional methods of forecasting election results involve interviewing a sample of the population questioning their vote intentions. These methods are expensive, time consuming and subject to interviewer bias which can influence voter opinion only by their body language when talking about candidates.The use of social media to infer voters’ opinions has been promising and people are increasingly exposing their opinions through social networks, influencing friends and defending their political beliefs by simply using smartphones. Taking into account this scenario where voters’ opinions are widely available in social networks, this study proposes a model for forecasting election results based on linear regression method where we use the polarity of the comments about candidates in editorial content in political news, as well as the approval and disapproval of this comment in the form of likes and dislikes, respectively. This model was inspired by studies that show the power of forecasting using Twitter and Facebook and aims to be a cheaper and agile alternative for traditional forecast methods. For the construction of the proposed model and test, we collected comments from the main brazilian online newspaper conducted, approximately, 4 months before the municipal elections of 2016 to 14 municipalities and a total of 70 candidates.",Dissertação___VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,WILLIAM WANDERLEY DA SILVA,UNIVERSIDADE FEDERAL FLUMINENSE,05/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Election forecast;Online newspaper;Sentiment analysis;Social network;Collective intelligence',-,ANA CRISTINA BICHARRA GARCIA,0,Previsão de eleições;Jornais online;Análise de sentimentos;Mídias Sociais;Inteligência coletiva,COMPUTAÇÃO (31003010046P4),-,"O processo de eleição é um dos mais importantes eventos para o futuro da sociedade, especialmente quando se trata de posições de prefeito, governador e presidente. O destino de uma nação pode ser drasticamente modificado pelo resultado de uma eleição. Efeitos na política e na economia são sentidos mesmo durante a campanha eleitoral. As pesquisas eleitorais são ferramentas poderosas que nos ajudam a minimizar as surpresas do resultado da eleição e a entender como está a percepção do candidato por meio do ponto de vista do eleitor. Fazer previsões é parte da natureza humana para antecipar acontecimentos e se sentir mais preparado para cenários futuros. A cultura de previsões está enraizada nas mais diversas áreas, tais como: mercado financeiro ou esportes, com a finalidade de preparar melhor as equipes para os adversários e, também, estar presente frequentemente nos ramos de apostas. Métodos tradicionais de previsão de resultado de eleição envolvem entrevistar uma amostra da população questionando sobre suas intenções de voto em relação aos candidatos que estão disputando determinada eleição. Estes métodos são caros, consomem tempo e estão sujeitos ao viés do entrevistador, que pode influenciar na opinião do eleitor apenas por sua linguagem corporal ao falar sobre os candidatos. O uso de mídias sociais para inferir a opinião dos eleitores tem se mostrado promissor e se utiliza do fato de que as pessoas estão cada vez mais expondo suas opiniões nas redes sociais, influenciando amigos e defendendo suas convicções de uma forma rápida e eficiente, por meio do uso de seus smartphones, que facilitam a exposição de suas opiniões no âmbito político e em redes sociais. Analisando este cenário, onde as opiniões dos eleitores estão de forma bastante aberta nas redes sociais, esta dissertação propõe um modelo de previsão de resultados de eleições baseado no método de regressão linear, no qual consideramos a polaridade dos comentários sobre candidatos realizados em conteúdo editorial em notícias políticas, bem como a aprovação e desaprovação deste comentário em forma de “likes” e “dislikes”, respectivamente. Este modelo foi inspirado em estudos que mostram o poder de previsão utilizando Twitter e Facebook e tem como objetivo ser uma alternativa mais barata e ágil para os métodos tradicionais de pesquisa de opinião. Para a construção e teste do modelo proposto, coletamos comentários dos principais jornais online do Brasil, realizados, aproximadamente, há 4 meses antes das eleições municipais de 2016 em relação ao processo de eleição de 14 municípios, com um total de 70 candidatos.",DISSERTAÇÃO,"Um Modelo de Previsão de Resultado de Eleições Baseado em Comentários, Likes e Dislikes em Mídias Sociais de Conteúdo Editorial",4999908,
"The Cell Formation Problem is a NP-hard optimization problem which consists of grouping a set of machines into cells, dedicated to the production of a family of parts, minimizing intercell moves and cell machine subutilization. These machine-part groups aim at reducing costs and increase the efficiency of a cellular manufacturing system. This problem is well studied and explored in the literature, with hundreds of papers and algorithms developed in the last decades. However, the inclusion of several relevant practical factors is a more recent concern that leads to an increase in the problem complexity, making it difficult to include many factors simultaneously. In this study, we solve the Cell Formation Problem considering Operation Sequence and Alternative Process Plans, where the sequence of operations are given and for each part one process plan must be chosen. We propose an algorithm based on ILS metaheuristic using a VND method with a random ordering of neighborhoods in local search phase. The proposed algorithms are compared with some available algorithms in the literature, presenting meaningful results in a reasonable computational time.",Tese___VERSÃO FINAL.pdf,ALGORITMOS E OTIMIZAÇÃO,IVAN CESAR MARTINS,UNIVERSIDADE FEDERAL FLUMINENSE,07/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Metaheuristics;Cellular Manufacturing;Group Technology',-,FÁBIO PROTTI,0,Meta-heurísticas;Manufatura Celular;Tecnologia de Grupo,COMPUTAÇÃO (31003010046P4),-,"O Problema de Formação de Células de Manufatura é um problema NP-difícil em que dado um conjunto de máquinas e de partes (de produtos) a serem processadas, deseja-se agrupar as máquinas em células e as partes em famílias de modo que cada família seja alocada a uma célula distinta, minimizando o número de deslocamento de partes entre as células ao mesmo tempo em que se maximiza o aproveitamento (número de operações) de cada célula. Este problema em si já é bem estudado e explorado na literatura, com centenas de artigos e heurísticas desenvolvidas nas ultimas décadas. No entanto, a inclusão de diversos fatores práticos relevantes é uma preocupação mais recente que leva a um aumento na complexidade do problema, tornando difícil a inclusão de muitos desses fatores simultaneamente. Neste trabalho iremos tratar do Problema de Formação de Células de Manufatura com Sequência de Operações e Processos Alternativos, no qual são incluídas informações sobre a sequência de operações das partes juntamente com a possibilidade de escolha entre múltiplos planos de processamento. Para tal, propomos inicialmente um algoritmo baseado na meta-heurística ILS para a versão clássica do problema e em seguida desenvolvemos uma extensão deste algoritmo para a resolução do problema com a inclusão desses novos requisitos. O algoritmo proposto é comparado com os métodos existentes na literatura, apresentando resultados superiores em um tempo computacional baixo.",TESE,Algoritmo para o Problema de Formação de Células de Manufatura com Sequência de Operações e Processos Alternativos,5000326,
"Some organizations have to assign and manage facilities in an optimized way. Those activities involve many stakeholders with multiple conflicting objectives. Multi-objective optimization evolutionary algorithms have been successfully applied to several complex synthetic and real-world multi-objective problems (MOPs). Although these algorithms have proved themselves as a valid approach to the MOP, there is still need for improvements on the performance of the search process. This work introduces a novel approach meant for bringing collective intelligence methods into the optimization process carried out by evolutionary multi-objective optimization algorithms. In particular, it describes the extension of some well-known algorithms (Non-dominated Sorting Genetic Algorithm-II, S-metric Selection Evolutionary Multi-objective Algorithm, Strength Pareto Evolutionary Algorithm 2) to include collective online preferences into the optimization process. In these new methods —called CI-NSGA-II, CI-SMS-EMOA and CI-SPEA2—, groups of decision makers can highlight the regions of the Pareto frontier that are more relevant to them as to focus the search process mainly on those areas. Additionally, the integration of interactivity and cooperation into the evolutionary algorithms refines users’ preferences and improves the reference points throughout the evolutionary progress. Rather than a unique or small group of decision makers with unilateral preferences, the application of dynamic group preferences aggregates consistent collective reference points and creative solutions to enhance multi-objective results. In order to analyze the results, three new performance indicators based on preferences are introduced to evaluate the quality of approximations set. As part of this work, the algorithms’ performances are tested when faced with some synthetic problems as well as a real-world case of facility location. The experiments demonstrate the advantages of a collective intelligence operator integrated into the multi-objective evolutionary algorithm.",Tese___VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,DANIEL LOPES CINALLI,UNIVERSIDADE FEDERAL FLUMINENSE,06/04/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'collective intelligence;preferences;reference points;evolutionary multi- objective optimization algorithms;facility location problem',-,ANA CRISTINA BICHARRA GARCIA,0,collective intelligence;preferences;reference points;evolutionary multi- objective optimization algorithms;facility location problem,COMPUTAÇÃO (31003010046P4),-,"Some organizations have to assign and manage facilities in an optimized way. Those activities involve many stakeholders with multiple conflicting objectives. Multi-objective optimization evolutionary algorithms have been successfully applied to several complex synthetic and real-world multi-objective problems (MOPs). Although these algorithms have proved themselves as a valid approach to the MOP, there is still need for improvements on the performance of the search process. This work introduces a novel approach meant for bringing collective intelligence methods into the optimization process carried out by evolutionary multi-objective optimization algorithms. In particular, it describes the extension of some well-known algorithms (Non-dominated Sorting Genetic Algorithm-II, S-metric Selection Evolutionary Multi-objective Algorithm, Strength Pareto Evolutionary Algorithm 2) to include collective online preferences into the optimization process. In these new methods —called CI-NSGA-II, CI-SMS-EMOA and CI-SPEA2—, groups of decision makers can highlight the regions of the Pareto frontier that are more relevant to them as to focus the search process mainly on those areas. Additionally, the integration of interactivity and cooperation into the evolutionary algorithms refines users’ preferences and improves the reference points throughout the evolutionary progress. Rather than a unique or small group of decision makers with unilateral preferences, the application of dynamic group preferences aggregates consistent collective reference points and creative solutions to enhance multi-objective results. In order to analyze the results, three new performance indicators based on preferences are introduced to evaluate the quality of approximations set. As part of this work, the algorithms’ performances are tested when faced with some synthetic problems as well as a real-world case of facility location. The experiments demonstrate the advantages of a collective intelligence operator integrated into the multi-objective evolutionary algorithm.",TESE,Integrating Collective Intelligence into  Multi-Objective Optimization Evolutionary Algorithms: Interactive Preferences and Reference Points for a Facility Location Problem,5001431,
"Power consumption in large-scale processing environments, such as cloud computing platforms and data centers, is a major issue nowadays, where the environmental sustainability of these environments is a priority premise. An efficient power management is critical to reduce the operating costs and the environmental impact of these environments. However, this is a complex task, since the power management must observe several factors such as the QoS of the supported applications, scalability, heterogeneity of the processing resources, power and performance profiling of these resources, among others. This work presents a model for managing Virtual Machines (VMs) on server clusters, in addition to providing energy savings, our model has linear scalability and is independent of the processing platform. We define a default processing virtual web server, named as Quantum Virtual Machine (QVM). A set of QVM performs a Logical Web Server (LWS), which operates in a flexible manner, changing its performance and power consumption depending on the workload of the applications. Concepts of agile VM clone, co-allocation of VMs in the same core, and Dynamic Voltage and Frequency Scaling (DVFS) are used in the model, enabling rapid configuration actions and a fine-grained QoS control. In addition, we propose a runtime modeling of the processing resources, dynamically defining their power consumption and processing capacity, avoiding the need for a previous profiling of the processing environment. Experiments evaluate the effectiveness of the proposed model by means of power consumption reduction and QoS violations as compared to the Linux CPU governors and state-of-the-art energy-aware approaches. The results show our model conserves up to 51.8% of the energy required by a cluster designed for peak workload scenario, with a negligible impact on the application’s performance.",tese___VERSÃO FINAL.pdf,SISTEMAS DE COMPUTAÇÃO,ANDRE FELIPE DE ALMEIDA MONTEIRO,UNIVERSIDADE FEDERAL FLUMINENSE,12/04/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Power-aware computing;resource management;scalability;virtualization;server clusters',-,ORLANDO GOMES LOQUES FILHO,0,Eficiência energética de servidores;gerenciamento de recursos;escalabilidade;virtualização;clusters de servidores,COMPUTAÇÃO (31003010046P4),-,"O elevado consumo de energia em ambientes de processamento de larga escala, como plataformas de computação em nuvem e data centers, mostra-se uma questão central nos dias atuais onde a sustentabilidade ambiental desses ambientes é uma premissa prioritária. Prover um gerenciamento energético eficiente é fundamental para reduzir os custos de operação e o impacto ambiental desses ambientes. Entretanto, esta é uma tarefa complexa, pois o gerenciamento energético deve lidar com diversos fatores como qualidade de serviço das aplicações suportadas, escalabilidade, heterogeneidade dos recursos de processamento, perfil energético e capacidade de processamento desses recursos, dentre outros. Este trabalho apresenta a Máquina Virtual Quântica, um modelo para gerenciamento de servidores virtuais de aplicação (Virtual Machines - VMs) em clusters de processamento. Além de prover economia de energia e garantir a qualidade de serviço das aplicações suportadas, nosso modelo tem escalabilidade linear e define um padrão de servidor virtual de processamento denominado de VM Quântica. Um conjunto de Máquinas Virtuais Quânticas constitui um Servidor Lógico Web (SLW), que opera de forma flexível ajustando seu consumo de energia e seu desempenho de acordo com a carga de requisições das aplicações. Conceitos de clone ágil e co-alocação de VMs e DVFS (Dynamic Voltage and Frequency Scaling) são utilizados em nosso modelo, viabilizando ações ágeis de reconfiguração do cluster e um controle fino de QoS das aplicações. Além disso, propomos uma modelagem em tempo de execução dos recursos de processamento, definindo dinamicamente o consumo de energia e a capacidade de processamento dos mesmos, dispensando assim a necessidade de uma análise prévia do ambiente de processamento. Os experimentos avaliam o nosso modelo por meio das métricas de consumo de energia e taxa de violação de QoS em comparação às políticas nativas do Linux para gerenciamento energético de servidores, e a modelos do estado da arte da área de gerenciamento energético de clusters. Os resultados demonstram que nosso modelo é capaz de economizar até 51.8% da energia consumida em um cluster que opera em sua capacidade máxima, com um impacto irrelevante no desempenho das aplicações.",TESE,Quantum Virtual Machine: a Dynamic Approach for Managing Power and Performance in Virtualized Clusters,5003051,
"This work deals with an extension of the Path Tubes method for the solution of the time-dependent Navier-Stokes equations for an incompressible Newtonian fluid. Departing from a physically intuitive methodology based on the theoretical basis of the mechanics of continuous media, a robust numerical technique is obtained. This version of the Path Tubes method draws on a semi-Lagrangian time-discretization that employs the Reynolds'   transport theorem, and a localization approach, to establish an implicit semi-Lagrangian algorithm that allows the use of classical schemes for spatial discretization, such as central-difference formulas, without the need to use upwind techniques, or high-order corrections for time derivatives. Some of the extensive numerical tests are shown herein, in particular for Reynolds' numbers typical of advection dominated flows. The tests show the method is accurate, even for coarse grids.",Tese_VERSÃO FINAL.pdf,COMPUTAÇÃO CIENTÍFICA E SISTEMAS DE POTÊNCIA,FABIO PACHECO FERREIRA,UNIVERSIDADE FEDERAL FLUMINENSE,27/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Path Tubes method;Semi-Lagrangian algorithm;Navier-Stokes equation;Implicit scheme;Interpolation formula;Advection dominated flow',-,MAURICIO KISCHINHEVSKY,0,Método do Tubo de Trajetórias;Algoritmo semi-Lagrangiano;Equação de Navier-Stokes;Esquema Implícito;Fórmula de interpolação;Fluxo dominado pela advecção,COMPUTAÇÃO (31003010046P4),-,"O presente trabalho trata de uma extensão do método do Tubo de Trajetórias para resolver as equações de Navier-Stokes dependentes do tempo para um fluido newtoniano incompressível. O resultado é uma metodologia fisicamente intuitiva cuja formulação se baseia nos fundamentos teóricos da mecânica dos meios contínuos. Esta versão do método do Tubo de Trajetórias baseia-se numa discretização temporal semi-Lagrangiana que emprega o teorema de transporte de Reynolds e uma abordagem de localização para estabelecer um algoritmo semi-Lagrangiano implícito que permite o uso de esquemas clássicos para a discretização espacial, tais como fórmulas de diferenças centradas, sem a necessidade de usar técnicas de ``upwind'' ou correções de alta ordem para derivadas no tempo. Após testes numéricos usando valores diferentes para o número de Reynolds típicos de fluxos dominados pela advecção, o método proposto mostrou-se preciso e capaz de trabalhar com malhas menos refinadas.",TESE,Aplicação do Método do Tubo de Trajetórias à Solução Numérica das Equações de Navier-Stokes,5005918,
"A usual way to collect data in a Wireless Sensor Network (WSN) is by the support of a special agent, called data mule, that moves between sensor nodes and performs all communication between  them. Using this problem as base, we tackled two versions of the problem: the first one where the mule has a global view, i.e., has the complete knowledge of all sensors and possible routes between them, and the second one where the mule must to decide its route based only on local informations obtained from the neighbors of the current visited sensor. The first version dealt with the Data Mule Scheduling Problem (DMSP). The DMSP is  $\mathcal{NP}$-hard since it is a generalization of the Traveling Salesman Problem. In DMSP, in addition to the Data Mule Routing, it is necessary to plan the speed that this mule will use and also to schedule the attendance of the sensors in this route. Exact mathematical programming models and sequential heuristic algorithms for the problem are studied. Two different formulations using mixed integer linear programming and methods based on the GRASP and GVNS metaheuristics were proposed, in addition we generated instances for their validation. In the second version, the focus is on the construction of the route that  the data mule must follow to serve all nodes in the WSN. This second version deals with the case when the data mule does not have a global view of the network, i.e., a prior knowledge of the network as a whole. Thus, at each node,  the data mule makes a decision about the next node to be visited based only on a limited local knowledge of the WSN. Considering this realist scenario, two locality sensitive heuristics are proposed. These heuristics differ by the criterion of choice of the next visited node, while the first one uses a  simpler greedy choice, the second one uses the geometric concept of convex hull. They were executed in instances of the literature and their results were compared both in terms of route length and  in number of sent messages as well. For this version, some theoretical results, a mathematical formulation, and some lower bounds for the global view scenario are also proposed, in order to provide some parameters to evaluate the quality of the solutions given by the locality sensitive heuristics.",Tese_VERSÃO FINAL.pdf,SISTEMAS DE COMPUTAÇÃO,PABLO LUIZ ARAUJO MUNHOZ,UNIVERSIDADE FEDERAL FLUMINENSE,04/05/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Wireless Sensor Networks;Data Mule Routing;Locality Sensitive Heuristics',-,LUCIA MARIA DE ASSUMPCAO DRUMMOND,0,Redes de sensores sem fio;Roteamento de Mula de Dados;Heurísticas Sensíveis à localidade,COMPUTAÇÃO (31003010046P4),-,"Uma maneira usual de coletar dados em uma Rede de Sensores sem Fio (\textit{Wireless Sensor Networks} -- WSN) é atráves de um agente especial, chamado mule de dados, que se move entre os nós sensores e realiza toda a comunicação entre eles. Ao se tratar da coleta de dados em uma WSN utilizando a mula de dados como meio de comunicação entre os sensores da rede, foram escolhidas duas abordagens para esse problema: a primeira onde a mula de dados tem uma visão global, ou seja, tem o conhecimento completo de todos os sensores e possíveis rotas entre eles; e a segunda onde a mula deve decidir sua rota baseada apenas em informações locais obtidas dos vizinhos do sensor que está sendo visitado em dado momento. A primeira abordagem tratou o Problema de Sequenciamento de Mula de Dados (Data Mule Scheduling Problem DMSP). O DMSP é um problema classificado como NP-Difícil, uma vez que é uma generalização do Problema do Caixeiro Viajante (Traveling Salesman Problem). Ao tratarmos o DMSP, além de realizar a definição da rota, é necessário planejar a velocidade que esta mula utilizará em seu trajeto e também sequenciar o atendimento dos sensores nesta rota. São estudados modelos de programação matemática  e algoritmos heurísticos sequenciais para o problema. Foram propostas formulações matemáticas utilizando programação linear inteira mista e métodos baseados nas metaheurísticas GRASP e GVNS. Além disso, instâncias para a validação dos métodos foram geradas. Na segunda abordagem, o foco está na construção da rota que a mula de dados deve seguir para atender todos os nós na WSN. Esta segunda versão aborda o caso em que a mula de dados não possui uma visão global da rede, ou seja, um conhecimento prévio da rede como um todo. Assim, em cada nó, a mula de dados toma uma decisão sobre o próximo nó a ser visitado com base apenas em um conhecimento local limitado do WSN. Considerando este cenário realista, duas heurísticas sensíveis à localidade são propostas. Estas heurísticas diferem entre si pelo critério de escolha do próximo nó visitado, enquanto a primeira usa uma escolha gulosa mais simples, a segunda usa o conceito geométrico de envoltória convexa. Eles foram testadas em instâncias da literatura e seus resultados foram comparados tanto em termos de duração da rota quanto em número de mensagens enviadas. Para esta versão, alguns resultados teóricos, uma formulação matemática e alguns limites inferiores para o cenário de visão global também são propostos, a fim de fornecer alguns parâmetros para avaliar a qualidade das soluções fornecidas pelas heurísticas sensíveis à localidade.",TESE,Data Gathering in Sensor Networks with Data Mules: Global and Local Approaches,5007273,
"Constant evolution of computational systems has made the use of approaches to the management of security requirements need continuous and growing. In these systems, one of the key components of security is access control. Access control policies are a means of defining authorized and unauthorized accesses to objects. Such policies permit, prohibit or oblige subjects to perform actions on objects. In systems where multiple policies are described conflicts among such policies can arise. Two policies are in conflict when the fulfillment of one policy violates the other policy. Some types of conflicts between policies can be detected by observing the overlap of policy elements (subject, action and object), for example, one conflict occurs when one policy permits (or obliges) a given subject to do a given action to a particular object and another policy prohibits the same subject to do the same action to such object. Other type of conflict can also occur through implicit relationships between subject, object and action.  In such case, the conflicts occur between policies that regulate the behavior of different but related subjects, specifying different but related actions that access different but related objects. This work proposes an approach to detect these types of conflicts among different policies, focusing mainly on the second type of conflict, mentioned above, which we call indirect conflict. In this sense, the contributions of this work include: A definition of the policy used; a set of relationships that can occur between the attributes that make up a policy (organization, subjects, roles, objects, visions and actions); a set of propagation rules and conflict rules. Three scenarios with real systems policies are described in the analysis to detect conflicts. Our Conflict Detector tool receives as input a set of access control policies, a set of relationships between the attributes of these policies and provides as an output, in case of conflicts, the identification of the conflict and the respective policies analyzed.",Tese_VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,LAURA COSTA SARKIS,UNIVERSIDADE FEDERAL FLUMINENSE,26/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'access control policies;conflict between policies;conflict detection;indirect conflict',-,VIVIANE TORRES DA SILVA,0,Políticas de Controle de Acesso;Conflitos entre políticas;Detecção de conflitos;Conflitos Indiretos,COMPUTAÇÃO (31003010046P4),-,"A constante evolução dos sistemas computacionais faz com que o uso de abordagens para a gestão de requisitos de segurança seja uma necessidade contínua e crescente. Nestes sistemas, um dos principais componentes de segurança é o controle de acesso. Políticas de controle de acesso são um meio de definir acessos autorizados e não autorizados aos objetos. Estas políticas autorizam, proíbem ou obrigam sujeitos a realizar ações sobre objetos. Nos sistemas em que existem várias políticas descritas, conflitos entre tais políticas podem surgir. Duas políticas estão em conflito, quando o cumprimento de uma política viola o cumprimento da outra política. O conflito entre políticas pode ser detectado de maneira direta, observando a sobreposição de atributos da política (isto é, sujeitos, ações e objetos) e de maneira indireta, através da análise dos relacionamentos implícitos entre os atributos das duas políticas verificadas. Neste último caso, os conflitos ocorrem entre as políticas que regulam o comportamento de diferentes, mas relacionados sujeitos, especificando ações diferentes, mas relacionadas, que acessam diferentes, mas relacionados, objetos. Este trabalho propõe uma abordagem para detectar esses tipos de conflitos entre as diferentes políticas, concentrando-se principalmente no segundo tipo de conflito, que denominamos de conflito indireto. Nesse sentido, as contribuições deste trabalho incluem: a definição da política utilizada; um conjunto de relacionamentos que podem ocorrer entre os atributos que compõe uma política (organização, sujeitos, papéis, objetos, visões, ação e atividades); um conjunto de regras de propagação e de regras de conflitos. Três cenários com políticas de sistemas reais são descritos na análise para detectar conflitos. A ferramenta desenvolvida Conflict Detector recebe como entrada um conjunto de políticas de controle de acesso, um conjunto de relacionamentos entre os atributos destas políticas e fornece como saída, no caso da existência de conflitos, a identificação do conflito e as políticas que estão em conflito.",TESE,Uma Abordagem para Detecção de Conflitos Indiretos entre Políticas de Controle de Acesso,5021106,
"Open-source software development is nowadays inserted in a distributed and collaborative context, which enables to receive external contributions. An emerging paradigm employed for the systematization of these contributions is named pull request. According to this paradigm, external developers wishing to contribute to a project fork the project repository, make their changes, and send a pull request to the project's core team, who will review the contribution and decide whether or not to integrate it into the repository. Pull requests may contain bug fixes, code refactorings, or new features, for example. Currently, few information about the nature of pull requests is known in the scenario of open-source projects. Some work investigated pull requests characteristics related to acceptance, lifetime, and reviewers assignment. However, all of these studies neglected the exploration of certain aspects that are still open. In this thesis we performed a set of studies based on the extraction of association rules from 88 open-source projects, detailing the nature of their 132,660 pull requests through: (1) the identification of frequent patterns from thousands of pull requests; (2) the assessment of the extent and strength of these patterns; and (3) a qualitative analysis that explains the occurrence of some patterns. Our results indicate that physical characteristics of the pull requests, collaborators profile, social aspects of the process, and location of contributions are factors that influence, in different intensities, on: the acceptance/rejection, lifetime, and reviewers assignment. The identification of these patterns can support developers and project managers in understanding the nature of pull requests and guide them to practices that maximize the benefits of this collaboration paradigm.",Tese_VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,DARICELIO MOREIRA SOARES,UNIVERSIDADE FEDERAL FLUMINENSE,27/06/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Pull Request;Association Rules;Acceptance;Lifetime;Reviewers Assignment',-,LEONARDO GRESTA PAULINO MURTA,0,Pull Request;Regras de Associação;Aceitação;Tempo de Vida;Atribuição de Revisores,COMPUTAÇÃO (31003010046P4),-,"O desenvolvimento de software livre está comumente inserido num contexto distribuído e colaborativo, o que permite o recebimento de contribuições externas. Um paradigma emergente empregado para a sistematização dessas contribuições é denominado pull request. Nesse paradigma, colaboradores externos que desejam contribuir com um projeto criam um fork a partir do repositório do projeto, fazem suas alterações e enviam um pull request à equipe principal, que revisará a contribuição e decidirá sobre a incorporação ou não ao repositório.  Pull requests podem conter correção de bugs, refatoração de código ou adição de novas funcionalidades, por exemplo. Atualmente, poucas informações sobre a natureza dos pull requests são conhecidas no cenário de projetos de software livre. Alguns trabalhos investigaram características de pull requests relacionadas à aceitação, tempo de vida e atribuição de revisores. No entanto, esses estudos negligenciam a exploração de questões importantes e que estão em aberto. Nesta tese, realizamos um conjunto de estudos baseados na extração de regras de associação sobre 88 projetos de software livre, detalhando a natureza dos seus 132.660  pull requests por meio da: (1) identificação de padrões frequentes a partir de milhares de pull requests; (2) avaliação da extensão e força desses padrões; e (3) análise qualitativa que explica a ocorrência de alguns deles. Nossos resultados indicam que as características físicas dos pull requests, o perfil dos colaboradores, aspectos sociais do processo e a localização das contribuições são fatores que influenciam, em diferentes intensidades de força, na aceitação/rejeição, no tempo de vida e na atribuição de revisores. A identificação desses padrões pode apoiar desenvolvedores e gerentes de projeto na compreensão da natureza dos pull requests e guiá-los a práticas que permitam extrair ao máximo os benefícios desse paradigma de colaboração.",TESE,On the Nature of Pull Requests: a Study about this Collaboration Paradigm over Open-Source Projects Using Association Rules,5021109,
"During the software development process, artifacts are constructed and manipulated by multiple developers working in parallel. A common practice to manage parallel development is the use of branches. Eventually, these branches need to be reintegrated through a merge operation. In this process, if conflicts arise, the developers need to communicate to reach consensus about the desired resolution. The developers must ensure that the result complies with the objective of the work, and that there is no high-level conflict (syntactic and semantic), as these conflicts are more difficult to identify automatically. For this reason, inviting the right developers to a collaborative merge session is fundamental. However, this task can be difficult especially when many different developers have made significant changes on each branch over a large number of files. The main goal of this work is to present TIPMerge, an approach conceived to recommend participants for collaborative merge sessions. TIPMerge analyzes the project history and builds a ranked list of developers who are the most appropriate to integrate a pair of branches, considering their changes in the branches, in the previous history, and the dependencies among files across branches. Our results show an average normalized improvement of 15.14% (median 25.01%) for top-1 and 43.45% (median 51.66%) for top-3 of the ranking compared to the majority classes, i.e., developers who performed most merges. Although useful for choosing a specific developer to perform the merge, usually picking the top developers in such ranking is not effective for collaborative merge sessions, as the top developers may have overlapping knowledge due to changes over the same files. In order to support collaborative merge, TIPMerge employs optimization techniques to choose developers with complementary knowledge, aiming at maximizing the joint knowledge coverage. Our results show an average normalized improvement of 47.31% (median 48.48%) for the joint knowledge coverage when using the optimization techniques employed by TIPMerge for assembling teams of three developers for collaborative merge in comparison to choosing the top-3 developers in the ranked list.",Tese_VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,CATARINA DE SOUZA COSTA,UNIVERSIDADE FEDERAL FLUMINENSE,28/06/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Collaborative Merge;Developers Assignment;Merge of Branches;Optimization',-,LEONARDO GRESTA PAULINO MURTA,0,Merge Colaborativo;Alocação de Desenvolvedores;Merge de Ramos;Otimização,COMPUTAÇÃO (31003010046P4),-,"Durante o processo de desenvolvimento de software, os artefatos são construídos e manipulados por vários desenvolvedores que trabalham em paralelo. Uma prática comum para gerenciar o desenvolvimento paralelo é o uso de ramos. Em algum momento, esses ramos precisam ser reintegrados através de uma operação de merge. Neste processo, em caso de conflitos, os desenvolvedores precisam se comunicar para chegar a um consenso sobre a resolução desejada. Os desenvolvedores devem garantir que o resultado esteja em conformidade com o objetivo do trabalho e que não haja conflitos de alto nível (sintático e semântico), mais difíceis de serem identificados de forma automática. Por essa razão, convidar os desenvolvedores adequados para uma sessão de merge colaborativo é fundamental. No entanto, esta tarefa pode ser difícil, especialmente quando muitos desenvolvedores diferentes realizaram alterações significativas em cada ramo em um grande número de arquivos. Assim, o objetivo principal deste trabalho é apresentar TIPMerge, uma abordagem concebida para recomendar participantes para sessões de merge colaborativo. TIPMerge analisa o histórico do projeto e cria um ranking de desenvolvedores mais apropriados para integrar dois ramos, considerando as mudanças nos ramos, na história prévia e as dependências entre os arquivos modificados em diferentes ramos. Nossos resultados mostram uma média de melhoria normalizada de 15,14% (mediana 25,01%) para o top-1 e 43,45% (mediana 51,66%) para o top-3 do ranking comparado com as classes majoritárias, i.e., desenvolvedores que mais realizaram merges. Embora seja útil para a escolha de um desenvolvedor para executar o merge, geralmente escolher os primeiros desenvolvedores do ranking não é eficaz para sessões de merge colaborativo, pois os primeiros desenvolvedores podem ter conhecimento sobreposto devido a alterações nos mesmos arquivos. A fim de apoiar o merge colaborativo, TIPMerge utiliza técnicas de otimização para escolher desenvolvedores com conhecimentos complementares, com o objetivo de maximizar a cobertura conjunta do conhecimento. Nossos resultados mostram uma média de melhoria normalizada de 47,31% (mediana 48,48%) para a cobertura de conhecimento conjunto ao utilizar as técnicas de otimização empregadas por TIPMerge para reunir equipes de três desenvolvedores para merge colaborativo em comparação com a escolha dos top-3 desenvolvedores do ranking.",TESE,Recommending Developers for Collaborative Merge Sessions,5021111,
"The Minimum Coloring Cut Problem (MCCP) is defined as follows: given a connected (undirected) graph G with colored edges, find an edge cut E' of G (a minimal set of edges whose removal renders the graph disconnected) such that the number of colors used by the edges in E' is minimum. The computational complexity of this problem is unknown, although Faria et al. [10] have demonstrated that the MCCP is NP-complete if the input graph is a digraph. In this work, two approaches (deterministic and probabilistic) are presented based on Variable Neighborhood Search to solve this problem. The presented algorithms are able to find all the optimum solutions described in the literature.",Dissertação___VERSÃO FINAL.pdf,ALGORITMOS E OTIMIZAÇÃO,AUGUSTO CESAR BORDINI BRAGA,UNIVERSIDADE FEDERAL FLUMINENSE,10/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Minimum Coloring Cut Problem;Combinatorial Optimization;Graph Theory;Variable Neighborhood Search;Label Cut Problem',-,FÁBIO PROTTI,0,Problema do Corte Global de Coloração Míinima;Otimizaçãao Combinatória;Teoria dos Grafos;Variable Neighborhood Search;Problema do Corte Rotulado,COMPUTAÇÃO (31003010046P4),-,"O Problema do Corte Global de Coloração Mínima (PCGCM) é definido conforme segue: dado um grafo conexo (não-direcionado) G com arestas coloridas, o objetivo é encontrar um corte de arestas E' de G (um conjunto minimal de arestas cuja remoção transforma o grafo em desconexo) tal que o número de cores usadas nas arestas em E' seja mínimo. Não é conhecida a complexidade computacional deste problema, embora Faria et al. [10] tenham demonstrado que o PCGCM é NP-completo se o grafo de entrada for um digrafo. Neste trabalho, são apresentadas duas abordagens (determinística e probabilística) baseadas na metaheurística Variable Neighborhood Search para resolver este problema. Os algoritmos apresentados são capazes de achar todas as soluções ótimas conhecidas na literatura.",DISSERTAÇÃO,Novos Algoritmos para o Problema do Corte Global de Coloração Mínima,5024192,
"In distributed software development, many projects have adopted the “integration manager” workflow to organize the collaboration process. In this workflow, the developer (requester) sends his or her contribution through a request, called pull request, which contains title, description, author, and the code needed to fix bugs or add features. The pull request must be reviewed by a member of the core team (integrator), who decides to reject or accept it. In many open-source projects, the pull requests integration process has a high demand for requests and a shortage of integrators’ time, which increases the number of opened pull requests and the average time for integrations. On the other hand, integrators are interested in reducing the time to integrate the pull requests and ensure the code quality. In this decision-making process, it may be extremely useful to predict information, such as the most appropriate integrator to integrate and the lifetime of a pull request. Some researches have already explored these forecasting scenarios. However, these researches differ in many aspects related to the materials and methods used: sets of predictive attributes, classification and regression techniques, experimental processes, and quantity of projects. In this context, the main objectives of this thesis are: to compare the different sets of predictive attributes used in previous works with the set proposed here and to evaluate whether attributes selection techniques can identify more adequate subsets of attributes in order to improve the performance of the tasks of predicting integrators and lifetime of pull requests. In the experiments, the sets of attributes were evaluated with different classification, regression, and attribute selection strategies. Compared to previous approaches, our proposal to recommend integrators showed the best accuracy in 29 out of the 32 projects considering the Top-1 recommendation and reached the best normalized improvement averages for the recommendations Top-1 (19,93%), Top-3 (41,91%), and Top-5 (52,60%). In the prediction of lifetime, our proposal also presented the best normalized improvement averages when compared to other approaches, obtaining the best accuracy in 18 out of the 20 projects used and normalized improvement average of 14,68%.",Tese_VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,MANOEL LIMEIRA DE LIMA JUNIOR,UNIVERSIDADE FEDERAL FLUMINENSE,11/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Pull Request;Integrators;Lifetime;Classification;Regression and Attribute Selection',-,ALEXANDRE PLASTINO DE CARVALHO,0,Pull Request;Integradores;Tempo de Vida;Classificação;Regressão e Seleção de Atributos,COMPUTAÇÃO (31003010046P4),-,"No desenvolvimento distribuído de software, muitos projetos têm adotado o fluxo de trabalho “gerente de integração” para organizar o processo de colaboração. Nesse fluxo, o desenvolvedor (solicitante) envia sua colaboração por meio de uma solicitação, denominada pull request, que contém título, descrição, autor e o código necessário para corrigir um bug ou adicionar uma funcionalidade. A solicitação deve ser revisada por um membro da equipe principal do projeto (integrador), que decide rejeitar ou aceitar o código. Em muitos projetos open-source, o processo de integração de pull requests possui grande demanda de solicitações e pouca oferta de tempo dos integradores, o que aumenta a quantidade de pull requests abertos e o tempo médio das integrações. Por outro lado, os integradores estão interessados em reduzir o tempo de integração dos pull requests e garantir a qualidade do código. Nesse processo de tomada de decisões, pode ser extremamente útil prever informações como, por exemplo, o integrador mais apropriado para integrar e o tempo de vida de um pull request. Alguns trabalhos já exploraram esses cenários de previsão. No entanto, esses trabalhos se diferenciam em vários aspectos referentes aos materiais e métodos utilizados: conjuntos de atributos preditivos, técnicas de classificação e regressão, processos experimentais e quantidade de projetos. Nesse contexto, os principais objetivos desta tese são: comparar os diferentes conjuntos de atributos preditivos utilizados em trabalhos anteriores com o conjunto aqui proposto e avaliar se técnicas de seleção de atributos podem identificar subconjuntos de atributos mais adequados para melhorar o desempenho das tarefas de prever integradores e tempo de vida de pull requests. Nos experimentos, os conjuntos de atributos foram avaliados com diferentes algoritmos de classificação, regressão e estratégias de seleção de atributos. Comparando com abordagens anteriores, nossa proposta para recomendar integradores apresentou a melhor acurácia em 29 dos 32 projetos considerando a recomendação Top-1 e atingiu as melhores médias de ganho normalizado para as recomendações Top-1 (19,93%), Top-3 (41,91%) e Top-5 (52,60%). Na previsão do tempo de vida, nossa proposta também apresentou as melhores médias de ganho normalizado quando comparada com outras abordagens, obtendo a melhor acurácia em 18 dos 20 projetos utilizados e um ganho normalizado médio de 14,68%.",TESE,Previsão de Integradores e Tempo de Vida de Pull Requests,5025942,
"Efficient query processing in large XML datasets is still a challenging problem, especially because there is no standard model and algebra to handle XML data and drive query optimization. In the literature, authors suggest to physically fragment and distribute the database, so that queries can be executed in parallel to increase performance. However, physical fragmentation is designed to benefit a set of frequent queries, and applications that require ad-hoc queries over XML datasets, such as OLAP, usually present poor performance in such settings. To solve this problem, previous work has proposed the use of virtual partitioning, which consists in replicating the dataset in several processing nodes and defining data partition at query runtime. However, Simple Virtual Partitioning may suffer from load imbalance in the presence of data skew, which negatively impacts the query processing time. To overcome this limitation and improve query processing performance, in this thesis, we contribute by proposing three skew-aware strategies for the parallel processing of ad-hoc XML queries: Master-Slave Simple XML Virtual Partitioning, Adaptive XML Virtual Partitioning and Dynamic Adaptive XML Virtual Partitioning. In addition, we implemented a prototype for an intra-query parallelism architecture that applies different strategies of virtual partitioning in a shared nothing XML database cluster. Experimental evaluation results with the TPC-H XML benchmark show sublinear speedup in most of the queries executed with these virtual partitioning strategies in a cluster. In particular, the Dynamic Adaptive XML Virtual Partitioning is the most adequate approach for heavyweight queries, speeding up the query processing time up to 6.2 times in a scenario with 16 processing nodes when compared to a single node.",TESE_VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,LUIZ AUGUSTO MATOS DA SILVA,UNIVERSIDADE FEDERAL FLUMINENSE,12/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Load balance;Parallel query processing;Virtual partitioning;XML;XQuery',-,VANESSA BRAGANHOLO MURTA,0,Balanceamento de carga;Fragmentação virtual;Processamento paralelo de consultas;XML;XQuery,COMPUTAÇÃO (31003010046P4),-,"O processamento de consultas em grandes bases de dados XML ainda é um problema desafiador, especialmente porque não existe um modelo e nem uma álgebra padrão para o processamento eficiente de consultas a esse tipo de dados. Na literatura, autores sugerem fragmentar fisicamente e distribuir o banco de dados, para que as consultas possam ser executadas em paralelo. Entretanto, a fragmentação física é projetada para beneficiar um conjunto de consultas frequentes. Dessa forma, aplicações que requerem o processamento de consultas ad hoc sob bases de dados XML, tais como as OLAP, apresentam baixo desempenho ao utilizar esse tipo de fragmentação. Para resolver este problema, trabalhos anteriores propõem o uso da fragmentação virtual, que consiste em replicar o banco de dados em vários nós de processamento e definir a fragmentação dos dados em tempo de execução da consulta. Porém, a Fragmentação Virtual Simples pode sofrer com desbalanceamento de carga na presença de distorção de dados, o que impacta negativamente no tempo de processamento da consulta. Para superar esta limitação e aumentar o desempenho do processamento de consultas, propomos nesta tese três estratégias para o processamento paralelo de consultas XML ad hoc: Fragmentação Virtual Simples com balanceamento de carga Mestre-Escravo, Fragmentação Virtual Adaptativa e Fragmentação Virtual Adaptativa Dinâmica. Além disso, implementamos um protótipo de software para a arquitetura de um processador paralelo intraconsultas que utiliza as diferentes estratégias de fragmentação virtual em um cluster de banco de dados XML sem compartilhamento de recursos. Resultados de nossa avaliação experimental com o benchmark TPC-H XML mostraram aceleração sublinear na maioria das consultas executadas com estas estratégias de fragmentação virtual em um cluster de computadores. Em particular, a Fragmentação Virtual Adaptativa Dinâmica se mostrou como a mais adequada para o processamento eficiente de consultas XML ad hoc de alto custo, acelerando o tempo de processamento dessas consultas em até 6,2 vezes no cenário com 16 nós quando comparado ao ambiente centralizado.",TESE,Processamento Paralelo de Consultas XML Ad Hoc com Fragmentação Virtual,5025947,
"Information-Centric Networking (ICN) is a new communication paradigm to the Internet. Different from the TCP/IP architecture focused on the interconnection between end systems, ICN introduces a new communication model that aims at retrieving a con- tent requested by a user regardeless of the logical or physical location of this content. There are several proposed architectures to this new paradigm. One of these proposals found in the literature is the Content Centric Networking (CCN).
	One of the main features of the CCN is that both routers and end systems can store in cache previously received contents to increase its availability. In-network caching increases the CCN robustness against traditional denial of service attacks (DoS). With CCN, intermediate nodes can satisfy content requests before these messages reach the content source. Thus, there is no guarantee that the attack will impact the victim node. Nevertheless, new types of DoS attacks have emerged in order to explore several CCN features.
	This thesis proposes and evaluates a countermeasure to mitigate the producer-consumer collusion attack in the CCN architecture called Cache nFace. This is a kind of DoS attack where malicious consumers and producers act in collusion, generating, publishing, and manipulating content popularity. The goal of the attack is to increase the probability of a legitimate consumer retrieves contents directly from the producer, increasing the content retrieval time. The proposed countermeasure mitigates this attack by dividing the cache of a node in sub-caches. Each sub-cache stores only the contents requested through a specific network interface. The goal is to increase the robustness of the cache under attack. Without the countermeasure, if a particular node is in the path between the consumer and the malicious producer, its entire cache is compromised under attack. In this case, the malicious consumer requests contents produced by the malicious producer at high rate in order to remove legitimate content from the cache of intermediate nodes. The probability that all network interfaces of a node are under attack simultaneously, however, is small. This is the key assumption of Cache nFace.
	In the analysis, we consider two network topologies: a tree-topology with 32 nodes and a mesh-topology with 192 nodes. The position and the number of attackers are varied for both topologies. We consider the following metrics: the average retrieval time for legitimate contents, the malicious cache occupation of routers, the average cache miss rate for legitimate contents, and the percentage of legitimate contents retrieved from the legitimate producer. Results show that the Cache nFace reduces the attack efficiency by up to 50% and surpasses the other proposal found in the literature in all scenarios analyzed.",Tese_VERSÃO FINAL.pdf,SISTEMAS DE COMPUTAÇÃO,ANDRE LUIZ NASSERALA PIRES,UNIVERSIDADE FEDERAL FLUMINENSE,14/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Future Internet;CCN;Security;Denial of Service',-,IGOR MONTEIRO MORAES,0,Internet do Futuro;CCN;Segurança;Negação de Serviço,COMPUTAÇÃO (31003010046P4),-,"As Redes Orientadas a Conteúdo (ROC) são um novo paradigma de comunicação para a Internet. As ROCs propõem um novo modelo de comunicação que, diferentemente da arquitetura TCP/IP centrada na interconexão entre sistemas finais, busca recuperar o conteúdo requisitado por um usuário independente da localização física ou lógica desse conteúdo. Existem diversas propostas de arquiteturas para esse novo paradigma. Uma dessas propostas, e que vem tendo destaque na literatura é a arquitetura Content Centric Networking (CCN).
	Uma das principais características da CCN é que tanto roteadores, quanto sistemas finais podem armazenar conteúdos recebidos em cache para aumentar sua disponibilidade. Essa característica torna a arquitetura mais robusta a ataques de negação de serviço (Denial of Service - DoS) tradicionais. Isso ocorre porque nós intermediários podem atender as solicitações de conteúdo sem que as solicitações cheguem diretamente à fonte do conteúdo. Então não existem garantias que o ataque afetará o nó vítima. Porém, com essa nova arquitetura novos tipos de ataques de negação de serviço surgiram explorando características presentes na CCN.
	Essa tese propõe e avalia uma contramedida para comedir o ataque de negação de serviço por conluio produtor-consumidor na arquitetura CCN chamada de Cache nFace. Nesse ataque, consumidores e produtores maliciosos agem em conluio, gerando, disponibilizando e manipulando a popularidade de conteúdos. Assim, aumenta-se a probabilidade de um consumidor legítimo recuperar o conteúdo diretamente do produtor, aumentando o seu tempo de recuperação. A contramedida proposta comede este ataque, dividindo o cache de um nó em sub-caches. Cada sub-cache armazena somente os conteúdos solicitados através de uma interface de rede específica. Isso aumenta a robustez do cache sob ataque. Sem a contramedida, se um determinado nó encontra-se no caminho entre o consumidor e o produtor malicioso, seu cache inteiro é comprometido sob ataque. Isso ocorre porque o consumidor malicioso solicita dados produzidos pelo produtor malicioso numa taxa elevada ao ponto de remover conteúdos legítimos do cache dos nós intermediários. Porém, a probabilidade de todas as interfaces de rede de um nó receberem tráfego de ataque ao mesmo tempo é pequena. Essa é a premissa para o bom funcionamento do Cache nFace.
	A análise é feita em duas topologias, uma em árvore com 32 nós, e outra em malha, com 192 nós. Variam-se a posição e a quantidade de atacantes para ambas topologias. As métricas utilizadas são tempo médio de recuperação de conteúdos legítimos, a ocupação maliciosa média do cache dos roteadores, a taxa média de erros de cache dos conteúdos legítimos e o percentual de conteúdos legítimos recuperados do produtor legítimo. Os resultados mostram que o Cache nFace reduz em até 50% a eficiência do ataque e supera a outra proposta encontrada na literatura em todos os cenários analisados.",TESE,Negação de Serviço por Conluio Produtor-Consumidor em Redes Orientadas a Conteúdo,5026511,
"Brazilian public policies for the inclusion of people with disabilities alert both to the need to include such people in the labor market and to the provision of an inclusive educational system. Currently the promotion of social inclusion is supported by laws that guarantee the reservation of places and the right of these people to take up positions in the public administration, which depends on the previous approval in public contests. This approval relies on the educational and professional background of the candidates. For many, this education passes through the entrance to Federal Institutions of Higher Education, that currently select integral students through the note obtained in the National High School Examination (Enem). The Enem is an evaluation instrument applied annually by the Ministry of Education (MEC). In addition to assessing the candidate’s performance, it is used as a selection criteria for various federal universities and government programs that provide benefits to students. Some of the candidates who perform the Enem are people who have some type of visual impairment and need specialized care to perform the test. In 2014, 1966 candidates with blindness and 11543 candidates with low vision underwent the examination. Despite the efforts of MEC – and the companies hired to prepare and apply the exam – to remove barriers and promote equality, some candidates report frustrating experiences during the tests, such as the lack of preparation of the readers. In this scenario, the use of a computational tool could be an alternative to promote the autonomy and equality of candidates with blindness or low vision. However, any tool intended for this purpose must be carefully designed so as not to impair the demonstration of skills and abilities of such candidates. In this work, we identify the needs of the visually impaired candidates in the context of performing computer-delivered tests to investigate opportunities for appropriate interaction for this audience in digital exams, using the Research-Action method. In a cyclic process composed of four stages, we evaluate the accessibility, usability and functionality of a prototype designed from the functional and non-functional requirements identified, observing aspects related to the interaction that must be considered in the development of a computational tool for adequate delivery of the Enem this audience.",Dissertacao_Versão final.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,HEDI CARLOS MININ,UNIVERSIDADE FEDERAL FLUMINENSE,12/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Accessibility;Usability;Digital Exam;Visual disability;ENEM',-,DANIELA TREVISAN,0,Acessibilidade;Usabilidade;Exame digital;Deficiência visual;ENEM,COMPUTAÇÃO (31003010046P4),-,"As políticas públicas brasileiras para a inclusão de pessoas com deficiência alertam tanto para a necessidade de inclusão de tais pessoas no mercado de trabalho como também para a oferta de um sistema educacional inclusivo. Atualmente o fomento à inclusão social é apoiado por leis que garantem a reserva de vagas e o direito destas pessoas de ingressarem em cargos na administração pública, o que depende da aprovação prévia em concurso público de provas e que resvala, consequentemente, no aspecto da formação educacional e profissional dos candidatos. Para muitos, esta formação passa pelo ingresso em Instituições Federais de Ensino Superior (IFES), que atualmente selecionam alunos integrantes por meio da nota obtida no Exame Nacional do Ensino Médio (Enem). O Enem é um instrumento de avaliação aplicado anualmente pelo Ministério da Educação (MEC) que, além de avaliar o desempenho do candidato, é utilizado como instrumento de seleção para diversas universidades federais e programas governamentais que concedem benefícios aos estudantes. Parte dos candidatos que realizam o Enem são pessoas que possuem algum tipo de deficiência visual e necessitam de atendimento especializado para realização da prova. Em 2014, 1966 candidatos com cegueira e 11543 candidatos com baixa visão realizaram o exame. Apesar dos esforços das empresas prestadoras de concurso e do MEC para eliminar barreiras e promover a igualdade, alguns candidatos relatam experiência frustrantes durante a realização das provas, como a falta de preparação dos ledores. Neste cenário, a utilização de uma ferramenta computacional se apresenta como uma alternativa possível para promover a autonomia e igualdade dos candidatos com cegueira ou baixa visão. Entretanto, qualquer ferramenta destinada a este propósito deve ser cuidadosamente projetada para que não prejudique a demonstração de competências e habilidades de tais candidatos. Neste trabalho, identificamos as necessidades dos candidatos com deficiência visual no contexto de realização de testes entregues por computadores para investigarmos oportunidades de interação adequada para este público na realização de exames digitais, utilizando, para tanto, o método Pesquisa-Ação (PA). Em um processo cíclico composto por quatro etapas, avaliamos a acessibilidade, usabilidade e funcionalidade de um protótipo concebido a partir dos requisitos funcionais e não funcionais identificados, observando aspectos relacionados com a interação que devem ser considerados no desenvolvimento de uma ferramenta computacional para entrega adequada do Enem a este público.",DISSERTAÇÃO,Acessibilidade em Exames Digitais para Deficientes Visuais,5026614,
"The evolution of the natural interaction between man and computer has represented a positive and promising impact for virtual reality (VR) applications. There is a growing interest in developing new approaches and technologies to improve the user experience so that it can be as natural and immersive as possible. Thus, this work aims to introduce a new concept of natural interaction in virtual environments using the upper limbs (arm, forearm and hand), with the combination of two types of sensors, classified as depth cameras and inertial sensors. While depth cameras allow precise tracking of the forearm, hand and fingers, their limited field of view restricts the range of the movements. On the other hand, the inertial sensors offer more freedom of movement, since they are not based on cameras and computer vision. However, they are not accurate enough to capture in detail the hands motions. In this work, we present a combination strategy using both types of sensors, aiming to improve the user experience by providing a robust interface control. To test the proposed solution, a VR game based on the use of the proposed strategy was developed. An experimental study with users was also developed and the results show that the proposed solution outperforms the use of the sensors separately, mainly in terms of performance and fun. Although our proposal is focused on VR games, it can also be an important interaction interface for any other VR based application.",Dissertação_Eider_Final.pdf,COMPUTAÇÃO VISUAL,EIDER CARLOS PAULINO DA SILVA,UNIVERSIDADE FEDERAL FLUMINENSE,13/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Virtual reality;depth cmeras;inertial sensors;interpolation;natural interaction',-,ANSELMO ANTUNES MONTENEGRO,0,Realidade virtual;câmeras de profundidade;sensores inerciais;interpolação;interação natural,COMPUTAÇÃO (31003010046P4),-,"A evolução da interação natural entre homem e computador tem representado um impacto positivo e promissor para aplicações em VR (Virtual Reality). Existe um crescente interesse no desenvolvimento de novas abordagens e tecnologias para melhorar a experiência do usuário de forma que seja tão natural e imersiva quanto possível. Assim, este trabalho visa introduzir um novo conceito de interação natural em ambientes virtuais usando os membros superiores (braço, antebraço e mão), com a combinação de dois tipos de sensores, classificados como câmeras de profundidade e sensores inerciais. Enquanto as câmeras de profundidade permitem o rastreamento preciso do antebraço, mãos e dos dedos, o alcance do campo de visão limitado restringe a área de captura de movimentos. Por outro lado, os sensores inerciais oferecem mais liberdade de movimento, uma vez que não são baseados em câmeras e visão computacional. No entanto, eles não possuem precisão suficiente para capturar em detalhes os movimentos das mãos. Neste trabalho, apresentamos uma estratégia de combinação usando os dois tipos de sensores, com o objetivo de melhorar a experiência do usuário fornecendo um controle de interface robusto. Para testar a solução proposta, foi desenvolvido um jogo em VR baseado no uso da estratégia proposta. Foi também elaborado um estudo experimental com usuários e os resultados mostram que a solução proposta supera o uso dos sensores separadamente, principalmente em termos de desempenho e diversão. Embora a nossa proposta esteja focada em jogos para VR, pode ser também uma interface de interação importante para qualquer outra aplicação baseada em VR.",DISSERTAÇÃO,Desenvolvimento de um Mecanismo de Interação para Ambientes de Realidade Virtual Baseada em Movimentos de Membros Superiores,5026965,
"Stories are crafted with series of actions and events that carry the notion of cause and consequence, to what we call experience. Therefore, they constitute an important method of learning, entertainment and, more generally, of communication between people. Deal with stories in computational form would allow the development of systems better in each of these competences. To this end, works on the eld of storytelling have been studying how to represent, generate and present story automatically. In the context of story generation, one of the most used methods involves the simulation of virtual worlds inhabited by agents that reason and act as characters. Just like people, characters act thinking about each other, creating plans to manipulate, disturb or help the others, and thus are capable of make their stories more interesting. In this work it is presented a methodology, purposely prone to errors, to allow the characters to plan their own actions considering the actions of the others and, consequently, demonstrate imperfect social behavior. This is made by modeling their knowledge with support to theory of mind and uncertainty, and by using a perception simulation process. Given that to anticipate the actions of the others it is necessary to have their knowledge, it was also developed and combined abductive reasoning processes, to allow the characters to try to understand what others have in mind given their actions. The methodology was tested preliminarily with the generation of stories based on the Little Red Riding Hood fable, and then with the generation of stories based on a haunted house scenario. The stories resulting from the haunted house scenario were evaluated through the application of survey that, comparing them with stories generated without the methodology, indicated an increase of verisimilitude and narrative coherence.",Tese_Versão final.pdf,COMPUTAÇÃO VISUAL,DAVID BATISTA CARVALHO,UNIVERSIDADE FEDERAL FLUMINENSE,12/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'storytelling;computational creativity;theory of mind;social behavior;social planning;abductive reasoning;perception simulation',-,ESTEBAN WALTER GONZALEZ CLUA,0,storytelling;criatividade computacional;teoria da mente;comportamento social;planejamento social;raciocínio abdutivo;simulação de percepções,COMPUTAÇÃO (31003010046P4),-,"Histórias são construídas com séries de ações e eventos que carregam consigo a noção de causa e consequência, ao que damos o nome de experiência. Por isso, elas constituem um método importante de aprendizado, entretenimento e, de uma maneira geral, de comunicação entre pessoas. Lidar com histórias de forma computacional permitiria o desenvolvimento de sistemas melhores em cada uma dessas competências. Para este m, trabalhos da área de storytelling vem estudando como representar, gerar e apresentar histórias automaticamente. No contexto de geração de histórias, um dos métodos mais utilizados envolve a simulação de mundos virtuais habitados por agentes, que raciocinem e atuem como personagens. Assim como pessoas, personagens atuam pensando uns nos outros, criando planos para manipular, atrapalhar ou ajudar os outros, e assim conseguem tornar suas histórias mais interessantes. Neste trabalho é apresentada uma metodologia, propositalmente propensa a erros, para permitir aos personagens planejar as próprias ações considerando ações alheias e, consequentemente, demonstrar comportamento social imperfeito. Isso é feito através da modelagem do conhecimento com suporte a teoria da mente e a incertezas, e do uso de um processo de simulação de percepções. Dado que para antecipar as ações dos outros é necessário ter o conhecimento deles, também são foram desenvolvidos e combinados processos de raciocínio abdutivo, para permitir aos personagens tentar entender o que os outros têm em mente dadas as suas ações. A metodologia foi preliminarmente testada pela geração de estórias com base na fábula Chapeuzinho Vermelho, e em seguida pela geração de estórias com base em um cenário de casa mal-assombrada. As estórias resultantes do cenário da casa mal-assombrada foram avaliadas através da aplicação de um questionário que, comparando-as com estórias geradas sem a metodologia, indicou um aumento de verossimilhança e coerência narrativa.",TESE,Comportamento Social de Personagens na Geração de Estórias por Computador,5026981,
"Software inspections are an efficient mean to improve quality. Learning Styles (LS) have been used to detect an individual’s preferences to acquire and process information according to different dimensions. In related work concerning requirements inspections, inspection teams with different LS showed more effective at detecting defects than teams with similar LS. The goal of this work is to investigate the influence of inspector’s LS on the effectiveness and efficiency of design inspections. A quasi-experiment was conducted using a subset of a real requirements document and corresponding design diagrams with seeded defects, characterizing each of the participants according to their LS. A script to combine inspectors into nominal inspection teams was implemented. For design inspections no influences of LS were detected in the individual inspection, but teams with similar LS (homogeneous teams) were more effective and efficient than teams with dissimilar LS (heterogeneous teams). However, replications are needed, including more participants and different contexts, to reinforce the obtained results.",Dissertacao_Versão final.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,GISELE DOMINGOS ALVES,UNIVERSIDADE FEDERAL FLUMINENSE,18/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'software design inspection;inspection teams;learning styles;quasi-experiment',-,MARCOS KALINOWSKI,0,Inspeção de software;projeto de software;diagramas de software;equipes de inspeção;estilos de aprendizado;estudo experimental;quasi-experimento,COMPUTAÇÃO (31003010046P4),-,"As inspeções de software são meios eficientes de aumentar a qualidade. O estilo de aprendizado (Learning Style - LS) tem sido utilizado para detectar as preferências de um indivíduo quanto à aquisição e processamento de informação. Em trabalhos relacionados, sobre inspeção de requisitos, equipes de inspeção com diferentes LS se mostraram mais eficazes na detecção de defeitos do que as equipes com LS similares. O objetivo desse trabalho é investigar a influência do LS do inspetor na eficácia e na eficiência das inspeções em diagramas de projeto de software. Um quasi-experimento foi conduzido utilizando um subconjunto de requisitos real e os diagramas correspondentes a ele com defeitos semeados, caraterizando cada participante de acordo com seu LS. Também foi utilizado um script para combinar os inspetores em equipes nominais de inspeção. Para inspeção de diagramas não foram detectadas  influências do LS na inspeção individual, porém equipes com LS similares (equipes homogêneas) foram mais eficazes e eficientes do que as equipes com LS diferentes (equipes heterogêneas). Entretanto, para reforçar os resultados obtidos são necessárias mais replicações do estudo, envolvendo mais participantes e diferentes contextos.",DISSERTAÇÃO,Investigando o Impacto de Estilos de Aprendizado em Equipes de Inspeção de Projeto de Software,5028175,
"Given a graph G=(V,E) and a threshold   (0,1], a -clique is any subset C of V such that the density of the graph induced in G by C is greater than or equal to . The maximum cardinality -clique problem amounts to finding a maximum cardinality subset C* of the vertices in V such that the density of the graph induced in G by C* is greater than or equal to the threshold . In the first part of this dissertation, we propose an exact algorithm for solving the maximum quasi-clique problem, based on a quasi-hereditary property. Computational experiments show that the new algorithm is competitive with the best formulations in the literature solved by CPLEX. The algorithm also provides a new upper bound that is consistently tighter than previously existing bounds. In the second part of this dissertation, we propose a hybridization of a biased random-key genetic algorithm with the exact algorithm developed in the first part of the dissertation. Computational results show that the hybrid approach outperforms the original genetic algorithm.",Dissertação_VERSÃO FINAL.pdf,ALGORITMOS E OTIMIZAÇÃO,JOSE ANGEL RIVEAUX MERIÑO,UNIVERSIDADE FEDERAL FLUMINENSE,25/07/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Maximum cardinality quasi-clique problem;maximum quasi-clique problem;maximum clique problem;graphs;biased random-key genetic algorithms;metaheuristics;graph density',-,CELSO DA CRUZ CARNEIRO RIBEIRO,0,Problema da quase-clique de cardinalidade máxima;problema da quase-clique máxima;problema da clique máxima;grafos;algoritmos genéticos baseados em chaves aleatórias;metaheurísticas;densidade do grafo,COMPUTAÇÃO (31003010046P4),-,"Dado um grafo G=(V,E) e um limitante   (0,1], uma -clique é qualquer subconjunto C de V tal que a densidade do grafo induzido em G por C é maior ou igual a . O problema da -clique de cardinalidade máxima consiste em determinar um subconjunto de cardinalidade máxima C* dos nós de V tal que a densidade do grafo induzido em G por C* seja maior ou igual ao limitante . Na primeira parte dessa dissertação, propõe-se um algoritmo exato para o problema da quase-clique máxima, baseado em uma propriedade quase-hereditária. Experimentos computacionais mostram que o novo algoritmo é competitivo com as melhores formulações exatas da literatura resolvidas pelo CPLEX. O algoritmo também fornece uma nova cota superior que é consistentemente mais justa do que as cotas já conhecidas. Na segunda parte da dissertação, propõe-se a hibridização de um algoritmo genético com chaves aleatórias tendenciosas com o algoritmo exato desenvolvido na primeira parte da dissertação. Resultados computacionais mostram que o enfoque híbrido tem melhor desempenho do que o algoritmo genético original.",DISSERTAÇÃO,An Exact Algorithm for the Maximum Quasi-Clique Problem,5028878,
"[Context] Problems in Requirements Engineering (RE) can lead to serious consequences during the software development lifecycle. [Goal] The goal of this dissertation is to propose empirically-based guidelines that can be used by different types of organisations, according to their process model (agile or plan-driven), to help them preventing RE problems. [Method] To achieve this goal, information collected in a survey on RE problems was used. Data from 228 organisations of 10 different countries was analysed to propose the guidelines. Thereafter, the guidelines were evaluated and refined based on feedback from experts in the field. [Results] From the collected survey data, the RE problems considered by the respondents as the most critical ones, their causes and mitigation actions were identified and organised by clusters of process model. Finally, the causes and mitigation actions of the critical problems of each cluster were analysed to get further insights into guidelines for potentially preventing them. The feedback from the experts allowed refining the guidelines. [Conclusions] This dissertation provides the resulting guidelines, which can be used, according to the characteristic context of the companies, as a starting point to support preventing critical RE problems.",Dissertaçao_versão final.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,PRISCILLA MAFRA DE CARVALHO MARQUES,UNIVERSIDADE FEDERAL FLUMINENSE,18/07/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'guidelines;problem prevention;defect prevention;requirements engineering',-,MARCOS KALINOWSKI,0,diretrizes;prevenção de problemas;prevenção de defeitos;engenharia de requisitos,COMPUTAÇÃO (31003010046P4),-,"[Contexto] Problemas na Engenharia de Requisitos (ER) podem levar a sérias consequências durante o ciclo de vida de desenvolvimento de um software. [Objetivo] O objetivo dessa dissertação é propor diretrizes que poderão ser utilizadas por diferentes tipos de empresas, de acordo com seu modelo de processo (ágeis e direcionadas a planos), para prevenir problemas da ER. [Método] Para atingir tal objetivo, informações coletadas de um projeto focado em problemas da ER foram utilizadas. Dados referentes à 228 empresas de 10 países diferentes foram analisados para compor as diretrizes. Em seguida, as diretrizes foram avaliadas e refinadas com base no feedback de especialistas da área. [Resultados] A partir dos dados coletados, os problemas considerados pelos participantes como mais críticos da ER, suas causas e possíveis ações de mitigação foram identificadas e organizadas em grupos de acordo com o modelo de processo. Finalmente, as causas e ações de mitigação dos problemas considerados mais críticos de cada grupo de modelo de processo foram analisadas para embasar diretrizes que possam ser utilizadas para prevenir tais problemas. A avaliação pelos especialistas permitiu o refinamento das diretrizes. [Conclusões] Esta dissertação disponibiliza as diretrizes resultantes, que podem ser utilizadas, de acordo com o contexto característico de cada empresa, como ponto de partida para apoiar a prevenção de problemas críticos da ER.",DISSERTAÇÃO,Guidelines For Preventing Requirements Engineering Problems,5029912,
"Tools for exploration, analysis and discovery of new knowledges are the key to enabling a large amount of data to make more sense to citizens. However, data is not understood efficiently when only common visual representations, such as tables and lists, are used. Large datasets are best assimilated when we have more elaborate visualizations that allow us to find patterns and recognize correlations between variables. Building and using data visualizations that are clear, accurate, efficient and consistent can be a very powerful solution for identifying trends and highlighting outliers, for example. However, disorientation and misinterpretation of readers is a recurring problem when we use visualization. Increasingly, we realize the need to provide citizens with better interpretable data visualizations, which show information more clearly to the user. In this paper, we propose two novel evaluations of visualizations – the presentation quality assessment and the interpretability assessment. To facilitate the understanding and use of these instruments, we proposed also a process that defines the steps that must be taken to carry out each of the two evaluation activities. The purpose of this process is to enable its users to create better views, making the data understandable, recognizable, and interpretable to readers in an effective and accurate manner. We carried out experiments with users from diverse backgrounds to validate the proposed approaches. Based on the results of the experiments, we could conclude that it is possible to use the two evaluation methods to measure both the presentation quality the interpretability of visualizations. Therefore, using the evaluation methods proposed in this work to improve the creation of data visualizations as a whole, is a feasible aspiration.",dissertacao_versão final.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,RAISSA DOS SANTOS BARCELLOS,UNIVERSIDADE FEDERAL FLUMINENSE,03/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Data visualization;Public transparency;Interpretability',-,JOSE VITERBO FILHO,0,Visualização de dados;Transparência pública;Interpretabilidade,COMPUTAÇÃO (31003010046P4),-,"Ferramentas para exploração, análise e descoberta de novos conhecimentos são a chave para permitir que uma grande quantidade de dados faça mais sentido para os cidadãos. Porém, os dados não são compreendidos de maneira eficiente quando são utilizadas apenas representações visuais comuns, como tabelas e listas. Grandes conjuntos de dados são melhores assimilados quando contamos com visualizações mais elaboradas, que permitam encontrar padrões e reconhecer correlações entre variáveis. Construir e utilizar visualizações de dados que sejam claras, precisas, eficientes e coerentes pode ser uma solução muito poderosa para identificar tendências e destacar outliers, por exemplo. Porém, desorientação e má interpretação diante dos leitores é um problema recorrente quando utilizamos visualização. Cada vez mais, percebe-se a necessidade de prover ao cidadão visualizações de dados melhor interpretáveis, que passem informações mais claramente ao usuário. Neste trabalho, são apresentadas propostas de duas avaliações – a avaliação da qualidade da apresentação de visualizações e a avaliação da interpretabilidade de visualizações. Para facilitar o entendimento e uso desses instrumentos, propõe-se também um processo que define os passos que devem ser executados para a realização de cada uma das duas atividades de avaliação. A finalidade deste processo é permitir que seus utilizadores criem melhores visualizações, tornando os dados, nelas representados, compreensíveis, reconhecíveis, e interpretáveis aos usuários, de maneira eficiente e acurada. Experimentos com usuários de diversos perfis foram realizados com o intuito de validar as avaliações propostas. Em posse dos resultados dos experimentos, concluiu-se que é possível utilizar as duas avaliações para mensurar tanto a qualidade da apresentação de visualizações, quanto a interpretabilidade das mesmas. Logo, melhorar a geração de visualização de dados como um todo, utilizando as avaliações propostas neste trabalho é uma aspiração factível.",DISSERTAÇÃO,Avaliação da Qualidade e Interpretabilidade de Visualizações de Dados,5032579,
"In Wireless Sensor Networks, nodes typically employ batteries that cannot be recharged or replaced. Hence, optimizing energy consumption is a major concern. Among the several solutions already proposed, schedule-based asynchronous duty cycle methods are the simplest because they do not require mechanisms, protocols or specific hardware for clock synchronization between nodes. These solutions, however, result in high latency for multi-hop communication. In this work, we show how existing asynchronous mechanisms can benefit from a low level of synchronism, with resolution of slots. We show, using numerical simulations and simple mathematical models, that the use of specific offsets between clocks of neighboring nodes according to their distances to the sink node drastically reduces latency. Additionally, we implemented a routing protocol prototype with the proposed low resolution synchronization functionalities for the TinyOS plattaform. With this implementation, we were able to run network simulations using the well-known TOSSIM simulator. The results of these simulations agree with the results from the other evaluation methods, consistently showing representative gains in terms of communication delay.",dissertacao_VERSÃO FINAL.pdf,SISTEMAS DE COMPUTAÇÃO,ANDRE RICARDO DE CARVALHO SARAIVA,UNIVERSIDADE FEDERAL FLUMINENSE,04/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Wireless Sensor Networks;Duty Cycle;Asynchronous Mechanisms',-,DIEGO GIMENEZ PASSOS,0,Rede de Sensores Sem Fio;Duty Cycle;Mecanismos Assíncronos,COMPUTAÇÃO (31003010046P4),-,"Nas Redes de Sensores Sem Fio, nós tipicamente operam por baterias não recarregáveis ou substituíveis. Logo, a otimização do consumo energético é uma das principais preocupações. Dentre as várias soluções já propostas, os métodos de duty cycle assíncrono baseados em escalonamento se mostram os mais simples por não demandarem mecanismos, protocolos ou hardwares específicos para sincronização de relógio entre os nós. Esses métodos, no entanto, resultam em alta latência para comunicação de múltiplos saltos. Neste trabalho, demonstramos como os mecanismos assíncronos já existentes podem se beneficiar de um baixo nível de sincronismo, em resolução de slots. Mostramos com simulações numéricas e modelos matemáticos simplificados que o uso de offsets específicos entre os relógios de nós vizinhos, de acordo com suas distâncias ao nó sorvedouro, reduz drasticamente a latência. Além disso, implementamos sobre a plataforma TinyOS um protótipo de protocolo de roteamento com as funcionalidades propostas de sincronização de baixa resolução. Com base nesta implementação, realizamos simulações de rede utilizando o simulador TOSSIM. Os resultados dessas simulações corroboram os resultados das demais avaliações, mostrando ganhos representativos em termos de latência de comunicação.",DISSERTAÇÃO,Reduzindo a Latência de Comunicação em Múltiplos Saltos dos Mecanismos de Duty Cycle Assíncronos Baseados em Escalonamento através de Sincronização de Baixa Resolução,5032613,
"The world population has been rising on a large scale and this directly reflects on the electricity consumption. Given this scenario, techniques for accurately forecasting energy consumption are very useful, since these data can be applied in decision-making and good planning aimed at providing uninterrupted and reliable energy. There are several techniques that are usually used to forecast energy consumption, among them, regression techniques have been highlighted as the most used in forecasting electricity consumption. Different regression approaches, with specific purposes to solve problems of certain countries or regions, are found in the works presented in the literature. Classical techniques such as linear regression, multilayer perceptron neural network and support vector regression, but also emerging techniques of optimization and fuzzy logic, are usually employed. Forecasting energy consumption with the most accurate value possible is not a trivial task and depends on a number of factors. The most popular models for estimating energy demand are defined based on consumption history and some socioeconomic factors. According to empirical studies, one of the most important factors for forecasting energy consumption is Gross Domestic Product (GDP). Based on this, some models are restricted to treating only energy consumption and GDP. One of the more recent works dealing with the subject at the national level presented a fuzzy logic-based prediction model using consumption, GDP and population and obtained good results. This work aims to evaluate, through an experimental study, the performance of classical regression techniques - linear regression, multilayer perceptron neural network and support vector regression - in energy consumption forecast in the Brazilian scenario. In addition, we verified whether the inclusion of additional socioeconomic data, such as minimum wage, dollar exchange rate, average energy consumption and oil price, would contribute to obtaining a more efficient model. When compared to the results available in the literature for the national scenario, our approach demonstrated superior performance in some situations.",Dissertação___VERSÃO FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,LEONARDO PIO VASCONCELOS,UNIVERSIDADE FEDERAL FLUMINENSE,08/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'energy consumption;regression;linear regression;multilayer perceptron neural network;support vector regression',-,JOSE VITERBO FILHO,0,consumo de energia;regressão;regressão linear;redes neurais perceptron multicamadas;máquina de vetores de suporte para regressão,COMPUTAÇÃO (31003010046P4),-,"A população mundial vem se elevando em grande escala e isso reflete diretamente no consumo de energia elétrica. Diante deste cenário, técnicas para a previsão do consumo de energia de forma acurada se apresentam com grande utilidade, pois estes dados podem ser aplicados em tomadas de decisões e um bom planejamento direcionados ao provimento ininterrupto e confiável de energia. Existem diversas técnicas que são usualmente empregadas para a previsão do consumo de energia, dentre estas, a técnica de regressão vem se destacando como uma das mais utilizadas na previsão do consumo de eletricidade. Diferentes abordagens de regressão, com objetivos específicos de resolver problemas de determinados países ou regiões, são encontradas nos trabalhos apresentados na literatura. São usualmente empregadas técnicas clássicas como regressão linear, redes neurais perceptron multicamadas e máquina de vetores de suporte para regressão, mas também técnicas emergentes de otimização e lógica nebulosa. Prever o consumo de energia com o valor mais preciso possível não é uma tarefa trivial e depende de uma série de fatores.  Os modelos mais populares para estimar a demanda de energia são definidos baseando-se no histórico de consumo e alguns fatores socioeconômicos. Segundo estudos empíricos, um dos fatores mais importantes para a previsão do consumo de energia é o Produto Interno Bruto (PIB). Baseando-se nisso alguns modelos gerados se restringem a tratar somente o consumo de energia e o PIB. Um dos trabalhos mais recentes a tratar o assunto no escopo nacional apresentou um modelo de previsão baseado em lógica fuzzy utilizando o consumo juntamente com o PIB e a população e obteve bons resultados. Esse trabalho tem por objetivo avaliar, a partir de um estudo experimental, o desempenho de técnicas de regressão clássicas - regressão linear, redes neurais perceptron multicamadas e máquina de vetores de suporte para regressão - na previsão do consumo de energia no cenário nacional. Além disso, verificamos se a inclusão de dados socioeconômicos adicionais, como salário mínimo, taxa de câmbio, tarifa média do consumo de energia e preço do barril de petróleo, contribuiriam para a obtenção de um modelo mais eficiente. Quando comparada aos resultados disponíveis na literatura para o cenário nacional, nossa abordagem demonstrou uma performance superior em algumas situações.",DISSERTAÇÃO,Um Estudo de Técnicas de Regressão para Previsão Eficiente de Consumo de Energia,5033789,
"Disorders of the thyroid gland are very common health problems. Thermography is a diagnostic tool that can be used to detect superficial abnormalities in an organism. Some authors have stated that they detect thyroid nodules by segmenting hot regions in thermograms even using simple methodologies. Medical diagnosis uses more information that is considered comprehensively, and accurate diagnosis is not feasible unless done by using multiples aspects. The main objective of this work is to study the possibility of using infrared images in the thyroid nodule analysis, and to include in this analysis patient´s data that enable a future development of a diagnostic aid program. The work is based on the induction of a temperature variation over the patient and the study of their return to steady state, which is called Dynamic Infrared Thermography. This work presents a general review of topics related to the analysis of thyroid nodules, develops a protocol for the acquisition of thermal thyroid imaging, creates and publishes a database with thermal images in addition to clinical and physiological data from patients of Antônio Pedro University Hospital of the Fluminense Federal University. Nodal position identification is provided on at least one thermal image of each patient who agreed to participate voluntarily in this research and signed a consent form. The possibility of identification of nodules (benign and malignant) from the region of the neck and upper part of the chest (denominated region of interest - ROI) in the infrared images was verified. For this, data were created from series of intensities (related to temperatures) from randomly selected points (from node regions and not nodules) and tracked in each thermogram. Were analyzed the difference from first to fifth order of these series, as well as the values of the sum of each such differences. It was found that it is not possible to identify healthy regions of regions with nodules with the employed method, but there are indications that malignant nodules present higher temperature than benign nodules (requiring more cases with proven malignancy to support this assumption). It was studied the possibility of developing systems to aid the segmentation of thyroid nodules in thermograms, concluding that it is possible to develop an interactive (or semi-automatic) system for this purpose, which would be of great help for later analyzes that are more sophisticated than aim to verify the viability of the use of thermal images in the classification of thyroid nodules as malignant or benign, or in the identification of nodular areas in these images.",Dissertação_VERSÃO FINAL.pdf,COMPUTAÇÃO VISUAL,JOSÉ RAMON GONZÁLEZ MONTERO,UNIVERSIDADE FEDERAL FLUMINENSE,08/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'infrared thermography;nodules;cancer;thyroid;temperature;intensity series;region growing',-,AURA CONCI,0,termografia infravermelha;nódulos;câncer;tireóide;temperatura;séries de intensidade;crescimento de regiões,COMPUTAÇÃO (31003010046P4),-,"Os distúrbios da glândula tireóide são problemas de saúde muito frequentes. A termografia é uma ferramenta de diagnóstico possível de ser usada para detectar algumas formas mais superficiais de anormalidades em um organismo. Alguns autores têm afirmado que detectam nódulos da tireóide ao segmentar as regiões quentes em termogramas mesmo usando metodologias simples. O diagnóstico médico utiliza diversas informações que são consideradas de forma abrangente e, um diagnóstico preciso é inviável apenas se feito sob um único ponto de vista. O objetivo principal deste trabalho é estudar a possibilidade do uso de imagens infravermelhas na análise de nódulos de tireóide, e incluir nesta análise outros dados do paciente que possibilitem um futuro desenvolvimento de um sistema de auxílio ao diagnóstico. O trabalho tem como base a indução de uma variação de temperatura no paciente e o estudo do seu retorno ao estado de equilíbrio, a denominada Termografia Infravermelha Dinâmica. Esse trabalho apresenta uma revisão geral de temas relacionados com a análise de nódulos da tiróide, desenvolve um protocolo para a aquisição de imagens térmicas de tireóide, cria e pública um banco de dados com as imagens térmicas além de dados clínicos e fisiológicos dos pacientes do ambulatório de Endocrinologia do Hospital Universitário Antônio Pedro da Universidade Federal Fluminense. É disponibilizada uma identificação da posição dos nódulos sobre pelo menos uma imagem térmica de cada paciente que aceitou participar voluntariamente nesta pesquisa e assinou um termo de consentimento. Foi verificada a possibilidade de identificação de nódulos (benignos e malignos) a partir da região do pescoço e parte superior do tórax (denominada de região de interesse) nas imagens infravermelhas. Para isso foram criados dados com séries de intensidades (relacionadas com as temperaturas) a partir de pontos selecionados randomicamente (das regiões de nódulo e não nódulos) e rastreados em cada termograma. São analisadas também as diferenças da primeira até a quinta ordem dessas séries, bem como os valores do somatório destas séries. Foi conferido que por esse método não é possível identificar regiões saudáveis de regiões com nódulos, mas existem indícios que nódulos malignos apresentam maior temperatura que nódulos benignos (precisando de mais casos com malignidade comprovada para respaldar essa suposição). Foi estudada a possibilidade de desenvolvimento de sistemas que auxiliem na segmentação de nódulos tireoidianos em termogramas, concluindo que é possível o desenvolvimento de um sistema interativo (ou semi-automático) com esse objetivo, o que seria de grande auxílio para posteriores análises mais sofisticadas que visem verificar a viabilidade do uso de imagens térmicas na classificação de nódulos da tireóide como malignos ou benignos, ou na identificação de áreas de nódulos nestas imagens.",DISSERTAÇÃO,Um Estudo sobre a Possibilidade do Uso de Imagens Infravermelhas na Análise de Nódulos de Tireóide,5034438,
"Wireless Networks are prone to collisions and interferences from other radiation sources due to the broadcast nature of the wireless channels. Collisions may occur when two or more stations sharing the physical medium transmit simultaneously. Contention-based protocols for medium access control (MAC) aim at reducing collisions in these networks. However, the presence of hidden terminals can still cause high levels of collision, especially as their number and the network load grow. Estimating the collision probability in wireless links may support and improve the decisions taken by routing protocols, rate adaptation algorithms, and other mechanisms. This work presents the CPE, a novel proposal to estimate the packet collision probability caused by the presence of hidden terminals in wireless networks. The proposed method is based on information regarding medium usage, which can be exchanged between two-hop neighbors. In order to evaluate the proposal, simulations were conducted comparing it to two other models found in related literature. In addition, this work presents an application of the model through simulation of an environment using a quality-aware routing metric. The results demonstrate that the model outperforms the other mechanisms in precision for the probability of collision estimation, while its application provides greater stability in the route selection and also higher throughput.",Dissertacao_versão final.pdf,SISTEMAS DE COMPUTAÇÃO,RODOLFO PIRES BULHOES,UNIVERSIDADE FEDERAL FLUMINENSE,09/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'wireless networks;collisions;hidden terminals;collision probability',-,CELIO VINICIUS NEVES DE ALBUQUERQUE,0,redes sem fio;colisões;estações ocultas;probabilidade de colisão,COMPUTAÇÃO (31003010046P4),-,"Redes sem fio são vulneráveis a eventos de colisão e interferência provenientes de outras fontes transmissoras devido à própria natureza de difusão do meio sem fio. Colisões podem ocorrer quando duas ou mais estações que compartilham o meio físico transmitem simultaneamente. Protocolos de acesso ao meio baseados em mecanismos de contenção têm como objetivo a redução dos eventos de colisão. Entretanto, a presença de estações ocultas pode ocasionar altas taxas de colisão nestas redes. A estimativa da probabilidade de colisão em enlaces sem fio pode auxiliar protocolos de roteamento, métodos para controle automático de taxa e outros mecanismos a fundamentar suas respectivas decisões de maneira mais adequada. Este trabalho apresenta o CPE, uma nova proposta para estimar a probabilidade de colisão em redes sem fio na presença de estações ocultas. O método proposto é baseado em informações acerca do uso do meio físico pelas estações participantes da rede, informações estas que podem ser trocadas entre vizinhos de dois saltos. Para avaliação do modelo proposto, este trabalho apresenta uma comparação com outros dois mecanismos encontrados na literatura. Adicionalmente, é apresentada uma aplicação do modelo através de simulação em um ambiente com roteamento baseado em métrica ciente de qualidade. Os resultados demonstram que o modelo supera os demais mecanismos em precisão das estimativas de probabilidade de colisão, enquanto a sua aplicação provê maior estabilidade na escolha de rotas e também maior vazão.",DISSERTAÇÃO,Estimativa de Probabilidade de Colisão em Redes Sem Fio,5034655,
"GPUs have established a new baseline for power efficiency and computing power, delivering larger bandwidth and more computing units in each new generation. Modern GPUs support the concurrent execution of kernels to maximize resource utilization, allowing kernels to better exploit idle resources. However, the decision on the simultaneous execution of different kernels is made by the hardware, and sometimes GPUs do not allow the execution of remaining blocks of other kernels, even with the availability of resources. In this work, we present an in-depth study on the simultaneous execution of kernels in the GPU. We present the necessary conditions for executing kernels simultaneously, we list the factors that influence competition and propose a model that describes performance degradation. Finally, we validate the model using kernels of real-world applications with different use intensities of computation and memory.",Dissertação_versão final.pdf,SISTEMAS DE COMPUTAÇÃO,ROMMEL ANATOLI QUINTANILLA CRUZ,UNIVERSIDADE FEDERAL FLUMINENSE,04/09/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Concurrent Kernels;Multiprogramming;GPU computing',-,ESTEBAN WALTER GONZALEZ CLUA,0,Kernels concorrentes;Multiprogramação;computação em GPU,COMPUTAÇÃO (31003010046P4),-,"As GPUs estabeleceram uma nova linha de base em relação à eficiência de energia e capacidade de computação, oferecendo maiores larguras de banda e mais unidades de computação em cada nova geração. As GPUs modernas suportam a execução simultânea de kernels para maximizar a utilização dos recursos, permitindo que kernels possam explorar melhor os recursos ociosos. Entretanto, a decisão da execução simultânea de kernels diferentes é realizada pelo hardware e muitas vezes as GPUs não permitem a execução de blocos remanescentes de outros kernels, mesmo com a disponiblidade de recursos. Neste trabalho, realizamos um estudo aprofundado sobre a execução simultânea de kernels na GPU. Apresentamos as condições necessárias para executar kernels simultaneamente, listamos os fatores que influenciam a concorrência e propomos um modelo que pode determinar a redução de desempenho. Finalmente, validamos o modelo utilizando kernels de aplicações reais com diferentes intensidades de computação e uso de memória.",DISSERTAÇÃO,Analyzing and Estimating the Performance of Concurrent Kernels Execution in GPUs,5047745,
"Focusing on improving the user experience regardless of the device used, interactive multimedia applications should provide self-tuning layouts. Some authoring languages as HTML5 and CSS3 allow the author to declare policies so that the application’s layout can adapt itself according to changes in the application context. However, those polices have limited expressiveness providing very basic relations such as the size of objects A and B must be equal. Other languages used for multimedia authoring, such as NCL and SMIL, do not provide native facilities for the document author to develop adaptive layouts. So, authors must define in details where each media object will be presented in a device screen, making authoring very hard.

This thesis proposes STyLe, an XML-based template language that provides constraints for specifying adaptive and dynamic spatial layouts. An adaptive layout allows authors to create generic presentation characteristics that adapt a document presentation layout depending on the number of media objects it contains, reducing the authoring effort to specify spatial layouts. An adaptive and dynamic spatial layout is an extension of an adaptive layout in such a way that the presentation characteristics of the media objects can be changed at runtime in response to event occurrences in document presentation, such as user interaction or live edition. STyLe provides this facility by defining the spatial layout of a hypermedia document through spatial constraints.

As proof of concept, the NCL multimedia authoring language and the XTemplate template authoring language have been extended to use STyLe for the design of document spatial layouts. In addition, an architecture capable of interpreting this language and performing the necessary changes in order to dynamically update NCL media object presentation characteristics at runtime is proposed and implemented.

This thesis also evaluates the performance of the proposed solution for dynamically adapting layout during NCL document execution measuring the response time for layout adaptation in different STyLe template examples.",Tese_versão final.pdf,SISTEMAS DE COMPUTAÇÃO,GLAUCO FIOROTT AMORIM,UNIVERSIDADE FEDERAL FLUMINENSE,29/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Dynamic Spatial Layout;Adaptive Layouts;STyLe;NCL;Spatial Constraints;XTemplate;Template Authoring',-,DEBORA CHRISTINA MUCHALUAT SAADE,0,Leiautes Dinâmicos;Leiautes Adaptativos;STyLe;NCL;Restrições espaciais;XTemplate;Autoria de Templates,COMPUTAÇÃO (31003010046P4),-,"Visando melhorar a experiência do usuário, independentemente do dispositivo utilizado, aplicações multimídia interativas podem oferecer leiautes autoajustáveis que são modificados dependendo das condições de visualização apresentadas. Algumas linguagens de autoria, como HTML5 e CSS3, permitem que o autor do documento declare diretivas para que o leiaute da aplicação responda a mudanças no contexto do usuário. Entretanto, essas diretivas têm expressividade limitada oferecendo relações muito básicas como, por exemplo, a que especifica que o tamanho do objeto A deve ser igual ao tamanho de B. Outras linguagens para autoria multimídia, como NCL e SMIL, não fornecem facilidades para que o autor do documento possa desenvolver leiautes adaptáveis, obrigando o autor a definir detalhadamente a região em que cada objeto de mídia visual ocupará na tela de cada dispositivo, o que torna a autoria bastante trabalhosa.

Esta tese propõe STyLe, uma linguagem de templates baseada em XML e que utiliza restrições para especificar leiautes espaciais adaptativos e dinâmicos para aplicações multimídia. Um leiaute espacial adaptativo é uma abordagem que possibilita que autores possam criar características de apresentação genéricas que adaptam o leiaute espacial especificado ao número de objetos de mídia de um dado documento, diminuindo o esforço de autoria na criação de leiautes espaciais. Um leiaute espacial adaptativo e dinâmico é uma extensão de um leiaute adaptativo de tal forma que as características de apresentação dos objetos de mídia possam ser alteradas em tempo de execução e em resposta a ocorrência de eventos na apresentação do documento, tais como interação do usuário ou edição ao vivo de partes do documento. STyLe fornece esta facilidade definindo o leiaute espacial de um documento hipermídia através de restrições espaciais.

Como prova de conceito, a linguagem de autoria multimídia NCL e a linguagem de autoria de templates XTemplate foram estendidas para utilizar STyLe na concepção dos leiautes espaciais dos documentos. Além disso, uma arquitetura capaz de interpretar as restrições espaciais contidas em um template e construir ou atualizar corretamente o leiaute espacial de um documento é proposta e implementada para o uso de STyLe em conjunto com NCL.

Esta tese também avalia o desempenho da solução apresentada para adaptar dinamicamente o leiaute espacial em tempo de execução de um documento NCL, medindo o tempo de resposta para adaptação de leiautes em diferentes exemplos de uso de STyLe.",TESE,Leiautes Dinâmicos para Documentos Multimídia Baseados em Templates,5055701,
"Wireless mesh networks are low cost and easy to deploy multi-hop wireless networks. These networks are useful in situations where there is little to none previous network infrastructure and network communication is necessary. However, wireless mesh networks still face challenges that limit their performance. One such challenge is the variability in routing metrics that causes constant changes in path choice. The oscillation in route choice causes suboptimal paths to be used, and may result in other unwanted effects such as routing loops. To tackle this challenge, this work proposes a different method for estimating the frame delivery probability of a link, an information that is commonly the base for routing metrics. This method is based on the concept of statistical hypothesis testing, and maintains a fixed estimation for the frame delivery probability until the behavior of the link in the recent past significantly deviates from the expectation. The proposed method was evaluated using simulations comparing it with the traditional method based on an Exponentially Weighted Moving Average. The results show that the proposed method is, indeed, capable of significantly reducing the variability in link quality estimates. Moreover, in terms of more traditional network performance metrics, simulations performed in a large number of instances demonstrate a slight preponderance of better results in terms of throughput in comparison to the traditional method of Exponentially Weighted Moving Average, thus corroborating the idea that reducing routing variability is connected to improvements in performance. The results also suggest that there is room for improvement in the proposed method, in terms of a faster detection of condition changes in the link.",Dissertação_versão final.pdf,SISTEMAS DE COMPUTAÇÃO,BRUNO DOS SANTOS SILVA,UNIVERSIDADE FEDERAL FLUMINENSE,27/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Wireless mesh networks;multi-hop wireless networks;wireless routing;routing metrics',-,DIEGO GIMENEZ PASSOS,0,Redes em malha sem fio;redes sem fio de múltiplos saltos;roteamento sem fio;métricas de roteamento,COMPUTAÇÃO (31003010046P4),-,"Redes em malha sem fio são redes sem fio de múltiplos saltos, de implantação simples e de baixo custo. Essas redes são úteis em situações em que há pouca ou nenhuma infraestrutura disponível e redes de comunicação são necessárias. No entanto, redes em malha sem fio ainda encontram desafios que limitam seu desempenho. Um desses desafios é a variabilidade das métricas de roteamento que causa constantes mudanças em escolha de rotas. A oscilação na escolha de rotas faz com que caminhos subótimos sejam utilizados, além de poderem resultar em outros efeitos indesejados como loops de roteamento. Para enfrentar esse desafio, este trabalho propõe um método diferente para estimar a probabilidade de entrega de quadros de um enlace, uma informação que é comumente usada como base por métricas de roteamento. Esse método se baseia no conceito de teste de hipóteses estatístico, e mantém uma estimativa fixa para uma probabilidade de entrega de quadros até que o comportamento do enlace no passado recente desvie significativamente do esperado. O método proposto foi avaliado usando simulações que o comparam com o método tradicional que utiliza uma Média Móvel Exponencialmente Ponderada. Os resultados mostram que o método proposto é, de fato, capaz de reduzir significativamente a variabilidade das estimativas de qualidade dos enlaces sem fio. Além disso, em termos de métricas mais tradicionais de desempenho de rede, simulações realizadas em um grande número de cenários mostraram uma ligeira preponderância de melhores resultados em termos de vazão com o uso do método proposto em comparação ao método tradicional de Média Móvel Exponencialmente Ponderada, assim corroborando a ideia de que a redução na variabilidade da métrica de roteamento está conectada a melhoras no desempenho. Os resultados apontam ainda espaço de melhora no método proposto no que tange a detecção mais rápida de mudanças no comportamento do enlace.",DISSERTAÇÃO,Redução da Variabilidade das Métricas de Roteamento para Redes em Malha sem Fio,5057140,
"The signal-tuned Gabor functions are Gaussian-modulated sinusoids whose parameters are defined by a given signal – the tuning signal –, of which the signal-tuned functions yield an exact representation. Such functions were originally introduced as kernels of a variant Gabor transform which has proven able to accurately detect the spatial and spectral signatures of the analyzed signal, even when the traditional Gabor approach fails. Here we propose and analyze a model, based on the signal-tuned framework, for the receptive fields and responses of cells in the primary visual cortex (V1). Specifically, the receptive fields of V1 cells are modeled as signal-tuned functions in the spatial (complex cells) or spectral (simple cells) domains, while the respective responses are given by the corresponding signal-tuned transforms. Such model replicates the neurophysiological responses of both cell types to standard stimuli, while also emulating receptive-field dependence on the stimulus set, as experimentally observed. Next, we introduce an extended version of the signal-tuned receptive fields, based on coding functions of the derivatives, in any order, of the tuning signal, obtained by incorporating Hermite polynomials to the original signal-tuned functions. We then define the responses of the simple-class and complex-class signal-tuned cells, show that they obey Shrödinger equations, and, based on this, propose a quantum-like interpretation of the signal-tuned framework.",Tese_versão final.pdf,COMPUTAÇÃO VISUAL,MARCOS SANGES DO AMARAL,UNIVERSIDADE FEDERAL FLUMINENSE,29/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Gabor Functions;Primary Visual Cortex;Schrdinger Equation;Quantum-Like Interpretation',-,JOSE RICARDO DE ALMEIDA TORREAO,0,Funções de Gabor;Córtex Visual Primário;Equação de Schrödinger;Interpretacão Quantum-Like,COMPUTAÇÃO (31003010046P4),-,"As funções sintonizadas de Gabor são senóides moduladas por Gaussianas, cujos parâmetros ficam determinados por um dado sinal – o sinal de sintonia –,  para o qual as funções sintonizadas fornecem uma representação exata. Tais funções foram originalmente propostas como núcleos de uma variante da transformada de Gabor, que se mostrou capaz de identificar com precisão eventos espaciais e espectrais do sinal analisado, mesmo em situaçõoes em que a abordagem de Gabor tradicional falha. No presente trabalho, nós propomos e analisamos um modelo, baseado na representação sintonizada, para os campos receptivos e as respostas de células do córtex visual primário (V1). Mais especificamente, os campos receptivos das células de V1 são modelados por funções sintonizadas do domínio espacial (células complexas) ou espectral (células simples), enquanto as respectivas respostas são fornecidas pelas transformadas sintonizadas correspondentes. Este modelo permite replicar as respostas neurofisiológicas de ambos os tipos de células a estímulos-padrão, além de reproduzir a variação dos seus campos receptivos de acordo com a classe do estímulo considerado. Em seguida, nós introduzimos uma versão generalizada dos campos receptivos sintonizados, com base em funções de codificação das derivadas, em qualquer ordem, do sinal de sintonia, obtidas pela incorporação de polinômios de Hermite às funções sintonizadas originais. A partir daí, nós definimos as respostas das classes de células que nós denotamos como da classe simples e da classe complexa, demonstramos que elas obedecem equações de Schrödinger e, com base nisto, propomos uma interpretação quantum-like para a abordagem sintonizada.",TESE,Abordagem Sintonizada de Gabor para a Modelagem de Neurônios do Córtex Visual Primário,5058402,
"The use of Content-Centric Networking architecture (CCN) in wireless mesh networks has become a broad field of research. One reason is the combination of cache deployment by intermediate nodes and the low implementation cost of wireless mesh networks. Caching by intermediate nodes can increase the throughput of WMNs. The reason for this increased performance is that popular contents can be retrieved from neighboring nodes to consumers. Thus, requests and contents do not have to traverse the entire consumer-producer path which reduces loss probability. However, the need for more research in this theme regarding the new challenges faced in the use of content-centric wireless mesh networks is notorious. One of these challenges is the broadcast storm problem. This broadcast storm is caused more specifically by the interest packets that request contents. Depending on the content request rate, the broadcast storm can become a limiting problem in wireless mesh networks based on the CCN architecture. The literature presents several proposals for protocols and mechanisms to reduce the problem of the broadcast storm caused by interest packets. Despite the existence of proposals, we emphasize that for multiple consumers requesting the same contents and scenarios with multiple producers, there is still a need for proposals that reduce negative impacts of the broadcast storm problem and increase the network throughput.

This thesis proposes three mechanisms: Probabilistic Interest Forwarding (PIF), Retransmission-Counter-based Interest Forwarding (ReCIF), and ReCIF + PIF. The first one defines a probability to forward interest packets. The second one limits the number of interest packets forwarded based on the number of previous forwarding actions of these packets. The third one is a hybrid approach that combines the forwarding criteria of the two previous mechanisms. The performance of a content-centric wireless mesh network is evaluated with the three proposed mechanisms and also with the default CCN forwarding mechanism and Listen First Broadcast Later (LFBL) protocol. The performance of such network is also compared with the OLSR protocol in a wireless mesh network based on the TCP/IP stack. Results show that the proposed mechanisms provide a higher delivery ratio than OLSR. Also, our proposals outperform the default forwarding mechanism by up to 22% regarding data delivery rate in dense scenarios with a high number of hops between source and destination and provide 49% lower delivery delay than the default CCN. One of our mechanisms, PIF, outperforms LFBL regarding data delivery rate and delivery delay by up to 92% and 55% respectively for high saturation levels.",tese_final.pdf,SISTEMAS DE COMPUTAÇÃO,DALBERT MATOS MASCARENHAS,UNIVERSIDADE FEDERAL FLUMINENSE,31/08/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'CCN;Routing;Wireless Networks;CCWMN',-,IGOR MONTEIRO MORAES,0,CCN;roteamento;redes sem-fio;CCWMN;redes de cache,COMPUTAÇÃO (31003010046P4),-,"O emprego da arquitetura Content-Centric Networking (CCN) em redes em malha sem fio tem se tornado um amplo campo de pesquisa. Um dos motivos é a possibilidade de combinar as vantagens de uso do cache em nós intermediários da rede com o baixo custo de implementação de redes em malha sem fio. A utilização de caches por nós intermediários pode aumentar a vazão em redes em malha sem fio. O motivo deste aumento de vazão se deve ao fato de que conteúdos populares e conteúdos perdidos podem ser requisitados aos nós mais próximos do nó consumidor, sem a necessidade de percorrer todo o caminho até o nó produtor, proporcionando uma redução na probabilidade de perda. No entanto é notória a necessidade de estudos frente aos novos desafios enfrentados na utilização de redes em malha sem fio orientadas a conteúdo. Um desses desafios é reduzir tempestade de broadcast de pacotes CCN, mais especificamente os pacotes de interesse usados para solicitar um dado conteúdo. Dependendo da taxa de requisição de conteúdos esta tempestade pode se tornar um problema limitante nas redes em malha sem fio baseadas na arquitetura CCN. A literatura apresenta algumas propostas de protocolos e mecanismos destinados a reduzir o problema de tempestade de broadcast por pacotes de interesse. Apesar das propostas existentes, é possível destacar que para cenários de múltiplos consumidores requisitando os mesmos conteúdos e cenários com múltiplos produtores, ainda existe a necessidade de propostas que reduzam os impactos negativos da tempestade de broadcast e aumentem a vazão da rede. 

Esta tese propõe três mecanismos chamados: Probabilistic Interest Forwarding (PIF), Retransmission-Counter-based Interest Forwarding (ReCIF) e o ReCIF + PIF. O primeiro mecanismo define uma probabilidade de encaminhamento de pacotes de interesse. O segundo limita o número de pacotes de interesse com base em encaminhamentos prévios desses pacotes. O terceiro é uma abordagem híbrida que combina os critérios de encaminhamento dos dois mecanismos anteriores. O desempenho das redes em malha sem fio, baseadas na CCN é avaliado com os três mecanismos propostos e também com o mecanismo de encaminhamento padrão CCN e com outro protocolo utilizado na literatura, o Listen First Broadcast Later (LFBL). O desempenho das redes orientadas a conteúdo também é comparado com uma rede em malha sem fio com base na pilha TCP/IP executando o protocolo OLSR. Os resultados mostram que os mecanismos propostos fornecem uma taxa de entrega de conteúdo maior do que a fornecida pela rede TCP/IP com OLSR. Além disso, os mecanismos propostos superam o mecanismo de encaminhamento CCN padrão em até 22 % em termos de taxa de entrega de conteúdos em cenários densos com alto número de saltos entre a origem e o destino e proporcionam um atraso de entrega 49% menor do que a CCN padrão. Quando comparados com o LFBL, um dos mecanismos propostos, PIF, apresentou ganhos de 92% e 55% em termos de taxa de entrega e atraso respectivamente.",TESE,Limiting Interest-Packet Forwarding in Content-Centric Wireless Mesh Networks,5058405,
"The Bin Packing Problem is one of the most studied problems in the Combinatorial Optimization area. However, its practical applications add additional constraints to the classical definition. Therefore, this thesis aimed to study new resolution methods for two bin packing variants, the Bin Packing Problem with Conflicts (BPPC) and the Bin Packing Problems with Dependencies (BPPD). These problems have many real applications, such as in timetabling problems, in cloud computing management resources, in transportation and logistics areas, among others.

	To solve these two variants, heuristic and exact methods were developed. For the PBPC, an approach based on the metaheuristic Iterated Local Search was proposed. This method is combined with several classes of local and large-scale neighborhoods. We introduce O(1) evaluation procedures for classical local-search moves, polynomial variants of ejection chains and assignment neighborhoods, an adaptive set covering-based neighborhood, and finally a controlled use of 0-cost moves to further diversify the search. The overall method produces solutions of high quality on the classical benchmark instances of the literature. Extensive computational experiments are conducted to measure the respective contribution of each proposed neighborhood. For PBPD, a formal definition of the problem is presented with a mathematical formulation. An exact method is proposed to solve the problem, more specifically a Branch-and-price algorithm. To test this approach, new instances for the problem are created from the PBPC benchmark instances.",tese_versão final.pdf,ALGORITMOS E OTIMIZAÇÃO,RENATHA OLIVA CAPUA,UNIVERSIDADE FEDERAL FLUMINENSE,01/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Bin Packing with Conflicts;Bin Packing with Dependencies;Iterated Local Search;Large Neighborhood Search;Branch-and-price',-,LUIZ SATORU OCHI,0,Bin Packing com Conflitos;Bin Packing com Dependências;Iterated Local Search;Busca em Vizinhança Larga;Branch-and-price,COMPUTAÇÃO (31003010046P4),-,"O Problema de Bin Packing é um dos problemas mais estudados na área de Otimização Combinatória. No entanto, as aplicações práticas do problema acrescentam restrições adicionais à definição clássica dificultando sua resolução utilizando os métodos existentes. Portanto, nesta tese são estudados novos métodos de resoluções para duas variantes do bin packing, o Problema de Bin Packing com Conflitos (PBPC) e o Problema de Bin Packing com Dependências (PBPD). Esses problemas surgem em diversas aplicações reais, tais como no problema de alocação de horários, na alocação de recursos de computação em nuvem, na área de transporte e logística, entre outros.

	Para resolver estas duas variantes, foram desenvolvidos métodos heurísticos e exatos. Para o PBPC foi proposta uma abordagem baseada na metaheurística Iterated Local Search. Esse método é combinado com várias classes de vizinhanças locais e de larga escala. Foram introduzidos ainda procedimentos de avaliação em O(1) dos movimentos clássicos da busca local, variantes polinomiais do ejection chains e da vizinhança assignment, além de uma vizinhança adaptativa baseada no problema de cobertura de conjuntos, e finalmente um uso controlado de movimentos de custo-0 como mecanismo de diversificação da busca. O método desenvolvido produz soluções de alta qualidade em instâncias da literatura. Experimentos computacionais foram realizados para medir a respectiva contribuição de cada vizinhança proposta. Para o PBPD, é apresentado uma definição formal do problema com um modelo matemático de programação linear inteira mista. Um método exato é proposto para resolver o problema, mais especificamente, um algoritmo de branch-and-price. Para testar essa abordagem são apresentadas novas instâncias para o problema criadas a partir das instâncias do PBPC existentes na literatura.",TESE,Métodos de Resolução para o Problema de Bin Packing com Conflitos e para o Problema de Bin Packing com Dependências,5058409,
"Routing protocols for Delay and Disruption Tolerant Networks (DTNs) are prone to suffer with malicious behavior of nodes. Particularly, in the acknowledgment counterfeiting attack, malicious nodes forge positive acknowledgments, also known as ACKs, in order to negatively impact network performance by removing from nodes' buffers messages which were not yet delivered to their destinations. This thesis proposes new countermeasures against the acknowledgement counterfeiting attack in DTNs. The proposed countermeasures are named DRAC (DRop ACknowledged messages first) and DRAC-SF (DRop ACknowledged messages first and Stop Forwarding) and work as follows. When an ACK is received, DRAC and DRAC-SF do not immediately drop the message for which this ACK was generated. Instead, in case of buffer overflow drop priority is given to this message. Additionally, DRAC-SF also stops forwarding and replicating messages for which ACKs were received.  We emphasize that DRAC and DRAC-SF do not rely in any authentication method because both countermeasures do not try to identify malicious nodes. The analysis considers traces of four real networks, seven routing protocols and two distinct attacker models. Results show our proposals decrease the efficiency of the acknowledgement counterfeiting attack. In addition, we highlight that DRAC-SF outperforms the main countermeasure against this kind of attack found in literature in 88% of the evaluated scenarios, providing higher delivery rates up to 135%.",tese_versão final.pdf,SISTEMAS DE COMPUTAÇÃO,JULIANO FISCHER NAVES,UNIVERSIDADE FEDERAL FLUMINENSE,06/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Delay and Disruption Tolerant Networks;positive acknowledgements;security',-,IGOR MONTEIRO MORAES,0,Redes tolerantes a atrasos e desconexões;reconhecimentos positivos;segurança,COMPUTAÇÃO (31003010046P4),-,"Os protocolos de roteamento para Redes Tolerantes a Atrasos e Desconexões (Delay and Disruption Tolerant Networks - DTNs) são suscetíveis a comportamentos maliciosos dos nós. Particularmente, no ataque de falsificação de reconhecimentos positivos, nós maliciosos forjam reconhecimentos positivos, ou ACKs, com o objetivo de causar impacto negativo no desempenho da rede removendo mensagens que ainda não chegaram ao destino dos buffers dos nós. Este trabalho propõe novas contramedidas contra o ataque de falsificação de reconhecimentos positivos em DTNs. As contramedidas são denominadas DRAC (Drop Acknowledged Messages First) e DRAC-SF (Drop Acknowledged Messages First and Stop Forwarding) e funcionam como se segue. Quando ACKs são recebidos, as propostas não descartam imediatamente as mensagens para as quais estes reconhecimentos foram gerados. Ao invés disso, elas alteram a prioridade destas mensagens na fila de descarte de modo que estas mensagens terão prioridade para serem descartadas em caso de estouro do buffer. Adicionalmente, a contramedida DRAC-SF para de encaminhar e replicar mensagens para as quais ACKs foram recebidos. Ressalta-se que DRAC e DRAC-SF não se baseiam em nenhum método de autenticação, visto que não tentam identificar quais são os nós maliciosos. A análise considera quatro cenários reais de mobilidade, sete protocolos de roteamento e dois modelos de ataque distintos. Os resultados mostram que as propostas reduzem a eficiência do ataque de falsificação de reconhecimentos positivos. Adicionalmente, cabe destacar o desempenho da proposta DRAC-SF, que supera o desempenho da principal contramedida existente na literatura em 88% dos cenários avaliados, alcançando taxas de entrega superiores em até 135%.",TESE,Contramedidas ao Ataque de Falsificação de Reconhecimentos Positivos em Redes Tolerantes a Atrasos e Desconexões,5058434,
"Developing software to take advantage of heterogeneous hardware resources in High-Performance Computing (HPC) scenarios is a complex task and has many associated variables, but also creates many possibilities and opportunities for solving larger problems that a single processor cannot handle. Depending on the mapping of tasks to the hardware, vastly different execution times are possible. This thesis proposes and implements a novel model for self-optimizing the overall execution time of independent tasks in heterogeneous environments by using the abstraction concept of a virtual group of processor cores provided by StarPU library. We design and implement an innovative model that uses the combination of virtual groups of processors, scheduling policies and estimations, optimizing the overall execution time and using just a small fraction of the time required to execute the tasks. This approach allows us to automatically achieve the self-optimization of the overall execution time, leading to the minimization of the overall execution time of tasks, allowing developers to automatically take advantage and extract more performance from the same available hardware.",Tese_FINAL.pdf,COMPUTAÇÃO VISUAL,JOAO GABRIEL FELIPE MACHADO GAZOLLA,UNIVERSIDADE FEDERAL FLUMINENSE,12/09/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'StarPU;CUDA;Heterogeneous Computing',-,ESTEBAN WALTER GONZALEZ CLUA,0,StarPU;CUDA;Computação Heterogênea,COMPUTAÇÃO (31003010046P4),-,"Desenvolver software para tirar proveito de recursos de hardware heterogêneos em cenários de computação alto desempenho é uma tarefa complexa e possui muitas variáveis associadas, criando porém uma vasta possibilidade e oportunidades para resolver problemas grandes que um único processador não seria capaz. Dependendo do mapeamento de tarefas para o hardware disponível, são possíveis diferentes tempos de execução. Este trabalho propõe e implementa um modelo inovador para otimizar o tempo geral de execução de tarefas independentes em ambientes heterogêneos, através da utilização do conceito abstrato de grupo virtual de processadores implementado pela biblioteca StarPU. Foi projetado e implementado um modelo que usa a combinação de grupos virtuais de processadores, políticas de agendamento e estimativas, para otimizar o tempo de execução geral, utilizando apenas uma pequena fração do tempo necessário para executar as tarefas. Essa abordagem nos permite alcançar automaticamente a otimização do tempo de execução geral, levando a minimização do tempo geral de execução das tarefas, permitindo aos desenvolvedores extrair mais desempenho a partir do mesmo hardware.",TESE,A Framework for Task Distribution in Multi-CPU and Multi-GPU Systems,5058439,
"Scientific workflows are models composed of activities, data and dependencies whose objective is to represent a computer simulation. In general, these workflows demand for many computational resources as their executions may involve numerous programs processing a considerable volume of data. Thus, the use of High Performance Computing (HPC) environments allied to parallelization techniques becomes imperative. Over the last years, Cloud Computing has been continuously improved and hence appears as a good candidate to the composition of HPC environments, due to advantages such as easy acquisition, low up-front investiment and virtually non-existent resource limit. However, failure occurrence in such environments is rather a reality than a possibility. There are many fault tolerance techniques that may be applied or adapted to the scientific workflow context, such as re-execution, checkpoint-restart and replication. This work explores these techniques and analyzes their application to real scientific workflow use cases. The checkpoint-restart and replication techniques were implemented in the SciCumulus Scientific Workflow Management System (SWfMS) and compared to the re-execution technique, used as baseline as it is the most commonly adopted fault tolerance technique by current SWfMS. Finally, an initial study on machine learning techniques was conducted in order to assist scientists in the task of choosing the most appropriate fault tolerance technique for a given task, enabling choices well suited for both the task and the currently available computational environment. The use of this fault tolerance framework improved the resilience of the SciCumulus SWfMS at the cost of a low overhead, producing more stable workflow executions in a faster and cheaper manner - with up to 30% improvements obtained at the experiments - even in adverse situations.",Dissertação_PEDIDO DE BANCA.pdf,SISTEMAS DE COMPUTAÇÃO,LEONARDO ARAUJO DE JESUS,UNIVERSIDADE FEDERAL FLUMINENSE,05/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'cloud computing;high performance computing;scientific workflows;fault tolerance techniques',-,LUCIA MARIA DE ASSUMPCAO DRUMMOND,0,computação em nuvem;computação de alto desempenho;workflows científicos;tolerância a falhas,COMPUTAÇÃO (31003010046P4),-,"Workflows científicos são modelos compostos por atividades, dados e dependências cujo objetivo é representar uma simulação computacional. No geral, estes workflows demandam por muitos recursos computacionais, pois suas execuções envolvem diferentes programas e um grande volume de dados. Desta forma, o uso de ambientes de Computação de Alto Desempenho (CAD) e técnicas de paralelização se tornam essenciais. Nos últimos anos, a Computação em Nuvem vem amadurecendo e se mostrando como uma boa alternativa para a composição destes ambientes, devido a vantagens como facilidade de aquisição, baixo custo inicial e limite de recursos virtualmente inexistente, entre outras. Entretanto, a ocorrência de falhas em tais ambientes deve ser tratada como regra e não como exceção. Existem diversas técnicas que podem ser aplicadas ou adaptadas ao contexto de workflows científicos com o objetivo de tornar suas execuções capazes de suportar falhas com o mínimo impacto possível, tais como reexecução, checkpoint-restart e replicação. Este trabalho explora estas técnicas e as analisa aplicadas em casos de workflows científicos reais. Foram implementadas as técnicas checkpoint-restart e replicação no Sistema de Gerência de Workflos Científicos (SGWfC) SciCumulus, e estas foram comparadas à técnica de reexecução, técnica mais comumente adotada pelos SGWfC atuais. Por fim, foi realizado um estudo inicial sobre técnicas de aprendizado de máquina que auxiliam o cientista na tomada de decisão a respeito de qual técnica seria melhor aplicada a uma determinada tarefa, possibilitando escolhas mais adequadas ao ambiente computacional disponível e a tarefa a ser executada. O uso deste framework de tolerância a falhas melhorou a resiliência do SGWfC SciCumulus ao custo de um reduzido overhead, proporcionando a execução de workflows científicos de forma mais estável, rápida e barata - com ganhos de até 30% nos experimentos realizados - mesmo em situações de adversidade.",DISSERTAÇÃO,Tolerância a Falhas Em Workflows Científicos Executados Em Nuvens Computacionais,5058446,
"A eficiência da mobilidade urbana é colocada em questão quase que diariamente por grande parte da população urbana mundial. Sabendo disso, os administradores das grandes cidades gastam boa parte do tempo monitorando e planejando melhorias sobre os sistemas de transportes. Para facilitar a execução destas ações, temos a utilização dos Sistemas Inteligentes de Transportes (SIT) que disponibilizam aos administradores diversos tipos de ferramentas computacionais.
O sucesso obtido pela utilização dos SITs fez com que as entidades detentoras de informações dos transportes públicos das cidades se sentissem motivadas a divulgá-las ao público em geral, para que novos estudos nesta área possam ser desenvolvidos.
Aproveitando-se desta tendência, este trabalho utiliza-se dos dados dos ônibus públicos da cidade do Rio de Janeiro, no Brasil, disponibilizados de forma aberta pela própria prefeitura, para extrair de forma automática algumas de suas principais informações operacionais. Mais especificamente, são inferidas as regiões de garagens, pontos inicias e finais e a rota, rua a rua, das linhas. Estas informações são de extrema importância para os administradores públicos e para os usuários, pois os administradores de posse destas podem entender e planejar melhor o sistema de transporte. De outro lado, também são de grande valia aos usuários pois, quando são informados de forma correta e atualizada podem elevar a confiança, aceitação e satisfação no uso do transporte.",Dissertação_versão final.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,DANIEL PADILHA TORQUATO DANTAS,UNIVERSIDADE FEDERAL FLUMINENSE,11/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Dados Urbanos;Map-matching;Rota de nibus;SIT  Urban Analytics;Bus Routing;ITS',-,ANTONIO AUGUSTO DE ARAGAO ROCHA,0,Dados Urbanos;Map-matching;Rota de ônibus;SIT  Urban Analytics;Bus Routing;ITS,COMPUTAÇÃO (31003010046P4),-,"A eficiência da mobilidade urbana é colocada em questão quase que diariamente por grande parte da população urbana mundial. Sabendo disso, os administradores das grandes cidades gastam boa parte do tempo monitorando e planejando melhorias sobre os sistemas de transportes. Para facilitar a execução destas ações, temos a utilização dos Sistemas Inteligentes de Transportes (SIT) que disponibilizam aos administradores diversos tipos de ferramentas computacionais.
O sucesso obtido pela utilização dos SITs fez com que as entidades detentoras de informações dos transportes públicos das cidades se sentissem motivadas a divulgá-las ao público em geral, para que novos estudos nesta área possam ser desenvolvidos.
Aproveitando-se desta tendência, este trabalho utiliza-se dos dados dos ônibus públicos da cidade do Rio de Janeiro, no Brasil, disponibilizados de forma aberta pela própria prefeitura, para extrair de forma automática algumas de suas principais informações operacionais. Mais especificamente, são inferidas as regiões de garagens, pontos inicias e finais e a rota, rua a rua, das linhas. Estas informações são de extrema importância para os administradores públicos e para os usuários, pois os administradores de posse destas podem entender e planejar melhor o sistema de transporte. De outro lado, também são de grande valia aos usuários pois, quando são informados de forma correta e atualizada podem elevar a confiança, aceitação e satisfação no uso do transporte.",DISSERTAÇÃO,Estimando Informações de Linhas de Ônibus a partir de Dados Históricos de GPS,5058451,
"In science, an analysis of large volumes of data is modeled as a scientific experiment, involving some issues such as data storage and formatting, program chaining and the definition of execution environment during simulations. Scientists have used scientific workflows to express and model computations and experiments on data. Due to complexity of the workflows and also the volume of data involved, these have been executed on distributed environments, through workflow parallel programming models. The MapReduce (MR) model has been widely used in the specification of scientific experiments, especially those that analyze a large volume of data. From the MR, frameworks such as Hadoop and Spark were created, which allow the manipulation and analysis of the data in a distributed way, as well as managing the execution of the experiments on distributed environments. However, the execution of intensive data workflows on distributed environments managed by MR frameworks still presents open challenges. Although it is not a complex task to install these frameworks, there are many parameters to be configured to execute a workflow. In addition, to exploit the parallelism offered by the environment it is necessary to partition the input data. There are several data partitioning strategies and aspects such as: knowledge of the partitioning criterion by the application, partition size and load balancing impact the workflow performance. Thus, in order to execute an MR workflow efficiently, the scientist must tune several configuration parameters related to the framework and data partitioning. The correlations between these parameters, workflow, and the execution environment make the configuration of such parameters a complex and difficult task for the scientist. In this thesis, an approach is proposed that can be applied in tuning the execution parameters configuration of workflows MR in distributed environments. The approach is based on (i) collecting the workflow execution time using several values in the parameters configuration, (ii) applying machine learning techniques in order to find the values and parameters that execute the workflow efficiently and (iii) use the same techniques to generate the predictive model to previously know the performance of a parameter configuration in later executions of workflow MR. The experiments presented in this thesis showed that the proposed approach to parameter setting leads to efficient performance of MR workflow in a distributed environment.",tese_versão final.pdf,SISTEMAS DE COMPUTAÇÃO,DOUGLAS ERICSON MARCELINO DE OLIVEIRA,UNIVERSIDADE FEDERAL FLUMINENSE,30/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Scientific Workflows;Parameter Tuning;Machine Learning',-,DANIEL CARDOSO MORAES DE OLIVEIRA,0,Workflows Científicos;Configuração de Parâmetros;Aprendizado de Máquina,COMPUTAÇÃO (31003010046P4),-,"Na ciência a análise de grandes volumes de dados é modelada como experimento científico, envolvendo algumas questões como o armazenamento dos dados e formatos dos mesmos, encadeamento dos programas e definição do ambiente de execução usados durante as simulações. Cientistas têm usado workflows científicos para exprimir e modelar computacionalmente análises e experimentos sobre dados. Devido à complexidade de processamento dos workflows e também o volume de dados envolvido, estes tem sido executados em ambientes distribuídos, em conjunto com modelos de programação paralela do workflow. O modelo MapReduce (MR) tem sido muito utilizado na especificação de experimentos científicos, em especial, aqueles que analisam um grande volume de dados. A partir do MR foram criados frameworks, como Hadoop e Spark, que permitem a manipulação e análise dos dados de forma distribuída, além de realizarem o gerenciamento da execução dos experimentos em ambientes distribuídos. No entanto, a execução de workflows intensivos de dados em ambientes distribuídos gerenciados por frameworks MR ainda apresenta desafios em aberto. Embora exista uma certa facilidade na instalação desses frameworks, há muitos parâmetros a serem configurados para execução de um workflow. Além disso, para explorar o paralelismo oferecido pelo ambiente é necessário o particionamento dos dados de entrada. Existem diversas estratégias de particionamento de dados e aspectos como:  conhecimento do critério de particionamento por parte da aplicação, tamanho das partições e o balanceamento de carga interferem no desempenho do workflow. Com isso, para executar um workflow MR de forma eficiente, o cientista deve ajustar diversos parâmetros de configuração dos frameworks e do particionamento dos dados de entrada. As correlações que existem entre estes parâmetros, o workflow e o ambiente de execução tornam o ajuste da configuração de tais parâmetros uma tarefa complexa e difícil para o cientista. Nesta tese, é proposta uma abordagem que pode ser aplicada no ajuste da configuração dos parâmetros de execução de workflows MR em ambientes distribuídos. A abordagem é baseada em (i) coletar o tempo de execução do workflow utilizando diversos valores na configuração dos parâmetros, (ii) aplicar técnicas de aprendizado de máquina afim de encontrar os valores e parâmetros que executam o workflow de forma eficiente e (iii) utilizar as mesmas técnicas para gerar o modelo preditivo para conhecer previamente o desempenho de uma configuração de parâmetros em execuções posteriores do workflow MR. Os experimentos apresentados nesta tese mostraram que a abordagem proposta para configuração de parâmetros conduz a um desempenho eficiente do workflow MR em um ambiente distribuído.",TESE,Otimização da Execução de Workflows Intensivos de Dados em Frameworks MapReduce,5135072,
"With users being charged for what they use, Cloud Computing has placed an emphasis on more efficient job executions and better resource utilization. In virtualized environments like Clouds, allocating a fixed quantity of resources to a job a priori may result in the underutilization of the underlying shared physical machines. This sub-utilization occurs often since applications frequently consume, for at least some part of their execution, less than the amount of resources reserved for them, which under current practice is usually their maximum requirement. The motivation for this practice is to avoid resource overloading, which occurs when the simultaneous demands of executing jobs exceed the capacity of that resource, since this can have a negative impact on the execution times of the jobs competing for that resource. Vertical elasticity could be adopted as a means to reduce this impact, by resizing Virtual Machines (VMs) dynamically, in conjunction with VM suspension and/or migration being used when the physical machine is about to be overloaded.  However, migration can be costly and should be employed with care. In order to reduce the number of migrations but still avoid overloading the host, it is important to find an effective initial VM allocation that takes into consideration the behavior of each application. This scheduler must work in unison with the local host elasticity controllers.  Existing approaches address the problems of elasticity and allocation separately. This work presents and evaluates a job scheduling architecture for elastic memory managed virtualized environments.The MEMiC (Memory Elasticity Management in Clouds) architecture is a two-tier VM scheduler for jobs that aims to predict the impact of caused by competition for server memory resources in a Cloud-like environment while also supports VM migration.

In order to analyze this proposal, experiments where executed in order to check the proposal in comparison some of the main approaches in the area. In the presented results it was possible to perceive that the MEMiC approach presents better results in comparison to the other approaches analyzed predicting and avoiding the negative interference and migration. The MEMiC approach presented the best results for the tested workloads and was more stable independent of the input workload.",tese_PEDIDO DE BANCA.pdf,SISTEMAS DE COMPUTAÇÃO,HENRIQUE DE MEDEIROS KLOH,UNIVERSIDADE FEDERAL FLUMINENSE,28/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Distributed Systems;Scheduling;Shared environments',-,EUGENE FRANCIS VINOD REBELLO,0,Sistemas Distribuídos;Computação em Nuvem;Escalonamento;Placement;Ambientes compartilhados;Elasticidade,COMPUTAÇÃO (31003010046P4),-,"Com os usuários sendo cobrados pelo que eles usam, a Cloud Computing colocou ênfase em execuções de jobs e uso de recursos mais eficientes. Em ambientes virtualizados como Clouds, alocar uma quantidade fixa de recursos para um job a priori pode resultar na subutilização das máquinas físicas compartilhadas. Esta subutilização ocorre muitas vezes, uma vez que as aplicações frequentemente consomem menos, em parte de sua execução, do que a quantidade de recursos normalmente reservados para elas, o que, de acordo com a prática atual, geralmente é o requisito máximo. A motivação para esta prática é evitar a sobrecarga de recursos, que ocorre quando as demandas simultâneas de execução de jobs excedem a capacidade desse recurso, uma vez que isso pode ter um impacto negativo nos tempos de execução dos jobs que competem por esse recurso. 

A elasticidade vertical tem sido utilizada como uma solução para este problema redimensionando dinamicamente as máquinas virtuais ou VMs (Virtual Machine) e a migração de VMs é utilizada para auxiliar a elasticidade quando a máquina física fica sobrecarregada. No entanto, a migração é onerosa e deve ser evitada. Para minimizar a migração e permitir evitar a sobrecarga do host é importante um escalonamento inicial eficiente para poder determinar o comportamento de cada aplicação e trabalhar em conjunto com o controlador de elasticidade.  Este trabalho apresenta e avalia uma arquitetura de escalonamento de jobs para ambientes virtualizados com elasticidade de memória. A arquitetura MEMiC (Memory Elasticity Management in Clouds) é um escalonador de VM de duas camadas para jobs que visa prever o impacto causado pela concorrência por recursos de memória do host em um ambiente semelhante a uma nuvem, além de suportar a migração da VM. 

Para analisar esta proposta, foram executados experimentos para verificar a proposta em comparação com algumas das abordagens principais na área. Nos resultados apresentados, foi possível perceber que a abordagem MEMiC apresenta melhores resultados em comparação com as outras abordagens analisadas, prevendo e evitando a interferência negativas e a migração. A abordagem MEMiC apresentou os melhores resultados para os workloads testados e foi mais estável independente do workload de entrada.",TESE,Escalonamento de Jobs em Ambientes Virtualizados com Elasticidade,5135080,
"This thesis deals with several aspects and concepts of graph theory, focusing on partition problems and path convexities. Regarding the partition problems, we first consider the M-Partition problem proposed by Feder, Klein, Hell, and Motwani, that consists of verifying if the vertex set of a given graph can be partitioned according to some conditions imposed by a symmetric, square matrix M defined over 0,1,* where the elements in the main diagonal represent internal relations between vertices in each of the m parts: vertices in part i can be completely adjacent (M_{ii}=1, and, in this case, part i is a clique); completely non-adjacent (M_{ii}=0, and, in this case, part i is an independent set); or have no adjacency restrictions (M_{ii}=*). The same values 0,1,* applies to the rest of the matrix, representing external relations between parts, where M_{ij}=1 means that parts i and j are completely adjacent; M_{ij}=0 means that parts i and j are completely non-adjacent; and M_{ij}=* means that parts i and j have no adjacency restrictions. From the M-partition problem, we propose the study of two specific matrix problems: the P_k-Partition problem and the (k,0)^{j}-Partition problem. The P_k-Partition problem consists of finding (if any) a partition of the vertex set such that, by choosing a vertex in each partition, the graph induced by such vertices is a P_k, for any choice of such vertices; therefore, we study conditions for a graph to be P_k-partitionable. The (k,0)^{j}-Partition problem consists of studying the M-Partition problem on restricted matrices M such that their main diagonal contains only 0's and the last row/column contains only 1's and/or *'s. Regarding the path convexities, which are defined over special collections of paths in graphs - for example, the collection of shortest paths, associated with the geodetic convexity; or the collection of induced paths, associated with the monophonic convexity -, we propose a general framework able to represent several well-known convexities, and show that a set of problems is NP-complete when we consider such framework in its more generic formulation. However, when restricted to specific configurations, some problems may still be hard, or admit polynomial algorithms according to the chosen convexity.",tese_final.pdf,ALGORITMOS E OTIMIZAÇÃO,JOAO VINICIUS CORREA THOMPSON,UNIVERSIDADE FEDERAL FLUMINENSE,18/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Graph Theory;Path Convexities;M-Partitions',-,LOANA TITO NOGUEIRA,0,Teoria dos Grafos;Convexidades de Caminhos;M-Partições,COMPUTAÇÃO (31003010046P4),-,"Esta tese aborda diversos conceitos e aspectos de Teoria dos Grafos, tendo como foco principal problemas de partição e de convexidades de caminhos. Para os problemas de partição, consideramos originalmente o problema da M-Partição proposto por Feder, Hell, Klein e Motwani que consiste em verificar se o conjunto de vértices de um dado grafo G pode ser particionado segundo as condições impostas por uma matriz M quadrada e simétrica de ordem m sob os valores 0,~1,~*, onde os elementos na diagonal principal representam as relações internas entre os vértices de cada uma das m partes, podendo os vértices de uma parte i serem completamente adjacentes (M_{ii}=1, e neste caso a parte i é uma clique); completamente não adjacentes (M_{ii}=0, e a parte i é um conjunto independente); ou não terem nenhuma restrição (M_{ii}=*). Os mesmos valores (0,1,*) são aplicáveis ao restante da matriz, representando as relações externas, onde M_{ij}=1 indica que todos os elementos da parte i são adjacentes a todos os elementos da parte j; M_{ij}=0 indica que nenhum elemento da parte i é adjacente a algum elemento da parte j; e M_{ij}=* indica que não existe restrição de adjacência entre as partes i e j. A partir do problema da M-Partição, propusemo-nos a estudar duas configurações específicas: P_k-Partição e (k,0)^{j}-Partição. O problema da P_k-Partição de um grafo G consiste em encontrar uma partição dos vértices de G de forma que, ao escolhermos um vértice de cada partição, o grafo induzido por estes vértices é um P_k, para quaisquer que sejam os vértices escolhidos; assim, estudamos condições para que um grafo seja P_k-particionável. O problema da (k,0)^{j}-Partição consiste em estudar o problema da M-Partição em que as matrizes M são restritas àquelas em que a diagonal principal contenha apenas 0's e a última linha/coluna contenha apenas 1's e/ou *'s. Para as convexidades de caminhos, que são definidas através de coleções especiais de caminhos em grafos - por exemplo, a coleção dos caminhos mínimos, associada à convexidade geodésica; ou a coleção dos caminhos induzidos, associada à convexidade monofônica -, propusemos uma estrutura geral (framework) capaz de representar diversas convexidades conhecidas, e mostramos que um conjunto de problemas é NP-completo quando mantemos esta estrutura na sua forma mais genérica. No entanto, quando restritos a configurações específicas, mostramos que alguns problemas podem se manter difíceis ou apresentar algoritmos polinomiais de acordo com a convexidade escolhida e do problema em questão.",TESE,Partições & Convexidades de Caminhos em Grafos,5186553,
"This thesis introduces concepts from mathematical morphology, an established field in visual computing, to the field of machine learning. Mathematical morphology operations are sensitive to shapes and density, aspects that are not sufficiently exploited in usual machine learning techniques. Two implementations of classifiers that use these techniques are proposed along with a theoretical framework. In classification, the main concept is to label the space around the training instances, growing this labelling at each iteration. The growth conforms to a new distance metric proposed herein, which is substantially faster than Manhattan and Euclidean distances when it comes to iterations of discrete neighbourhoods. At last, these labelled spaces are used to classify unlabelled instances. The proposed classifiers outperformed 14 classifiers in 5 out of 8 datasets, which is a strong initial evidence of the predictive power of the approach, which is novel and is in its early stages of development. Furthermore, a clusterization algorithm that is based on morphological reconstruction is proposed. In this case, the algorithm is faster than state-of-the-art techniques while providing clusterization results that preserve shapes and cluster density. The proposed clusterization scheme possesses a number of unique features such as an intrinsic sense of maximal clusters that can be created, provides a means of removing noise from datasets with no extra processing costs, enables a vast amount of growth patterns, which are controlled by structuring elements, etc. An experiment with more than 70 volunteers also indicated that clustering algorithms that are sensitive to shapes and density produce more human-like clusterizations.",tese_final.pdf,COMPUTAÇÃO VISUAL,ERICK OLIVEIRA RODRIGUES,UNIVERSIDADE FEDERAL FLUMINENSE,06/12/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'morphological classier;machine learning;data mining;supervised learning;unsupervised learning;clusterization;mathematical morphology;image dilation;k-nn;set theory;k-means;cluster density;cluster shape',-,AURA CONCI,0,classicadores morfológicos;aprendizado de máquina;aprendizado supervisionado;aprendizado não supervisionado;clusterização;morfologia matemática;dilatação de imagens;k-NN;teoria dos conjuntos;densidade de clusters;forma de clusters,COMPUTAÇÃO (31003010046P4),-,"Esta tese introduz conceitos da morfologia matemática, um campo já estabelecido dentro da computação visual, ao aprendizado de máquina. Técnicas da morfologia matemática exploram características de forma e densidade, aspectos que não são devidamente explorados por algoritmos de machine learning tradicionais. Duas implementações de classificadores são apresentadas conjuntamente ao framework teórico proposto. Na classificação, a ideia principal é rotular o espaço ao redor das instâncias de treino, crescendo esse rotulamento a cada iteração. Nas implementações propostas, este crescimento respeita uma nova métrica/distância que também é proposta nesta tese. Esta métrica é mais rápida que as distâncias de Manhattan e Euclidiana nas iterações de vizinhanças discretas. Uma vez que o espaço de classificação encontra-se razoavelmente rotulado, este espaço é utilizado como modelo para classificar instâncias não rotuladas. Os classificadores propostos obtiveram melhores resultados em relação a 14 outros classificadores em 5 de 8 bases de dados, o que é uma forte evidência inicial do poder de predição da proposta, que é inovadora e ainda está em seus passos iniciais de desenvolvimento. Além disso, um algoritmo de clusterização que é baseado na reconstrução morfológica também é proposto. Neste caso, o algoritmo é mais rápido que o estado-da-arte ao passo em que provê resultados onde formas e densidade dos clusters também são preservadas. O algoritmo de clusterização possui características únicas como um senso intrínseco de clusters máximos que podem ser criados, uma forma de remover ruído das bases sem custos extra de processamento, permite alterações no padrão de crescimento dos clusters ao modificar os elementos estruturantes, entre outras. Um experimento considerando mais de 70 voluntários também demonstrou que clusterizações realizadas com algoritmos sensíveis às formas e densidade são mais próximos de clusterizações humanas.",TESE,Introducing Mathematical Morphology in Machine Learning,5378903,
"Norms in multi-agent systems are used as a mechanism to regulate the behavior of autonomous and heterogeneous agents and to maintain the social order of the society of agents. They prescribe how agents should ideally behave. Norms describe actions that must be performed (obligations), actions that can be performed (permissions) and actions that cannot be performed (prohibitions) by a given entity in a certain situation. One of the challenges in designing and managing systems governed by norms is to deal with normative conflicts. Two norms are in conflict when the fulfillment of one causes the violation of the other, and vice-versa. Such conflicts make the norm-compliant agent ""paralyzed"", i.e., whatever the agent does (or refrains from doing) will lead to a social constraint being broken. Several researches have been proposed mechanisms to detect conflicts between norms. However, there is a category of normative conflicts not investigated yet in the design phase that can only be detected if we know information about the runtime execution of the system. Such conflicts are called here Potential Normative Conflicts (PNC). Potential Normative Conflicts are normative conflicts that may happen during the multi-agent system execution depending on execution order of runtime events of the system. This work presents two approaches, based on execution scenarios, to detect Potential Normative Conflicts in multi-agent systems. In the first approach, the system designer provides a set of norms and possible examples of system execution scenarios. As output, the normative system returns normative conflicts that would happen if such scenarios were executed in the multi-agent system. In the second approach, the system designer provides only a set of norms and the normative system generates the execution scenarios that would cause normative conflicts if such scenarios would be executed in the system. Both approaches detect normative conflicts between pair of norms.",TeseMairon_VERSÃO_FINAL.pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,MAIRON DE ARAUJO BELCHIOR,UNIVERSIDADE FEDERAL FLUMINENSE,11/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Norms;Normative conflict;Multi-agent systems;OWL;SWRL',-,VIVIANE TORRES DA SILVA,0,Normas;Conflito normativo;Sistemas multi-agentes;OWL;SWRL,COMPUTAÇÃO (31003010046P4),-,"Normas em sistemas multi-agentes são usadas como um mecanismo para regular o comportamento de agentes autônomos e heterogêneos e para manter a ordem na sociedade de agentes. Elas prescrevem como os agentes idealmente deveriam se comportar. As normas definem ações que devem ser executadas (obrigações), ações que podem ser executadas (permissões), e ações que não podem ser executas (proibições) por determinada entidade em certa situação. Um dos desafios no projeto e gerenciamento de sistemas governados por normas é garantir que as normas especificadas não estejam em conflito. Duas normas estão em conflito quando o cumprimento de uma causa a violação da outra, e vice-versa. Tais conflitos fazem com que o agente obediente às normas fique “paralisado”: não importa o que ele faça (ou deixe de fazer), ele irá violar uma das normas, ou seja, irá provocar a quebra de uma restrição social. Existem vários trabalhos na literatura que lidam com os conflitos entre normas em sistemas multi-agentes. Porém, existe uma categoria de conflitos, não investigado ainda na fase de design, que somente podem ser detectados se soubermos de alguma informação sobre a execução do sistema multi-agentes. Tais conflitos são chamados aqui de Potenciais Conflitos Normativos (PCN). Potenciais Conflitos Normativos são conflitos entre normas que podem acontecer durante a execução dos sistemas multi-agentes, dependendo da ordem de execução dos eventos do sistema. Este trabalho apresenta duas abordagens baseadas em cenários de execução para detectar Potenciais Conflitos Normativos em sistemas multi-agentes. Na primeira abordagem, o designer fornece ao sistema normativo um conjunto de normas e possíveis exemplos de cenários de execução do sistema. Como saída, o sistema normativo retorna os conflitos normativos que aconteceriam caso tais cenários fossem executados no sistema multi-agentes. Na segunda abordagem, o designer do sistema fornece somente um conjunto de normas e o sistema normativo gera os cenários de execução que causariam os conflitos normativos caso tais cenários aconteçam no sistema multi-agentes. Ambas as abordagens fazem a detecção do conflito entre pares de normas.",TESE,Detecção de Potenciais Conflitos Normativos que Dependem da Ordem de Execução dos Eventos nos Sistemas Multi-Agentes,5379466,
"This thesis presents theoretical and practical contributions on the use of column generation over set partitioning formulations. On the theoretical side, the set packing and set partitioning polytopes associated with a binary n-row matrix having all possible 2^n-1 columns are analyzed. We show the precise relation between those polytopes: with very few exceptions, every facet-inducing inequality for the former polytope is also facet-inducing for the latter polytope, and vice-versa. Moreover, we characterize the multipliers that induce Chvátal-Gomory rank 1 clique facets and give several families of multipliers that yield other facets for arbitrarily large dimensions. On the practical side, we present a novel branch-and-price algorithm over a set partitioning formulation for the Minimum Latency Problem (MLP), a variant of the Traveling Salesman Problem. The proposed algorithm strongly relies on new features for the ng-path relaxation, the current best-performing path relaxation employed by many exact algorithms for routing problems. Such new features are not built over any strong assumption on the MLP and can be potentially used in several other problems. Computational experiments over TSPLIB instances are reported, showing that several instances not solved by the current state-of-the-art method can now be solved.",tese_final.pdf,ALGORITMOS E OTIMIZAÇÃO,TEOBALDO LEITE BULHOES JUNIOR,UNIVERSIDADE FEDERAL FLUMINENSE,19/12/2017,INGLES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Set Partitioning;Column Generation;Rank 1 Cuts;Polyhedral Combinatorics;Routing;ng-paths',-,FÁBIO PROTTI,0,Particionamento de Conjuntos;Geração de Colunas;Cortes de posto 1;Combinatória Poliédrica;Roteamento;ng-paths,COMPUTAÇÃO (31003010046P4),-,"Esta tese apresenta resultados teóricos e práticos a respeito do uso de geração de colunas a partir de formulações de particionamento de conjuntos. Na parte teórica, são estudados os poliedros de particionamento e empacotamento de conjuntos associados a uma matriz binária com n linhas e 2^n-1 colunas. Um forte relação entre esses poliedros é provada: com algumas poucas exceções, as inequações que induzem facetas desses poliedros são iguais. Além disso, caracterizam-se os multiplicadores que induzem as inequações de clique que podem ser obtidas por uma única aplicação do procedimento de arredondamento de Chvátal-Gomory (cortes de posto 1) , bem como apresentam-se famílias de multiplicadores que induzem facetas para dimensões arbitrárias (não necessariamente cliques).  Na parte prática, é apresentado um algoritmo branch-and-price para o Problema da Latência Mínima (PLM), uma variante do Problema do Caixeiro Viajante. O algoritmo proposto utiliza uma formulação de particionamento de conjuntos e novas ideias para melhorar o desempenho da relaxação de caminho conhecida como ng-path. Essas novas ideias não são baseadas em nenhuma hipótese forte em relação ao PLM, tendo o potencial de serem aplicadas em outros problemas. Experimentos computacionais mostram que o algoritmo proposto supera o estado da arte, sendo capaz de resolver várias instâncias nunca antes resolvidas.",TESE,Column Generation over Set Partitioning Formulations: Theory and Practice,5380164,
"One of the challenges for the growth of the Web of Data is entity linking. The more interlinked the data is, the more valuable it will be. Currently available datasets still have a large unexplored potential for interlinking. Ranking techniques contribute to this task by scoring datasets according to the likelihood of finding entities related to those of a target dataset. Ranked datasets can be either manually selected to furtherer linking discovery tasks, or automatically scanned by programs that mine related entities in their content. Automated processes would typically scan all datasets in a given upper slice of the rank. Metrics such as nDCG better capture the degree of adherence of the ranking to users expectations in order to allow discrimination between ranking models. On the other hand, to automated processes, best ranks are those that can recall more relevant datasets in smaller upper slices of the ranks. In this case, the Recall at Rank Position would better discriminate ranking models. This work presents empirical comparisons between different ranking models and argues that different algorithms could be used depending on whether the rank is manually or automatically handled and, also, depending on the available metadata of the datasets. Experiments indicate that ranking algorithms that performed best with nDCG do not always have the best Recall at Position, for high recall levels. Best algorithms, under the automatic perspective, may find the same number of datasets with related entities by inspecting a slice of the rank at least 10% smaller. Moreover, depending on the set of features used for ranking, the nDCG can increase 20%.",dissertacao_final (1).pdf,ENGENHARIA DE SISTEMAS E INFORMAÇÃO,ANGELO BATISTA NEVES JUNIOR,UNIVERSIDADE FEDERAL FLUMINENSE,18/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL FLUMINENSE,b'Linked Data;Entity Linking;Recommendation;Dataset;Ranking;Empirical Evaluation',-,LUIZ ANDRE PORTES PAES LEME,0,Dados Interligados;Recomendação;Conjuntos de Dados;Ranqueamento;Avaliação Empírica,COMPUTAÇÃO (31003010046P4),-,"Um dos desafios para o crescimento da Web de Dados é a interligação de entidades. Quanto mais interligada, mais valiosa será. Os conjuntos de dados atualmente disponíveis ainda possuem um grande potencial inexplorado de interligação. As técnicas de ranqueamento contribuem para esta tarefa ranqueando conjuntos de dados de acordo com a probabilidade de encontrar entidades relacionadas às de um conjunto de dados alvo. Conjuntos de dados ranqueados podem ser selecionados manualmente para posterior uso em tarefas de interligação de entidades, ou automaticamente percorridos por programas de mineração de interligações. Processos automatizados tipicamente varrem todos os conjuntos de dados em uma determinada fatia superior do ranque. Métricas como o nDCG capturam melhor o grau de adesão do ranking às expectativas dos usuários, a fim de permitir a discriminação entre os modelos de ranqueamento. Por outro lado, para processos automatizados, os melhores ranques são aqueles que encontram mais conjuntos de dados relevantes na fatia do ranque pesquisada. Neste caso, a medida do recall em cada posição do ranque melhor discriminaria os modelos de classificação. Este trabalho apresenta comparações empíricas entre diferentes modelos de classificação e argumenta que diferentes algoritmos podem ser usados dependendo se a seleção do ranque é manual ou automatizada, e também, dependendo dos metadados disponíveis dos conjuntos de dados. Os experimentos indicaram que os algoritmos de ranking com melhor desempenho no nDCG nem sempre têm o melhor recall para níveis de elevados dessa medida. Os melhores algoritmos, sob a perspectiva automática, podem encontrar o mesmo número de conjuntos de dados com entidades relacionadas, inspecionando uma fatia do ranking pelo menos 10% menor. Além disso, dependendo do conjunto de metadados usados para classificação, o nDCG pode aumentar 20%.",DISSERTAÇÃO,Uma Abordagem Adaptável para Busca de Datasets para Interligação de Entidades,5632119,
.,,CIENCIA DA COMPUTACAO,WASHINGTON LUIS DE SOUZA RAMOS,UNIVERSIDADE FEDERAL DE MINAS GERAIS,03/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Hyperlapse',"ROBÓTICA, VISÃO COMPUTACIONAL E PROCESSAMENTO GRÁFICO",ERICKSON RANGEL DO NASCIMENTO,0,Aceleração de vídeo;Hyperlapse;Informação semântica;Visão egocêntrica,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),Bolsa de produtividade CNPq/2016: Computação Visual: Extração Semântica e Mapeamento 3D para Criação de Imagens Foto-Realísticas,"O surgimento de dispositivos móveis pessoais de baixo custo e câmeras portáteis e, a crescente capacidade de armazenamento de sites de compartilhamento de vídeos têm impulsionado um crescente interesse em vídeos em primeira pessoa. Câmeras vestíveis, em especial, podem operar por horas sem a necessidade de manuseio contínuo, possibilitando a criação de vídeos de longa duração, o que os torna tediosos e visualmente desagradáveis.  Algoritmos de Hyperlapse visam reduzir vídeos longos e monótonos em vídeos de curta duração e sem transições abruptas entre os quadros. Um aspecto importante não abordado por tais algoritmos é a importância de cada trecho para o usuário. Neste trabalho, propomos uma metodologia inovadora capaz de resumir e estabilizar vídeos egocêntricos extraindo e analisando a informação semântica nos quadros. Através de experimentos, comprovamos a superioridade da nossa abordagem sobre às presentes na literatura no que diz respeito à semântica.",DISSERTAÇÃO,Semantic Hyperlapse For Egocentric Videos,4992561,
.,,CIENCIA DA COMPUTACAO,FABRICIO HORACIO SALES PEREIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,30/03/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Antecipao da Interao;Definies de configurao;Gerncia do Legado Digital;Interao Humano-Computador;Mtodo de Inspeo Semitica',-,RAQUEL OLIVEIRA PRATES,0,Antecipação da Interação;Definições de configuração;Gerência do Legado Digital;Interação Humano-Computador;Método de Inspeção Semiótica,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Sistemas que permitem a gerência do Legado Digital póstumo de um usuário abrangem uma série de configurações com efeitos a longo prazo.  Esses sistemas implementam diferentes formas de tratar temporalidade de informações e papeis assumidos pelos usuários. Além disso, abrangem desafios relacionados ao direito sucessório e custos emocionais dos envolvidos. Ao contrário de outros trabalhos que exploram ferramentas de gestão do Legado Digital, nossa abordagem evidencia a qualidade de uso da comunicabilidade e como os projetistas tratam as questões associadas à morte em seus sistemas. Nossa metodologia utiliza o Método de Inspeção Semiótica (MIS), desafios de antecipação da interação, análise de uma amostra de sistemas atuais e a prototipação de um Sistema Gerenciador de Legado Digital póstumo para ampliar o entendimento acerca das tecnologias relacionadas às práticas de comunicação e interação em torno da morte.",DISSERTAÇÃO,Investigação Sobre Antecipação dos Impactos da Interação Em Sistemas de Gerência de Legado Digital de Usuários,4999191,1
.,,CIENCIA DA COMPUTACAO,ARMANDO HONORIO PEREIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,31/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Caixeiro Viajante;Coleta e Entrega;Planos de cortes;Problema de Roteamento de Veculos;Restries de Carregamento',-,SEBASTIAN ALBERTO URRUTIA,0,Caixeiro Viajante;Coleta e Entrega;Planos de cortes;Problema de Roteamento de Veículos;Restrições de Carregamento,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"No Problema do Caixeiro Viajante com Coleta e Entrega sob Múltiplas Pilhas um único veículo deve atender um conjunto de requisições de coleta e entrega. Cada requisição é definida por duas localizações, a localização de coleta onde um item deve ser carregado no veículo, e a localização de entrega onde o item previamente carregado deve ser entregue. Enquanto transportados, os items são armazenados em pilhas de capacidade limitada. Cada pilha deve seguir a política Last-In-First-Out. O que significa que somente o último item carregado em cada pilha está disponível para entrega. O objetivo do problema é encontrar uma rota para o veícula que atenda todas as requisições e que minimize a distância percorrida.

Nesta dissertação, propomos três novas formulações de programação inteira para o problema junto com desigualdades válidas. A primeira formulação é uma reformulação de um modelo na literatura, baseado em uma observação, um conjunto de varíaveis do modelo é eliminado, o que resulta também na eliminação de um conjunto de restrições, com isso o objetivo é obter a solução de relaxação linear mais rápida. A segunda formulação é um misto de dois modelos na literatura. E a terceira formulação, no melhor do nosso conhecimento, é uma formulação compacta para o problema.

Propomos algoritmos de planos de corte para essas novas formulações. Os algoritmos são testados em instâncias de benchmark e os resultados computacionais são comparados com a literatura. Instâncias para as quais uma solução ótima era conhecida previamente na literatura são resolvidas mais eficientemente nesse trabalho. Também, novos certificados de otimalidade são fornecidos para várias instâncias.",DISSERTAÇÃO,Formulações e algoritmos exatos para o problema do caixeiro viajante com coleta e entrega sob múltiplas pilhas,4999193,
"Change is a routine in software development. In the case of APIs provided by libraries and frameworks changes can be backward-incompatible, breaking contracts with client applications. In this dissertation, we perform two studies on API breaking changes. We assess (i) the frequency of breaking changes, (ii) their behavior over time, (iii) the impact on clients, (iv) the characteristics of libraries with high frequency of breaking changes, (v) the reasons why developers introduce them, and (vi) developers awareness on the risks associated to breaking changes. Our large-scale analysis on 317 real-world Java libraries, 9K releases, and 260K client applications shows that (i) 14.78% of the API changes break compatibility, (ii) the frequency increases over time, (iii) 2.54% of API clients are impacted, and (iv) systems with higher frequency of breaking changes are larger, more popular, and more active. We also survey API developers to reveal a list of five reasons why they break APIs.",,CIENCIA DA COMPUTACAO,JOSE LAERTE PIRES XAVIER JUNIOR,UNIVERSIDADE FEDERAL DE MINAS GERAIS,02/05/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Breaking Changes;Compatibilidade;Evoluo de Bibliotecas',ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,MARCO TULIO DE OLIVEIRA VALENTE,0,Breaking Changes;Compatibilidade;Evolução de Bibliotecas,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),Bolsa de produtividade CNPq/2016: Modularidade e Qualidade de Software em Linguagens Dinâmicas,"Change is a routine in software development. In the case of APIs provided by libraries and frameworks changes can be backward-incompatible, breaking contracts with client applications. In this dissertation, we perform two studies on API breaking changes. We assess (i) the frequency of breaking changes, (ii) their behavior over time, (iii) the impact on clients, (iv) the characteristics of libraries with high frequency of breaking changes, (v) the reasons why developers introduce them, and (vi) developers awareness on the risks associated to breaking changes. Our large-scale analysis on 317 real-world Java libraries, 9K releases, and 260K client applications shows that (i) 14.78% of the API changes break compatibility, (ii) the frequency increases over time, (iii) 2.54% of API clients are impacted, and (iv) systems with higher frequency of breaking changes are larger, more popular, and more active. We also survey API developers to reveal a list of five reasons why they break APIs.",DISSERTAÇÃO,Historical and impact analysis of API breaking changes,5005902,
"A common strategy for improving the understanding of natural language queries is to annotate them with semantic information mined from a knowledge base. Nevertheless, queries with different intents may arguably benefit from specialized annotation strategies. For instance, some queries could be effectively annotated with a single entity or an entity attribute, others could be better represented by a list of entities of a single type or by entities of multiple types. In this dissertation, we propose a framework for learning semantic query annotations suitable to the target intent of each query. Thorough experiments on a public benchmark show that our approach can significantly improve state-of-the-art intent-agnostic approaches based on Markov random fields and learning to rank. Our results further demonstrate the consistent effectiveness of our approach for queries of various target intents, lengths, and difficulty levels, as well as its robustness to noise in intent detection.",,CIENCIA DA COMPUTACAO,RAFAEL GLATER DA CRUZ MACHADO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,07/04/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Anotao Semntica de Consultas;Aprendizado de Ranqueamento;Recuperao de Informao',-,RODRYGO LUIS TEODORO SANTOS,0,Anotação Semântica de Consultas,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Compreender a intenção de busca por trás de uma consulta é uma tarefa desafiadora devido principalmente à ambiguidade inerente à linguagem natural. Uma estratégia comum para auxiliar nesse processo é anotar as consultas com informações semânticas extraídas de uma base de conhecimento. Porém, consultas com intenções diferentes podem se beneficiar de estratégias de anotação distintas. Por exemplo, algumas consultas podem ser anotadas com uma única entidade ou com o atributo de uma entidade, outras podem ser melhor representadas por uma lista de entidades de um único tipo ou por entidades de vários tipos distintos, enquanto outras podem ser simplesmente ambíguas. Nesta dissertação, propomos um arcabouço para o aprendizado de anotações semânticas sensíveis à intenção de busca implícita em cada consulta. Experimentos utilizando uma coleção de teste publicamente disponível demonstram que a abordagem proposta supera significativamente abordagens agnósticas à intenção baseadas em campos aleatórios de Markov e em aprendizado de ranqueamento. Os resultados demonstram ainda a consistência da abordagem para consultas com diferentes intenções, tamanhos, e níveis de dificuldade, bem como a sua robustez ao ruído na detecção de intenções.",DISSERTAÇÃO,Intent-aware Semantic Query Annotation,5015375,
"Software development is a knowledge-intensive industry. For this reason, concentration of knowledge in software projects tends to be more risky than in other domains. To express this risk, practitioners proposed the notion of Truck Factor, as a measure of concentration of knowledge in software projects. There are also algorithms that estimate this measure automatically, usually by extracting and processing data from software repositories. However, we still lack large studies that assess the results of Truck Factor algorithms. Therefore, in this dissertation, we first carried out a comparative study of such algorithms. We also investigate the relation between the concepts of Truck Factor and Core Developers, showing that the former are in most cases a subset of the latter. Finally, we analyze the relation between Truck Factor and software architectures. As a result, we recommend that measures of Truck Factor should consider the relative importance of the classes in a software project.",Mivian Marques Ferreira.pdf,CIENCIA DA COMPUTACAO,MIVIAN MARQUES FERREIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,05/06/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Mining software repositories;Truck Factor',-,MARCO TULIO DE OLIVEIRA VALENTE,0,Code ownership;Core developers;Developer tunover,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Software development is a knowledge-intensive industry. For this reason, concentration of knowledge in software projects tends to be more risky than in other domains. To express this risk, practitioners proposed the notion of Truck Factor, as a measure of concentration of knowledge in software projects. There are also algorithms that estimate this measure automatically, usually by extracting and processing data from software repositories. However, we still lack large studies that assess the results of Truck Factor algorithms. Therefore, in this dissertation, we first carried out a comparative study of such algorithms. We also investigate the relation between the concepts of Truck Factor and Core Developers, showing that the former are in most cases a subset of the latter. Finally, we analyze the relation between Truck Factor and software architectures. As a result, we recommend that measures of Truck Factor should consider the relative importance of the classes in a software project.",DISSERTAÇÃO,Concentration Of Knowledge in Software Projects: An Empirical Assessment,5015376,
.,JulioAlbinati.pdf,CIENCIA DA COMPUTACAO,JULIO ALBINATI CORTEZ,UNIVERSIDADE FEDERAL DE MINAS GERAIS,16/01/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'sistema de alerta;dengue;processos gaussianos',-,GISELE LOBO PAPPA,0,sistema de alerta;dengue;processos gaussianos,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Dengue é uma doença presente em todas zonas tropicais do mundo, afetando quase 400 milhões de pessoas ao redor do mundo todos os anos. Como não há tratamento ou vacinas disponíveis para o público geral, a dengue só pode ser contida através do controle populacional do mosquito transmissor do vírus e identificando rapidamente novos focos da doença através de modelos preditivos capazes de estimar, de forma acurada, o número de casos de dengue em uma determinada área e período de tempo. 

O Brasil é responsável pelo maior número de casos confirmados de dengue nas Américas, atingindo mais de um quarto do número total de casos no continente. Motivado por esse cenário, o principal objetivo desse trabalho é desenvolver um modelo para predição de número de casos de dengue em cidades brasileiras. Para tanto, exploramos o framework não-paramétrico e bayesiano de inferência utilizando processos gaussianos, um método que reside na interseção entre modelos interpretáveis e estado-da-arte.

O modelo proposto é equipado com uma função de covariância espaço-temporal. O componente temporal explora dependências locais e sazonalidade, sendo expresso através de uma função quasi-periódica. Já o componente espacial é definido por meio de uma matriz de covariância entre cidades, que é aprendida com base nos dados apenas, sem nenhuma intervenção humana. Além disso, propusemos uma metodologia para extender o modelo proposto de forma a utilizar dados de fontes online, como dados do Twitter, no cenário mais realista onde os dados epidemiológicos são fornecidos com atraso. Assim, os dados online atuam como proxy para os dados epidemiológicos.

Conduzimos uma análise experimental extensiva para analizar a acurácia do modelo proposto, bem como a sua extensão para o cenário descrito acima. Verificamos que as propostas obtiveram predições mais acuradas quando comparadas a formulações alternativas, incluindo um modelo previamente proposto para previsão de incidência de dengue no Brasil. Nossos resultados foram particularmente interessantes no cenário onde os valores de incidência são categorizados em níveis de incidência - baixa, média ou alta -, onde o modelo obteve uma área sob a curva ROC mediana superior a 0.90,comparada à area de 0.74 obtida pela melhor formulação alternativa.",DISSERTAÇÃO,A Spatio-temporal Gaussian Process-based Model For Forecasting Dengue Fever Incidence,5018930,1
"In this work, we tackle the creation of 3D meshes from concept art. We present an automatic 3D model reconstruction method, which delivers a textured 3D triangulated mesh. Creating 3D meshes from real-life objects can be a laborious task. A wide-spread modeling approach is performed in a coarse-to-refined basis. An artist starts modeling from scratch a coarse representation of the given model and then, refines it iteratively. Although this process is consolidated in the industry, it is slow and expensive. Our proposed method uses a set of only three pieces concept art of the object in different viewpoints as input. The resulting mesh is intended to replace the coarse modeling step. Also, we propose a novel constraint to improve the silhouette carving methods, which were originally not capable of modeling concave parts. We evaluate the quality of our 3D meshes by assessing its speed up with 3D artists and similar methods. Our results show a speed-up of 14% by providing the coarse model.",SergioNunes.pdf,CIENCIA DA COMPUTACAO,SERGIO NUNES DA SILVA JUNIOR,UNIVERSIDADE FEDERAL DE MINAS GERAIS,03/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Computao Grfica;Gerao de Textura Automtica;Modelagem baseada em rascunho;Modelagem Tridimensiona',-,ERICKSON RANGEL DO NASCIMENTO,0,Modelagem Tridimensional;Modelagem baseada em rascunho;Computação Gráfica;Geração de Textura Automática,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Neste trabalho é abordado o problema de criação de modelos 3D a partir de artes conceituais para aplicações 3D de tempo real, como por exemplo, Jogos Eletrônicos. É apresentado uma metodologia automática para reconstrução 3D baseada no método 'silhouette carving' no qual cria malhas 3D texturizadas a partir de 3 desenhos bidimensionais. A criação de malhas 3D de objetos da vida real para aplicações em tempo real pode ser uma tarefa trabalhosa, que é comumente produzia manualmente, demandando recursos específicos e altamente especializados como artistas 2D e 3D. Uma técnica difundida na indústria de jogos eletrônicos é a criação iterativa a partir de uma modelo grosseiro para um refinado. Nesse processos, um artista 3D começa a modelagem do zero criando uma representação grosseira do objeto final e vai refinando-o iterativamente. Embora, esse processo esteja consolidado na indústria e vem sendo usado assertivamente na prática para a criação de modelos de alta qualidade, ele é lento e custoso. A metodologia apresentada nesse trabalho requer apenas três artes conceituais do objeto alvo em pontos de vista diferente, e gera uma malha 3D e um mapa de textura como resultado. Essa malha é proposta de ser utilizada como o modelo grosseiro no primeiro paço da modelagem iterativa. Além disso, é proposto uma nova abordagem para melhorar a qualidade dos métodos baseados em 'silhouette carving', os quais originalmente não conseguem modelar partes côncavas. A qualidade dos modelos 3D gerados é avaliado com experimentos entre artistas 3D, onde é avaliado o quanto a metodologia acelera a criação de modelos 3D; e com metodologias similares e baseadas em foto-consistência. Medindo o quão acurado é a malha gerada quando comparadas às outras metodologias. Nossos resultados demonstram que a metodologia acelera na média em 14% a criação de conteúdo 3D quando provido o modelo grosseiro automaticamente.",DISSERTAÇÃO,From 2d To 3d by drawing,5018984,1
.,,CIENCIA DA COMPUTACAO,PAULO VIANA BICALHO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,17/01/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Modelgem de Tpicos;Textos curtos',-,GISELE LOBO PAPPA,0,Modelgem de Tópicos;Textos curtos,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Textos curtos são frequentemente encontrados na Web, e incluem mensagens publicadas em mídias sociais, mensagens de status, comentários de blogs, etc. Descobrir os tópicos ou assuntos presentes neste tipo de mensagens é crucial para uma ampla gama de aplicações, como análise de contexto e caracterização de usuários. No entanto, extrair tópicos de textos curtos é desafiador. Isto porque existe uma dependência dos métodos convencionais, como Latent Dirichlet Allocation (LDA), da co-ocorrência de palavras, que em textos curtos são raras.

Dados os desafios dessa tarefa, esta dissertação propõe um arcabouço para modelagem de tópicos em textos curtos que trabalha expandindo os documentos originais, transformando-os em pseudo-documentos maiores e com mais informações. No arcabouço proposto, os documentos são decompostos em componentes (palavras, bigramas ou n-gramas) definidos sobre um espaço métrico, capaz de fornecer informações sobre a similaridade entre esses componentes. Apresentamos então duas especializações do nosso arcabouço que, apesar de simples, são eficazes e eficientes para a geração de pseudo-documentos a partir dos documentos de texto curto originais.

Enquanto o primeiro método, CoFE (Co-Frequency Expansion), considera a co-ocorrência de palavras para definir o espaço métrico, o segundo, DREx (Distributed Representation-based Expansion), baseia-se em representações vetoriais de palavras. Os pseudo-documentos gerados podem ser dados como entrada para qualquer algoritmo
de modelagem de tópicos, o que torna nossa abordagem ainda mais genérica e flexível. 

Comparamos os resultados das estratégias propostas com cinco algoritmos estado-da-arte que seguem duas estratégias: geram pseudo-documentos ou modificam os métodos convencionais de extração de tópicos. Os métodos foram avaliados em sete conjuntos de dados usando a métrica de qualidade de tópico Normalized Pointwise Mutual Information (NPMI) e também no contexto de classificação de documentos. Resultados experimentais mostram que o DREx com a representação vetorial gerada pelo método Glove supera os métodos existentes, obtendo valores mais elevados de NPMI e melhores valores de macro-F1, com ganhos de até 15% neste último.",DISSERTAÇÃO,A general framework to expand short text for topic modeling,5019031,1
"Geometric primitives defined by OGC and ISO standards, implemented in most modern spatially-enabled DBMS, are unable to capture the semantics of richer representation types, as found in current geographic data models. Moreover, relational DBMSs do not extend referential integrity mechanisms to cover spatial relationships and to support spatial integrity constraints. Rather, they usually assume that all spatial integrity checking will be carried out by the application, during the data entry process. Therefore, this work presents AST-PostGIS, an extension for PostgreSQL/PostGIS that incorporates advanced spatial data types and implements spatial integrity constraints. The extension reduces the distance between the conceptual and the physical designs of spatial databases, by providing richer representations for geo-object and geo-field geometries. It also offers procedures to assert the consistency of spatial relationships during data updates.",LuisEduardoOliveiraLizardo.pdf,CIENCIA DA COMPUTACAO,LUIS EDUARDO OLIVEIRA LIZARDO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,22/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Banco de Dados Espaciais;Modelagem de Banco de Dados;OMT-G;Restries de Integridade Espaciais;Sistema de Informao Geogrfica',-,CLODOVEU AUGUSTO DAVIS JUNIOR,0,Banco de Dados Espaciais;Modelagem de Banco de Dados;Sistema de Informação Geográfica;OMT-G;Restrições de Integridade Espaciais,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"As primitivas geométricas definidas pelos padrões OGC e ISO, implementadas na maioria dos sistemas gerenciadores de banco de dados (SGBD) com suporte para dados espaciais, são incapazes de capturar a semântica de tipos ricos de representação, como os encontrados nos atuais modelos de dados geográficos. Além disso, os SGBDs relacionais não estendem os mecanismos de integridade referencial para cobrir relações espaciais e para suportar restrições de integridade espacial. Em vez disso, eles geralmente assumem que toda a verificação de integridade espacial será realizada pela aplicação, durante o processo de entrada de dados. Além de não ser prático, se o DBMS suportar muitas aplicações, isto pode levar a implementações redundantes.  Por isso, este trabalho apresenta o AST-PostGIS, uma extensão para PostgreSQL/PostGIS que incorpora no SQL tipos de dados espaciais avançados e implementa restrições de integridade espaciais. A extensão reduz a distância entre os projetos conceituais e físicos de banco de dados espaciais, fornecendo representações ricas para geometrias do tipo geo-objeto e geo-campo. Ele também oferece funçoes para garantir a consistência das relações espaciais durante as atualizações de dados. Outras funções podem ainda serem utilizadas antes de impor restrições de integridade espacial pela primeira vez para verificar a consistência inicial do banco de dados. O uso do AST-PostGIS é ilustrado em um pequeno projeto de banco de dados geográfico urbano, mapeando seu esquema conceitual para a implementação física em SQL estendido.",DISSERTAÇÃO,A SQL Extension To Support Advanced Spatial Data Types And Integrity Constraints,5019036,1
"Software Product Line (SPL) is a set of systems that share features. Since the components of a SPL should be easy to maintain, developers have to detect code anomalies that harm the SPL maintainability. We have found several means to detect single code anomalies. Previous work assume that single anomalies suffice to characterize SPL maintenance problems, though an anomaly represents only a part of the problem. Thus, previous studies have difficulties in characterizing anomalies that indicate SPL maintenance problems. In this dissertation, we observe that anomalies may be interconnected, thereby forming so-called agglomerations. Two or more anomalies form an agglomeration in SPL when they affect the same SPL structural element. We then assess to what extent (non-)agglomerated anomalies represent sources of a specific problem: instability. Our findings suggest that feature hierarchy agglomeration indicates up to 89% of sources of instability, i.e., more than non-agglomerated anomalies.",EduardoMoreiraFernandes.pdf,CIENCIA DA COMPUTACAO,EDUARDO MOREIRA FERNANDES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,22/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Anomalia de Cdigo;Instabilidade;Linha de Produtos de Software',-,EDUARDO MAGNO LAGES FIGUEIREDO,0,Anomalia de Código;Instabilidade;Linha de Produtos de Software,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Uma Linha de Produtos de Software (LPS) é um conjunto de sistemas de software que compartilham características comuns e variáveis. Para prover reúso em larga escala, os componentes de uma LPS devem ser de fácil manutenção. Portanto, desenvolvedores devem identificar as estruturas de código anômalas - isto é, as anomalias de código - que prejudicam a manutenção de LPSs. Caso contrário, mudanças em uma LPS podem eventualmente propagar-se a características sem aparente inter-relação e afetar diversos produtos da LPS. Após revisarmos a literatura, encontramos algumas estratégias de detecção e várias ferramentas para detecção de anomalias de código. Em geral, tanto as estratégias quanto as ferramentas apresentam resultados de detecção similares, e algumas ferramentas são compatíveis com LPS. Assim, assumimos que a detecção de anomalias individuais de código é um problema suficientemente tratado pela literatura. Trabalhos anteriores frequentemente assumem que anomalias isoladas são suficientes para caracterizar problemas de manutenção em LPS, ainda que cada anomalia possa representar uma visão parcial, insignificante ou inexistente da extensão de um problema. Portanto, tais estudos possuem dificuldades em caracterizar estruturas anômalas que indiquem problemas de manutenção em LPS. Nesta dissertação, estudamos o contexto de cada anomalia e observamos que algumas delas podem estar interconectadas, formando as chamadas aglomerações de anomalias. Duas ou mais anomalias compõem uma aglomeração em LPS se afetam em conjunto uma característica, uma hierarquia de características ou um componente. Caracterizamos três tipos de aglomeração de anomalias de código em LPS e investigamos o potencial de anomalias aglomeradas, ou não-aglomeradas, em representarem, no contexto de LPS, fontes de um problema de manutenção específico: instabilidade. Analisamos diversas versões de quatro LPSs orientadas por características. Nossos resultados sugerem que aglomeração em hierarquia de características pode indicar até 89% de fontes de instabilidade em LPS, provendo melhores resultados em comparação a anomalias não-aglomeradas.",DISSERTAÇÃO,Anomaly Agglomeration As Sign Of Product Line Instabilities,5019039,1
"The creation of high-level programming models to facilitate the development of parallel applications and the coordinated usage of processing units in heterogeneous architectures are broadly discussed topics in desktop and server systems. However, in the emerging scenario of mobile architectures, there have been few evaluations and discussions so far. For this reason, this work presents important contributions to reduce the complexity of developing parallel applications for mobile heterogeneous devices. The goal of this work is to analyze the overall scenario of parallel programming in mobile platforms -- focusing on Android OS -- and present an alternative solution to reduce the complexity of creating applications that take advantage of current mobile heterogeneous architectures. In order to do that, this work analyses low-level frameworks for parallel programming in Android, presenting a solution to produce high-performance code with low programming effort and low-energy consumption.",,CIENCIA DA COMPUTACAO,WILSON DE CARVALHO MOREIRA JUNIOR,UNIVERSIDADE FEDERAL DE MINAS GERAIS,23/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Dispositivos Heterogneos;Dispositivos Mveis;Programao Paralela',-,RENATO ANTONIO CELSO FERREIRA,0,Dispositivos Heterogêneos;Dispositivos Móveis;Programação Paralela,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A densidade de componentes eletrônicos em um único chip tem crescido por meio século. Mantendo esta tendência por longos anos, a indústria de microprocessadores tem continuamente lançado produtos mais poderosos, permitindo o desenvolvimento de aplicações mais complexas e que exigem maior capacidade computacional. Especialmente nos últimos dez anos, os fabricantes tem mostrado que a direção tomada para atender à crescente demanda por recursos computacionais das aplicações recentes é aumentar o número de unidades de processamento (PUs) num mesmo empacotamento físico, criando o que é conhecido hoje como arquitetura multi-núcleo. Adicionalmente ao aumento no número de núcleos, arquiteturas desktop e servidor tem adotado diferentes tipos de PUs nas chamadas arquiteturas heterogêneas: computadores que incluem CPUs multi-núcleo e também outros processadores de propósito especial -- sendo GPUs um favorito dentre eles.

A criação de modelos de programação de alto nível para facilitar o desenvolvimento de aplicações e do uso coordenado de PUs em arquiteturas heterogêneas são tópicos largamente discutidos em sistemas desktop e servidor. Entretanto, no emergente cenário de arquiteturas de dispositivos móveis, poucas avaliações e discussões foram feitas até o momento. Desta forma, este trabalho apresenta contribuições importantes para reduzir a complexidade de desenvolver aplicações paralelas para dispositivos móveis heterogêneos.

Assim, o principal objetivo deste trabalho é analisar o atual cenário de programação paralela em plataformas móveis heterogêneas -- focando no sistema operacional Android -- e apresentar uma abstração de programação alternativa para reduzir a complexidade de criar aplicações que utilizem múltiplas unidades de processamento nos atuais dispositivos móveis. Afim de fazê-lo, este trabalho analisa frameworks de baixo nível para programação paralela em Android, apresentando um compilador fonte-para-fonte para traduzir o código criado da abstração proposta para representações de alto desempenho em frameworks de mais baixo nível. Esta abordagem trouxe ganhos de desempenho e consequente redução de consumo de energia das aplicações.",DISSERTAÇÃO,Parallel Programming Models For Mobile Devices,5019041,1
"SPMD programming languages for SIMD hardware such as C for CUDA, OpenCL or ISPC have contributed to increase the programmability of SIMD accelerators and graphics processing units. However, SPMD languages still lack the flexibility offered by low-level SIMD programming on explicit vectors. To close this expressiveness gap while preserving the SPMD abstraction, this work introduces the notion of Function Cal Re-Vectorization. We present a formal semantics of CREV, and an its implementation on the ISPC compiler. To validate our idea, we have used CREV to implement some classic algorithms, including string matching, depth first search and Bellman-Ford, with minimum effort.",,CIENCIA DA COMPUTACAO,RUBENS EMILIO ALVES MOREIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,21/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Compilador;SIMD;Vetorizacao',-,FERNANDO MAGNO QUINTAO PEREIRA,0,Compilador;SIMD;Vetorizacao,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Placas gráficas programáveis (GPUs) e alguns processadores, como Xeon Phi
e Skylake Xeon, suportam o modelo de execução SIMD ({\em Single Instruction,
Multiple Data}).
Apesar do interesse crescente, as linguagens conhecidas para a programação
de máquinas SIMD, a saber, C para CUDA e OpenCl, possuem semântica SPMD
({\em Single Program, Multiple Data}), permitindo divergências de execução entre
{\em threads}.
Para usar diretivas SIMD, em geral, todas as {\em threads} de um grupo de
processamento devem estar ativas -- restri\c{c}\{a}o contra-intuitiva sob linguagens
SPMD.
Para usar diretivas SIMD, em geral, todas as {\em threads} de um grupo de
processamento devem estar ativas -- restrição contra-intuitiva sob linguagens
SPMD.
Em particular, tal restrição dificulta a implementação correta de
{\em paralelismo dinâmico}: a capacidade de iniciar um novo grupo de
processamento a partir de um bloco de {\em threads} já em execução.
As implementações atuais de paralelismo dinâmico têm custos com alocação
de recursos e escalonamento.
Nesta dissertação propomos uma forma de implementar paralelismo dinâmico que é mais
eficiente e simples de ser usada.
Com tal propósito, resgata-se uma antiga construção de linguagens SIMD: os
blocos {\tt Everywhere}.
A principal contribuição deste trabalho é a descrição formal da semântica dessa
construção.
Para tanto, cria-se uma máquina SIMD abstrata, e mostra-se como blocos
{\em everywhere} podem ser implementados sobre ela.
Tal máquina encontra-se hoje disponível para uso público, via uma implementação
Prolog, e pode ser usada para simular outros tipos de construções no mundo SIMD.
A partir desta semântica, implementamos CREV em ISPC, um compilador
desenvolvido pela Intel para a geração de código vetorizável.",DISSERTAÇÃO,Function Call Re-vectorization,5019043,1
"ompilers implement a number of code optimizations that rely on precise memory-related information to generate efficient code. The effectiveness of such transformations is bound by the inherent imprecision of static methods designed to extract useful memory insights. We present a symbolic analysis that combines static and dynamic information to infer access bounds for memory operations. Our method is able to derive symbolic bounds for 98% of the memory accesses. We present two distinct clients. The first, a hybrid pointer disambiguation technique, uses our access range analysis to provide the compiler with more precise alias information, generating binaries that are 10% faster. The second, a framework that automatically annotates code for GPU execution, uses our memory range inference to generate directives capable of transferring data to the external device, allowing speedups of over 100x in an Nvidia architecture, and over 50x in an ARM architecture.",PericlesRafaelOliveiraAlves.pdf,CIENCIA DA COMPUTACAO,PERICLES RAFAEL OLIVEIRA ALVES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,03/02/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Anlise de intervalos;Anotao de cdigo para GPU;Otimizao de cdigo;Compiladores;Desambiguao de apontadores',-,FERNANDO MAGNO QUINTAO PEREIRA,0,Análise de intervalos;Anotação de código para GPU;Compiladores;Desambiguação de apontadores;Otimização de código,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,.,DISSERTAÇÃO,Enabling code optimizations through hybrid analysis of memory access ranges,5019046,1
"Stereotypes can be viewed as oversimplified ideas about social groups.   The main goal of this work is to identify stereotypes for female physical attractiveness in images available in search engines results. We also examine  the influence of globalization on the formation of stereotypes in the Web. Two factors associated with the queries are analyzed: language and location. We queried Google and Bing for beautiful and ugly women and proposed a methodology to understand how race and age manifest in the observed stereotypes. Our findings demonstrate the existence of stereotypes associated with  black and old women. We identified common stereotypes within results from countries with the same language, which are eliminated when queries were limited to local sites. Based on that, we show that results from search engines are biased towards the language, which leads to stereotypes that are often quite different from the majority of the female population of the country.",CamilaSouzaAraujo.pdf,CIENCIA DA COMPUTACAO,CAMILA SOUZA ARAUJO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,24/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'esteretipos de beleza;mquinas de busca;vis algortmico',-,WAGNER MEIRA JUNIOR,0,estereótipos de beleza;máquinas de busca;viés algorítmico,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Estereótipos podem ser vistos como ideias simplificadas sobre grupos sociais, evoluindo de acordo com mudanças sociais e culturais. Alguns estereótipos e preconceitos encontrados no mundo real são refletidos no mundo virtual. A internet tem estreitado a distância entre as culturas locais e globais, afetando de diferentes maneiras a percepção das pessoas sobre si e os outros. No contexto global da Internet, as plataformas de máquinas de busca são um importante mediador entre indivíduos e informações. O objetivo principal deste trabalho é identificar estereótipos associados à atratividade física feminina em imagens disponíveis nos resultados das máquinas de busca. Pretendemos também identificar a influência da globalização da internet e da cultura local na formação de estereótipos por meio de dois fatores: linguagem e localização. Nós conduzimos experimentos no Google e no Bing, realizados consultas por mulheres bonitas e feias. Em seguida, coletamos imagens e extraímos informações das faces. Primeiramente, propomos uma metodologia para compreender como raça e idade se manifestam nos estereótipos observados e como eles variam de acordo com os países e regiões. Nossos resultados demonstram a existência de estereótipos de atratividade física feminina, em particular estereótipos negativos para mulheres negras e estereótipos positivos para mulheres brancas em termos de beleza. Também encontramos estereótipos negativos associados a mulheres mais velhas. Em seguida, identificamos uma fração significativa de imagens replicadas em resultados de países com a mesma língua. No entanto, quando as consultas são limitadas a sites locais, mostramos que a existência de imagens comuns entre países é praticamente eliminada. Com base nisso, argumentamos que os resultados das máquinas de busca são enviesados em relação a linguagem utilizada, o que leva a certos estereótipos de beleza que muitas vezes são bastante diferentes da maioria da população feminina do país.",DISSERTAÇÃO,Identifying stereotypes in the online perception of physical attractiveness,5019059,1
.,,CIENCIA DA COMPUTACAO,FERNANDO AUGUSTO FREITAS DA SILVA DA NOVA MUSSEL,UNIVERSIDADE FEDERAL DE MINAS GERAIS,29/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Abridores e bloqueadores;Biologia Sistmica;Canais e bombas inicas;Frmacos e toxinas;Verificao de modelos probabilsticos',-,WAGNER MEIRA JUNIOR,0,Abridores e bloqueadores;Biologia Sistêmica;Canais e bombas iônicas;Fármacos e toxinas;Verificação de modelos probabilísticos,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Verificação probabilística de modelos é uma técnica de verificação formal para modelar e analisar sistemas estocásticos. Pode ser aplicada diretamente a sistemas biológicos que apresentam comportamento estocástico, incluindo sistemas de transporte transmembrânico de íons. Esses sistemas são responsáveis por trocar íons através da membrana celular e participam de diversos processos biológicos, tais como a contração do músculo cardíaco. Nesse trabalho nós modelamos e analisamos quatro modelos diferentes da bomba de sódio e potássio, um sistema de transporte, e suas interações com uma toxina chamada Palitoxina (PTX), que perturba o comportamento da bomba. Nossos modelos sugeriram que altas concentrações de energia celular e sódio inibem a ação de PTX, enquanto que potássio aumenta a mesma. Essa análise fornece um melhor entendimento do comportamento de sistemas de transporte celular, sendo complementar a outras abordagens tais como simulações, e pode levar ao desenvolvimento de fármacos.",DISSERTAÇÃO,Towards terabyte-scale outlier detection using GPUs,5019062,1
.,,CIENCIA DA COMPUTACAO,JEFFERSON RAYNERES SILVA CORDEIRO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,21/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Flexibilizao;Rdio Definido por Software;Redes sem fio;Subcamada MAC',-,LUIZ FILIPE MENEZES VIEIRA,0,Flexibilização;Rádio Definido por Software;Redes sem fio;Subcamada MAC,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"As redes sem fio atuais são bastante dinâmicas e abrigam uma grande diversidade de aplicações com necessidades bem diferentes em relação a requisitos como latência e taxa de erros. Esse cenário de diversidade gera uma demanda por equipamentos e redes mais flexíveis, que se adaptem às características do contexto. Na subcamada MAC, um dos problemas que ainda persistem é o fato de os protocolos não serem eficazes em todos os cenários. Protocolos baseados em CSMA, por exemplo, tendem a ser eficientes em redes esparsas e ineficientes em redes mais congestionadas. Já aqueles baseados em TDMA apresentam comportamento oposto. O presente trabalho aborda esse problema flexibilizando o controle de acesso ao meio, colocando cada protocolo em atividade na situação em que é mais eficiente. O projeto foi desenvolvido utilizando SDR (Software Defined Radio), uma tecnologia que possibilita a criação de protocolos com alto grau de flexibilidade.",DISSERTAÇÃO,Fs-mac: um sistema para flexibilização da subcamada mac em redes sem fio,5019068,1
"In this work we consider a wireless ad-hoc network deployed on a finite street lattice, where communication between nodes is affected by regularly spaced obstacles. The critical transmission range for connectivity in such networks grows with the size of the grid, which might impair the feasibility of low-power wireless technologies in large networks. We therefore analyze how the connectivity of such networks in sub-critical scenarios, where the transmission range is insufficient to establish connectivity, can be improved by introducing a connected backbone of base stations. We formulate the problem of positioning a minimum number of base stations, such that every connected component is covered by at least one base station, and refer to it as the Obstructed Wireless Network Backbone Cover Problem (OWN-BC). We prove that OWN-BC is \NP-complete and propose a 2-approximation algorithm to find solutions with guaranteed quality. Furthermore, we perform a series of simulations to illustrate the performance of the algorithm and characterize scenarios in which the proposed algorithm obtains optimal solutions in polynomial time with high probability.",ManassesFerreira.pdf,CIENCIA DA COMPUTACAO,MANASSES FERREIRA NETO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,22/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Algoritmos aproximativos;Escalabilidade;Limites fundamentais;Modelagem analtica;NP-Completude;Redes de Comunicao sem fio obstrudas;Validao de modelos',-,OLGA NIKOLAEVNA GOUSSEVSKAIA,0,Algoritmos aproximativos;Escalabilidade;Limites fundamentais;Modelagem analítica;NP-Completude;Redes de Comunicação sem fio obstruídas;Validação de modelos,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Nesse trabalho é considerada uma rede sem fio ad hoc realizada em uma grade regular quadrada, na qual a comunicação entre os dispositivos é influenciada por obstáculos regularmente espaçados. O raio crítico de transmissão para obter conectividade nesse tipo de rede cresce muito rapidamente com o tamanho da grade, o que inviabiliza o uso de tecnologias sem fio de baixo consumo de energia. Avalia-se portanto como introduzir eficientemente uma infraestrutura conectada de pontos de acesso em cenários de raio de transmissão insuficiente para obter conectividade. Formula-se o problema de posicionar o menor número de pontos de acesso, de tal modo que todo componente conectado seja coberto por pelo menos uma estação base. Prova-se que tal problema é NP-Completo e propõe-se um algoritmo 2-aproximativo para obter soluções com garantia de qualidade. Realiza-se simulações e demonstra-se analiticamente a chamada Probabilidade Polinomial que estabelece a chance nosso algoritmo obter a solução ótima.",DISSERTAÇÃO,Infraestrutura de Acesso em Redes sem Fio Obstruídas: da Intratabilidade à Conectividade,5019073,1
.,DavidMichaelQuirinoNelson.pdf,CIENCIA DA COMPUTACAO,DAVID MICHAEL QUIRINO NELSON,UNIVERSIDADE FEDERAL DE MINAS GERAIS,24/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Anlise Tcnica;Aprendizado de Mquina;Long Short-Term Memory;Mercado de Aes;Redes Neurais Recorrentes',-,ADRIANO CESAR MACHADO PEREIRA,0,Análise Técnica;Aprendizado de Máquina;Long Short-Term Memory;Mercado de Ações;Redes Neurais Recorrentes,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Prever variações de preço em bolsas de valores é um grande desafio devido ao fato
que este é um ambiente imensamente complexo, caótico e dinâmico. Existem diversos
estudos de variadas áreas buscando encarar tal desafio, e abordagens baseadas em
Aprendizado de Máquina são o foco de muitos deles. Existem vários exemplos em que
algoritmos de Aprendizado de Máquina foram capazes de alcançar resultados satisfatórios
quando realizando tal tipo de previsão. Este trabalho estuda a aplicação de redes
Long Short-Term Memory nesse problema, de previsão de tendências de preços de ações
e com base no histórico de preços juntamente com indicadores de análise técnica. Para
este objetivo, um modelo de previsão foi projetado, uma série de experimentos foram
executados e seus resultados analisados em relação a uma variedade de métricas para
avaliar se este tipo de algoritmo apresenta melhorias quando comparado com outros
métodos de Aprendizado de Máquina e estratégias de investimento. Os resultados obtidos
foram satisfatórios, obtendo uma acurácia média de até 55,9% ao prever se o preço
de uma determinada ação irá subir ou não no futuro imediato. O modelo também
foi avaliado sob aspectos financeiros mostrando resultados promissores em termos de
retorno financeiro.",DISSERTAÇÃO,Uso de Redes Neurais Recorrentes Para Previsão de Séries Temporais Financeiras,5019077,1
.,,CIENCIA DA COMPUTACAO,LUIS GUILHERME SILVA PENA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,17/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Acordo de chave;Advantage Distillation;Criptografia;Information Reconciliation;Segurana Incondicional',-,JEROEN ANTONIUS MARIA VAN DE GRAAF,0,Acordo de chave;Advantage Distillation;Criptografia;Information Reconciliation;Segurança Incondicional,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Criação e distribuição de chaves criptográficas é parte fundamental do estudo de segurança da informação. Abordamos protocolos com a segurança baseada em teoria da informação, considerados como o modelo mais seguro possível e que provê a segurança incondicional.
O acordo de chave é constituído por três fases: Advantage Distillation, Information Reconciliation e Privacy Amplification. A primeira delas é responsável por garantir vantagem a Alice e Bob sobre Eve, a segunda deverá corrigir os erros existentes nas cadeias de bits e a última eliminará a informação que foi vazada nas fases anteriores. Ao analisar abordagens encontradas na literatura, foi possível estabelecer a relação entre cada uma das fases e definir os protocolos que apresentaram o melhor desempenho na criação de um chave final. Ao final do estudo, é apresentada a relação entre a cadeia inicial de bits e o canal de distribuição pelo tamanho final da senha secreta e sua segurança.",DISSERTAÇÃO,Análise de Protocolos de Acordo de Chaves Baseados Em Teoria da Informação,5019078,1
.,ViniciusVelosodeMeloGarcia.pdf,CIENCIA DA COMPUTACAO,VINICIUS VELOSO DE MELO GARCIA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,17/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,"b'Casamento de Padro, Compreenso de Linguagem Natural, Sistemas de Resposta'",-,ADRIANO ALONSO VELOSO,0,"Casamento de Padrão, Compreensão de Linguagem Natural, Sistemas de Resposta",CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Neste trabalho é apresentado um modelo para o Mecanismo Humano de Processamento Textual e uma implementação deste modelo na forma de uma linguagem de programação de nome JSpy.
Este sistema é capaz de descrever informação estruturada de forma muito semelhante à feita pelos seres humanos.
Isso possibilita que significados semânticos complexos sejam expressados em poucas linhas de código e com naturalidade.
A avaliação do sistema é feita resolvendo-se um conjunto de problemas, propostos por Weston et al., que avaliam diferentes habilidades de processamento textual.
A resolução destes prolemas não apenas é feita em poucas linhas de código como supera os resultados do atual estado da arte.",DISSERTAÇÃO,JSPY: um modelo objetivo para compreenção de linguagem natural,5019080,1
.,,CIENCIA DA COMPUTACAO,ANDRE LLOYD DWIGHT PERLEE HARDER,UNIVERSIDADE FEDERAL DE MINAS GERAIS,30/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Aprendizado de Mquina;Descida de Gradiente;Modelos Markovianos',-,ADRIANO ALONSO VELOSO,0,Aprendizado de Máquina;Descida de Gradiente;Modelos Markovianos,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O estudo de sequências de eventos e séries temporais se encontra no cerne do pensamento científico, tal que a meta principal de uma grande parcela de nossas empreitadas intelectuais se sumariza em prever, explicar, ou modelar esse tipo de fenômeno, que se instancia no mundo por via de series de, por exemplo, eventos climáticos, linguagem, observações empíricas, e ações motoras. Um grupo de métodos que tem sido exitoso em modelar séries e sequências são os Modelos Markovianos de tempo e estado discreto. Estes modelos oferecem um ferramental variado, podendo ser empregados em tarefas como predição, explanação e simulação, às custas de uma modelagem mais simplificada do mundo. O uso destes modelos, no entanto, vem acompanhado de dificuldades em adequar-los a contextos específicos: A criação de estados semanticamente úteis, e novas estratégias para expressar a função de transição tradicionalmente requerem alterações complexas sobre otimizadores como o algoritmo de Baum Welch.

Nesse trabalho, hipotetizamos que a otimização por via de Descida de Gradiente é um substituto eficaz para formas mais tradicionais de se otimizar Cadeias de Markov discretas. Ademais, acreditamos que o uso de tais técnicas permitirá uma maior flexibilidade ao usuário na definição da semântica dos estados e do modelo de transição. Para esse fim, desenvolvemos uma estratégia para obter otimizadores de descida de gradiente para essa classe de modelos, que validamos por via de comparações objetivas e subjetivas com a saída obtida pelo algoritmo Baum-Welch. Em seguida, propomos dois modelos práticos que objetivam demonstrar a flexibilidade do novo otimizador. O primeiro utiliza um modelo hibrido de Rede Convolucional/Modelo Markoviano na  classificação de dígitos do MNIST, enquanto o segundo ilustra um modelo de transição mais complexo, hierárquico, que trata dados multi-dimensionais e multi-escala, o qual validamos prevendo valores de temperatura. Nossos resultados mostram que descida de gradiente é um método viável para otimizar Modelos Markovianos Discretos, e que o modelo resultante é altamente flexível.",DISSERTAÇÃO,Extending Markov Models Through Gradient Descent Optimization,5020185,1
.,,CIENCIA DA COMPUTACAO,PRICILA RESENDE RODRIGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,31/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'busca acadmica;design de interfaces;interao humano-computador;linguagem visual;string de busca',-,RAQUEL OLIVEIRA PRATES,0,busca acadêmica;design de interfaces;interação humano-computador;linguagem visual;string de busca,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A busca por artigos é uma constante no contexto acadêmico. Contudo, a definição de uma string pode consumir muito tempo e, somado a isso, cada sistema de busca possui uma sintaxe própria. A definição de strings apoiada pelo uso de linguagens visuais, permite ao usuário a compreensão da estrutura da string e amplia sua capacidade de expressar a necessidade de informação, ao mesmo tempo em que pode otimizar seu tempo, gerando consultas mais completas. Assim, o objetivo deste projeto foi a definição de uma linguagem visual para buscas acadêmicas. Para a definição da linguagem visual VILAS e design da interface da ferramenta web desenvolvida para interação com a linguagem, foram aplicados conceitos de design da Gestalt, usabilidade e também da Engenharia Semiótica.  Assim, é possibilitado ao usuário não somente a definição e manipulação de uma string de busca, como também a tradução da string para a sintaxe de diferentes sistemas, em uma representação visual minimalista e significativa.",DISSERTAÇÃO,Uma linguagem visual para definição de strings de busca acadêmica,5020199,1
"Recent years have witnessed the emergence of sharing economy marketplaces. A prominent example in the travel industry is Airbnb, which connects travelers with hosts, allowing both to exchange cultural experiences in addition to the economic transaction. Inspired by recent marketing analyses of repurchase intent behavior on Airbnb, we propose a learning-to-rank approach for lodging recommendation, aimed to infer the user's perception of several dimensions involved in choosing which lodging to book. We devise features to capture the user's price sensitivity, their perceived value of a lodging option, the risk involved in choosing it rather than other options, the authenticity of the experience it could provide, and its overall perception by other users. Through a comprehensive evaluation using publicly available Airbnb data, we demonstrate the effectiveness of our proposed model compared to several alternative recommendation baselines, including a simulation of Airbnb's own recommender.",LuisRaulSanchezVasquez.pdf,CIENCIA DA COMPUTACAO,LUIS RAUL SANCHEZ VAZQUEZ,UNIVERSIDADE FEDERAL DE MINAS GERAIS,30/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Economia Compartilhada;Sistemas de Recommendao',-,RODRYGO LUIS TEODORO SANTOS,0,Economia Compartilhada;Sistemas de Recommendação,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Nos últimos anos, temos observado o surgimento de mercados de economia compartilhada, os quais permitem aos usuários compartilhar bens e serviços diretamente uns com os outros. Um exemplo proeminente na indústria de viagens é a Airbnb, uma empresa online que conecta viajantes a anfitriões, permitindo a troca de experiências culturais, além da transação econômica. Inspirados por recentes pesquisas de mercado sobre intenções de compra no Airbnb, propomos uma abordagem de aprendizado de ranqueamento para recomendação de hospedagens, com vistas a inferir a percepção do usuário quanto a diversas dimensões envolvidas na decisão de escolha de uma hospedagem. Em particular, desenvolvemos um modelo capaz de captar a sensibilidade do usuário quanto aos preços praticados, sua percepção de valor na escolha de uma hospedagem específica, o risco envolvido na escolha comparado a outras alternativas, a autenticidade da experiência proporcionada e a percepção geral das opiniões de outros usuários. Através de uma avaliação abrangente usando dados publicamente disponíveis da Airbnb, demonstramos a eficácia do modelo proposto em comparação a uma série de recomendadores alternativos, incluindo uma simulação do recomendador da própria Airbnb.",DISSERTAÇÃO,Repurchase intention for lodging recommendation,5020214,1
.,,CIENCIA DA COMPUTACAO,BRUNO LAPORAIS PEREIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,31/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Aprendizado Online;Feedback Implcito;Recomendao de Msicas;Sistemas de Recomendao',-,RODRYGO LUIS TEODORO SANTOS,0,Aprendizado Online;Feedback Implícito;Recomendação de Músicas;Sistemas de Recomendação,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O sucesso proeminente de serviços de streaming tem gerado cada vez mais desafios para a recomendação de músicas. Em particular, em um cenário de streaming, as músicas são consumidas sequencialmente dentro de uma sessão, a qual deve atender tanto preferências históricas quanto preferências eventuais, ocasionadas por mudanças repentinas no contexto do usuário. Nesta dissertação, propomos uma nova abordagem de aprendizado online para recomendação de músicas, com vistas a aprender continuamente a partir do feedback do usuário. Em contraste a abordagens de aprendizado online existentes para recomendação de música, utilizamos o feedback implícito como único sinal de preferência do usuário em cada ponto no tempo. Além disso, representamos cada música utilizando um conjunto de atributos contínuos. Uma avaliação rigorosa utilizando sessões coletadas do Last.fm demonstra a efetividade da nossa abordagem em aprender mais rápido e melhor comparada ao estado-da-arte para aprendizado online.",DISSERTAÇÃO,Recomendação online de músicas usando feedback implícito,5020227,1
.,ThiagoResendePereiraPrado.pdf,CIENCIA DA COMPUTACAO,THIAGO RESENDE PEREIRA PRADO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,31/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Recommendation;Review;Social Network',-,MIRELLA MOURA MORO,0,Recommendation;Review;Social Network,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Aplicativos de revisão online recomendam as revisões mais úteis para um cliente ler. Sobre revisões de locais (ou pontos de interesse), para o proprietário do estabelecimento é importante ter uma maneira rápida e confiável de identificar as revisões relevantes para melhorar os serviços ofertados, uma vez que lidam com volumes potencialmente grandes de dados  (muitos usuários escrevendo muitos comentários para muitos itens). Neste trabalho, introduzimos um novo problema: identificar a utilidade de uma revisão para o proprietário de um estabelecimento. Portanto, propomos criar a classificação das revisões de acordo com sua relevância para a tomada de decisões, ou seja, direcionado aos proprietários e não aos clientes. O ranking proposto considera aspectos e sentimentos presentes nas revisões. Finalmente, a nossa avaliação experimental considera um ground truth  e um baseline  também propostos neste trabalho e mostra que a nossa solução está muito próxima da classificação ideal.",DISSERTAÇÃO,Review recommendation for points of interest's owners,5020248,1
.,,CIENCIA DA COMPUTACAO,MATEUS ANDRADE REZENDE,UNIVERSIDADE FEDERAL DE MINAS GERAIS,02/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Cascade Correlation Neural Networks;Cross-Entropy Method;General Game Playing;Monte Carlo Tree Search;Upper Confidence Bounds for Trees',-,LUIZ CHAIMOWICZ,0,Cascade Correlation Neural Networks;Cross-Entropy Method;General Game Playing;Monte Carlo Tree Search;Upper Confidence Bounds for Trees,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Um agente de General Game Playing (GGP) deve ser capaz de jogar efetivamente diferentes jogos talvez com algum processo inicial de aprendizagem. Dadas as regras de um jogo qualquer, como gerar um agente inteligente que seja competitivo em comparação a agentes específicos para o jogo? Neste trabalho, propomos um método denominado UCT-CCNN para o aprendizado off-line de função de valor para estados de jogos de tabuleiro. No método UCT-CCNN inúmeras partidas são jogadas pelos agentes MCTS com política da árvore conhecida como Upper Confidence Bounds for Tree (UCT) em um processo off-line que gera uma base de dados de exemplos de estado-utilidade. A partir desses exemplos uma função de valor para os estados de jogo é aprendida com o uso de redes neurais construtivas denominadas Cascade Correlation Neural Networks. Os jogos Othello e Trilha foram submetidos ao método UCT-CCNN e os agentes obtidos foram capazes de ganhar de agentes específicos do domínio.",DISSERTAÇÃO,Desenvolvimento e avaliação de uma metodologia para geração de agentes genéricos para jogos de tabuleiro,5020365,1
"It is well known that predicting the outcome of sport events from feature-based models is a very challenging task. This work quantifies this difficulty by proposing a scale that measures the impact of luck and skill on final results of sports leagues. It was collected and analyzed all games from 1503 seasons of 84 countries in 4 different sports: basketball, soccer, volleyball and handball. Through the proposed scale, it is possible to measure the competitiveness by countries and by sports. Another contribution of this work is the fitting of a probabilistic graphical model in order to learn about the teams' skills in situations where skill has a large impact on the results according with the proposed scale. The model finds on National Basketball Association (NBA) championship a correlation of about 0.7 between the estimated skill and the number of wins throughout the season.",RaquelAoki.pdf,CIENCIA DA COMPUTACAO,RAQUEL YURI DA SILVEIRA AOKI,UNIVERSIDADE FEDERAL DE MINAS GERAIS,26/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Anlise Esportiva;Estatstica Bayesiana;Modelos Grficos Probabilsticos',-,RENATO MARTINS ASSUNCAO,0,Análise Esportiva;Estatística Bayesiana;Modelos Gráficos Probabilísticos,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Prever o resultado de eventos esportivos é uma tarefa muito desafiadora. Este trabalho quantifica essa dificuldade através de um coeficiente que mede a distância entre o resultado final observado em ligas esportivas e o idealizado em competições completamente balanceadas em termos de habilidade. Este coeficiente indica a presença relativa de sorte e habilidade no campeonato. Foram coletados e analisados todos os jogos de 198 ligas esportivas, compostas de 1503 temporadas, oriundas de 84 países diferentes em 4 esportes: basquete, futebol, voleibol e handebol. Foi medida a competitividade por país e esporte. Também foram identificadas em cada temporada quais equipes deveriam ser removidas para que a liga ficasse completamente aleatória. Surpreendentemente, não é necessária a remoção de muitas equipes. Outra contribuição deste trabalho um modelo gráfico probabilístico cujo objetivo é aprender sobre as habilidades das equipes e decompor o peso relativo da sorte e da habilidade em cada partida. O componente da habilidade foi separado em variáveis associadas às características da equipe. O modelo também permite estimar como 0.36 a probabilidade do pior time, o chamado underdog, vencer uma partida na liga americana de basquete NBA. Como mostrado na primeira parte deste trabalho, a sorte está substancialmente presente mesmo nos campeonatos mais competitivos, o que parcialmente explica porque modelos sofisticados e complexos dificilmente conseguem ter resultados melhores que modelos mais simples na tarefa de prever resultados esportivos.",DISSERTAÇÃO,Um arcabouço computacional para caracterizar sorte e habilidade em ligas esportivas,5020374,1
"Oceans represent around 2/3 of Earths surfaces. It is estimated that 95% of their volume remains unknown by humans. In this context, underwater wireless sensor networks (UWSNs) have been shown as a viable technology for aquatic monitoring and exploration.  However, these networks have low performance in terms of data collection, latency and energy consumption, due to the underwater acoustic communication characteristics. In this work, we propose jointly topology control and opportunistic routing solutions for improving data collection and prolonging UWSNs lifetime. Simulation results show that the UWSNs opportunistic routing performance is significantly improved by means of topology control, increasing the data delivery ratio and reducing the network energy consumption.",RodolfoWandersonCoutinho.pdf,CIENCIA DA COMPUTACAO,RODOLFO WANDERSON LIMA COUTINHO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,17/03/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'roteamento oportunstico;redes de sensores aquticas;avaliao de desempenho;controle de topologia',-,ANTONIO ALFREDO FERREIRA LOUREIRO,0,roteamento oportunístico;redes de sensores aquáticas;avaliação de desempenho;controle de topologia,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Cerca de 2/3 da superfície do planeta Terra é coberta por oceanos. Estimasse que 95% do volume desses ambientes ainda permanece desconhecido.  Nesse contexto, redes de sensores sem fio sub-aquáticas (RSSFAs) vêm se mostrando como uma tecnologia viável para o monitoramento e exploração de ambientes aquáticos. Entretanto, esse tipo de rede apresenta baixo desempenho em termos de coleta de dados, latência e consumo de energia, dada as características da comunicação acústica sub-aquática. Neste trabalho, é proposto soluções combinadas de controle de topologia e roteamento oportunístico para melhorar a coleta de dados e prolongar o tempo de vida de RSSFAs.  Resultados de simulação mostram que o desempenho de protocolos de roteamento oportunístico, essenciais em RSSFAs, é melhorado significativamente com o controle da topologia da rede,  levando a um incremento da taxa de entrega dos dados e redução do gasto de energia da rede.",TESE,Topology control and opportunistic routing in underwater sensor networks,5020429,1
"The identification of reputable entities is an important task in business, education, and in many other fields. In general, the reputation of an entity reflects its public perception, which touches a variety of aspects that may impact the identity of the entity, such as its competence, integrity, and trustworthiness. Indeed, more reputable entities are presumably a better fit for most purposes. Reputation is a widespread notion in society, albeit an arguably ill-defined one. As a subjective and multi-faceted concept, quantifying reputation is challenging. As a result, existing attempts to quantify reputation rely on either manual assessments or on a restrictive definition of reputation.

In this thesis, instead of relying on a single and precise definition of reputation, we propose to exploit the transference of reputation among entities in order to identify the most reputable ones. To this end, we present a conceptual framework of reputation flows and propose a metric based on it, which we call the P-score metric. This framework consists of a novel random walk model to infer the reputation of a target set of entities with respect to suitable sources of reputation. By using it, one could better understand how the reputation flows between distinct entities in a reputation graph.

We instantiate our model in academic search to address three common ranking tasks, namely, research group ranking, author ranking, and publication venue ranking. By relying on publishing behavior as a reputation signal, we demonstrate the effectiveness of our model in contrast to standard citation-based approaches for identifying reputable venues, authors as well as research groups in the broad area of computer science.",,CIENCIA DA COMPUTACAO,SABIR RIBAS,UNIVERSIDADE FEDERAL DE MINAS GERAIS,06/04/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Academic Search;Random Walks;Reputation Flows',-,BERTHIER RIBEIRO DE ARAUJO NETO,0,Academic Search;Random Walks;Reputation Flows,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"The identification of reputable entities is an important task in business, education, and in many other fields. In general, the reputation of an entity reflects its public perception, which touches a variety of aspects that may impact the identity of the entity, such as its competence, integrity, and trustworthiness. In this thesis, instead of relying on a single and precise definition of reputation, we propose to exploit the transference of reputation among entities in order to identify the most reputable ones. To this end, we present a conceptual framework of reputation flows and propose a metric based on it, which we call the P-score metric. This framework consists of a novel random walk model to infer the reputation of a target set of entities with respect to suitable sources of reputation. We instantiate our model in academic search to address three common ranking tasks, namely, research group ranking, author ranking, and publication venue ranking.",TESE,Random walks on the reputation graph,5020798,1
"In social networks, tie strength has been largely explored in different contexts, such as information diffusion and evaluation of scientific productivity. In this thesis, we measure the strength of co-authorship ties in non-temporal and temporal social networks. In sum, the main contributions are: (1) a survey and a taxonomy of social professional networks; (2) an analysis of how topological properties relate to tie strength in non-temporal networks, which reveal how different properties explain variations in tie strength; (3) a new metric called tieness that better differentiates tie strength in distinct levels in non-temporal networks; (4) a new algorithm to measure tie strength called STACY (Strength of Ties Automatic-Classifier over the Years), which is able of better identifying strong ties than a state of the art algorithm; and (5) a new computational model called temporal_tieness to classify tie strength in temporal networks with low computational cost.",MicheleAmaralBrandão.pdf,CIENCIA DA COMPUTACAO,MICHELE AMARAL BRANDAO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,20/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Fora dos Relacionamentos;Rede Social;Redes Temporais',-,MIRELLA MOURA MORO,0,Força dos Relacionamentos;Rede Social;Redes Temporais,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O estudo de relacionamentos sociais tem sido utilizado para construir modelos rigorosos que revelam a evolução de redes sociais e seus dinamismos. Uma propriedade dos relacionamentos sociais é a força, a qual tem sido aplicada em diferentes contextos como por exemplo: difusão de informação, análises de padrões em logs de comunicação e avaliação da produtividade científica de pesquisadores. Especialmente, analisar a força dos relacionamentos permite investigar como diferentes relacionamentos desempenham papéis distintos e identificar o impacto em nível micro e macro na rede. O objetivo desta tese é medir a força dos relacionamentos de co-autoria em redes sociais acadêmicas não-temporais e temporais. As principais contribuições são: (1) uma revisão do estado-da-arte e uma taxonomia para redes sociais profissionais, que contextualizam o problema abordado neste trabalho; (2) uma análise de como propriedades topológicas relacionam-se com a força dos relacionamentos, pois nossos resultados mostram que diferentes propriedades topológicas explicam variações na força dos relacionamentos de co-autoria, dependendo da área de pesquisa; (3) uma nova métrica chamada tieness que é fácil de calcular e melhor diferencia a força dos relacionamentos em diferentes níveis em redes sociais de co-autoria não-temporais; (4) uma análise da dinâmica das forças dos relacionamentos ao longo do tempo através de dois algoritmos, um já existente e um proposto aqui, chamado STACY (Strength of Ties Automatic-Classifier over the Years); e (5) um novo modelo computacional chamado temporal_tieness que diretamente classifica com baixo custo computacional a força dos relacionamentos em redes sociais temporais de co-autoria.",TESE,"Tie strength in co-authorship social networks: analyses, metrics and a new computational model",5020809,1
"Matrix factorization (MF) and neighborhood models are the two most successful approaches in the  context of Collaborative Filtering (CF). Recently, graph-based models have emerged as an effective alternative despite little attention has been devoted to them. In this dissertation, we exploit collaborative data to improve MF and graph-based recommendation.  We propose a graph-based approach that exploits three-step paths starting from the target user in the user-item bipartite graph and relies on the scoring functions, which exploit distributional aspects of the ratings given by users in order to extract their latent information. Likewise, we propose a  MF  approach that exploits both user-user and item-item similarities extracted from the raw user-item rating matrix. We experiment with several publicly available datasets against relevant baselines. The results attest the value of our methods as they provide significant gains in several settings.",RamonPereiraLopes.pdf,CIENCIA DA COMPUTACAO,RAMON PEREIRA LOPES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,09/05/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Estatstica Bayesiana;Fatorao de Matrizes;Filtragem Colaborativa;Grafos;Sistemas de Recomendao',-,RENATO MARTINS ASSUNCAO,0,Estatística Bayesiana;Fatoração de Matrizes;Filtragem Colaborativa;Grafos;Sistemas de Recomendação,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Fatoração de Matrizes (FM) e modelos de vizinhança são as duas abordagens mais
difundidas para Sistemas de Recomendação no contexto de Filtragem Colaborativa.
Recentemente, modelos de recomendação baseados em grafo surgiram como uma alter-
nativa àquelas abordagens Apesar de pouca atenção ter sido dedicada a esses modelos.
Neste trabalho, exploramos dados colaborativos com o objetivo de prover recomen-
dações mais precisas no contexto de FM e recomendação baseada em grafos. Para
esse fim, propomos abordagens baseadas em grafo e três funções de pontuação baseada
no paradigma Bayesiano, e um novo modelo Bayesiano para FM. Nossa abordagem
baseada em grafos, explora caminhos de comprimento três a partir do usuário alvo
no grafo de relacionamentos entre usuários e itens, equanto as funções de pontuação,
por sua vez, exploram aspectos distribucionais das notas dadas pelos usuários a fim de
extrair informações latentes. Nossa abordagem baseada em FM, por sua vez, explora
similaridades tanto entre usuários quanto entre itens, de modo que as similaridades
são computadas a partir da matriz de interaccão entre usuários e itens. Avaliamos o
desempenho dos nossos métodos em várias coleções de teste disponíveis publicamente
e comparamos com outras abordagens da literatura sob um conjunto de métricas. Os
resultados mostram que os nossos métodos superam aqueles comparados em vários
cenários.",TESE,Similarity-enhanced collaborative filtering,5020811,1
.,BrunoLuan.pdf,CIENCIA DA COMPUTACAO,BRUNO LUAN DE SOUSA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,07/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Bad Smell;Mtricas de Software;Padres de Projeto',-,MARIZA ANDRADE DA SILVA BIGONHA,0,Bad Smell;Métricas de Software;Padrões de Projeto,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Bad Smells são sintomas que aparecem no código fonte de um software e que possivelmente indicam a presença de um problema estrutural que demanda refatoração do código. Padrões de projeto são soluções conhecidas como boas práticas de desenvolvimento de software e auxiliam na produção de software com qualidade e estrutura flexível. Alguns trabalhos na literatura destacam o uso dos padrões de projeto para remoção de bad smells. Contudo, outros trabalhos identificam relações de coocorrência entre padrões de projeto e bad smells. Tendo em vista a relevância de ambos conceitos no contexto de Engenharia de Software, é importante conhecer as situações em que o uso de padrões de projeto podem estar associadas ao surgimento de bad smells. Baseado nisso, o objetivo deste trabalho é estudar as ocorrências de bad smells em software desenvolvidos com padrões de projeto. Para atingir esse objetivo, inicialmente foi realizada uma revisão sistemática da literatura, a fim de se compreender o atual estado da arte referente a padrões de projeto e bad smells. Além disso, foi realizado um estudo com cinco projetos Java, a fim de (i) investigar se o uso de padrões de projeto reduzem a ocorrência de bad smells, (ii) identificar coocorrências entre padrões de projeto com bad smells, e (iii) identificar as principais situações que levaram os sistemas de software a apresentar essas relações de coocorrência. Os resultados do estudo indicam que apesar de alguns padrões de projeto apresentarem baixa relação de coocorrência com bad smells, essas soluções não necessariamente evitam o surgimento de bad smells. Além disso, a análise das situações de coocorrência de bad smells e padrões de projeto indica que o mal planejamento de padrões de projeto e seu uso inadequado impactam diretamente no surgimento de bad smells.",DISSERTAÇÃO,Estudo de coocorrências de padrões de projeto e bad smells usando métricas de software,5023626,
"In this dissertation, we study the reputation of publication venues and graduate programs in Computer Science (CS) with focus on its subareas. For that we adopt the 37 CS subareas defined by Microsoft Academic Research and extend the usability of a reputation metric based on Markov networks, called P-score. More specifically, we study the impact to the reputation of CS conferences, journals, and graduate programs in Brazil and US when subareas are taken into account. Our experiments suggest that the extended P-scores yield better results when compared with citation counts. We also present an overview of current research directions of Brazil and US, i.e. on which subareas they have the most prominent work nowadays. This analysis of reputation on a per subarea basis provides additional insights for university officials, funding agencies directors, and government officials who need to decide how to allocate limited research funds.",Alberto Hideki Ueda.pdf,-,ALBERTO HIDEKI UEDA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,14/07/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Busca Acadmica;P-score;Reputao;Sub-reas',-,BERTHIER RIBEIRO DE ARAUJO NETO,0,Busca Acadêmica;P-score;Reputação;Sub-áreas,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Nesta dissertação, analisamos a reputação de veículos de publicação e programas de pós-graduação em Ciência da Computação (CC) com foco em suas sub-áreas. Para realizar esta tarefa, consideramos as 37 sub-áreas em CC definidas pela Microsoft Academic Research e estendemos uma métrica de reputação baseada em redes de Markov, denominada P-score (Publication Score). Mais especificamente, examinamos o impacto obtido na reputação de conferências, periódicos e em programas de pós-graduação no Brasil e nos Estados Unidos (EUA) em CC, ao considerarmos suas sub-áreas. Nossos experimentos sugerem que a metodologia proposta produz resultados melhores que métricas basedas em citações. Nós também apresentamos um panorama das direções de pesquisa atuais do Brasil e dos EUA, que seja, em quais sub-áreas estes países possuem mais trabalhos de destaque no momento. Esta análise de reputação sob a perspectiva de sub-áreas fornece informações adicionais para administradores de universidades, diretores de agências de fomento à pesquisa e representantes do governo que precisam decidir como alocar recursos de pesquisa limitados. Por exemplo, em CC, sabemos que o volume de publicações científicas nos EUA é significantemente superior ao volume da pesquisa brasileira. Porém, este trabalho mostra que as sub-áreas em CC em que cada país possui maior impacto científico são divergentes.",DISSERTAÇÃO,Reputation in computer science on a per subarea basis,5038124,
"Vehicles' internal control systems rely on physical sensors to acquire data about their operation and malfunctions. Nowadays, manufacturers invest in better and more diverse sensors, however, there are aspects still unmonitored. Virtual sensors were proposed to replace failing physical sensors, but their applications have grown to include new aspects of a vehicle. The number of high-quality sensors available in modern cars enabled the creation of virtual sensors that extract new information from raw data.

This work presents the design of vehicular virtual sensors, which is divided into three stages, namely, collection, operation and presentation of new data. To detail this process, in the first stage a data collection and its preprocessing are presented. The prepared data is then fed into operators, divided into four categories with proofs of concept that show their behavior. Finally, to illustrate the presentation stage, three examples of virtual sensors are provided.",,CIENCIA DA COMPUTACAO,ANDRE BROCHADO CAMPOLINA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,31/08/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Sensores Veiculares;Sensores Virtuais',-,ANTONIO ALFREDO FERREIRA LOUREIRO,0,Sensores Veiculares;Sensores Virtuais,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Sistemas de controle internos de veículos dependem de sensores físicos para adquirir dados sobre seus estados de operação e defeitos. Atualmente, fabricantes investem em sensores mais diversos e de melhor qualidade, no entanto, ainda existem aspectos para os quais não existem sensores físicos. Sensores virtuais foram propostos inicialmente para substituir sensores físicos em falha, porém suas aplicações cresceram a ponto de monitorar novas características do funcionamento de um veículo. O número de sensores de alta qualidade em um carro moderno permitiu a criação de sensores virtuais que extraem informações complexas a partir de dados brutos. Uma tecnologia importante nesse contexto é o sistema On Board Diagnostic (OBD), que serve como interface de coleta para os sensores embarcados.

Este trabalho apresenta o processo de projeto de sensores virtuais veiculares. Para descrever as diferentes etapas da concepção de um sensor virtual, esse processo é dividido em três principais estágios, sendo a coleta, operação e apresentação dos novos dados. O estágio de coleta é responsável por adquirir e preparar dados brutos de sensores para alimentar os operadores. Na etapa de operação, os operadores são responsáveis por efetivamente transformar dados de entrada em novos tipos de informação. Finalmente, no estágio de apresentação, os novos dados calculados são ajustados em unidades e formatos para serem apresentados aos usuários finais de um sensor virtual, sejam eles os próprios veículos, motoristas ou companhias.

Para detalhar a apresentação de cada um desses passos, no primeiro estágio é apresentado o processo de coleta dos dados e seu preprocessamento. Os dados prontos são então fornecidos aos operadores, que estão divididos em quatro categorias com provas de conceito para mostrar seu comportamento. Finalmente, para apresentar o processo completo, particularmente a etapa de apresentação, três exemplos de sensores virtuais também são apresentados.",DISSERTAÇÃO,On the design of virtual sensors using vehicular sensor data,5047786,1
"Sentiment analysis has become a very important tool for analysis of social media data. Despite the large number of existent methods developed, there is no single one which fits well in all cases and data sources. In this work, we propose to combine several very popular and effective state-of-the-practice sentiment analysis methods, by means of an unsupervised bootstrapped strategy for classification of polarity. Our solution was thoroughly tested considering thirteen datasets in several domains. The experimental results demonstrate that our combined method (aka 10SENT) improves the effectiveness of the classification task, but more important, it solves a key problem in the field. Our self-learning approach is also very independent of the base methods, which means that it is highly extensible to incorporate any new additional method. Finally, we investigate a transfer learning and active learning approach for sentiment analysis and show the potential of this technique to final results.",,CIENCIA DA COMPUTACAO,PHILIPE DE FREITAS MELO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,26/06/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Anlise de Sentimento;Aprendizado de Mquina;Aprendizado No-supervisionado;Processamento de Linguagem Natural',-,FABRICIO BENEVENUTO DE SOUZA,0,Análise de Sentimento;Aprendizado de Máquina;Aprendizado Não-supervisionado;Processamento de Linguagem Natural,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A análise de sentimentos se tornou uma ferramenta muito importante para análise de dados de mídia social. Existem vários métodos desenvolvidos para este campo de pesquisa, vários deles trabalhando muito diferentes uns dos outros, cobrindo aspectos distintos do problema e estratégias diversas. Apesar do grande número de técnicas existentes, não existe uma única que se encaixe bem em todos os casos e origem de dados. Além disso, no caso de abordagens supervisionadas, pode ser muito difícil obter dados rotulados para estratégias que exigem treinamento, principalmente para novas aplicações. Neste trabalho, propomos combinar vários métodos populares de análise de sentimento do atual estado-da-arte e eficazes, por meio de uma estratégia não-supervisionada com uso de bootstrapping para classificação de polaridade. Nossa solução foi completamente testada considerando treze diferente conjuntos de dados em vários domínios, como opiniões de produtos, comentários e mídias sociais. Os resultados experimentais demonstram que o nosso método combinado (conhecido como 10SENT) melhora a eficácia da tarefa de classificação, mas mais importante, ele resolve um problema-chave no campo. Nosso método aparece consistentemente entre os melhores métodos em vários tipos de bases de dados, o que significa que ele pode produzir os melhores resultados (ou perto de melhor) em quase todos os contextos considerados, sem quaisquer custos adicionais. A nossa abordagem de auto-aprendizagem é também muito independente dos métodos base, o que significa que é altamente extensível incorporar qualquer novo método adicional que possa ser desenvolvido no futuro. Finalmente, investigamos duas abordagens de transfer learning e active learning para a análise de sentimento e mostramos o potencial dessas técnicas para melhorar nossos resultados.",DISSERTAÇÃO,An Unsupervised Approach Based On Self-learning For The Combination Of Sentiment Analysis Methods,5082002,1
"Face Recognition is one of the most relevant problems in computer vision as we consider its importance to areas such as surveillance, forensics and psychology. In fact, a real-world recognition system has to cope with several unseen individuals and determine either if a given face image is associated with a subject registered in a gallery of known individuals or if two given faces represent equivalent identities. In this work, not only we combine hashing functions, embedding of classifiers and response value histograms to estimate when probe samples belong to the gallery set, but we also extract relational features to model the relation between pair of faces to determine whether they are from the same person. Results show that our method continues effective for both open-set face identification and verification tasks regardless of the dataset difficulty.",,CIENCIA DA COMPUTACAO,RAFAEL HENRIQUE VARETO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,16/10/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Viso computacional;Vigilncia;Reconhecimento de face;Identificao;Verificao',-,WILLIAM ROBSON SCHWARTZ,0,Visão computacional;Vigilância;Reconhecimento de face;Identificação;Verificação,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O reconhecimento de faces é um dos problemas mais relevantes em visão computacional quando consideramos sua importância em áreas como vigilância, ciência forense e psicologia. De fato, um sistema de reconhecimento que representa o mundo real deve lidar com vários indivíduos desconhecidos e determinar se uma dada imagem está associada a um sujeito registrado em uma galeria de indivíduos conhecidos ou se dois rostos representam identidades equivalentes. Neste trabalho, não só combinamos funções de indexação, coleção de classificadores e histogramas para estimar quando imagens faciais pertencem à galeria, mas também modelamos a relação entre pares de faces para determinar se elas são da mesma pessoa. Os resultados mostram que o nosso método continua eficiente tanto na verificação e identificação de galeria aberta, independentemente da dificuldade dos datasets.",DISSERTAÇÃO,Face Recognition based on a Collection of Binary Classifiers,5096140,1
"Environmental boundaries, such as the borderline of a forest fire or an oil spill, pose a significant threat for living organisms. Anticipating the dynamics of these phenomena is a potentially life-saving indicator to support efficient and effective evacuations or to dispel the hazard. We propose a decentralized coordination method that allows multiple robots to efficiently sample and predict the behavior of environmental boundaries. Our method does not require a priori information about the boundary dynamics. We validate our proposal through experiments with actual robots. We show how the accuracy of our estimation method is affected by different factors. We demonstrate experimentally that our method can estimate and predict non-convex boundaries even with noisy measurements and inaccurate actuators.",DavidJulianSaldanha.pdf,CIENCIA DA COMPUTACAO,DAVID JULIAN SALDANA SANTACRUZ,UNIVERSIDADE FEDERAL DE MINAS GERAIS,14/07/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Sistemas Multi-Rob;Estimao e Predio de Contornos;Monitoramento de Ambientes',-,MARIO FERNANDO MONTENEGRO CAMPOS,0,Sistemas Multi-Robô;Estimação e Predição de Contornos;Monitoramento de Ambientes,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Environmental boundaries, such as the borderline of a forest fire or an oil spill, pose a significant threat for living organisms. Anticipating the dynamics of these phenomena is a potentially life-saving indicator to support efficient and effective evacuations or to dispel the hazard. We propose a decentralized coordination method that allows multiple robots to efficiently sample and predict the behavior of environmental boundaries. Our method does not require a priori information about the boundary dynamics. We validate our proposal through experiments with actual robots. We show how the accuracy of our estimation method is affected by different factors. We demonstrate experimentally that our method can estimate and predict non-convex boundaries even with noisy measurements and inaccurate actuators.",TESE,Detecting and predicting environmental boundaries with a team of robots,5382873,1
"Classificação automática de dados é uma tarefa chave para resolver,eficazmente,diversos problemas práticos, sendo de grande benefício para indústrias e sociedade. Florestas Aleatórias (RF) é um dos classificadores mais usados, dado sua alta acurácia. A despeito de diversas aplicações bem sucedidas,tal classificador não é imune a desafios: quando aplicado a dados de alta dimensionalidade e ruidosos,RF pode se sobreajustar,limitando sua capacidade de generalização. Propomos duas soluções para mitigar tal problema: LazyNN_RF e BROOF. Baseados em RFs,ambos focam em sub-regiões específicas do espaço de entrada, objetivando filtrar dados irrelevantes ao construir a regra de predição,minimizando o sobreajuste dos mesmos. Considerando o domínio de classificação automática de texto, representado por 16 coleções reais, ambas as técnicas propostas obtiveram o notável resultado de serem os que mais figuraram entre os melhores classificadores, dentre 14 classificadores estado-da-arte de referência.",,CIENCIA DA COMPUTACAO,THIAGO CUNHA DE MOURA SALLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,21/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Avaliao Experimental;Boosting;Classificao;Classificadores Postergados;Random Forests',-,MARCOS ANDRE GONCALVES,0,Avaliação Experimental;Boosting;Classificação;Classificadores Postergados;Random Forests,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Classificação automática de dados é uma tarefa chave para resolver,eficazmente,diversos problemas práticos, sendo de grande benefício para indústrias e sociedade. Florestas Aleatórias (RF) é um dos classificadores mais usados, dado sua alta acurácia. A despeito de diversas aplicações bem sucedidas,tal classificador não é imune a desafios: quando aplicado a dados de alta dimensionalidade e ruidosos,RF pode se sobreajustar,limitando sua capacidade de generalização. Propomos duas soluções para mitigar tal problema: LazyNN_RF e BROOF. Baseados em RFs,ambos focam em sub-regiões específicas do espaço de entrada, objetivando filtrar dados irrelevantes ao construir a regra de predição,minimizando o sobreajuste dos mesmos. Considerando o domínio de classificação automática de texto, representado por 16 coleções reais, ambas as técnicas propostas obtiveram o notável resultado de serem os que mais figuraram entre os melhores classificadores, dentre 14 classificadores estado-da-arte de referência.",TESE,Random forest based classifiers for high dimensional noisy data classification,5388073,1
"JavaScript is the most popular programming language for the Web. Although the language is prototype-based, developers can emulate class-based abstractions to master the increasing complexity of their applications. Identifying classes in legacy JavaScript code can support these developers in understanding and maintaining their code. In this thesis, we present a strategy to detect class-based abstractions in the source code of legacy JavaScript systems. We found evidence that structures emulating classes are present in 74% of the studied legacy systems. We also demonstrate how to use a static type-checker to infer types that correspond to class references. We perform a study to identify coupling information between class-like structures in which we achieve precision of 100%, and the values of recall reach 80% for dependencies in general and range from 85% to 96% for associations. Finally, we present a set of reasons that can lead developers to postpone/reject the adoption of ES6 classes.",Leonardo Humberto silva.pdf,CIENCIA DA COMPUTACAO,LEONARDO HUMBERTO GUIMARAES SILVA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,15/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Compreenso de programas;Engenharia reversa;JavaScript',-,MARCO TULIO DE OLIVEIRA VALENTE,0,Compreensão de programas;Engenharia reversa;JavaScript,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"JavaScript is the most popular programming language for the Web. Although the language is prototype-based, developers can emulate class-based abstractions to master the increasing complexity of their applications. Identifying classes in legacy JavaScript code can support these developers in understanding and maintaining their code. In this thesis, we present a strategy to detect class-based abstractions in the source code of legacy JavaScript systems. We found evidence that structures emulating classes are present in 74% of the studied legacy systems. We also demonstrate how to use a static type-checker to infer types that correspond to class references. We perform a study to identify coupling information between class-like structures in which we achieve precision of 100%, and the values of recall reach 80% for dependencies in general and range from 85% to 96% for associations. Finally, we present a set of reasons that can lead developers to postpone/reject the adoption of ES6 classes.",TESE,Identifying classes in legacy javascript code,5391389,1
"Two robust optimization NP-Hard problems are studied in this thesis: the min-max regret WSCP and the min-max regret MCLP. The uncertain data in these problems is modeled by intervals and only the minimum and maximum values for each interval are known. While the min-max regret WSCP is still a theoretical problem, the min-max regret MCLP has an application in disaster logistics which is investigated in this thesis. Four mathematical formulations, three exact algorithms and five heuristics were developed and applied to both problems. Computational experiments showed that the exact algorithms efficiently solved 14 out of 75 instances generated to the min-max regret WSCP and all realistic instances created to the min-max regret MCLP. For the simulated instances that was not solved to optimally in both problems, the heuristics developed in this thesis found solutions, as good as, or better than the best exact algorithm in almost all instances.",,CIENCIA DA COMPUTACAO,AMADEU ALMEIDA COCO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,06/10/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Combinatorial optimization;Meta-heuristics;Algorithms;Operations research;Logistics',-,THIAGO FERREIRA DE NORONHA,0,Combinatorial optimization;Meta-heuristics;Algorithms;Operations research;Logistics,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Dois problemas NP-Difíceis de otimização robusta foram estudados nesta tese: o Problema min-max regret de Cobertura de Conjuntos Ponderado  (min-max regret WSCP, do inglês min-max regret  Weighted Set Covering Problem) e o Problema min-max regret de Cobertura e Localização Maximal (min-max regret  MCLP, do inglês min-max regret Maximal Coverage Location Problem). O min-max regret WSCP e o min-max regret MCLP são respectivamente, versões de otimização robusta do Problema de Cobertura de Conjuntos Ponderado e do Problema de Cobertura e Localização Maximal. Em ambos os problemas, o conjunto de parâmetros incertos é modelado por intervalos onde apenas os valores mínimo e máximo são conhecidos. Além disso, uma aplicação do min-max regret MCLP em logística pós-desastres é investigada nesta tese e, para esta aplicação, dois outros critérios de otimização robusta foram derivados a partir do min-max MCLP: o max-max MCLP e o max-min MCLP. Em termos de métodos, quatro formulações matemáticas, três algoritmos exatos e cinco heurísticas foram desenvolvidos e aplicados para ambos os problemas. Experimentos computacionais mostraram que os algoritmos exatos resolveram 14 de 75 instancias geradas para o min-max regret  WSCP e todas as instâncias realísticas criadas para o min-max regret  MCLP. Além disso, em quase todas as instâncias que não foram resolvidas na otimialidade, as heurísticas propostas nesta tese encontraram soluções tão boas quanto ou melhores que aquelas retornadas por meio dos algoritmos exatos. Quanto à aplicação na logística pós-desastres, os três modelos de otimização robusta (max-max MCLP, max-min MCLP and min-max regret  MCLP) encontraram soluções similares para os cenários realísticos gerados a partir dos dados dos terremotos que atingiram Catmandu, Nepal em 2015.",TESE,"Robust covering problems: formulations, algorithms and application",5391479,1
"P2P live streaming systems, and their algorithms for constructing and maintaining the network overlay, often face issues of high playback latency and buffer underflows. In particular, as peers establish more partnerships to increase opportunity for exchanging media, the overhead of control messages increases and sophisticated neighbourhood filtering techniques are required to maintain media distribution efficiency. We present the Partnership Class Constraint Algorithm (PCCA), which constructs and maintains the network overlay with low complexity. The algorithm groups peers into classes according to their contributions to media upload in the overlay. PCCA enforces restrictions on the number of partnerships peers can establish and on which groups can establish partnerships, pulling cooperative peers close to the source and pushing uncooperative peers to the edge of the overlay. Our experiments show that PCCA can sustain 50% of uncooperative peers without disruption to media distribution.",EliseuMiguel.pdf,CIENCIA DA COMPUTACAO,ELISEU CESAR MIGUEL,UNIVERSIDADE FEDERAL DE MINAS GERAIS,17/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'P2P;Peer-to-Peer Mesh Overlay;Flash Crowd;Free Rider;Network',-,SERGIO VALE AGUIAR CAMPOS,0,P2P;Peer-to-Peer Mesh Overlay;Flash Crowd;Free Rider;Network,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A transmissão de vídeo já representa o maior tráfego na Internet. Este tipo de transmissão depende de distribuição em grande escala em redes de distribuição de conteúdo (CDNs), que incorrem em custos significativos em sua construção e uso. A distribuição de conteúdo de vídeo por meio de sistemas par-a-par, por outro lado, reduz a dependência e o custo de CDNs. Nas redes par-a-par, o conteúdo é compartilhado em uma sobreposição topológica na rede física. Isso é fundamental para o desempenho da rede. Infelizmente, a distribuição par-a-par é repleta de problemas QoE. Por exemplo, a chegada simultânea de um grande número de pares, conhecida como flash crowd, pode afetar a topologia da rede e interromper a transmissão de conteúdo. Além disso, em cenários em que os usuários têm uma largura de banda de contribuição limitada, os sistemas par-a-par precisam de mecanismos importantes para o incentivo de contribuição de mídia por pares. Os algoritmos atuais de transmissão em sistemas par-a-par em tempo real que constroem e mantêm a topologia de rede geralmente enfrentam problemas de latência na reprodução e descontinuidade de mídia. Quando os pares alcançam um número maior de parceiros, tanto o aumento de mensagem de controle quanto as técnicas sofisticadas de filtragem de vizinhanças de pares são necessárias para fornecer a mídia sem interrompção de fluxo. Os problemas são particularmente desafiandores quando a porcentagem de free riders na rede é alta, o que é um caso frequentemente. Para lidar com esses desafios e ao mesmo tempo mitigar os efeitos negativos dos free riders, apresentamos o Algoritmo de Restrição de Classe de Parceria (PCCA) que constrói e mantém a topologia de rede com foca em parâmetros simples de configuração de redes par-a-par. O algoritmo estabelece o conceito de classes de pares em que os pares são agrupados pela contribuição da mídia para a rede. Essas contribuições são usadas para estabelecer critérios de parceria entre as classes, o que chamamos de Restrições de Parceria do Peer (PPC). Cada uma dessas classes configura seus pares com um número limitado de parceiros externos para permitir que os pares enviem pacotes em ordem de chegada das solicitações e evitar, com isso, qualquer técnica sofisticada de filtragem de vizinhança, o que reduz significativamente a complexidade do sistema. Além disso, as restrições nas classes de pares impedem a competição de parcerias entre free riders e pares cooperativos. Esta falta de concorrência de parceria é fundamental para: (i) facilitar e acelerar o processo de novos ingresso de novos pares na sobreposição; e (ii) ajudar o PCCA a diminuir a latência da reprodução e a descontinuidade da mídia, aproximando a classe dos pares que mais contribuem ao servidor de mídia enquanto empurra os free riders para a borda da rede. Nossos experimentos mostram que o PCCA garante que a rede possa sustentar 50% dos cavaleiros livres sem interferir nas parcerias de pares cooperativas da sobreposição. Encontramos uma maneira simples de construir e manter as topologias de sobreposição mais robustas para suportar uma grande quantidade de free riders com base em parâmetros de configuração de sistemas peer-to-peer. Nossas estratégias exigem baixa complexidade de implementação e, além disso, incorrem em baixo acréscimo de sobrecarga de mensagens. Mais importante, uma vez que estamos baseando nossa solução no comportamento de parâmetros de configuração das redes par-a-par, acreditamos que nossa solução pode ser combinada com abordagens par-a-par atuais que lidam com flash crowd e free riders para alcançar soluções menos complexas e mais eficientes.",TESE,Overlay Construction Strategies for Peer-to-Peer Live Streaming Systems,5391858,1
"Cloud storage is a data-intensive Internet service that synchronizes files from user devices, such as PCs and smartphones, with the cloud, offering means for users to easily backup data and perform collaborative work. This service has attracted a large interest from end users, industry and academia. Dropbox, which is one of leading companies in this market, report having more than 500 million registered users with 1 billion files upload daily. Despite the large popularity of cloud storage, little is known about the implications of typical user behavior patterns and policies adopted by popular services to the costs and benefits for end users and service providers. This thesis aims at investigating such cost-benefit tradeoffs for both parts jointly. Using data collected from Dropbox in various networks, we have developed new service policies, analytic models, and workload analysis tools to support providers and users in improving jointly their benefits in this kind of service.",,CIENCIA DA COMPUTACAO,GLAUBER DIAS GONCALVES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,18/12/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Cloud Storage Service;User Behavior;Cost-benefit analysis;CacheUtility;Dropbox',-,JUSSARA MARQUES DE ALMEIDA GONCALVES,0,Dropbox;Armazenamento em Nuvem - Comportamento de Usuário - Análises de Custo e Benefício - Cache;Utilidade,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O armazenamento em nuvem é um serviço de Internet de uso intensivo de dados que oferece aos usuários finais facilidades para backup de dados e trabalhos colaborativos, sendo que os arquivos em dispositivos dos usuários são sincronizados automaticamente com a nuvem. Este serviço já é um dos mais populares na Internet, e provedores bem conhecidos, como Dropbox, Google e a Microsoft, enfrentam uma forte concorrência por clientes. Apesar da popularidade de serviços armazenamento na nuvem, pouco se sabe sobre as implicações dos padrões de comportamento dos usuários e das políticas adotadas por serviços populares nos custos e benefícios dos próprios usuários assim como provedores de serviço. Nossa pesquisa tem como objetivo investigar os compromissos entre esses custos e benefícios para ambas as partes conjuntamente. Em particular, propomos modelos analíticos para ajudar os provedores a avaliar a eficácia de políticas alternativas que visem aumentar a rentabilidade e a satisfação dos usuários. Nesse sentido, essa pesquisa é organizada em três objetivos complementares: (1) caracterização do comportamento típico do usuário e modelagem dos padrões de carga de trabalho derivados desse comportamento em serviços de armazenamento em nuvem; (2) investigação do impacto do compartilhamento de conteúdo nos custos dos provedores de serviço e no nível de atividade do usuário; e (3) modelagem dos compromissos entre custos e benefícios de usuários finais e do provedor do serviço de armazenamento na nuvem. Esses objetivos foram investigados utilizando dados derivados de tráfego de rede do Dropbox, que atualmente é um dos serviços de armazenamento em nuvem mais populares. Usando dados coletados passivamente de quatro redes diferentes, foram obtidas as seguintes contribuições relacionadas a cada objetivo de pesquisa: (1) um modelo de comportamento de usuário hierárquico e um gerador de carga de trabalho sintético com base nesse modelo; (2) a avaliação do impacto de compartilhamentos no custo do serviço através de downloads que podem ser evitáveis, arquiteturas de sincronização alternativas e características sociais que quantificam o nível de atividade do usuário; e (3) modelos analíticos, especificamente funções de utilidade, para analisar custos e benefícios de novas políticas do serviço em vários cenários, identificando se e quando uma solução que atende aos interesses do provedor e usuários pode ser alcançada.",TESE,On the Cost-Benefit Tradeoffs of Cloud Storage Services for End Users and Service Providers,5393134,1
"Despite numerous efforts that explore demographic aspects in social media, it is still unclear whether social media perpetuates old inequalities from the offline world. In this dissertation, we attempt to identify gender and race of Twitter users located in U.S. using advanced image processing algorithms from Face++. We investigate how different demographic groups connect with each other and differentiate between them regarding linguistic styles and also their interests. Furthermore, we extract the absolute ranking difference of top phrases between demographic groups. Our analysis shows that white and male users tend to attain higher positions in Twitter. There are differences in the way of writing across different demographic as well as in the topic of interest. We hope our effort can stimulate the development of new theories of demographic information in the online space. We developed and deployed the Who Makes Trends? Web-based service available at https://goo.gl/oon5ot",Johnnatan Messias.pdf,CIENCIA DA COMPUTACAO,JOHNNATAN MESSIAS PEIXOTO AFONSO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,09/06/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Demographic;Gender and Race;Inequality;Linguistic Patterns;Twitter',-,FABRICIO BENEVENUTO DE SOUZA,0,Demographic;Gender and Race;Inequality;Linguistic Patterns;Twitter,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Social media is considered a democratic space in which people connect and interact with each other regardless of their gender, race, or any other demographic aspect. Despite numerous efforts that explore demographic aspects in social media, it is still unclear whether social media perpetuates old inequalities from the offline world. In this dissertation, we attempt to identify gender and race of Twitter users located in U.S. using advanced image processing algorithms from Face++. We investigate how different demographic groups (i.e. male/female, asian/black/white) connect with each other and differentiate between them regarding linguistic styles and also their interests. We quantify to what extent one group follow and interact with each other and the extent to which these connections and interactions reflect in inequalities in Twitter. We also extract linguistic features from 6 categories (affective attributes, cognitive attributes, lexical density and awareness, temporal references, social and personal concerns, and interpersonal focus) in order to identify the similarities and differences in the messages they share in Twitter. Furthermore, we extract the absolute ranking difference of top phrases between demographic groups. As a dimension of diversity, we also use the topics of interest that we retrieve from each user. Our analysis shows that users identified as white and male tend to attain higher positions in Twitter, in terms of the number of followers and number of times in other user's lists. There are clear differences in the way of writing across different demographic groups in both gender and race domains as well as in the topic of interest. We hope our effort can stimulate the development of new theories of demographic information in the online space. Therefore, we developed and deployed the Who Makes Trends? Web-based service available at http://twitter-app.mpi-sws.org/who-makes-trends/.",DISSERTAÇÃO,Characterizing Interconnections  and Linguistic Patterns in Twitter,5590632,1
"In this dissertation, we evaluate thoroughly the impact and potential of deep learning alternative representations as a way to improve results in Learning to Rank tasks. We use stacked autoencoders to create hundreds of alternative representations of several databases, and evaluate those under the prism of their impact in the performance of several traditional Learning to Rank algorithms. In other words, we ask how easy or hard it is to improve the representation of Learning to Rank data using autoencoders, and how better can these representations be. We use the autoencoder to uniformily walk the domain of possible representations, so to know how useful can the autoencoder be for such task as well. We see in our analysis that it is possible, although difficult, to improve the databases representations in a relevant way, obtaining results that surpass state-of-the-art performances of the traditional ranking algorithms. We see, as well, that there are autoencoder hyperparameter sets that tend to generate better results.",,CIENCIA DA COMPUTACAO,ALBERTO DE SA CAVALCANTI DE ALBUQUERQUE,UNIVERSIDADE FEDERAL DE MINAS GERAIS,23/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Aprendizado de Mquina;Aprendizado de Ordenao de Documentos;Recuperao de Informao;Representaes',-,RENATO ANTONIO CELSO FERREIRA,0,Aprendizado de Máquina;Aprendizado de Ordenação de Documentos;Recuperação de Informação;Representações,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Neste trabalho, nós avaliamos de forma abrangente o impacto e potencial que o aprendizado profundo de representações alternativas tem enquanto aprimorador de resultados em tarefas de ordenação de documentos. Nós utilizamos autoencoders empilhados para criar um conjunto de centenas de representações alternativas de diversas bases de dados, e as avaliamos sob a ótica do desempenho de diversos algoritmos tradicionais de aprendizado de ordenação de documentos. Em outras palavras, procuramos saber o quão fácil ou dificíl é aprimorar a representação dos dados, usando autoencoders, que esses algoritmos utilizam em suas tarefas de aprendizado, e o quão melhores tais representações podem ser. Utilizamos o autoencoder para percorrer o domínio de representações possíveis de forma uniforme, de modo a, também, procurar entender o quão útil o autoencoder é para tal tarefa. Vemos em nossas análises que é possível, embora difícil, aprimorar a representação dos dados de forma relevante, obtendo resultados superiores ao estado da arte nas tarefas de ordenação de documentos. Vemos também que há conjuntos de hiperparâmetros do autoencoder que tendem a gerar resultados melhores.",DISSERTAÇÃO,Recodificação de atributos para learning to rank usando autoencoders,5591004,1
"Sentiment analysis has become a key tool for several social media applications. Despite the significant interest in this theme and amount of research efforts in the field, almost all existing methods are designed to work with only English content. So, we provide an extensive quantitative analysis of existing multi-language approaches against state-of-the-art English methods with the help of machine translation tools in fourteen different human labeled datasets in various idioms. Our results suggest that simply translating the input text on a specific language to English and then using one of the existing best methods developed to English can be better than the existing language specific efforts evaluated. As a final contribution to the research community, we release the iFeel 3.0 system, a web framework for multilingual sentence-level sentiment analysis. We hope our system setups a new baseline for future sentence-level methods developed in a wide set of languages.",,CIENCIA DA COMPUTACAO,MATHEUS LIMA DINIZ ARAUJO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,26/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Anlise de Sentimentos;Minerao de Opinio;Multilnge;Redes Sociais Online;Traduo Automtica',-,FABRICIO BENEVENUTO DE SOUZA,0,Análise de Sentimentos;Mineração de Opinião;Multilíngüe;Redes Sociais Online;Tradução Automática,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A análise de sentimentos se tornou uma ferramenta chave em aplicações voltadas para mídias sociais, isto inclui a classificação da opinião dos usuários sobre produtos e serviços, o monitoramento político das mídias durante campanhas eleitorais e até mesmo a influência no mercado de ações. Existem diferentes ferramentas para análise de sentimentos que exploram variadas técnicas, normalmente elas dependem de dicionários léxicos ou aprendizado de máquina. Apesar do significante interesse neste tema e o grande esforço investido nesta área pela comunidade científica, quase todos os métodos existentes para análise de sentimentos foram direcionados para o contexto da língua inglesa. A maioria das estratégias para análise em diferentes línguas consiste na adaptação de um léxico já existente em inglês, sem apresentar validações ou comparações com linhas de base. Neste trabalho, realizamos uma abordagem diferente para resolver o problema de análise de sentimentos em diferentes línguas. Para isto, avaliamos dezesseis métodos voltados à análise de sentimentos em sentenças criadas para o inglês e os comparamos com três abordagens geradas para línguas específicas. A partir de quatorze conjuntos de dados em diferentes línguas, rotulados por humanos, realizamos uma extensa análise quantitativa das abordagens criadas para múltiplos idiomas. Nossos resultados sugerem que a simples tradução automática do texto de entrada da língua específica para o inglês e em seguida a utilização dos métodos estado da arte criados para o inglês pode ser melhor que os métodos existentes desenvolvidos para uma língua específica. Nós também classificamos os métodos de acordo com sua capacidade de predição e identificamos aqueles métodos que alcançam os melhores resultados utilizando a tradução automática entre as diferentes línguas. Como contribuição final para a comunidade acadêmica, compartilhamos os códigos, conjuntos de dados e o sistema iFeel 3.0, um arcabouço para análise de sentimentos em sentenças para múltiplas línguas. Esperamos que nossa metodologia se torne uma linha de base para o desenvolvimento de novos métodos de análise de sentimentos ao nível de sentenças em múltiplas línguas.",DISSERTAÇÃO,A comparative study of machine translation for multilingual sentence-level sentiment analysis,5591337,1
"The Next Release Problem (NRP) is an optimization problem of the requirements specification area, studied in the research area of Search Based Software Engineering (SBSE). The SBSE has been mainly applied in cost estimation, requirements, testing, debugging, management and design problems. The complex formulation of NRP makes it an NP-Hard problem, being used heuristics and metaheuristics to solve it efficiently. The decision of a release is important during the software development for meeting deadlines and budget. Although it is a widely studied problem in traditional software development, there are few studies when it is applied to contexts where is used agile development methodologies such as Scrum. This paper aims to address the problem from this agile context by reviewing existing models and heuristics and proposing a new formulation of the problem, so it better represents the reality of software development of agile teams. An algorithm was developed with support of experiments to match the proposed approach, presenting mixed results. An analysis of integration between models and techniques of other management problems of Software Engineering, such as Allocation of Teams and Cost Estimation, was performed to minimize possible disadvantagens of this approach.",Ivan Italo.pdf,CIENCIA DA COMPUTACAO,IVAN ITALO ITUASSU,UNIVERSIDADE FEDERAL DE MINAS GERAIS,29/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Metaheursticas;Problema do Prximo Release;Scrum;Search Based Software Engineering',-,GERALDO ROBSON MATEUS,0,Metaheurísticas;Problema do Próximo Release;Scrum;Search Based Software Engineering,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O Problema do Próximo Release (NRP) é um problema de otimização da área de especificação de requisitos, estudada na linha de pesquisa da Search Based Software Engineering (SBSE). Trata-se de um problema NP-Difícil, sendo usado heurísticas e metaheurísticas para solucioná-lo eficientemente. Embora seja um problema amplamente estudado no desenvolvimento tradicional de software, há poucos estudos quando este é aplicado à contextos onde se utilizam metodologias ágeis de desenvolvimento, como o Scrum. Este trabalho tem como objetivo abordar o problema partindo deste contexto ágil, analisando modelos e heurísticas existentes e propondo uma heurística e uma nova formulação para o problema, de forma que esta represente melhor a realidade de desenvolvimento de software de equipes ágeis.",DISSERTAÇÃO,Problema do próximo release integrado à gerência de projetos através de metodologias ágeis,5591438,1
.,Vagner Clementino Santos.pdf,CIENCIA DA COMPUTACAO,VAGNER CLEMENTINO DOS SANTOS,UNIVERSIDADE FEDERAL DE MINAS GERAIS,13/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Engenharia de Software;FGRM;Manuteno de Software;Melhorias',-,RODOLFO SERGIO FERREIRA DE RESENDE,0,Engenharia de Software;FGRM;Manutenção de Software;Melhorias,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"As mudanças de software são importantes e consomem boa parte dos recursos alocados ao longo do ciclo de vida de um software. Uma Requisição de Mudança (RM) descreve desde solicitações de correção de falhas até pedidos de melhoria de um software. O volume de RMs é, em geral, muito alto e exige uma ferramenta de gerência, que denominamos FGRM. Nesta defesa de dissertação apresentamos nosso trabalho que teve como objetivo a melhoria das versões atuais das FGRMs mais populares.
Nossa proposta é baseada na literatura científica e em levantamentos realizados junto à profissionais de desenvolvimento de software. Além da proposta de um conjunto de sugestões de melhorias apresentamos também a implementação de uma das sugestões na forma de uma extensão da plataforma Github.",DISSERTAÇÃO,Um estudo de ferramentas de gerenciamento de requisição de mudança,5591578,1
"Emotion recognition from speech is one of the key steps towards emotion intelligence in advanced human-machine interaction. It requires learning features that are robust and discriminative across diverse domains that differ in terms of language, spontaneity of speech, recording conditions, and types of emotion. In addition, it also presents some difficulties like learning scenario in which the features and labels may change substantially across domains beyond few labeled data for each domain. In this work we apply deep learning for learning transferable features to enable model adaptation from multiple source domains, given the sparseness of speech emotion data. As results, deep learning approach in scenarios with different domains is better than feature-engineering approach, mainly in datasets with natural speech.",,CIENCIA DA COMPUTACAO,ALISON DE OLIVEIRA MARCZEWSKI,UNIVERSIDADE FEDERAL DE MINAS GERAIS,17/07/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Adaptao de Domnio;Aprendizado Profundo;Computao Afetiva;Reconhecimento de Emoo',-,ADRIANO ALONSO VELOSO,0,Adaptação de Domínio;Aprendizado Profundo;Computação Afetiva;Reconhecimento de Emoção,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Reconhecimento de emoção através da fala é um ponto chave na direção da inteligência emocional em interações homem-máquina avançadas. Identificar emoções na fala humana requer aprender descritores que sejam robustos e discriminativos entre os mais diversos domínios, estes que se distinguem em termos de idioma, espotaneidade da fala, condições de gravação do áudio, além dos tipos de emoções expressas. Isso descreve um cenário de aprendizado que as distribuições disjuntas de descritores e labels estão sujeitas a divergências substanciais entre domínios. Neste trabalho, propomos uma arquitetura profunda que explora em conjunto uma rede convolucional para extração de descritores compartilhados entre domínios e uma rede LSTM para classificar emoções usando descritores específicos de domínio. Utilizamos descritores genéricos para permitir a adaptação de modelos a partir de vários domínios de origem, dada a espacidade de dados de fala e o fato de que os domínios alvo apresentam poucos dados rotulados. Um extenso experimento entre os datasets nos mais variados domínios revela que descritores genéricos proveem ganhos entre 4.3% e 78.6% no reconhecimento de emoção em fala. Nós avaliamos várias abordagens para adaptação de um domínio em outro, além disso, realizamos um estudo de ablação para entender quais domínios de origem mais contribuem para a efetividade geral no reconhecimento de emoção para um domínio alvo. Como análise complementar para entender a diferença na efetividade entre domínios e emoções, nós analisamos a divergência entre eles para entender melhor as razões pelas quais o processo de adaptação ao domínio alvo não é efetivo quando alguns outros domínios estão na base de dados fonte.",DISSERTAÇÃO,Learning transferable features from multiple source domains for speech emotion recognition,5591702,1
"Motivated by the Random Forest (RF) success, recently proposed RF-based classification approaches leverage the central RF idea of aggregating a large set of low-correlated trees. In this context, this work brings many new contributions to this line of research. We propose a new RF-based strategy (BERT) that applies the boosting in bags of extremely randomized trees. We empirically demonstrate that this new strategy, as well as the recently proposed BROOF and LazyNN_RF classifiers do complement each other, motivating us to stack them to produce an even more effective classifier. Finally, we exploit stacking strategy based on out-of-bag samples to considerably speedup the very costly training process. Our experiments in several datasets covering two high-dimensional and noisy domains provide strong evidence in favor of the benefits of our RF-based solutions. We show that BERT is among the top performers in the vast majority of analyzed cases, while retaining the unique benefits of RF.",Raphael Rodrigues Campos.pdf,CIENCIA DA COMPUTACAO,RAPHAEL RODRIGUES CAMPOS,UNIVERSIDADE FEDERAL DE MINAS GERAIS,21/07/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Aprendizado de Mquina;Classificao;Empilhamento;Ensemble;Floresta Aleatria',-,MARCOS ANDRE GONCALVES,0,Aprendizado de Máquina;Classificação;Empilhamento;Ensemble;Floresta Aleatória,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Floresta Aleatória (FA) é uma das estratégias mais bem-sucedidas para tarefas de classificação automática. Motivado por seu grande sucesso, recém-propostos métodos baseados em FA têm alavancado a ideia central da RF de agregar um grande conjunto de árvores de decisão com baixa correlação, que é inerentemente paralelizável e provê capacidade excepcional de generalização. Nesse contexto, esse trabalho provê várias novas contribuições para essa linha de pesquisa. Primeiramente, nós propomos uma nova estratégia baseada em FA (BERT) que aplica a técnica de \textit{boosting} em árvores extremamente aleatórias com \textit{bagging}. Segundo, nós demonstramos empiricamente que essa nova estratégia, assim como os recém-propostos classificadores BROOF e LazyNN\_RF complementam uns aos outros, motivando-nos a empilhá-los a fim de produzir um método ainda mais eficaz. Até onde sabemos, esse é a primeira estratégia que efetivamente combina as três principais estratégias de comitê de classificadores: empilhamento, \textit{bagging} (a base da FA) e \textit{boosting}. Por último, nós exploramos as instâncias \textit{out-of-bag (OOB)} para empilhar, eficientemente e sem viés, métodos baseados em \textit{bagging}, desse modo diminuindo consideravelmente o custoso processo de treino do procedimento de empilhamento. Nossos experimentos cobrindo dois domínios ruidosos e com alta dimensionalidade - classificação de tópicos e sentimentos - provê forte evidência em favor dos benefícios de nossas soluções baseadas em FA. Nós mostramos que o BERT está dentre os classificadores de mais alta efetividade na vasta maioria dos casos analisados, mantendo os benefícios únicos da FA (interpretabilidade, paralelização, fácil parametrização, capacidade de lidar com dados heterogêneos e valores faltantes).",DISSERTAÇÃO,Stacking bagged and boosted forests for classification of noisy and high-dimensional data,5591924,1
"MOBA games are currently one the most popular online game genres. In their basic gameplay, two teams of multiple players compete against each other to destroy the enemy's base, controlling a powerful unit known as 'hero'. Each hero has different abilities, roles and strengths. Thus, choosing a good combination of heroes is fundamental for the success in the game. In this dissertation we propose a recommendation system for selecting heroes in a MOBA game. We develop a mechanism based on association rules that suggests the more suitable heroes for composing a team, using data collected from a large number of Dota 2 matches. For evaluating the efficacy of the line-up, we trained a neural network capable of predicting the winner team with a 92% accuracy. The results of the recommendation system were very satisfactory with up to 76.4% success rate.",,CIENCIA DA COMPUTACAO,LUCAS AUGUSTO FERREIRA HANKE,UNIVERSIDADE FEDERAL DE MINAS GERAIS,31/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'pedir ao aluno',-,LUIZ CHAIMOWICZ,0,pedir ao aluno,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Jogos Multiplayer Online Battle Arena (MOBA) sao atualmente um dos mais populares generos de jogos online. Na sua jogabilidade basica, dois times de multiplos jogadores competem entre si a fim de destruir a base inimiga, controlando uma unidade poderosa denominada 'heroi'. Cada heroi tem diferentes habilidades, papeis e forcas. Consequentemente, escolher uma boa combinacao de herois e fundamental para o sucesso de um determinado time em uma partida. Nesse trabalho, e proposto um sistema de recomendacao para selecao de herois em uma partida de jogo MOBA. Foi desenvolvido um mecanismo baseado em regras de associacao que sugere o herois mais adequados para se compor um time, usando dados coletados de uma uma grande quantidade de partidas de Dota 2. Para avaliar a eficacia do line-up, foi treinada uma rede neural capaz de prever o time vencedor com uma acuracia de 92%. Os resultados do sistema de recomendacao foram muito satisfatorios com ate 76.4% de taxa de sucesso.",DISSERTAÇÃO,Sistema de recomendação de heróis para jogos MOBA utilizando aprendizado de máquina,5591992,1
"The compilers community has dedicated much time and effort in making context-sensitive analyses scalable, with great profit. However, the implementation of context-sensitive optimizations remains a challenge. The main problem is code size growth. Both function cloning and inlining are based on creating copies of all functions in the call path that leads to each optimization, even when that involves copying functions that are not optimized. We propose a solution for that problem. Using a combination of dynamic dispatch and a state machine to dynamically control the transitions between calling contexts, our method implements fully context-sensitive optimizations only needing to copy optimized functions. Experiments in the LLVM Test Suite and SPEC CPU2006 show our method scales better than cloning, using 8.5x less bytes to implement the same optimizations. We have also observed speed-ups of up to 20% on top of LLVM -O3 using fully context-sensitive constant propagation.",,CIENCIA DA COMPUTACAO,GABRIEL POESIA REIS E SILVA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,04/08/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Compiladores Otimizantes;Linguagens de Programao;Otimizaes Sensveis ao Contexto;Propagao de Constantes',-,FERNANDO MAGNO QUINTAO PEREIRA,0,Compiladores Otimizantes;Linguagens de Programação;Otimizações Sensíveis ao Contexto;Propagação de Constantes,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Construir análises sensíveis ao contexto escaláveis é um problema que vem sendo frequentemente trabalhado
pela comunidade de compiladores, com sucesso. Porém, a implementação de otimizações sensíveis ao contexto
continua sendo desafiadora. O principal problema que desencoraja os compiladores de implementarem tais
otimizações é o crescimento no tamanho do código. Com clonagem de funções ou inlining, duas técnicas conhecidas
para a implementação de especializações sensíveis ao contexto, o tamanho do código pode crescer exponencialmente
no pior caso. Ambas as técnicas são baseadas em criar cópias especializadas do código para cada contexto.
Contudo, as duas técnicas precisam criar cópias de todas as funções no caminho de chamadas que leva a cada
otimização, ainda que isto envolva copiar funções que não serão otimizadas. Neste trabalho, propomos uma solução
para este problema. Utilizando uma combinação de despacho dinâmico e uma máquina de estados para controlar as transições
entre os contextos dinamicamente, nosso método implementa otimizações completamente sensíveis ao contexto
necessitando apenas copiar as funções que serão otimizadas, mas não o caminho de chamadas até elas.
Apresentamos nossa abordagem em Minilog, uma linguagem mínima que possui todos os recursos necessários para aplicar
o método proposto, e provamos sua corretude. Implementamos nosso método na infraestrutura de compiladores LLVM,
utilizando-o para otimizar programas com propagação de constantes completamente sensível a contexto.
Nossos experimentos nos benchmarks do LLVM Test Suite e do SPEC CPU2006 mostram que nosso método escala significativamente
melhor em termos de espaço que clonagem de funções, gerando binários em média 2.7x menores, adicionando em média 8.5x menos
bytes ao implementar a mesma otimização. Os binários gerados utilizando nossa técnica tiveram performance muito semelhante aos
gerados com clonagem tradicional.
Além disso, utilizando essa classe de otimizações ainda pouco explorada, conseguimos speed-ups de até 20\% em alguns
benchmarks quando comparados a LLVM -O3.",DISSERTAÇÃO,Dynamic dispatch of context-sensitive optimizations,5592315,1
"Code smells can hinder the evolution and maintenance of software systems and can be detected by multiple techniques and tools. We evaluate and compare four tools, namely inFusion, JDeodorant, PMD, and JSpIRIT, using seven open source projects. We calculated the recall and precision of each tool in detecting God Class, God Method, and Feature Envy, using different methods to create reference lists. Agreement was also calculated between tools. Our main findings are that the evaluated tools present different recall and precision values for each smell. However, for all smells, percentage, AC1 and non-occurrence agreement were over 90%, confirming a high agreement on non-smelly classes and methods, regardless of differences between detection techniques. On the other hand, occurrence agreement was lower, ranging from 0.38% to 64.56%, confirming that regardless of similarities between detection techniques, each tool reports very different sets of classes and methods as code smells.",thanis paiva.pdf,CIENCIA DA COMPUTACAO,THANIS FERNANDES PAIVA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,11/08/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'anomalias de cdigo;ferramentas de deteco;mtricas de software',-,EDUARDO MAGNO LAGES FIGUEIREDO,0,anomalias de código;ferramentas de detecção;métricas de software,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Code smells can hinder the evolution and maintenance of software systems and can be detected by multiple techniques and tools. We evaluate and compare four tools, namely inFusion, JDeodorant, PMD, and JSpIRIT, using seven open source projects. We calculated the recall and precision of each tool in detecting God Class, God Method, and Feature Envy, using different methods to create reference lists. Agreement was also calculated between tools. Our main findings are that the evaluated tools present different recall and precision values for each smell. However, for all smells, percentage, AC1 and non-occurrence agreement were over 90%, confirming a high agreement on non-smelly classes and methods, regardless of differences between detection techniques. On the other hand, occurrence agreement was lower, ranging from 0.38% to 64.56%, confirming that regardless of similarities between detection techniques, each tool reports very different sets of classes and methods as code smells.",DISSERTAÇÃO,On the evaluation of code smells and detection tools,5592399,1
"Corpora used to learn open-domain Question-Answering (QA) models are typically collected from a wide variety of topics or domains. Since QA requires understanding natural language, open-domain QA models generally need very large training corpora. A simple way to alleviate data demand is to restrict the domain covered by the QA model, leading thus to domain-specific QA models. While learning improved QA models for a specific domain is still challenging due to the lack of sufficient training data in the topic of interest, additional training data can be obtained from related topic domains. Thus, instead of learning a single open-domain QA model, this work investigates domain adaptation approaches in order to create multiple improved domain-specific QA models. It is also demonstrated that this can be achieved by stratifying the source dataset, without the need of searching for complementary data unlike many other domain adaptation approaches. This work proposes a deep architecture that jointly exploits convolutional and recurrent networks for learning domain-specific features while transferring domain-shared features. That is, transferable features to enable model adaptation from multiple source domains. It is considered different transference and domain selection approaches designed to learn span-level and sentence-level QA models. The findings show that domain-adaptation improves performance, specially in sentence-level QA. It is also shown that span-level QA benefits from contextual information present in the sentence models.",GianluccaZuin.pdf,CIENCIA DA COMPUTACAO,GIANLUCCA LODRON ZUIN,UNIVERSIDADE FEDERAL DE MINAS GERAIS,14/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Question Answering;Deep Neural Networks;Domain Adaptation;Transfer Learning;Context Integration',-,ADRIANO ALONSO VELOSO,0,Pergunta-Resposta;Redes Neurais Profundas;Adaptação de Domínio;Transferência de Aprendizado;Integração de Contexto,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Geralmente coletamos dados de diversas fontes para montarmos um Corpus adequado para o aprendizado de modelos multi-domínio de pergunta-resposta (QA). Uma maneira simples de aliviar a demanda de dados é restringir o domínio abordado, levando assim à modelos específicos. Embora o aprendizado de modelos de QA em um único domínio ainda seja uma tarefa desafiadora, podemos obter instâncias adicionais por meio de domínios relacionados. Este trabalho investiga abordagens de adaptação a fim de obter vários modelos especializados no qual características gerais dos temas são compartilhadas enquanto características específicas dos domínios são aprendidas. São consideradas diferentes abordagens de transferência e de divisão de domínios desenvolvidas. Observou-se que a adaptação ao domínio resulta em ganhos de desempenho, em especial à nível de sentenças. Observou-se também que podemos ter um aumento no desempenho do modelo baseado em spans ao utilizar informações do QA em nível de sentenças.",DISSERTAÇÃO,Uso de adaptação de domínio e informação contextual em sistemas de pergunta-resposta,5664170,1
"In the last decades, we have observed a constant growth in the use of remote sensing images (RSI) for the monitoring of activities and phenomena on Earth allowing the development of several applications. The creation of thematic maps is one of the most common, since it allows the classification and analysis of the various objects that make up an image and can be used for many purposes, such as: monitoring, planning and recognition. 
In this sense, in this project, we decided to develop a method of generating thematic maps for the recognition of coffee crops in order to obtain data from this product, which, despite its great importance in the country's economy, Brazil being the largest producer of coffee Of the world and the second largest consumer, still uses methods of obtaining data manually.",,CIENCIA DA COMPUTACAO,RAFAEL MARLON PEREIRA COSTA BAETA CARREIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,04/09/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Classificao;Redes neuronais de convoluo;Sensoriamento remoto',-,JEFERSSON ALEX DOS SANTOS,0,Classificação;Redes neuronais de convolução;Sensoriamento remoto,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Nas ultimas décadas, temos observado um constante crescimento na utilização de imagens de sensoriamento remoto (ISR) para o monitoramento de atividades e fenômenos na Terra permitindo o desenvolvimento de diversas aplicações. Dentre as aplicações existentes a criação de mapas temáticos é uma das mais comuns, pois permite a classificação e análise dos vários objetos que compõe uma imagem podendo ser utilizado para muitos fins, tais como: monitoramento, planejamento e reconhecimento. Mapas temáticos são imagens construídas no intuito de identificar a categoria em que cada objeto presente pertence, sendo estas imagens geradas, normalmente, através da utilização de modelos treinados através de aprendizagem supervisionada que é um processo no qual o sistema é treinado para aprender padrões através da utilização de amostras rotuladas fornecidos pelo usuário. Nesse sentido, neste projeto, decidimos desenvolver um método de geração de mapas temáticos para o reconhecimento de colheitas de café visando auxiliar na obtenção de dados deste produto que, apesar da sua grande importância na economia do país, sendo o Brasil o maior produtor de café do mundo e o segundo maior consumidor, ainda utiliza métodos de obtenção de dados de forma manual. A identificação de áreas de café não é uma tarefa trivial. Algumas das dificuldades encontradas nesta atividade está relacionada ao fato do produto ser cultivado em diferentes áreas que possuem diferentes climas, relevos, altitudes e latitudes, o que permite a produção de vários tipos de café, produzinho uma grande variedade de padrões. As características do relevo também pode levar a ocorrência de sombras que alteram a percepção da informação espectral obtida pelos satélites, reduzindo ou mesmo eliminando-as. Além disso, o crescimento do café não ocorre de forma sazonal o que permite a existência de plantas de diferentes idades em um mesmo cultivo. Além dos desafios fornecidos pela identificação do café, temos os relacionados ao sensoriamento remoto que possui imagens imensas e uma grande quantidade de informações de objetos que podem interferir no desempenho dos algoritmos de aprendizagem de máquina. O método a ser desenvolvido neste trabalho baseia-se na combinação de redes neuronais de convolução em múltiplas escalas e a escolha por redes neuronais para o desenvolvimento deste projeto é atribuída ao fato de seu desempenho ser superior aos métodos tradicionais propostos em visão computacional e também por ainda não ser tão utilizado em tarefas relacionadas a área agrícola. A utilização de uma abordagem multi-escala está relacionada à variação do tamanho dos padrões encontrados em imagens de satélite e visa tornar o método mais robusto ao permitir que características distintas sejam aprendidas em cada uma das escalas e usadas de forma complementar e, para que as diferentes redes sejam combinadas é utilizada uma fusão através de votação majoritária.",DISSERTAÇÃO,Geographical mapping of coffee crops by using convolutional networks,5712207,1
.,,CIENCIA DA COMPUTACAO,LUIZ PAULO DAMILTON CORREA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,05/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Aplicativos de Encontro;Aplicativos Mveis;Coproduo;Pesquisa Qualitativa;Sistemas Colaborativos',-,RAQUEL OLIVEIRA PRATES,0,Aplicativos de Encontro;Aplicativos Móveis;Coprodução;Pesquisa Qualitativa;Sistemas Colaborativos,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Esta dissertação apresenta o projeto de um aplicativo móvel que viabiliza encontros de estudos no ambiente acadêmico, o EstudaJunto UFMG. Trata-se de um aplicativo de encontros e coprodução, em que todos os envolvidos na atividade são igualmente beneficiados e não há necessariamente um ofertante ou demandante de um serviço, como é o caso de uma atividade de estudos em dupla ou em grupo. A coprodução e o projeto como um todo é discutido a partir das seguintes etapas: (1) entrevistas com estudantes sobre suas atividades de estudo; (2) design e construção de uma versão interativa em Android do aplicativo; (3) avaliação do aplicativo com usuários; e (4) levantamento e discussão de atributos relevantes para este tipo de aplicação. Essa pesquisa propõe uma solução que pretende contribuir diretamente com a comunidade da UFMG e usa a proposta como estudo de caso para investigar as peculiaridades deste tipo de aplicação, que possibilita encontros offline e interações colaborativas.",DISSERTAÇÃO,EstudaJunto UfMG: um aplicativo móvel de encontros e coprodução para estudos na Universidade,5712398,1
.,Roberto Gontijo.pdf,CIENCIA DA COMPUTACAO,ROBERTO ALMEIDA GONTIJO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,14/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'anlise local de deslocamentos;interao hptica;mtodos de identificao de materiais;percepo em tempo real',-,MARIO FERNANDO MONTENEGRO CAMPOS,0,análise local de deslocamentos;interação háptica;métodos de identificação de materiais;percepção em tempo real,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O presente trabalho apresenta como principal objetivo a implementação de dois novos métodos para identificação de materiais: Método de Análise Local de Deslocamentos por Curvas de Nível (MALD-CN) e Método de Análise Local de Deslocamentos de Pontos Característicos (MALD-PC).  Em termos de análise de deslocamentos, o método de medição de dureza e o Método de Elementos Finitos Inverso (MEFI) correspondem às técnicas adotadas no estado da arte. De forma geral, os métodos de medição de dureza apresentam a desvantagem de necessitar de ambientes controlados. Já o MEFI requer grande tempo de execução, inviabilizando sua utilização em sistemas de tempo real crítico ou suave como sistemas robóticos. Os métodos MALD-CN e MALD-PC superam os principais problemas dos métodos, baseados em análise de deslocamentos, adotados na atualidade .",DISSERTAÇÃO,Identificação interativa da elasticidade de materiais por meio da extração de características visuais e geométricas,5712646,1
.,,CIENCIA DA COMPUTACAO,HENRIQUE SOUZA ROSSI,UNIVERSIDADE FEDERAL DE MINAS GERAIS,21/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Desordem de Integrao Sensorial;DIS DPS HMD;Head Mounted Display;Jogos Digitais;Realidade Virtual',-,RENATO ANTONIO CELSO FERREIRA,0,Desordem de Integração Sensorial;DIS DPS HMD;Head Mounted Display;Jogos Digitais;Realidade Virtual,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"No contexto da Desordem de Integração Sensorial (DIS), este presente trabalho apresenta uma solução de realidade virtual  um simulador de montanha russa, que utiliza os movimentos da cabeça captados através do Oculus Rift  a ser aplicada no tratamento DIS. Nosso objetivo é verificar se o uso de jogos digitais baseados em realidade virtual possui relevância neste tratamento por meio de seu uso na estimulação e integração dos diversos sistemas sensoriais. Para validar o potencial de uso no tratamento, convidamos 5 terapeutas ocupacionais que utilizaram o sistema durante 15 dias em seus tratamentos. Os resultados da pesquisa indicam que uso do nosso sistema foi útil no tratamento da DIS, pois, além dos terapeutas terem relatado indícios de estímulo aos 5 sentidos (visual, vestibular, olfativo, tato e proprioceptivo), eles também relataram indícios de relaxamento, maior concentração e mudanças de comportamento em pacientes que o utilizaram.",DISSERTAÇÃO,Imaginator: um sistema de realidade virtual para o auxílio no tratamento de transtornos de processamento sensorial,5723256,1
"Land-cover maps are one of the main sources of information for studies that support the creation of public policies in areas like urban planning and environmental monitoring. Their automatic creation involves learning to annotate all samples of a Remote Sensing Image (RSI) from just a few annotated by the user. Nevertheless, low-level descriptors like color and shape are not enough to produce a discriminative representation for the samples that represent objects that share similar visual appearance. In order to overcome this limitation, three methods to encode the context of the samples extracted from regions are proposed in this work: the first combines low-level representations from adjacent samples, the second one counts co-occurrences of visual words over a local area and the last one exploits ConvNets to compute deep contextual features. Confirming previous studies, the generated maps were improved by incorporating context in the representations used to feed the classifier.",,CIENCIA DA COMPUTACAO,TIAGO MOREIRA HUBNER CANCADO SANTANA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,28/09/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Classificao Baseada em Regies;Descritor Contextual;Mapas de Cobertura Terrestre;Sensoriamento Remoto',-,JEFERSSON ALEX DOS SANTOS,0,Classificação Baseada em Regiões;Descritor Contextual;Mapas de Cobertura Terrestre;Sensoriamento Remoto,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Desde o começo da década de 70, quando as primeiras Imagens de Sensoriamento Remoto (ISRs) tornaram-se disponíveis a partir de satélites civis, o mapeamento automático de cobertura terrestre tem sido um tema central de pesquisa no campo devido à sua importância socio-econômica: tais mapas são uma das principais fontes de informação para estudos que embasam a criação de políticas públicas em áreas como o planejamento urbano e o monitoramento ambiental, por exemplo. O processo para gerar os mapas a partir das imagens é frequentemente modelado como um problema de classificação supervisionada, onde algumas amostras de cada classe alvo anotadas pelo usuário na imagem são usadas para alimentar um classificador, que é utilizado para anotar as amostras restantes após o estágio de treinamento. À medida que a resolução espacial dos sensores usados para adquirir as imagens tornou-se mais fina, o paradigma de anotar e classificar pixels que foi dominante desde as primeiras abordagens teve que ser substituído pelo baseado em regiões, uma vez que cada objeto significativo contido em ISRs agora é composto de vários pixels. No entanto, descritores de baixo nível como cor e forma não são suficientes para produzir uma representação discriminativa para as amostras que representam objetos que compartilham aparência visual semelhante. Em tais situações, agregar informações da cena como um todo ou de objetos vizinhos pode ser útil para ajudar a distinguí-los. Explorando essa abordagem que está apenas começando a ser usada para o paradigma baseado em regiões, foram propostos três métodos para codificar o contexto de superpixels neste trabalho: o primeiro método modela cada vizinhança local composta de um superpixel e seus vizinhos como um Grafo de Adjacência de Regiões (GAR) e combina representações de baixo nível extraídas dos vértices e arestas em um único vetor de características que codifica tanto a aparência visual quanto o contexto do superpixel; o segundo codifica o contexto semântico de uma vizinhança local ao redor do superpixel contando a co-ocorrência de palavras visuais dentro dele e de seus vizinhos; e o último método proposto explora ConvNets para calcular características contextuais profundas a partir de estruturas de imagem com forma irregular, como é o caso dos superpixels. Confirmando estudos anteriores que mostraram que codificar contexto seja de pixels ou regiões é uma abordagem promissora, todos os três métodos propostos foram capazes de melhorar os mapas gerados ao incorporar contexto nas representações usadas para alimentar o classificador.",DISSERTAÇÃO,Encoding context from superpixels to improve land-cover maps,5733567,1
"The design of traditional wireless sensor networks prioritized reducing the cost, the energy consumption and memory usage at the expense of the throughput, but modern applications require using cameras and transmitting video data through the network. In this work, we propose a multipath routing algorithm to find two disjoint paths with the same parity between a pair of nodes in a dual-radio wireless sensor network. Combined with a forwarding scheme that alternates the radios throughout the paths, it allows all nodes in the paths to use both radios in parallel, doubling the throughput. We evaluated our algorithm and forwarding scheme in a real world testbed and we were able to double the throughput when compared with FastForward, the state-of-the-art protocol for dual-radio platforms, and achieve up to 96% of the theoretical limit.",,CIENCIA DA COMPUTACAO,NILDO DOS SANTOS RIBEIRO JUNIOR,UNIVERSIDADE FEDERAL DE MINAS GERAIS,06/10/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Multipath Routing;Wireless Sensor Networks;Dual-Radio',-,MARCOS AUGUSTO MENEZES VIEIRA,0,Roteamento Multicaminho;Redes de Sensores Sem Fio;Dois Rádios,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O projeto de uma Rede de Sensores Sem Fio depente muito da aplicação. Para aplicações tradicionais, como monitoração ambiental, smart buildings ou na agricultura, foram sempre priorizadas a redução do custo das plataformas, o gasto de energia e o uso de memória, em troca de ter uma vazão mais baixa. Aplicações modernas, como sistemas de segurança ou monitoramento de tráfego, geralmente exigem o uso de câmeras e transmissão de dados de vídeos pela rede. Para aplicações como essas, plataformas de dois rádios foram desenvolvidas, onde o seu projeto prioriza alcançar uma vazão maior e conservar a eficiência energética da rede. Nesse trabalho é proposto um algoritmo de roteamento multicaminho para encontrar dois caminhos disjuntos com a mesma paridade entre um par de nós da rede. O novo algoritmo de roteamento, combinado com um esquema de encaminhamento de pacotes que alterna os rádios ao longo do caminho, permite que todos os nós usem os dois rádios que possuem em paralelo a todo o tempo, dobrando a vazão em comparação com um esquema de um único caminho. Nós avaliamos nosso algoritmo e esquema de encaminhamento em um testbed físico contendo 100 nós da plataforma Opal, que possui dois rádios. Nosso protocolo conseguiu dobrar a vazão em comparação com o FastForward, o protocolo estado-da-arte para plataformas com dois rádios, e conseguiu atingir até 96% do limite teórico.",DISSERTAÇÃO,Multipath Routing for Dual-Radio Wireless Sensor Networks,5733955,1
"Point-of-interest (POI) recommendation in location-based social networks has been widely researched as a mechanism to help users discover new venues to visit in a city. Despite recent efforts, the currently available test collections trivially favor geography-aware POI recommenders by overlooking the fact that users will most often prefer nearby POIs anyway. As a result, it remains unclear what role other contextual factors play in a more realistic scenario where geography is constrained by definition given the user's location. To close this gap, we introduce a large-scale test collection for context-aware nearby POI recommendation, which includes several geography-constrained test cases, enriched with temporal and weather contexts in two large cities in the Americas. Also, we describe our proposed evaluation methodology for the nearby POI recommendation task and present a breakdown analysis of the state-of-the-art POI recommenders results as reference results for the new collection.",,CIENCIA DA COMPUTACAO,JORDAN SILVA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,27/09/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'POI;Ponto-de-Interesse;Recuperao de Informao;Rede social baseada em localizao;Sistemas de Recomendao',-,RODRYGO LUIS TEODORO SANTOS,0,POI;Ponto-de-Interesse;Recuperação de Informação;Rede social baseada em localização;Sistemas de Recomendação,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A recomendação de ponto de interesse (POI) em redes sociais baseadas em localização (LBSNs) têm sido amplamente pesquisada como um mecanismo para ajudar as pessoas a descobrirem novos locais para visitar em uma cidade. Apesar dos esforços recentes, atualmente, as coleções de teste disponíveis são enviesadas ao favorecimento de sistemas de recomendação de POIs sensíveis a geografia. Isso ocorre pois boa parte do sucesso desses sistemas é devido ao fato de que as pessoas preferencialmente tendem a visitar POIs próximos à elas. Como resultado, não está claro o papel que outros fatores contextuais desempenham em um cenário mais realista, onde a geografia é restrita por definição, dada a localização da pessoa. Para lidar com esse problema, apresentamos nossa metodologia de avaliação para a tarefa de recomendação de POIs próximos, onde as sugestões de POIs são restritas a uma área próxima da localização da pessoa. Além da tarefa proposta, apresentamos uma coleção de testes para recomendação de POI próximos, que inclui vários casos de teste com restrições geográficas, enriquecidos com contextos temporais e climáticos, distribuídos ao longo de um ano em duas grandes cidades nas Américas. Também, apresentamos uma análise detalhada da qualidade dos sistemas de recomendação de POIs estado-da-arte (e.g., BPRMF, Rank-GeoFM) como referência para a nossa coleção de testes.",DISSERTAÇÃO,Nearby places: on location-based  pruning for point-of-interest recommendation,5942279,1
"The increasing advancements of information systems allow more and more data to be generated and stored. In this context, machine learning algorithms appear as an alternative to process and generate knowledge from this immense amount of information. However, for machine learning algorithms to perform well, they need to be applied to properly represented data. Thus, the hypothesis of this work is that, based on the observation of online discussions about particular entities, it is possible to learn attributes that characterize and relate these entities to each other. Thanks to the popularization of the Internet, it is easy to find many records of people talking about the most diverse subjects, whether in discussion forums, social network interactions, or comments on web pages. To study this problem, we first analyze the potential that comments have for describing entities, and perform a task to select them according to their descriptive value. With the understanding generated by this study, we define the problem of online-discussions-based entity representation, propose a new benchmark for evaluation of representation methods within this context, and perform an evaluation of well-known techniques using three evaluation scenarios: (i) clustering, (ii) ordering and (iii) recommendation. Results show that there is no one method better than the others in all the analyzed tasks.",,CIENCIA DA COMPUTACAO,TULIO CORREA LOURES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,10/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Representation;Entity;Online Discussion;Comments',-,PEDRO OLMO STANCIOLI VAZ DE MELO,0,Representação;Entidades;Discussões Online;Comentários,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Os crescentes avanços dos sistemas de informação permitem que cada vez mais dados sejam gerados e armazenados. Nesse contexto, algoritmos de aprendizagem de máquina surgem como uma alternativa para processar e gerar conhecimento a partir dessa imensa quantidade de informação. No entanto, para que algoritmos de aprendizado de máquina tenham bom desempenho, é necessário que eles sejam aplicados em dados representados de forma adequada. Assim, a hipótese deste trabalho é que, a partir da observação das discussões online sobre entidades particulares, seja possível aprender atributos que as caracterizem e as relacionem umas com as outras. Graças à popularização da Internet, é fácil encontrar muitos registros de pessoas falando sobre os mais diversos assuntos, seja em fóruns de discussão, interações em redes sociais, ou comentários de páginas Web. Para estudar esse problema, primeiro analisamos o potencial que comentários têm para descrever entidades, e realizamos uma tarefa para selecioná-los de acordo com seu valor descritivo. Com o entendimento gerado por esse estudo, definimos o problema de representação de entidades baseada em discussões online, propusemos um novo benchmark para avaliação de métodos de representação dentro desse contexto, e realizamos uma avaliação de técnicas bem conhecidas usando três cenários de avaliação: (i) agrupamento, (ii) ordenação e (iii) recomendação. Resultados mostram que não há um método melhor que os outros em todas as tarefas analisadas.",DISSERTAÇÃO,Representação Distribuída de Entidades baseada em discussões online,5942936,1
"Agriculture is one of the most important activities developed today by man. Before its development, the form of human feeding was based on the hunting and gathering of foods and the world counted on a population of about 4 million people. Today the world has about 7 billion people and agriculture is constantly improving to produce food for all these people. New machines, seeds, and fertilizers were developed to increase the productivity of cultivated areas. The idea of ​​sustainable development became widely known after the United Nations Conference on Environment and Development in Rio de Janeiro in 1992 (ECO-92). It is estimated that by 2050 we will have a population of 9 billion people and the production of food to meet this demand must occur in a sustainable way. To achieve this goal, sustainable management of these agroecosystems must be implemented. This is a complex task due to the large number of variables involved, the difficulty of collecting this information and the definition of a model for sustainability assessment. In this work, a system for sustainability management in agroecosystems based on Data Science was developed. Tools for collection, structured storage, and analysis of information related to the sustainability of rural properties were developed. Our system was named IS@ Digital because it is based on the Agroecosystems Sustainability Index (ISA) methodology and validated on 100 properties in the state of Minas Gerais. As a result, we have the development of tools for data collection, visualization, and analysis related to the sustainability of rural properties. These tools facilitate the management of rural property sustainability indicators. We also have that with only 8 of the 21 indicators present in ISA we can identify the level of sustainability in more than 90% of cases.",,CIENCIA DA COMPUTACAO,EVANDRO CALDEIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,25/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Sustainability;Agroecosystems;Machine Learning;KDD;Knowledge Discovery;Characterization and Data Analysis',-,ADRIANO CESAR MACHADO PEREIRA,0,Sustentabilidade;Agroecossistemas;Aprendizado de Máquina;KDD;Descoberta de Conhecimento;Caracterização e Análise de Dados,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A agricultura é uma das mais importantes atividades desenvolvidas hoje pelo homem. Antes do seu desenvolvimento a forma de alimentação humana se baseava na caça e coleta de alimentos e o mundo contava com uma população de cerca de 4 milhões de pessoas. Hoje o mundo tem cerca de 7 bilhões de pessoas e a agricultura está em constante aprimoramento para produzir alimentos para todas essas pessoas. Foram desenvolvidas novas máquinas, sementes e fertilizantes para aumentar a produtividade das áreas cultivadas. A ideia do desenvolvimento sustentável se tornou amplamente conhecida após a Conferência das Nações Unidas sobre o Meio Ambiente e o Desenvolvimento no Rio de Janeiro em 1992 (ECO-92). Estima-se que em 2050 teremos uma população de 9 bilhões de pessoas e a produção de alimentos para suprir essa demanda deve ocorrer de forma sustentável. Para alcançar esse objetivo deve ser implementada uma gestão sustentável desses agroecossistemas. Essa é uma tarefa complexa devido ao grande número de variáveis envolvidas, a dificuldade para coleta dessas informações e a definição de um modelo para avaliação da sustentabilidade. Nesse trabalho foi desenvolvido um sistema para gestão de sustentabilidade em agroecossistemas baseado em Ciência de Dados. Foram desenvolvidas ferramentas para coleta, armazenamento estruturado e análise de informações relacionadas à sustentabilidade das propriedades rurais. Nosso sistema foi batizado com o nome de IS@ Digital, pois é baseado na metodologia Índice de Sustentabilidade de Agroecossistemas (ISA) e validado em 100 propriedades do estado de Minas Gerais. Como resultado temos o desenvolvimento de ferramentas para coleta, visualização e análise de dados relacionados à sustentabilidade de propriedades rurais. Essas ferramentas facilitam a gestão dos indicadores de sustentabilidade da propriedade rural. Também temos que com apenas 8 dos 21 indicadores presentes no ISA conseguimos identificar o nível de sustentabilidade em mais de 90% dos casos.",DISSERTAÇÃO,IS@ Digital: Um Sistema de Informação para Gestão Sustentável de Agroecossistemas,5944334,1
"Home networks have become increasingly large due to the number of connected smart devices. Indeed, the use of personal devices, like smartphones, tablets, smart TVs, results in a densification of such networks. In addition, new services such as streaming, cloud storage, and torrents have become popular. Therefore, the need for greater reliability and Quality of Service (QoS) arises, which requires a more efficient management of home networks. In these networks, usually when a problem occurs, the users are not able to inspect or troubleshoot it. Moreover, such users are usually responsible for setting up their own network. This situation is aggravated by the lack of tools that support the identification of the problem source, as well as its automatic solution. In this work, we propose HomeNetRescue, a software defined network service for home networks autonomous management. HomeNetRescue is designed for the detection, diagnosis and automatic solution in wired and wireless networks. Its architecture is generic and modular, which allows the addition of new applications and devices to be monitored. The service was modeled to be used by Internet Service Providers (ISP) to manage problems in their customers' home networks. In addition, the service can add automatic detection and troubleshooting capabilities to these networks, thus the networks become more stable and reliable. The use of HomeNetRescue can provide cost savings for the ISPs, generate lower demand for support services, and shorten recovery time in case of failures. We evaluate the prototype of HomeNetRescue in a real environment. In order to do that, we elaborated three use cases. With our results, we can conclude that HomeNetRescue provided individual and global benefits on the network. These benefits correspond to gains in the throughput, and reductions in delay and jitter of the stations' wireless transmissions. In first evaluation we describe two approaches for transmit power control of access points, one distributed and one centralized. In Uncontrolled Transmit Power Control use case, the service detects an increase in the amount of packet loss due to collisions, acts automatically on the network, thus increases transmission power, and combats bad wireless links. This results in a gain of 7% in the throughput of the analyzed access point. In Transmit Power Control with Coordination use case, the service improved the average throughput by 66%, reduced the delay by 36% and the jitter by 15% when compared to a network without power control. Finally, in Dynamic and Coordinated Selection of Channels use case, the service performs an orchestrated assignment of channels to network access points in a multi-AP environment. In this case, the throughput improves by 131%, and delay and jitter reduces by 46% and 24% respectively, when compared to algorithms for channel assignment of commodity access points.",AlissonRodriguesAlves.pdf,-,ALISSON RODRIGUES ALVES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,27/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Software Defined Network;Management;Home Networks;Troubleshooting',-,DANIEL FERNANDES MACEDO,0,Redes Definidas por Software;Gerenciamento;Redes Domésticas;Troubleshooting,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"A quantidade de dispositivos inteligentes inseridos nas redes domésticas está rapidamente crescendo, o que torna o gerenciamento de problemas mais complexo nessas redes. Além disso, a falta de conhecimento dos usuários e a inexistência de ferramentais para automaticamente diagnosticar e solucionar falhas agravam o problema. Nesta dissertação propomos o HomeNetRescue, um serviço que emprega o conceito de redes definidas por software para realizar o gerenciamento autônomo de redes domésticas. O HomeNetRescue é voltado para a detecção, diagnóstico e solução automática de problemas em redes com e sem fio. Avaliamos o protótipo do HomeNetRescue em três casos de uso, empregados em ambiente real, considerando vazão, atraso e jitter. Nossos resultados mostram que o HomeNetRescue foi capaz de aumentar a vazão da rede em 131%, diminuindo o atraso e o jitter das transmissões sem fio em 46% e 24%, respectivamente",DISSERTAÇÃO,HomeNetRescue: Um Serviço SDN para Detecção e Solução de Problemas em Redes Domésticas,5944560,1
"Software Defined Vehicular Network (SDVN) is a new network architecture inspired by the well-known Vehicular Ad Hoc Network (VANET), applying the concepts of Software Defined Network (SDN). The SDVN proposes a complete data flow management by a module that controls the routing actions. However, it is necessary to verify that the security requirements are still satisfied. This work presents the Sentinel, a new defense mechanism in order to detect flooding attack by time series analysis of packet flow and mitigate the attack creating a flow tree to find out the source of spoofed packets. We divided the results between the detection rate of victim vehicles and the efficiency of mitigation method. The algorithm was able to mitigate the attack flow in different scenarios and parameters. However, the speed of vehicles might decrease the efficiency due to the fast change of attack flow. Furthermore, we also propose some improvements to future approaches.",GabrieldeBiasi.pdf,CIENCIA DA COMPUTACAO,GABRIEL DE BIASI,UNIVERSIDADE FEDERAL DE MINAS GERAIS,01/11/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Network;Vehicles;SDVN',-,LUIZ FILIPE MENEZES VIEIRA,0,Network;Vehicles;SDVN,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Software Defined Vehicular Network (SDVN) is a new network architecture inspired by the well-known Vehicular Ad Hoc Network (VANET), applying the concepts of Software Defined Network (SDN). The SDVN proposes a complete data flow management by a module that controls the routing actions. However, it is necessary to verify that the security requirements are still satisfied. This work presents the Sentinel, a new defense mechanism in order to detect flooding attack by time series analysis of packet flow and mitigate the attack creating a flow tree to find out the source of spoofed packets. We divided the results between the detection rate of victim vehicles and the efficiency of mitigation method. The algorithm was able to mitigate the attack flow in different scenarios and parameters. However, the speed of vehicles might decrease the efficiency due to the fast change of attack flow. Furthermore, we also propose some improvements to future approaches.",DISSERTAÇÃO,Mitigating Denial of Service Attacks in Software Defined Vehicular Networks,5944748,1
"Lazy machine learning algorithms have to learn every time it is been given a new example, however knowing which example is being classified gives them the advantage of adjusting their knowledge search accordingly. The Lazy Associative Classifier (LAC) is a rule-based demand-driven lazy machine learning algorithm that takes advantage of the information present in the example being classified by focusing its effort on inducing only rules that cover that particular example. Each rule comes from a frequent pattern present in the data. While, associative classifiers, in general, suffer from searching frequent patterns among the large number of existing patterns within the data, LAC breaks that problem down into many subproblems, solving one small problem at a time. Rule-based algorithms are often caught in the dilemma of not knowing the best way to combine their rules in order to form the best possible classifier. Usually, the choosing of a rule metric followed by a simple voting is used (as simple as assigning an importance -- or weight -- of one to each rule and averaging the accounts by each class). This approach is easily proven to be frail. Furthermore, LAC uses all rules available, which can be considered a large quantity of rules, regardless of their prediction quality. In this work we use a boosting algorithm known as Confidence-Rated Adaboost in conjunction with LAC to form a new, more accurate and smaller (in number of rules present in each model) classifier algorithm called BLACk. We prove that our approach is superior in terms of accuracy to LAC and other associative classifier. Nevertheless, we show that the built classifiers model are less complex compared to those built by LAC.",,CIENCIA DA COMPUTACAO,VAUX SANDINO DINIZ GOMES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,14/11/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Data Mining;Associative Classification;Lazy Classifier;Boosting',-,LOIC PASCAL GILLES CERF,0,Aprendizado de Máquinas;Classificação Associativa;Algoritmo lazy;Boosting,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Frequentemente, algoritmos classificação associativa sofrem com problemas de processamento, dado o número padrões existentes nos dados. O Lazy Associative Classifier (LAC) supera essa adversidade decompondo o problema de procurar por todos os padrões em vários subproblemas menores. Contudo o LAC utiliza indiscriminadamente todas as regras que consegue formar. Isto pode levar à perda de acurácia e de interpretabilidade das classificações do algoritmo. Propomos o BLACk: um algoritmo de classificação associativa que utiliza boosting para montar um modelo aditivo com os mesmos padrões encontrados pelo LAC e que melhora a acurácia e a legibilidade do classificador. Foi possível comprovar estatisticamente que o BLACk é mais preciso que o LAC e que o número de regras do BLACk é algumas ordens de grandeza menor que o número de regras do LAC, o que o torna mais humanamente inteligível que o LAC.",DISSERTAÇÃO,BOOSTED LAZY ASSOCIATIVE CLASSIFIER,5944953,1
"Currently the computer science community has several powerful programs for performing dynamic analysis. Examples of these analyzers are: Valgrind, useful for performing several types of checks related to the running of programs; and aprof, used to aid in the search for asymptotic inefficiencies. However, these tools expect to receive as an input an executable program. This can disturb programmers to analyze specific code snippets, such as functions. Considering this difficulty, in this dissertation we present a set of techniques that, together, they enable us to execute isolated functions written in C language automatically. In particular, we focus on methods that deal with access to arrays through indexation. Besides that, it is our goal that, when executed, a target function does not contain invalid memory accesses caused by the data provided to execute it. With the ideas presented here, we developed a tool, amazonc, which enable us to execute functions in C isolated from the rest of the code. Experiments performed with the kernels of the Polybench reinforce the effectiveness of the method presented. Together with aprof, amazonc was able to reconstruct of the asymptotic complexity curves of all core functions in that benchmark. In addition, the reconstruction of the curves were made without Valgrind point out any kind of error related to bad memory access.",,-,MARCUS RODRIGUES DE ARAUJO,UNIVERSIDADE FEDERAL DE MINAS GERAIS,30/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Automatic Execution;Dynamic Analysis;Static Analysis;C Language',-,FERNANDO MAGNO QUINTAO PEREIRA,0,Execução Automática;Análise Estática;Análise Dinâmica;Linguagem C,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Atualmente a comunidade conta com diversos programas para a realização de análises dinâmicas. Exemplos desses analisadores são: Valgrind (verificações relacionadas ao uso de memória) e aprof (aponta ineficiências assintóticas). Entretanto, essas ferramentas esperam receber como entrada um programa executável. Isso pode dificultar a tarefa de programadores em analisar trechos de código específicos. Dada essa limitação, nesta dissertação são apresentadas um conjunto de técnicas que permitem a execução de um método em C de forma isolada. Em especial, focamos em funções que lidam com acessos a arranjos e é tido como objetivo que, ao ser executado, o método alvo não deve conter acessos de memória inválidos causados pelos dados usados para executá-lo. Com as ideias apresentadas aqui, foi construída uma ferramenta, amazonc, que possibilita a execução de funções em C de forma isolada. Experimentos realizados com as funções núcleos do Polybench reforçam a eficácia das técnicas apresentadas.",DISSERTAÇÃO,Execução de funções parciais em linguagem de programação C,5945128,1
"The Vehicle Routing Problem (VRP) with Multiple Time Windows is a generalization of VRP, where the customers have one or more time windows in which they can be visited. The best heuristic in the literature is a Hybrid Variable Neighborhood Tabu Search (HVNTS) that mostly deals with infeasible solutions, because it is assumed that one may not reach some regions of the search space without passing through infeasible solutions. In this dissertation, we propose a simpler Variable Neighborhood Search heuristic where all the computational effort is spent on searching for feasible solutions. Computational experiments showed that the proposed heuristic is competitive with the best heuristic in the literature.",,CIENCIA DA COMPUTACAO,HUGGO SILVA FERREIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,30/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Variable Neighborhood Search;Vehicle Routing Problem;Multiple Time Windows',-,THIAGO FERREIRA DE NORONHA,0,Busca em Vizinhanças Variáveis;Problema de Roteamento de Veículos;Múltiplas Janelas de Tempo,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O Problema de Roteamento de Veículos (VRP, do inglês Vehicle Routing Problem) com Múltiplas Janelas de Tempo é uma generalização do Problema de Roteamento de Veículos, onde os clientes têm uma ou mais janelas de tempo nas quais eles podem ser visitados. A melhor heurística na literatura, HVNTS, é uma hibridização das metaheurísticas Busca Tabu e Variable Neighborhood Search que trabalha principalmente com soluções inviáveis. Nesta dissertação, propomos uma heurística de Variable Neighborhood Search mais simples, onde todo o esforço computacional é gasto na busca de soluções viáveis. Experimentos computacionais mostraram que a heurística proposta é competitiva com a melhor heurística na literatura.",DISSERTAÇÃO,Heurística de busca em vizinhanças variáveis para o problema de roteamento de veículos com múltiplas janelas de tempo,5945409,1
"Problems that occur in components of a network system may cause service disruption, elevate the operating costs and contribute to material and imaterial damages to organizations. One option to minimize the impact of problems on the components of the system is to add more people able to perform tasks that enable the operation, maintenance and update of those components. But, this approach can be financially not feasible, besides other facts, due to cost associated with people. Another option that can minimize the impact is to adopt and integrate to the system automation elements capable of minimize manual intervention. In this context, we can highlight two approaches, which are not mutually exclusive, being the adoption of autonomic computing elements and robotic process automation (RPA). These approaches tend to be more scalable, centralized, less error prone and to offer more opportunities to standardize process if compared to solutions that rely solely on the increase of human resources to support the operation, maintance and upgrade of the system. On that direction, we verified a set o limitation on the tools and available related work that contemplate some of the principles that characterize autonomic computing and robotic process automation elements. In this work, we present the iServiceReliabilityEngineer (iSRE) platform, on which we design and implement, via a prototype, the mechanisms that make feasible, in a practical way, some characteristics found in autonomic computing and RPA. That prototype is evaluate through a series of experiments. We also show a qualitative analysis that highlights the theme relevance and the interest by professionals that work with activities related to network and distributed systems management.",AchilesCaldeira.pdf,CIENCIA DA COMPUTACAO,ACHILES CALDEIRA QUINTELLA SANTANA JUNIOR,UNIVERSIDADE FEDERAL DE MINAS GERAIS,14/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Network Systems;Self-Management Computer Systems;Network Systems Management',-,RENATO ANTONIO CELSO FERREIRA,0,Sistemas em Rede;Auto-Gerenciamento de Sistemas de Computação;Gerenciamento de Sistemas em Rede,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"O gerenciamento de grandes sistemas em rede pode ser ainda mais desafiador sem o suporte de ferramentas de automação. Nesse trabalho apresentamos a plataforma iServiceReliabilityEngineer (iSRE), na qual projetamos e implementamos, por meio de um protótipo, os mecanismos que viabilizam de forma prática, no contexto de gerenciamento de sistemas em rede, algumas das características das abordagens de computação autônoma e robotic process automation. O iSRE é avaliado por meio de experimentos. Também apresentamos uma avaliação qualitativa a respeito da relevância do tema e do interesse de profissionais que atuam em atividades relacionadas a gerenciamento de sistemas em rede.",DISSERTAÇÃO,iSRE: PLATAFORMA DE DEFINIÇÃO E EXECUÇÃO DE PROCEDIMENTOS DE SUPORTE EM SISTEMAS EM REDE,5945602,1
"In beyond silicon research, Quantum-dot Cellular Automata (QCA) emerged as a candidate for replacing the traditional CMOS logic circuits. QCA is a paradigm based on the exchange of information between cells that takes advantage of Coulomb's law. In this work, we present techniques that expand and accelerate the validation of designs created using QCADesigner simulator. The circuit is split in small independent regions which could be verified for common mistakes such as cell misplacements and synchronization issues. Those small circuits chops could also be arranged for parallel simulation allowing speed ups of up to five times. The output from such devices could be used to build a model where logical prepositions could be evaluated. The entire process could vastly reduce the time for full circuitry validation.",,CIENCIA DA COMPUTACAO,LUIZ HENRIQUE BORGES SARDINHA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,14/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Quantum-dot Cellular Automata;QCA;Verification;Simulation;Paralelization;Nanocomputing',-,OMAR PARANAIBA VILELA NETO,0,Autômatos Celulares com Pontos Quânticos;QCA;Verificação;Simulação;Paralelização;Nanocomputação,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"No campo de pesquisa pós-sílicio, o paradigma Autômatos Celulares com Pontos Quânticos (do inglês Quantum-dot Cellular Automata - QCA) surgiu como candidato para substituir os circuitos tradicionais CMOS. O QCA se baseia na transmissão de informações entre células tirando proveito de interações Coulombianas. Neste trabalho apresenta-se técnicas que tentam expandir e acelerar as possibilidades de validação de um circuito originalmente descrito usando a ferramenta QCADesigner. O circuito é dividido em várias partições menores independentes que podem ser verificadas por falhas comuns como malposição e falta sincronia. Essas mesmas fatias também podem ser arranjadas em fatias simuladas em paralelo. O processo de simulação apresentado é aproximadamente cinco vezes mais veloz que simulações comuns e permite a criação de um modelo onde proposições lógicas podem ser avaliadas. O uso completo desse processo espera reduzir drasticamente o tempo necessário para validação completa de um circuito.",DISSERTAÇÃO,Simulação paralela e verificação de circuitos de autômatos celulares com pontos quânticos,5947395,1
"Along the history, many researchers provided remarkable contributions to science, not only advancing knowledge but also in terms of mentoring new scientists. Currently, identifying and studying the formation of new researchers over the years is a challenging task. Once the current repositories of theses and dissertations are cataloged in a decentralized way through many digital libraries spread across the web. In this dissertation, we built a large collection of genealogic trees that shows the relationships between advisors and advisees both in the master's and doctor's degree. The genealogic trees was build from data extracted from curricula of all doctors registered at the Lattes Platform until April 2017. To do that, we developed an algorithm capable process data of each curricula, disambiguating names and finding both the explicit and implicit advisement's relationships between all researchers present in each of the collected curricula. Our work also include analysis of the academic genealogic trees and as well the differences between trees of different areas of knowledge. For such purpose, we defined metrics to aid understanding of the structure and the evolution of each different genealogic tree built. Our results show that some of the trees are more remarkable than others in the Brazilian science, according to ours metrics and also we detected difference between the trees of different areas of knowledge. More important, the genealogic trees built can be accessed through a web portal open to the public and thus is possible to understand more the advancement and contributions in terms of advisement of new researchers.",WellingtonJosedaDores.pdf,CIENCIA DA COMPUTACAO,WELLINGTON JOSE DAS DORES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,21/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Complex Networks;Genealogy Trees;Lattes Platform',-,ALBERTO HENRIQUE FRADE LAENDER,0,Redes Complexas;Árvores Genealógicas;Plataforma Lattes,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Ao longo da história, muitos pesquisadores contribuíram de maneira notável para a ciência, não apenas no avanço do conhecimento, mas também na mentoria de novos pesquisadores. Atualmente, identificar e estudar a formação de novos pesquisadores ao longo dos anos é uma tarefa desafiadora. Uma vez que os repositórios atuais contendo dados sobre as orientações estão catalogados de maneira descentralizada através de diversos repositórios digitais espalhados pela web. Nesta dissertação, construímos uma enorme coleção de árvores genealógicas acadêmicas que mostram as relações entre orientador-orientando tanto no mestrado quanto no doutorado. As árvores foram construídas a partir de dados extraídos dos currículos de todos os doutores cadastrados na Plataforma Lattes até a data de abril de 2017. Para isso, desenvolvemos um algoritmo capaz de processar os dados de cada currículo, desambiguar nomes e encontrar as relações tanto explícitas quanto implícitas de orientação entre os pesquisadores.",DISSERTAÇÃO,Um estudo sobre a genealogia acadêmica brasileira,5947961,1
"In Bioinformatics, the study of biological information such as genes has multiple uses in different research fields. Among the supporting algorithms, the Basic Local Alignment Search Tool (BLAST) is a popular and widely used program among researches given it's capacity to find similarities between biological sequences, such as amino acids of proteins or nucleotides sequences of DNA. This work proposes the use of a new data structure to implement and improve BLAST. The proposed architecture makes use of the Binary Decision Diagrams (BDD) to store the biological sequences and optimize resources, saving time and space. This new version of BLAST in which the algorithm is enhanced with the use of BDDs allowed us to construct the alignment of biological sequences through a different perspective, with the use of a data model that provided gains up to 23.8% in search time and reductions up to 65.7% in memory usage using real biological sequences. All of this without altering the BLAST origina",,CIENCIA DA COMPUTACAO,DEMIAN FREDERICO BUENO DE OLIVEIRA,UNIVERSIDADE FEDERAL DE MINAS GERAIS,21/12/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'BDD Blast',-,SERGIO VALE AGUIAR CAMPOS,0,BLAST;BDD;Bioinformatics,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"In Bioinformatics, the study of biological information such as genes has multiple uses in different research fields. Among the supporting algorithms, the Basic Local Alignment Search Tool (BLAST) is a popular and widely used program among researches given it's capacity to find similarities between biological sequences, such as amino acids of proteins or nucleotides sequences of DNA. This work proposes the use of a new data structure to implement and improve BLAST. The proposed architecture makes use of the Binary Decision Diagrams (BDD) to store the biological sequences and optimize resources, saving time and space. This new version of BLAST in which the algorithm is enhanced with the use of BDDs allowed us to construct the alignment of biological sequences through a different perspective, with the use of a data model that provided gains up to 23.8% in search time and reductions up to 65.7% in memory usage using real biological sequences. All of this without altering the BLAST origina",DISSERTAÇÃO,BDDBLAST - A MEMORY AND TIME EFFICIENT ARCHITECTURE FOR PAIRWISE ALIGNMENTS,5948154,1
"The link aggregation technique is a solution to the link saturation problem. This technique combines several physical links to create a virtual link with the sum of the respective bandwidths. Since the use of Software Defined Networks (SDN) in enterprise environments increases every day, the purpose of this work is to allow the SDN controller to configure the aggregation of links. Therefore, we have defined and implemented an architecture that it is configurable, scalable, and autonomous, and dynamically aggregates links. Three link aggregation algorithms were evaluated: hash, traffic analysis, and virtual round-robin. Compared with the well known LACP protocol, the proposed system performs a better distribution of the bandwidth, allows more aggregated interfaces, and increases the throughput between two endpoints in our tests.",,CIENCIA DA COMPUTACAO,RONALDO RESENDE ROCHA JUNIOR,UNIVERSIDADE FEDERAL DE MINAS GERAIS,27/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Communication systems;Computer networks;LACP;Link Aggregation;Software defined networks',SISTEMAS DE COMPUTAÇÃO,MARCOS AUGUSTO MENEZES VIEIRA,0,Agregação de enlaces;Redes Definidas por Software;OpenVSwitch;Redes Programáveis;LACP;OpenFlow;Open vSwitch,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),Bolsa de Produtividade CNPq/2013: Redes sem fio e de sensores.,"A técnica de agregação de enlaces é uma solução para o problema de saturação do enlace. Essa técnica combina várias interfaces físicas para criar um enlace virtual, somando as respectivas bandas existentes. Visto que a utilização das Redes Definidas por Software (SDN) em ambientes coorporativos aumenta a cada dia, a proposta deste trabalho é permitir o controlador SDN configurar a agregação de enlaces de forma dinâmica, autônoma e e auto-adaptável Como forma de avaliar nossa implementação, criamos três algoritmos com diferente premissas de agregação de enlaces: tabela hash, análise de tráfego e round-robin virtual. Todas as implementações foram testadas em ambientes virtuais e reais. A proposta deste trabalho é apresentar os testes da técnica de agregação de enlaces em ambientes SDN, técnica que até o momento não é muito abordada no ambiente acadêmico. Com isso, através dos resultados obtidos espera-se promover uma discussão sobre o assunto em trabalhos futuros.",DISSERTAÇÃO,Agregação dinâmica de links em ambientes de redes definidas por software,5948330,1
"Dynamic search in specialized domains is a challenging task, in which systems must learn about the user's need interactively. Despite recent initiatives to advance the state-of-the-art, limited progress has been achieved, with the best systems only marginally improving upon ad-hoc search systems. In this dissertation, we analyze the impact of several components of a typical dynamic search system on the effectiveness of the entire system. Through simulations, we discuss the impact of producing an initial ranking of documents, modeling the aspects underlying the user's query given his or her feedback, leveraging these aspects to improve the initial ranking, and deciding when to stop the interaction. In addition, we provide preliminary results on practical instantiations of each component. Our results using data from the TREC 2015-2016 Dynamic Domain track shed light on these components and provide directions for the design of effective dynamic search systems for specialized domains.",FelipeMoraesGomes.pdf,CIENCIA DA COMPUTACAO,FELIPE MORAES GOMES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,20/04/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Busca Dinmica;Eficcia de Sistemas de Busca',-,RODRYGO LUIS TEODORO SANTOS,0,Busca Dinâmica;Eficácia de Sistemas de Busca,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Busca dinâmica em domínios especializados e uma tarefa desafiadora, em que sistemas devem aprender sobre a necessidade do usuário através da sua exploração interativa. Apesar das recentes iniciativas para melhorar o estado da arte para essa tarefa, avanços limitados foram alcançados até então, com os melhores sistemas de busca dinâmica conseguindo melhorias marginais em relação aos sistemas de busca ad-hoc. Nesta dissertação, nós realizamos uma análise abrangente do impacto de vários componentes de um sistema de busca dinâmico genérico sobre a eficácia de todo o sistema. Através de uma série de simulações, discutimos o impacto da produção de um ranking inicial de documentos candidatos, da modelagem dos possíveis aspectos subjacentes a consulta do usuário com seu feedback, da utilização dos aspectos modelados para dinamicamente reordenar o ranking inicial de candidatos e da decisão de quando parar o processo interativo. Além disso, apresentamos resultados preliminares de instanciações praticas do componente de reranking dinâmico por meio de abordagens para diversificação interativa de resultados da busca. Nossos resultados usando dados das coleções de teste da TREC 2015-2016 Dynamic Domain track demonstram o impacto de cada componente e apresentam diretrizes para o projeto de sistemas eficazes para busca dinâmica em domínios especializados.",DISSERTAÇÃO,On effective dynamic search systems,6052176,1
"Architectural erosion denotes the gap observed between planned and implemented software architectures. The problem might be more severe in systems implemented in dynamically typed languages. The reasons are twofold: (i) some features provided by such languages make it more easy to break the planned architecture, and (ii) dynamic languages still lack tool support for architecture conformance. In this dissertation, we propose an architectural conformance and visualization approach for Ruby, a widely used dynamic language. The proposed solution, called ArchRuby, relies on static code analysis and includes a simple type inference heuristic. We evaluate ArchRuby in three real-world systems, including a system used by a major Brazilian telecommunication company. Using ArchRuby, we were able to identify 48 architectural violations in these systems. We also show that ArchRuby correctly infers 58% of the types of Ruby systems, when compared to dynamic techniques.",SergioHenriqueMirandaJr.pdf,CIENCIA DA COMPUTACAO,SERGIO HENRIQUE MIRANDA JUNIOR,UNIVERSIDADE FEDERAL DE MINAS GERAIS,29/03/2017,INGLES,UNIVERSIDADE FEDERAL DE MINAS GERAIS,b'Conformidade arquitetural;Linguagens dinmicas;Visualizao arquitetural de alto nvel',-,MARCO TULIO DE OLIVEIRA VALENTE,0,Conformidade arquitetural;Linguagens dinâmicas;Visualização arquitetural de alto nível,CIÊNCIAS DA COMPUTAÇÃO (32001010004P6),-,"Architectural erosion denotes the gap observed between planned and implemented software architectures. The problem might be more severe in systems implemented in dynamically typed languages. The reasons are twofold: (i) some features provided by such languages make it more easy to break the planned architecture, and (ii) dynamic languages still lack tool support for architecture conformance. In this dissertation, we propose an architectural conformance and visualization approach for Ruby, a widely used dynamic language. The proposed solution, called ArchRuby, relies on static code analysis and includes a simple type inference heuristic. We evaluate ArchRuby in three real-world systems, including a system used by a major Brazilian telecommunication company. Using ArchRuby, we were able to identify 48 architectural violations in these systems. We also show that ArchRuby correctly infers 58% of the types of Ruby systems, when compared to dynamic techniques.",DISSERTAÇÃO,Archruby: architecture conformance checking in dynamically typed languages,6054941,1
"The difficulties and limitations of the Learning Management Systems (LMS) in interoperating with other technologies have been a recurrent topic in the literature. It is known that even with all the support which is offered by these environments in conducting the learning activities, there is a gap related to the perception of teachers and tutors over activities done by the learners when they are out of the environments. Before this scenario, the regular use of instruments or tools used by them allowed that an emerging concept named Personal Learning Environments (PLE) to arise and to be characterised as an educational context. This context allows learners to execute learning activities independent from the ones planned in the formal learning environments; providing them autonomy to the execution and management of their own learning activities. Teachers and tutors of LMS based courses have only been using the available tools in the LMS to accompaniment and evaluation of their learners. Furthermore, they often use formative assessment paradigm in order to investigate the learners development based exclusively in the activities done in the LMS field. In that case, this thesis aimed the conception, development and evaluation of a service named “Formative Accompaniment Service”, which purpose is to identify informal learning activities that are done outside the LMS environments which are able to be integrated to the evaluation process or even activity accompaniment. The methodology proposed was designed under the principles and recommendations of the “Service Design Thinking” which are made of four phases, as it follows: exploration, creation, reflection and implementation. The participants involved in the process of conception and development added up to 134 users, 75 teachers and tutors and 49 learners, they belonged to different institutions and specialised discussion groups in Brazil. The evaluation of the service was divided into three phases: the first evaluated the expectation, in the second it was proposed five tasks to be executed by each evaluator, and eventually, in the third, it was evaluated the experience to use the service. This process counted with 13 experts that examined 28 items about the proposed service. The results got with it assure that the proposed service showed a satisfactory level of consistence related to the interest of teachers and tutors in doing it in their activities . Moreover, it allows that as using the service they are able to reflect over their actions as teachers and tutors and, in this way, strive for assertive actions in conducting a discipline or course, in an individual or collective point of view. The improving recommendations resulting of the evaluation were implemented, allowing the creation of a new version of service aligned with the real necessities of teachers and tutors.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,IVANILDO JOSE DE MELO FILHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,16/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'LMS. PLE. Service. Informal Learning. Formative Accompaniment.',MÍDIA E INTERAÇÃO,ALEX SANDRO GOMES,349,LMS. PLE. Serviço. Aprendizagem Informal. Acompanhamento Formativo.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"As dificuldades e as limitações dos Learning Management Systems (LMS) em interoperar com outras tecnologias tem sido um tema recorrente na literatura. É sabido que mesmo com todo o suporte oferecido por esses ambientes na condução das atividades de aprendizagem, existe uma lacuna relacionada à percepção dos professores e tutores sobre as atividades que são realizadas pelos aprendizes quando estes encontram-se fora deles. Diante desse cenário, o frequente uso de instrumentos ou ferramentas utilizadas por estes permitiram que um conceito emergente denominado Personal Learning Environments (PLE) surgisse e fosse caracterizado como um contexto educacional. Esse contexto permite aos aprendizes a executarem atividades de aprendizagem independente das planejadas nos ambientes formais de ensino fornecendo a eles autonomia para a execução e gerenciamento de suas atividades de aprendizagem. Professores e tutores de cursos baseados em LMS têm utilizado unicamente as ferramentas disponíveis nos LMS para o acompanhamento e avaliação dos aprendizes. Ademais, esses utilizam frequentemente o paradigma da avaliação formativa no intuito de averiguar o desempenho dos aprendizes baseado exclusivamente nas atividades realizadas dentro dos LMS. Sendo assim, esta tese tem como objetivo a concepção, desenvolvimento e avaliação de um serviço denominado de “Serviço de Acompanhamento Formativo”, cujo propósito é identificar atividades de aprendizagem informais que são realizadas fora dos ambientes LMS que podem ser integradas ao processo de avaliação ou de acompanhamento de atividades. A proposta metodológica foi delineada sob os princípios e recomendações do Design Thinking de Serviços formada por quatro fases a saber: exploração, criação, reflexão e implementação. Os participantes envolvidos no processo de concepção e desenvolvimento foram de 134 usuários, sendo 75 professores e tutores e 49 aprendizes, todos pertencentes a diferentes instituições e grupos de discussão especializados no Brasil. A avaliação do serviço foi dividida em três fases: a primeira avaliou a expectativa, na segunda foram propostas cinco tarefas a serem executadas por cada avaliador, e por fim, na terceira foi a avaliada a experiência do uso do serviço. Esse processo contou com 13 especialistas onde foram examinados 28 itens sobre o serviço proposto. Os resultados obtidos atestam que o serviço proposto apresentou um grau satisfatório de consistente em relação ao interesse de professores e tutores a fazerem uso em suas atividades. Além disso, permite que os mesmos possam, por meio do serviço, refletirem sobre suas ações docentes e de tutoria e, assim diligenciar ações assertivas na condução de uma disciplina ou curso, sejam estas do ponto de vista individual ou coletivo. As recomendações de melhoria resultantes da avaliação foram implementadas, permitindo a geração de uma nova versão do serviço alinhada as necessidades reais dos professores e tutores.",TESE,FORMATIVE ACCOMPANIMENT SERVICE IN E-LEARNING: INTEGRATION BETWEEN LMS AND PLE (SERVIÇO DE ACOMPANHAMENTO FORMATIVO NO E-LEARNING: INTEGRAÇÃO ENTRE LMS E PLE),5012400,1
"Academy and industry have been showing that the limited resources of mobile devices
might be supplemented by virtualized resources in a cloud infrastructure, emerging a new paradigm
called mobile cloud computing (MCC). While this solution substantially expands the
abilities of such gadgets, it also enforces a full-time dependency on wireless Internet connection.
Furthermore, issues such as battery charge depletion, mobile device faults, wireless network
instability, application bugs, and outages in the cloud service may represent obstacles in expansion
of the mobile cloud computing, since such issues may result in poor provision of the MCC
service. Hierarchical stochastic models are suitable for a concise description of the operation of
an MCC infrastructure, dealing with the large number of components that constitute this type
of system. Being such a recent paradigm, few efforts were conduced to identify the impact of
those types of faults on the system operation. In this way, this thesis provides stochastic models
to planning of mobile cloud computing infrastructures. This approach focus specially on the
impact of some factors on system availability and reliability, such as: different architectures of
mobile cloud computing; characteristics of communication protocols; critical components of
mobile devices, such as the battery; and the behavior of the software involved. Our evaluation
is based on hierarchical heterogeneous models and focuses on measures such as steady-state
availability, reliability, downtime, connectivity probability, average battery lifetime, and costs
of implementation and provisioning of the cloud infrastructure. We performed experiments to
provide subsidies to the input parameters of the proposed models here. In addition, software
tools have been developed to aid in the conduct of such experiments. The proposed models
allow comparisons between different forms of mobile cloud computing deployment, as well as
planning adjustments to the hardware and software infrastructure to ensure satisfactory service
levels.",,ARQUITETURA DE COMPUTADORES E SISTEMAS DIGITAIS,JEAN CARLOS TEIXEIRA DE ARAUJO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,09/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Mobile cloud computing. Stochastic models. Infrastructure planning. Battery lifetime.',SISTEMAS DIGITAIS,PAULO ROMERO MARTINS MACIEL,150,Mobile cloud computing. Modelos estocásticos. Planejamento de infraestruturas. Tempo de vida da bateria.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"academia e a indústria têm demonstrado que os recursos limitados dos dispositivos
móveis podem ser complementados por recursos virtualizados em uma infraestrutura de computação
em nuvem, surgindo assim um novo paradigma chamado mobile cloud computing (MCC).
Embora esta solução expanda substancialmente a capacidade de tais dispositivos, também impõe
uma dependência em tempo integral de conexão de Internet sem fio. Além disso, problemas
como o esgotamento da carga da bateria, falhas de dispositivos móveis, instabilidade das redes
sem fio, bugs de aplicativos e interrupções no serviço da nuvem podem representar obstáculos
na expansão da mobile cloud computing, uma vez que tais problemas podem resultar no
mal fornecimento do serviço da MCC. Modelos estocásticos hierárquicos são adequados para
descrever de forma concisa o funcionamento de uma infraestrutura de MCC, lidando com o
grande número de componentes que constituem esse tipo de sistema. Sendo um paradigma
tão recente, poucos esforços foram feitos para identificar os impactos destes tipos de falhas
sobre o funcionamento do sistema. Desta forma, esta tese provê modelos estocásticos para o
planejamento de infraestruturas de mobile cloud computing. Esta abordagem foca especialmente
no impacto de alguns fatores sobre a disponibilidade e confiabilidade do sistema, tais como:
arquiteturas distintas de mobile cloud computing; características de protocolos de comunicação;
componentes críticos dos dispositivos móveis, como a bateria; e o comportamento dos softwares
envolvidos. A avaliação adotada é baseada em modelos heterogêneos hierárquicos e se concentra
em métricas como a disponibilidade em estado estacionário, confiabilidade, downtime,
probabilidade de conectividade, tempo médio de vida da bateria, e os custos de implantação e
provisionamento da infraestrutura de nuvem. Os experimentos realizados fornecem subsídios aos
parâmetros de entrada dos modelos aqui propostos. Além disso, ferramentas de software foram
desenvolvidas para auxiliar na condução de tais experimentos. Os modelos propostos permitem
realizar comparações entre diferentes formas de implantação de mobile cloud computing, assim
como planejar ajustes na infraestrutura de hardware e software com o intuito de garantir níveis
de serviço satisfatórios.",TESE,PLANEJAMENTO DE INFRAESTRUTURAS DE MOBILE CLOUD COMPUTING BASEADO EM MODELOS ESTOCÁSTICOS,5018000,1
"The toy and game industries invest in hybrid play products. In these scenarios, the user interacts with the system through toys, using them both as input and output information, giving the toys the character of a playful interface for the user. Hybrid systems for entertainment are complex artifacts for using real and virtual information, presenting new challenges for designers, programmers and engineers. Adopting approaches that consider hybrid design from the concept of the products, can bring benefits both for the toy industry and for the gaming industry. The main objective of this research is to propose tools to facilitate the development of interactive solutions in the context of hybrid recreational systems. A systematic review of the literature covering articles published between 2008 and 2016 was carried out and expanded with the market contribution that mapped 120 hybrid recreational products. A relational model for interaction with hybrid gameplay was conceived and comprises interactive aspects relating three entities: things, the environment and people. The evaluation of the proposal consisted in applying the model as a practical tool for the creation of new hybrid leisure systems, in a course of graduation course lasting 16 weeks. The evaluation made it possible to propose improvements in the structure and nomenclature of the model. The proposed model synthesizes complex information from the ludic interface through a set of interactive aspects, allowing the user to conceptualize, plan, and document the hybrid systems. The application of the model as a creative tool allowed the development of six functional prototypes of connected games and toys, including the conceptual modeling of hybrid games and low fidelity prototypes. Summarized the contributions, the research pointed out topics for a future methodological approach.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,ANNA PRISCILLA DE ALBUQUERQUE,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/07/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'tangible user interfaces;hybrid games;playful interfaces;smart toys',MÍDIA E INTERAÇÃO,JUDITH KELNER,146,tangible user interfaces;hybrid games;playful interfaces;smart toys,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"As indústrias de brinquedos e jogos investem em produtos lúdicos híbridos. Nestes cenários, o usuário interage com o sistema através de brinquedos, utilizando-os tanto como entrada e saída de informação, atribuindo aos brinquedos o caráter de uma interface lúdica para o usuário. Sistemas híbridos para entretenimento são artefatos complexos por utilizarem informações reais e virtuais, apresentando novos desafios para designers, programadores e engenheiros. Adotar abordagens que considerem o design híbrido desde a conceituação dos produtos, poderá trazer benefícios tanto para a indústria de brinquedos como para a de jogos. O principal objetivo desta pesquisa é propor ferramentas para facilitar o desenvolvimento de soluções interativas no contexto de sistemas híbridos lúdicos. Uma revisão sistemática da literatura cobrindo artigos publicados entre 2008 e 2016 foi realizada e  ampliada com a contribuição mercadológica que mapeou 120 produtos híbridos lúdicos.  Um modelo relacional para interação com gameplay híbrido foi concebido e compreende aspectos interativos relacionando três entidades: as coisas, o ambiente e as pessoas. A avaliação da proposta consistiu em aplicar o modelo como ferramenta prática para a criação de novos sistemas híbridos lúdicos, em uma disciplina de curso de graduação com duração de 16 semanas. A avaliação viabilizou propor melhorias na estrutura e nomenclatura do modelo. O modelo proposto sintetiza informações complexas da interface lúdica através de um conjunto de aspectos interativos, permitindo ao usuário conceituar, planejar, e documentar os sistemas híbridos. A aplicação do modelo como uma ferramenta criativa permitiu o desenvolvimento de seis protótipos funcionais de jogos e brinquedos conectados, incluindo a modelagem conceitual de jogos híbridos e protótipos de baixa-fidelidade. Sumarizadas as contribuições, a pesquisa apontou tópicos para uma futura abordagem metodológica.",DISSERTAÇÃO,IOT4FUN: A RELATIONAL MODEL FOR HYBRID GAMEPLAY INTERACTION,5077300,1
"The Internet, so far, has evolved through ""patches"" and these changes made in the architecture are aimed at meeting new applications needs. We highlight the limitations of addressing and the need to implement quality of service (QoS) mechanisms especially for real-time applications. Due to these problems and the difficulties to meet new applications demands, new studies are being done with the aim of creating the ""Internet of the Future"" proposing new network architectures with the purpose of solving these problems still not solved. Among these new studies of network architectures for the Internet of the Future is RINA (Recursive InterNetworking Architecture) whose basic principle is InterProcess Communication (IPC). It is considered a clean slate architecture that aims to be an alternative to the conventional Internet model. This dissertation presents a comparative study of the performance of the data transfer services in RINA and TCP/IP architectures. The metric investigated and analyzed refers to the Round-Trip-Time (RTT) times of the data streams in packet transmission. To perform this study, factors such as bandwidth, packet sizes, and propagation delays are assessed applying the systematic approach of Raj Jain (1991). The results lead to the conclusion that the RINA architecture presents RTTs 3.71% better than the traditional architecture for the short distance networks, whereas for the long distance networks this improvement was of 9.81%. In the short distance networks, the improvement was smaller due to the smaller response times which makes the congestion controls the same for the two architectures. On the other hand, in the long distance networks the study concluded that RINA had a more efficient behavior in the transmission of packets with large propagation delays, because the control mechanisms in the RINA more effectively solve the congestion problem in the network due to its recursive nature.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,DJALMA ALMEIDA LIMA FILHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,30/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'RINA;TCP/IP, short distance networks, long distance networks, Data Transfer.'",REDES DE COMPUTADORES,JOSE AUGUSTO SURUAGY MONTEIRO,106,"INA, TCP/IP, Rede de pequena distância, Rede de longa distância, Transferência de Dados.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Ambiente de Monitoração para Redes para Experimentação e Novas Arquiteturas de Rede,"A Internet, até o momento, tem evoluído através de “remendos” e essas modificações realizadas na arquitetura têm como finalidade atender novas necessidades das aplicações. Destacamos as limitações do endereçamento e a necessidade de implementação de mecanismos de qualidade de serviço (QoS) sobretudo para as aplicações de tempo real. Devido a estes problemas e às dificuldades para atender novas demandas das aplicações, novos estudos estão sendo feitos com intuito de criar a “Internet do Futuro”, propondo novas arquiteturas de redes com a finalidade de resolver esses problemas ainda não solucionados. Dentre essas novas arquiteturas de redes para Internet do Futuro, encontra-se a RINA (Recursive InterNetworking Architecture) cujo princípio básico é a comunicação entre processos ou IPC (InterProcess Communication) e considerada uma arquitetura clean slate que tem como objetivo ser uma alternativa frente ao modelo convencional da Internet. Esta dissertação apresenta um estudo comparativo do desempenho do serviço de transferência de dados nas arquiteturas RINA e TCP/IP. A métrica investigada e analisada refere-se aos tempos de ida-e-volta (RTT – Round-Trip-Time) dos fluxos de dados na transmissão de pacotes. Para realizar o estudo, fatores como largura de banda, tamanhos de pacote e atrasos de propagação para os cenários de redes de pequena e longa distância são avaliados utilizando a abordagem sistemática de Raj Jain (1991). Os resultados levam à conclusão de que a arquitetura RINA apresenta RTTs 3,71% melhores em relação à arquitetura tradicional para as redes de pequena distância, enquanto que para as redes de longa distância essa melhora foi de 9,81%. Nas redes de pequena distância, a melhora foi menor por conta dos tempos de resposta menores o que torna os controles de congestionamento basicamente os mesmos para as duas arquiteturas. Por outro lado, nas redes de longa distância o estudo concluiu que a RINA teve um comportamento mais eficiente na transmissão de pacotes com grandes atrasos de propagação, porque os mecanismos de controle na RINA resolvem com mais eficácia o problema de congestionamento na rede devido à sua natureza recursiva.
R",DISSERTAÇÃO,Estudo comparativo do desempenho da transferência de dados nas arquiteturas RINA e TCP/IP,5135100,1
"Thousands of people in the world suffer from some mental disorder, such as depression or bipolar disorder, facing difficulties to tackle the challenges of everyday life. In the worst case, these conditions may lead to suicide. Although there are treatments for most of these problems, the majority of the affected persons do not seek help, due to lack of financial conditions or to the stigma surrounding these diseases. In order to help the people, some countries maintain governmental programs which offer support systems and supporting groups on the internet. In internet-based groups, it is possible to find hundreds of testimonials (personal letters) that convey experiences of people who have undergone or are undergoing these conditions. The aim is to inspire, motivate and show to people who suffer from mental disorders that they are not the only ones to face these challenges, and that is possible to deal with these problems and find proper treatment. However, in most cases finding relevant testimony to the context of a particular reader is a time-consuming and difficult task, as there are hundreds of them on the internet, and usually the texts are long and unstructured. In this scenario, Recommendation Systems are an interesting alternative to provide a personalized service for each particular reader. These systems automatically provide indications of items according to the particular interest of a user (or group of users). This MSc project developed a Text Recommendation System to assist people suffering from mental disorders. The recommended texts (testimonials) were collected from various internet sources and were stored in a local repository. The system was implemented based on information filtering techniques, consisting of two modules: content-based filtering and collaborative filtering. The experiments results were satisfactory, showing a high correlation between the evaluation given by the user and the evaluation given by the system for the same testimony.",,COMPUTAÇÃO INTELIGENTE,RAISA BRITO COSTA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,25/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Recommendation Systems. Content-Based Filtering. Collaborative Filtration. Text Processing. Mental Disorders. Depression. Anxiety. Bipolar disorder. Personal Testimonials.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,FLAVIA DE ALMEIDA BARROS,112,Sistemas de Recomendação. Filtragem baseada em Conteúdo. Filtragem Colaborativa. Processamento de Texto. Transtornos Mentais. Depressão. Ansiedade. Transtorno Bipolar. Depoimentos Pessoais.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Desenvolvimento de Chatterbots Agentes Conversacionais Incorporados em Jogos Sérios,"Milhares de pessoas no mundo sofrem de algum transtorno mental, como depressão ou transtorno bipolar, apresentando dificuldades de enfrentar os desafios do cotidiano.  No pior caso, essas condições levam as pessoas ao suicídio. Apesar de existirem tratamentos para a maioria desses problemas, a maior parte das pessoas afetadas não busca ajuda, por falta de condições financeiras ou devido ao estigma que rodeia essas doenças. Em virtude disso, existem programas mantidos por Governos de alguns países, como sistemas de apoio a pessoas nessas condições e grupos de apoio na internet. Nos grupos baseados na internet, é possível encontrar centenas de depoimentos que relatam experiências de pessoas que passaram ou estão passando por essas condições, buscando inspirar, motivar e mostrar para pessoas que sofrem que elas não são as únicas a enfrentar esses desafios, e que é possível conviver com esses problemas e tratá-los. Contudo, na maioria dos casos, encontrar um depoimento relevante ao contexto de um leitor particular é uma tarefa demorada e difícil, visto que existem centenas deles na internet e, na sua grande maioria, são textos longos e desestruturados. Nesse cenário, sistemas de recomendação são uma alternativa interessante para oferecer um serviço personalizado para cada leitor. Sistemas de recomendação fornecem automaticamente indicações personalizadas de itens, de acordo com o interesse particular de um usuário (ou grupo de usuários). Este projeto de mestrado desenvolveu um Sistema de Recomendação de Texto para auxiliar pessoas que sofrem de transtornos mentais. Os textos (depoimentos) recomendados foram coletados de diversas fontes da internet e estão armazenados em um repositório local. O sistema foi implementado utilizando técnicas de filtragem de informação, tendo dois módulos: um de filtragem baseada em conteúdo, e outro de filtragem colaborativa. Os resultados obtidos com os testes foram satisfatórios, apresentando uma forte correlação entre a avaliação dada pelo usuário e a avaliação dada pelo sistema para o mesmo depoimento.",DISSERTAÇÃO,REC-PSI: Um Sistema de Recomendação de Textos para Auxiliar Pessoas que Sofrem de Transtornos Mentais,5713600,1
"Activities in hazardous environments can be accomplished through the
teleoperation of robots. However, the absence of situational awareness during
the remote control of robots can lead to an increase in the difficulty in performing
tasks.
In order to identify the interface assistance needs, the use of augmented reality,
associated to information design to create a mixed reality interface is investigated
in this dissertation as an alternative to assist in visualization problem for
manipulation tasks. The survey of the state of the art was done through a
literature review, in wich, 36 articles were selected from 893 articles analyzed.
Aiming the identification of the user needs for interface aids, the robot
manipulation task was analyzed and described in six steps: Identification,
Handling, Localization, Intervention, Adjustment, and Inspection. Visual elements
in augmented reality were developed for each step of the task.
An interface prototype was developed for robots' teleoperation with the purpose
of evaluating the elements in user tests. The tests took place in a controlled
scenario where users performed three tasks of increasing complexity. 31
volunteers participated in the tests to evaluate usability through questionnaires
validated in the literature.
The results indicated positive results in terms of clarity, ease of visualization, and
ease of finding the necessary information, with 90.3% approval ratings. The
visual solutions used in the creation of the graphic elements of this dissertation
can be adapted for other robotic tasks.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,CAROLINA CANI DIAS LEDEBOUR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,21/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Human-robot interaction, design, teleoperation, advanced intefaces'",MÍDIA E INTERAÇÃO,JUDITH KELNER,117,"Interação Humano-robô, design, tele operação, intefaces avançadas",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Atividades em ambientes que apresentem risco aos humanos podem ser realizadas através da tele operação de robôs. Entretanto, a ausência de consciência situacional durante o controle remoto de robôs pode ocasionar no aumento da dificuldade na execução de tarefas..
Com o objetivo de identificar as necessidades de auxílio de interface, o uso de realidade aumentada, associado ao design da informação para criação de uma interfaces mistas é investigado nesta dissertação como alternativa para auxiliar neste problema de visualização para tarefas de manipulação. O levantamento do estado da arte foi feito através de uma revisão da literatura, onde 36 artigos foram selecionados a partir de 893 artigos analisados.
Com o objetivo de identificar as necessidades do usuário de auxílio de interface, a tarefa de manipulação de robôs foi analisada e descrita em seis etapas : Identificação, Manejo, Localização, Intervenção, Ajuste e Inspeção. Elementos visuais em realidade aumentada foram desenvolvidos para cada etapa da tarefa.
Foi elaborado um protótipo de interface para tele operação de robôs com o objetivo de avaliar os elementos em testes com usuários. Os testes aconteceram em um cenário controlado onde os usuários executaram três tarefas de grau de complexidade crescente. Participaram dos testes 31 voluntários para avaliar a usabilidade através de questionários validados pela literatura.
Os resultados alcançados demonstraram resultados positivos nos quesitos de clareza, facilidade de visualização, e facilidade de encontrar as informações necessárias, com índices de 90,3% de aprovação. As soluções visuais utilizadas na criação dos elementos gráficos desta dissertação podem ser adaptados para tarefas robóticas de outras naturezas.",DISSERTAÇÃO,DESIGN DE INFORMAÇÃO APLICADO AO DESENVOLVIMENTO DE INTERFACE DE REALIDADE AUMENTADA PARA INTERAÇÃO HUMANO-ROBÔ REMOTA,5005621,1
"Shape classification has multiple applications. In real scenes, shapes may contain severe
occlusions, hardening the identification of objects. In this work, a bayesian framework for
object recognition under severe and varied conditions of occlusion is proposed. The proposed
framework is capable of performing three main steps in object recognition: representation of parts,
retrieval of the most probable objects and hypotheses validation for final object identification.
Occlusion is dealt with separating shapes into parts through high curvature points, then tangent
angle signature is found for each part and continuous wavelet transform is calculated for each
signature in order to reduce noise. Next, the best matching object is retrieved for each part
using Pearson’s correlation coefficient as query prior, indicating the similarity between the
part representation and of the most probable object in the database. For each probable class,
an ensemble of Hidden Markov Model (HMM) is created through training with the one-class
approach. A sort of search space retrieval is created using class posterior probability given by
the ensemble. For occlusion likelihood, an area term that measure visual consistency between
retrieved object and occlusion is proposed. For hypotheses validation, a area constraint is set
to enhance recognition performance eliminating duplicated hypotheses. Experiments were
carried out employing several real world images and synthetical generated occluded objects
datasets using shapes of CMU_KO and MPEG-7 databases. The MPEG-7 dataset contains 1500
test shape instances with different scenarios of object occlusion with varied levels of object
occlusion, different number of object classes in the problem, and different number of objects in
the occlusion. For real images experimentation the CMU_KO challenge set contains 8 single
view object classes with 100 occluded objects per class for testing and 1 non occluded object per
class for training. Results showed the method not only was capable of identifying highly occluded
shapes (60%-80% overlapping) but also present several advantages over previous methods. The
minimum F-Measure obtained in MPEG-7 experiments was 0.67, 0.93 and 0.92, respectively
and minimum AUROC of 0.87 for recognition in CMU_KO dataset, a very promising result
due to complexity of the problem. Different amount of noise and varied amount of search
space retrieval visited were also tested to measure framework robustness. Results provided
an insight on capabilities and limitations of the method, demonstrating the use of HMMs for
sorting search space retrieval improved efficiency over typical unsorted version. Also, wavelet
filtering consistently outperformed the unfiltered and sampling noise reduction versions under
high amount of noise.",,COMPUTAÇÃO INTELIGENTE,FIDEL ALEJANDRO GUERRERO PENA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,22/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Severe occlusion. Hidden Markov Model. Wavelet Transform. Object Recognition.',REDES NEURAIS,GERMANO CRISPIM VASCONCELOS,108,Oclusão severa;Modelo Escondido de Markov. Transformada de Wavelet. Reconhecimento de Objetos.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A classificação da forma tem múltiplas aplicações. Em cenas reais, as formas podem
conter oclusões severas, tornando difícil a identificação de objetos. Neste trabalho, propõe-se
uma abordagem bayesiana para o reconhecimento de objetos com oclusão severa e em condições
variadas. O esquema proposto é capaz de realizar três etapas principais no reconhecimento
de objetos: representação das partes, recuperação dos objetos mais prováveis e a validação de
hipóteses para a identificação final dos objetos. A oclusão é tratada separando as formas em
partes através de pontos de alta curvatura, então a assinatura do ângulo tangente é encontrada
para cada parte e a transformada contínua de wavelet é calculada para cada assinatura reduzindo
o ruído. Em seguida, o objeto mais semelhante é recuperado para cada parte usando o coeficiente
de correlação de Pearson como prior da consulta, indicando a similaridade entre a representação
da parte e o objeto mais provável no banco de dados. Para cada classe provável, um sistema
de múltiplos classificadores com Modelos Escondido de Markov (HMM) é criado através de
treinamento com a abordagem de uma classe. Um ordenamento do espaço de busca é criada
usando a probabilidade a posterior da classe dada pelos classificadores. Como verosimilhança de
oclusão, é proposto um termo de área que mede a consistência visual entre o objeto recuperado
e a oclusão. Para a validação de hipóteses, uma restrição de área é definida para melhorar
o desempenho do reconhecimento eliminando hipóteses duplicadas. Os experimentos foram
realizados utilizando várias imagens do mundo real e conjuntos de dados de objetos oclusos
gerados de forma sintética usando formas dos bancos de dados CMU_KO e MPEG-7. O conjunto
de dados MPEG-7 contém 1500 instâncias de formas de teste com diferentes cenários de oclusão
por exemplo, com vários níveis de oclusões de objetos, número diferente de classes de objeto
no problema e diferentes números de objetos na oclusão. Para a experimentação de imagens
reais, o desafiante conjunto CMU_KO contém 8 classes de objeto na mesma perspectiva com
100 objetos ocluídos por classe para teste e 1 objeto não ocluso por classe para treinamento. Os
resultados mostraram que o método não só foi capaz de identificar formas altamente ocluídas
(60% - 80% de sobreposição), mas também apresentar várias vantagens em relação aos métodos
anteriores. A F-Measure mínima obtida em experimentos com MPEG-7 foi de 0.67, 0.93 e
0.92, respectivamente, e AUROC mínimo de 0.87 para o reconhecimento no conjunto de dados
CMU_KO, um resultado muito promissor devido à complexidade do problema. Diferentes
quantidades de ruído e quantidade variada de espaço de busca visitado também foram testadas
para medir a robustez do método. Os resultados forneceram uma visão sobre as capacidades
e limitações do método, demonstrando que o uso de HMMs para ordenar o espaço de busca
melhorou a eficiência sobre a versão não ordenada típica. Além disso, a filtragem com wavelets
superou consistentemente as versões de redução de ruído não filtradas e de amostragem sob
grande quantidade de ruído.",DISSERTAÇÃO,A BAYESIAN FRAMEWORK FOR OBJECT RECOGNITION UNDER SEVERE OCCLUSION,5006594,1
"The large amount of data available on the Web along with the ease access and representation
of these data create new challenges for those who wish to share data on the Web. In
general, there is no prior knowledge between the interests of who share the data, called data
producers, and the interests of who use, called data consumers. In this context, W3C proposed
a recommendation, called Data on the Web Best Practices (DWBP), that aims a common understanding
between data producers and data consumers. The DWBP deals with several aspects
related to sharing data on the Web, such as data format, data access, data identifiers and metadata.
Besides, over the years, a broad of solutions have been developed for the purpose of publishing
and sharing data on the Web. However, current data publishing solutions, which are responsible
for providing data catalogs and maintaining the communication interface between data producers
and data consumers, do not implement much of the guidelines proposed by the DWBP. Given
the lack of solutions that allow the management of shared data on the Web, this work has as
main objective to propose an architectural model for a Data on the Web Management System
(DWMS). We also identify the main requirements that a DWMS must meet to overcome the
limitations of existing solutions. In addition, we developed a proof of concept and we propose a
collection of services that aim to facilitate the definition, creation, maintenance, manipulation
and sharing of datasets on the Web among users and applications.",,BANCO DE DADOS,LAIRSON EMANUEL RODRIGUES DE ALENCAR OLIVEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,03/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Data Catalog. Best Practices. Data on the Web. DWMS. Services',BANCO DE DADOS,BERNADETTE FARIAS LOSCIO,114,Catalogação de Dados. Boas Práticas. Dados na Web. SGDW. Serviços,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A grande quantidade de dados disponível na Web, juntamente com a facilidade de
acesso e representação desses dados, criam novos desafios tanto para quem deseja publicar e
compartilhar dados na Web quanto para os que desejam usar tais dados. De modo geral, não
existe um conhecimento prévio entre os interesses de quem publica os dados, os produtores,
e os interesses de quem consome os dados, os consumidores. Nesse contexto, recentemente
foi proposta, pelo W3C, uma recomendação para Dados na Web, que busca um entendimento
comum entre os produtores e os consumidores e discursa sobre diferentes aspectos relacionados
ao compartilhamento de dados na Web, como formatos de dados, acesso, identificadores e
metadados. Ao longo dos anos, várias soluções foram desenvolvidas objetivando a publicação
e o compartilhamento desses dados na Web. No entanto, as soluções de publicação de dados
atuais, que são responsáveis por prover catálogos de dados e manter a interface de comunicação
entre os produtores e consumidores, não implementam boa parte das orientações propostas
pelo W3C como boas práticas. Dado que existe uma carência de soluções que possibilitem o
gerenciamento adequado dos dados compartilhados na Web, esta dissertação tem como principal
objetivo propor um modelo de arquitetura para um Sistema de Gerenciamento de Dados na Web
(SGDW). Pretende-se identificar os principais requisitos que um sistema desse tipo deve atender
para prover soluções para as limitações encontradas. Além disso, é proposta uma coleção de
serviços que visam facilitar a definição, criação, manutenção, manipulação e compartilhamento
dos conjuntos de dados na Web entre diversos usuários e aplicações.",DISSERTAÇÃO,UM MODELO DE ARQUITETURA PARA SISTEMAS GERENCIADORES DE DADOS NA WEB,5006980,1
"Exploratory testing is a software testing approach that emphasizes the tester’s experience
in the attempt to maximize the chances to find bugs and minimize the time effort applied
on satisfying such a goal. It is naturally a GUI-oriented testing activity for GUI-based
systems. However, in most cases, exploratory testing strategies may not be accurate enough
to reach changed code regions. To reduce this gap, in this work, we propose a way of
aiding exploratory testing by providing a GUI model of the regions impacted by internal
code changes (for example, as result of change requests to fix previous bugs as well as for
software improvement). We create such a delimited GUI model by pruning an original
GUI model, quickly built by static analysis, using a reachability relation between GUI
elements (i.e., windows, buttons, text fields, etc.) and internal source code changes (classes
and methods). To illustrate the idea we provide promising data from two experiments,
one from the literature and another from our industrial partner.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,JACINTO FILIPE SILVA REIS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,22/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'GUI Testing . Static Analysis . Swing Patterns . Exploratory Testing . Change Request . Release Notes',ENGENHARIA DE SOFTWARE,ALEXANDRE CABRAL MOTA,67,Teste de GUI . Análise Estática . Padrões Swing . Teste Exploratório . Solicitação de Mudança . Notas de Publicação,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Teste exploratório é uma abordagem de teste de software que enfatiza a experiência do
testador na tentativa de maximizar as chances de encontrar bugs e minimizar o esforço
de tempo aplicado na satisfação desse objetivo. É naturalmente uma atividade de testes
orientada à GUI aplicada em sistemas que dispõem de GUI. No entanto, na maioria
dos casos, as estratégias de testes exploratórios podem não ser suficientemente precisas
para alcançar as regiões de código alteradas. Para reduzir esta lacuna, neste trabalho nós
propomos uma forma de auxiliar os testes exploratórios, fornecendo um modelo de GUI
das regiões impactadas pelas mudanças internas de código (por exemplo, como resultado
de solicitações de mudanças para corrigir bugs anteriores, bem como, para realização de
melhorias do software). Criamos um modelo de GUI delimitado, podando um modelo de
GUI original, construído rapidamente através de análise estática, usando uma relação de
alcançabilidade entre elementos de GUI (janelas, botões, campos de textos) e alterações de
código interno (classes e métodos). Para ilustrar a ideia, nós fornecemos dados promissores
de dois experimentos, um da literatura e outro de nosso parceiro industrial.",DISSERTAÇÃO,AIDING EXPLORATORY TESTING WITH PRUNED GUI MODELS,5007079,1
"On the last decades, mobile communications rose from a costly technology which few
people could use to a ubiquitous systems used by most of people. On this, the technologies found
on mobile devices (such as hardware, software, communications and battery) need to evolve to
supply the new features (high data rate and continuous connectivity, for example) that demands a
higher energy consumption and, consequently, imply on reduction of the smartphone’s battery
duration. Knowing that the wireless communication contribute fairly to the energy consumption
of mobile devices and considering the huge increase of network traffic on the last years, efforts
have been made to find solutions to extend battery duration. One example is the integration
between different wireless networks types (e.g. 3G and Wi-Fi) that allows the device to select
which network interface should be used based on some optimization criteria, like the minimization
of the energy cost for each transferred byte. Currently, on Android smartphones, the Wi-Fi
network is chosen every time it is available, probably because of monetary issue. However, this
choice doesn’t guarantee that the energy consumption will be minimized. Against the above,
the objective of this dissertation is the proposition of a network interface dynamic selection
mechanism focused on minimizing the mobile device’s energy consumption, increasing the
battery duration. To that, several Machine Learning techniques are applied to predict the energy
cost for each transferred byte of each available network interface. Lastly, a comparison between
all predicted energy costs (of each network interface) is performed to define which technique
delivers the best performance. That way, it is possible to compare the energy cost estimates for
each network interface and select the one that decreases the energy consumption.",,ENGENHARIA DA COMPUTAÇÃO,LUCAS MINORU FERREIRA HARADA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,17/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Mobile devices, smartphone, energy consumption, energy cost, cellular network, 3G network,Wi-Fi network, network interface dynamic selection, machine learning, regression.'",ENGENHARIA DA COMPUTAÇÃO,DANIEL CARVALHO DA CUNHA,69,"Dispositivos móveis, consumo de energia, custo energético, rede celular, rede 3G, rede Wi-Fi, seleção dinâmica de interface de rede, aprendizagem de máquina, regressão.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Nas últimas décadas, as comunicações móveis evoluíram de um patamar de tecnologia cara utilizada por alguns poucos indivíduos para a condição de sistemas ubíquos usados pela maioria da população mundial. Diante desse cenário, as tecnologias presentes nos dispositivos móveis (hardware, software, comunicações e bateria) precisam evoluir para suprir as novas funcionalidades (altas taxas de dados e conectividade ininterrupta, por exemplo) que cada vez mais demandam um maior consumo de energia e, consequentemente, implicam na diminuição da vida útil da bateria dos smartphones. Sabendo que a tecnologia de transmissão sem fio contribui significativamente para o aumento do consumo de energia dos dispositivos móveis e considerando o crescimento exorbitante de tráfego de dados dos últimos anos, esforços tem sido realizados para se buscar soluções que estendam a autonomia das baterias. Um exemplo é a integração de diferentes tipos de redes sem fio (3G e Wi-Fi, por exemplo) que permitam ao dispositivo móvel selecionar a interface de rede com base em algum critério de otimização, como, por exemplo, a minimização do custo energético por cada byte transferido. Atualmente, em smartphones Android, sempre que redes Wi-Fi estão disponíveis, a interface de rede Wi-Fi do dispositivo móvel é naturalmente escolhida, muito provavelmente por questões financeiras. No entanto, tal escolha não garante que o consumo de energia associado será otimizado. Face ao exposto, o objetivo deste trabalho é a proposição de um mecanismo dinâmico de seleção de interface de rede focado em minimizar o consumo de energia do dispositivo móvel, permitindo um aumento da vida útil da bateria. Para isso, diversas técnicas de Aprendizagem de Máquina são empregadas no intuito de prever o custo energético por byte transferido de cada tipo de interface de rede disponível. Por fim, uma comparação dos custos energéticos (para cada interface de rede) obtidos por cada técnica de Aprendizagem de Máquina é realizada para indicar qual a melhor alternativa dentre as técnicas selecionadas. Dessa forma, é possível comparar as estimativas dos custos energéticos de cada interface e escolher aquela que diminui o consumo energético.",DISSERTAÇÃO,PROPOSTA DE UM MECANISMO DINÂMICO DE SELEÇÃO DE INTERFACE DE REDE DE DISPOSITIVOS MÓVEIS UTILIZANDO APRENDIZAGEM DE MÁQUINA,5007851,1
"The present dissertation was developed within the framework of the symbolic data
analysis of interval-valued type, and it is specially related to regression models. Symbolic data
are extensions of classic data types. In conventional data sets, objects are individualized, while
in symbolic data they are unified by relationships. At first, a deep review about the nature of
this kind of data and the main methodologies used for its analysis were performed. A new
capital asset pricing model (CAPM) has been proposed and tested for interval symbolic data.
The approach considered the daily variation of the price ranges in market assets according to
the maximum and minimum prices rather than the opening or closing prices, which have been
most popular in econometric applications with CAPM models. For calculations involving price
ranges and asset returns, the basic operations concerning the interval arithmetic were used. The
proposed model (iCAPM) is one of the most recent interval CAPM applications, in which the
estimate of the b-parameter is, in fact, an interval. On this occasion, a new interpretation was
proposed for this parameter in accordance with the traditional interpretation for the systematic
risk of the assets in the market. Two figurative examples involving the daily price ranges of
Microsoft and Amazon have been presented, using the market returns from the S&P500 index in
the period from November 1, 2013 to January 15, 2015. In accordance with the statistical tests
performed here, the results of the application of the proposed model (iCAPM) are statistically
consistent with a reliable explanation of the assets returns and the market returns in question.
Secondly, a non-linear regression model for interval-valued data was introduced (SNLRM-IVD),
which sets a single regression model to the midpoints (centers) and ranges of the intervals at once,
considering the t-Student distribution. The performance of the model was validated through
the statistical criterion of the average magnitude of the relative error, undergoing experiments
in the scope of Monte Carlo simulations in relation to several symbolic scenarios with outliers.
Finally, the proposed model was fitted to a real set of interval data. The main feature of this
SNLRM-IVD is that it provides estimators that are not sensitive to the presence of outliers.",,INTELIGÊNCIA COMPUTACIONAL,DAILYS MAITE ALIAGA REYES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'SDA. Symbolic Interval-valued Data. Linear and Non-linear Regression. CAPM. Outliers.',ANÁLISE DE DADOS SIMBÓLICOS E/OU NUMÉRICOS E MÉTODOS AFINS,RENATA MARIA MENDES CARDOSO,83,SDA. Dados Simbólicos de Tipo Intervalo. Regressão Linear e Não-linear. CAPM. Outliers.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A presente dissertação foi desenvolvida no marco da análise de dados simbólicos de tipo
intervalo, especificamente, em modelos de regressão. Os dados simbólicos são extensões de
tipos de dados clássicos. Em conjuntos de dados convencionais, os objetos são individualizados,
enquanto em dados simbólicos estes são unificados por relacionamentos. Primeiramente, foi
realizada uma revisão sobre dados desta natureza e das principais metodologias utilizadas para
sua análise. Um novo modelo de precificação de ativos de capital (CAPM pelas siglas em inglês)
foi proposto e testado para dados intervalares. A abordagem levou em conta a variação nos
intervalos de preços diários em ativos de mercado, observando os preços máximos e mínimos
ao invés dos preços de abertura ou fechamento que têm sido mais populares em aplicações
econométricas com modelos de CAPM. Para os cálculos envolvendo intervalos de preços e
retornos de ativos, as operações básicas da aritmética intervalar foram utilizadas. O modelo
proposto (iCAPM) é uma das mais recentes aplicações CAPM intervalares, em que a estimativa
do parâmetro b é um intervalo. Nesta ocasião, foi proposta uma nova interpretação para dito
parâmetro em conformidade com a interpretação tradicional para o risco sistemático de ativos na
área das finanças. Foram apresentados dois exemplos ilustrativos com os intervalos de preços
diários da Microsoft e de Amazon, usando os retornos do mercado derivados do índice S&P500
do 01 de novembro de 2013 ao 15 de janeiro de 2015. Em conformidade com os testes estatísticos
aqui realizados, os resultados da aplicação do modelo CAPM intervalar (iCAPM) proposto são
consistentes estatísticamente, com uma explicação confiável referente aos retornos dos ativos em
questão e aos retornos do mercado. Conjuntamente, foi introduzido um modelo de regressão
não-linear simétrica para dados simbólicos de tipo intervalo (SNLRM-IVD), o qual ajusta um
único modelo de regressão não-linear aos pontos médios (centros) e amplitudes (ranges) dos
intervalos considerando a distribuição de t-Student. O desempenho do modelo foi validado
através do critério estatístico da magnitude média do erro relativo, desenvolvendo experimentos
no âmbito de simulações de Monte Carlo em relação a vários cenários simbólicos com outliers.
Além do mais, o modelo proposto foi ajustado a um conjunto real de dados intervalares. A
principal característica deste modelo é que proporciona estimadores não sensíveis à presença de
outliers.",DISSERTAÇÃO,ENSAIOS DE MODELOS DE REGRESSÃO LINEAR E NÃO-LINEAR PARA DADOS SIMBÓLICOS DE TIPO INTERVALO,5008897,1
"Online learning aims to extract information from data streams composed of a large number
of examples. These flows often contain concept drifts that in most cases are characterized
as changes in data distributions. Concept drifts detectors are algorithms designed to work
with one or more base classifier in order to estimate the change positions and, when
necessary, replace the predictor to improve its accuracy. DDM, EDDM and STEPD are
simple, efficient and well-known detectors. Despite its effectiveness on small bases, DDM
tends to lose accuracy when faced with considerably large data sets. On the other hand,
EDDM works well with gradual databases, but achieves low accuracy on bases with abrupt
drifts. STEPD was designed to detect changes in distribution using a hypothesis test
between two proportions, however, statistically this test is not recommended for small
and/or imbalanced samples. This work proposes four new detectors (seven versions in
total) that aim to improve DDM, EDDM and STEPD. All the proposed methods are
inspired by statistical tests, where EMZD is based on hypothesis test between means of
two independent samples and FPDD, FSDD and FTDD are based on Fisher’s exact test.
Experiments with two base classifiers using 36 artificial data sets and three real-world
datasets demonstrated the effectiveness and efficiency of the proposed detectors. Regarding
the evaluation of detectors, one of the versions of the EMZD obtained the best accuracy
and the FPDD was the most accurate in the analysis of the concept drifts detections.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,DANILO RAFAEL DE LIMA CABRAL,UNIVERSIDADE FEDERAL DE PERNAMBUCO,03/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Statistic. Machine Learning. Concept Drifts in Data Streams.',ENGENHARIA DE SOFTWARE,ROBERTO SOUTO MAIOR DE BARROS,108,Estatística. Aprendizagem de Máquina. Mudanças de Conceitos em Fluxos de Dados.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O aprendizado online objetiva a extração de informações a partir de fluxos de dados
compostos de uma grande quantidade de exemplos. Esses fluxos frequentemente contêm
mudanças de conceitos que na maioria dos casos são caracterizadas como modificações
nas distribuições dos dados. Métodos detectores de mudanças de conceitos são algoritmos
projetados para trabalharem conjuntamente com um – ou mais – classificador base, a fim
de estimarem as posições das mudanças e quando necessário substituírem o preditor, com
o objetivo de melhorar a sua acurácia. DDM, EDDM e STEPD são exemplos de detectores
simples, eficientes e bem-conceituados. Apesar de sua eficácia em bases pequenas, o DDM
tende a perder precisão quando apresentado a conjuntos de dados consideravelmente
grandes. Por outro lado, o EDDM funciona bem com bases de dados graduais, porém
alcança baixos índices de acurácia em bases com mudanças de conceitos abruptas. O
STEPD, por sua vez, foi projetado para a detecção de mudanças de conceitos através do
teste de hipóteses entre duas proporções, entretanto, estatisticamente, esse teste não é
recomendado para amostras pequenas e/ou desbalanceadas. Este trabalho propõe quatro
novos detectores (formando o total de sete versões) que objetivam melhorar o DDM,
EDDM e STEPD. Todos os métodos propostos são baseados em testes estatísticos, sendo
o EMZD baseado no teste de hipóteses entre médias de duas amostras independentes e, o
FPDD, FSDD e FTDD baseados no teste exato de Fisher. Os experimentos realizados,
com dois classificadores base, usando 36 conjuntos de dados artificiais e três bases de dados
reais, demonstraram a eficácia e eficiência dos detectores propostos. No que diz respeito
a avaliação dos detectores, uma das versões do EMZD obteve as melhores acurácias e o
FPDD foi o mais preciso na análise das detecções de mudanças de conceitos.",DISSERTAÇÃO,TESTES ESTATÍSTICOS E DETECÇÕES DE MUDANÇAS DE CONCEITOS EM FLUXOS DE DADOS,5008905,1
"Symbolic Interval Data (SDA) is a paradigm which provides a framework for building,
describing, analyzing and extracting knowledge from data more complex such as intervals,
histograms, distribution of weights or list of values (categories). Typically, symbolic data
arise in two situations throughout data collecting and processing. Some data collected are
inherently symbolic and some become symbolic data after processing of huge data sets in
order to summarize them through classes of data. Polygonal data present in this work is a
multivariate complex structure of data that is able to store information from classes of
data. This work introduces a new framework for polygonal data analysis in the symbolic
data analysis paradigm. We show that polygonal data generalizes bivariate interval data.
To analyse polygonal data descriptive statistics and a linear regression model are proposed
for symbolic polygonal data. A Monte Carlo study of simulation are present to verify the
performance of prediction for polygonal data. Two real dataset are present.",,INTELIGÊNCIA COMPUTACIONAL,WAGNER JORGE FIRMINO DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,15/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Symbolic Data Analysis. Polygonal data. Descriptive Data. Regression',ANÁLISE DE DADOS SIMBÓLICOS E/OU NUMÉRICOS E MÉTODOS AFINS,RENATA MARIA MENDES CARDOSO,63,Análise de dados simbólicos. Dados poligonais. Regressão,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Análise de Dados Simbólicos (ADS) é um paradigma que fornece uma estrutura para
construir, descrever, analisar e extrair conhecimento de dados mais complexos como
intervalos, histogramas, distribuição de pesos ou lista de valores (categorias). Tipicamente,
os dados simbólicos surgem em duas situações, ao longo da coleta e processamento de
dados. Alguns dados coletados são inerentemente simbólicos e outros se tornam dados
simbólicos após o processamento de enormes conjuntos de dados, a fim de resumi-los
através de classes de dados. Dados poligonais, propostos neste trabalho, são estruturas
complexas multivariadas de dados que são capazes de armazenar informações de classes de
dados. Este trabalho introduz uma nova estrutura para análise de dados poligonais no
paradigma de análise de dados simbólicos. Mostramos que dados poligonais generalizam
dados de intervalos bivariados. Para análise de dados poligonais estatísticas descritivas e
um modelo de regressão linear são propostos. Estudo de simulação de Monte Carlo são
realizados para verificar o desempenho da previsão em dados poligonais. Dois conjuntos
de dados reais são apresentados.",DISSERTAÇÃO,ANÁLISE DE DADOS POLIGONAIS: UMA NOVA ABORDAGEM PARA DADOS SIMBÓLICOS,5008929,1
"Visual attention is the process by which the human brain prioritizes and controls visual
stimuli and it is, among other characteristics of the visual system, responsible for the fast way
in which human beings interact with the environment, even considering a large amount of
information to be processed. Visual attention can be driven by a bottom-up mechanism, in which
low level stimuli of the analysed scene, like color, guides the focused region to salient regions
(regions that are distinguished from its neighborhood or from the whole scene); or by a top-down
mechanism, in which cognitive factors, like expectations or the goal of concluding certain task,
define the attended location. This Thesis investigates the use of visual attention algorithms to
guide (and to accelerate) the search for objects in digital images. Inspired by the bottom-up
mechanism, a saliency detector based on the estimative of the scene’s background combined
with the result of a Laplacian-based operator, referred as BLS (Background Laplacian Saliency),
is proposed. Moreover, a modification in SURF (Speeded-Up Robust Features) local feature
detector/descriptor, named as patch-based SURF, is designed so that the recognition occurs
iteratively in each focused location of the scene, instead of performing the classical recognition
(classic search), in which the whole scene is analysed at once. The search mode in which the
patch-based SURF is applied and the order of the regions of the image to be analysed is defined
by a saliency detection algorithm is called BGMS. The BLS and nine other state-of-the-art
saliency detection algorithms are experimented in the BGMS. Results indicate, in average, a
reduction to (i) 73% of the classic search processing time just by applying patch-based SURF in
a random search, (ii) and to 53% of this time when the search is guided by BLS. When using
other state-of-the-art saliency detection algorithms, between 55% and 133% of the processing
time of the classic search is needed to perform recognition. Moreover, inspired by the top-down
mechanism, it is proposed the BGCO, in which the visual search occurs by prioritizing scene
descriptors according to its Hamming distance to the descriptors of a given target object. The
BGCO uses Bloom filters to represent feature vectors that are similar to the descriptors of the
searched object and it has constant space and time complexity in relation to the number of
elements in the set of the descriptors of the target. Experiments showed a reduction in the
processing time to 80% of the required time when the classic search is performed. Finally, by
using the BGMS and the BGCO in an integrated way, the processing time of the search was
reduced to 44% of the execution time required by the classic search.",,COMPUTAÇÃO INTELIGENTE,RAFAEL GALVAO DE MESQUITA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,24/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Visual search. saliency detection. visual attention. object recognition. local feature detectors/descriptors. matching.',PROCESSAMENTO DE IMAGENS,CARLOS ALEXANDRE BARROS DE MELLO,140,Busca visual.;detecção de saliência. atenção visual. reconhecimento de objetos. detecção e descrição de características locais. matching.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Atenção visual é o processo pelo qual o cérebro humano prioriza e controla o processamento
de estímulos visuais e é, dentre outras características do sistema visual, responsável pela
forma rápida com que seres humanos interagem com o meio ambiente, mesmo considerando
uma grande quantidade de informações a ser processada. A atenção visual pode ser direcionada
pelo mecanismo bottom-up, em que estímulos de baixo nível da cena, como cor, guiam o foco
atentivo para aquelas regiões mais salientes, ou seja, que se distinguam da sua vizinhança ou do
restante da cena; ou pelo mecanismo top-down, em que fatores cognitivos, como expectativas do
indivíduo ou o objetivo de concluir certa tarefa, definem a região de atenção. Esta Tese investiga
o uso de algoritmos de atenção visual para guiar (e acelerar) a busca por objetos em imagens
digitais. Inspirado no funcionamento do mecanismo bottom-up, um algoritmo de detecção de
saliências baseado na estimativa do background da cena combinado com o resultado de um
operador Laplaciano, denominado de BLS (Background Laplacian Saliency), é proposto. Além
disso, uma modificação no detector/descritor de características locais SURF (Speeded-Up Robust
Features), denominado de patch-based SURF, é desenvolvida para que o reconhecimento ocorra
iterativamente em certos locais em foco da cena, ao invés de executar o modo clássico de reconhecimento
(busca clássica), em que toda a cena é analisada de uma só vez. O modo de busca em
que o patch-based SURF é aplicado e a ordem das regiões analisadas da imagem é definida por
um algoritmo de detecção de saliência é referenciado como Busca Guiada por Mapa de Saliência
(BGMS). O BLS e outros nove algoritmos de detecção de saliências são experimentados na
BGMS. Resultados indicam, em média, uma redução para (i) 73% do tempo de processamento
da busca clássica apenas pela aplicação do patch-based SURF em uma busca aleatória, (ii) e
para 53% desse tempo quando a busca é guiada pelo BLS. Utilizando outros algoritmos de
detecção de saliências do estado da arte, entre 55% e 133% do tempo da busca clássica são
necessários para o reconhecimento. Além disso, inspirado pelo mecanismo top-down, é proposta
a Busca Guiada por Características do Objeto (BGCO) por meio da priorização de descritores
extraídos da cena em função da distância Hamming para os descritores de um determinado objeto
alvo. A BGCO utiliza filtros de Bloom para representar vetores de características similares aos
descritores do objeto buscado e possui complexidade de espaço e tempo constantes em relação
ao número de elementos na base de descritores do alvo. Experimentos demonstram uma redução
do tempo de processamento para 80% do tempo necessário quando a busca clássica é executada.
Concluindo, a partir da integração entre a BGMS e a BGCO (BGMS+BGCO) é possível reduzir
o tempo de execução da busca para 44% do tempo da busca clássica.",TESE,RECONHECIMENTO DE INSTÂNCIAS GUIADO POR ALGORITMOS DE ATENÇÃO VISUAL,5011167,1
"The automobile, previously seen as an isolated system, is now immersed in a connected environment
where the data exchanged inside the car and with the outside world has little or no security
measures against unwanted attackers. In this new scenario, automotive security must be one of
the most important architectural attributes of any car. This fact raises some challenges for OEMs,
since they must keep producing economically attractive cars, but with ever-increasing enticing
functionalities. This trade-off has a direct impact on the existing possibilities for the hardware
utilized to execute such functionalities, and hence its achievable processing power. Since security
measures generally involve the use of encryption methods, the required hardware performance
tends to escalate; when safety-critical applications are concerned, the hardware requirements
are even more severe due to maximum latency limitations. Advances in technology, especially
in dedicated processing modules for specific tasks and the increase in raw processing power
of processors, along with a redesigned architecture for the exchange of in-vehicle data communication,
turn feasible the implementation of functionalities in cars that previously suffered
from performance limitations. Ethernet arises as a high-bandwidth, scalable and future-proof
in-vehicle network technology and is the main component supporting this newly, redesigned
architecture.
This work aims to investigate the performance implications introduced by the use of cryptographic
protocols in the exchange of information among electronic modules in safety-critical automotive
systems. Confidentiality, authenticity and integrity are explored over different secret key sizes
and different payload data sizes on the layer 2 of the network. Hardware encryption acceleratorenabled
micro-controller units are utilized to accelerate cryptography-related calculations. The
latency of the data exchange is compared with the case where only a Cyclic Redundancy Check
(CRC32) is used, which has been the most commonly used method for integrity checking over
the last years in the automotive domain, as well as with the case with no verification. Low-cost
processing modules and an ordinary switch are utilized to experimentally demonstrate that the
common maximum latency values for safety-critical applications can be respected even after the
application of cryptography to protect data communication among electronic modules.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,EDILSON AUGUSTO SILVA JUNIOR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,09/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Cryptography;Embedded Systems, Automotive Networks, In-vehicle Networks, Intra-vehicular Networks, Ethernet, Security, Low-cost, Latency, Safety-critical'",SISTEMAS DISTRIBUÍDOS,DIVANILSON RODRIGO DE SOUSA CAMPELO,90,"Criptografia Sistemas Embarcados, Redes Automotivas, Redes Intra-veiculares, Ethernet, Segurança, Baixo Custo, Latência, Safety-critical, Sistemas Críticos",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O automóvel, antes visto como um sistema isolado, está agora imerso em um ambiente conectado
onde os dados trocados dentro do carro e com o mundo externo têm poucas ou nenhuma medidas
de segurança contra invasores indesejados. Neste novo cenário, a segurança automotiva precisa
ser um dos atributos arquiteturais mais importantes de qualquer carro. Este fato traz alguns
desafios aos fabricantes, já que eles devem continuar produzindo carros atrativos economicamente,
porém com cada vez mais funcionalidades atrativas. Esta troca tem um impacto direto
nas possibilidades existentes para o hardware utilizado para execução de tais funcionalidades,
e, portanto, seu máximo poder de processamento alcançável. Como as medidas de segurança
geralmente envolvem o uso de métodos de encriptação, o desempenho de hardware requerido
tende a intensificar-se rapidamente; quanto às aplicações safety-critical, os requerimentos de
hardware são ainda mais severos devido às limitações de latência máxima. Avanços na tecnologia,
especialmente nos módulos de processamento dedicados para tarefas específicas e o
aumento em poder bruto de processamento dos processadores, juntamente com uma arquitetura
redesenhada para o intercâmbio de dados intra-veiculares, tornam factível a implementação de
funcionalidades em carros que anteriormente sofriam limitações de desempenho. A Ethernet
surge como uma tecnologia de rede intra-veicular de grande largura de banda, escalável e à
prova do futuro, além de ser o principal componente dando base para esta nova e redesenhada
arquitetura.
Este trabalho tem como objetivo investigar as implicações no desempenho, devido ao uso de
protocolos de criptografia durante a troca de informações entre módulos eletrônicos em sistemas
automotivos safety-critical. Confidencialidade, autenticidade e integridade são explorados
através da utilização de diferentes tamanhos de chaves secretas e tamanhos de dados de carga
útil na camada 2 da rede. Micro controladores com hardware acelerador de encriptação são
utilizados para acelerar cálculos relacionados à criptografia. A latência da troca de dados é
comparada com o caso em que apenas o método Verificação de Redundância Cíclica (CRC32)
é utilizado, o qual tem sido o método de verificação de integridade mais comumente utilizado
no domínio automotivo até os últimos anos, e também com o caso em que nenhuma verificação
é feita. Módulos de processamento de baixo custo e um switch comum são utilizados para
demonstrar experimentalmente que os valores comuns de latência máxima para aplicações safetycritical
podem ser respeitados mesmo depois da aplicação da criptografia, para que se proteja a
comunicação dos dados entre módulos eletrônicos.",DISSERTAÇÃO,LOW-COST EXPERIMENTAL VALIDATION OF CRYPTOGRAPHIC PROTOCOLS FOR SAFETY-CRITICAL AUTOMOTIVE COMMUNICATION,5012397,1
"Fault modelling is essential to anticipate failures in critical systems. Traditionally, Static
Fault Trees are employed to this end, but Temporal and Dynamic Fault Trees have gained
evidence due to their enriched power to model and detect intricate propagation of faults
that lead to a failure.
In a previous work, we showed a strategy based on the process algebra CSP and Simulink
models to obtain fault traces that lead to a failure. From the fault traces we discarded
the ordering information to obtain structure expressions for Static Fault Trees. Instead of
discarding such an ordering information, it could be used to obtain structure expressions
of Temporal or Dynamic Fault Trees.
In this work we present: (i) an algebra of temporal faults (with a notion of fault propagation)
to analyse systems’ failures, and prove that it is indeed a Boolean algebra, and (ii) a
parametrized activation logic to express nominal and erroneous behaviours, including fault
modelling, provided an algebra and a set of operational modes. The algebra allows us
to inherit Boolean algebra’s properties, laws and existing reduction techniques, which
are very beneficial for fault modelling and analysis. With expressions in the algebra of
temporal faults we allow the verification of safety properties based on Static, Temporal or
Dynamic Fault Trees. The logic created in this work can be combined with other algebras
beyond those shown here. Being used with the algebra of temporal faults it is intended to
help analysts to consider all possible situations in complex expressions with order-related
operators, avoiding missing subtle (but relevant) faults combinations. Furthermore, our
algebra of temporal faults tackles the NOT operator which has been left out in other works.
We illustrate our work on simple but real case studies, some supplied by our industrial
partner EMBRAER.
Isabelle/HOL was used to mechanize the theorems proofs of the algebra of temporal faults.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,ANDRE LUIS RIBEIRO DIDIER,UNIVERSIDADE FEDERAL DE PERNAMBUCO,08/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Simulink, CSP, FDR, Fault Tree Analysis, Temporal Fault Trees, Dynamic Fault Trees, Isabelle/HOL, Pandora, Fault Injection'",ENGENHARIA DE SOFTWARE,ALEXANDRE CABRAL MOTA,168,"Simulink, CSP, FDR, Fault Tree Analysis, Temporal Fault Trees, Dynamic Fault Trees, Isabelle/HOL, Pandora, Fault Injection",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),VERIFICAÇÃO DE MODELOS VOLTADOS PARA REQUISITOS DE SISTEMAS AVIÔNICOS,"A modelagem de falhas é essencial na antecipação de defeitos em sistemas críticos. Tradicionalmente,
Árvores de Falhas Estáticas são empregadas para este fim, mas Árvores de
Falhas Temporais e Dinâmicas têm ganhado evidência devido ao seu maior poder para
modelar e detectar propagações complexas de falhas que levam a um defeito.
Em um trabalho anterior, mostramos uma estratégia baseada na álgebra de processos
CSP e modelos Simulink para obter rastros (sequências) de falhas que levam a um defeito.
A partir dos rastros de falhas nós descartamos a informação de ordenamento para obter
expressões de estrutura para Ávores de Falhas Estáticas. Ao contrário de descartar tal
informação de ordenamento, poderíamos usá-la para obter expressões de estrutura para
Árvores de Falhas Temporais ou Dinâmicas.
No presente trabalho apresentamos: (i) uma álgebra temporal de falhas (com noção
de propagação de falhas) para analisar defeitos em sistemas e provamos que ela é de
fato uma álgebra Booleana, e (ii) uma lógica de ativação parametrizada para expressar
comportamentos nominais e de falha, incluindo a modelagem de falhas a partir de uma
álgebra e um conjunto de modos de operação. A álgebra permite herdar as propriedades
de álgebras Booleanas, leis e técnicas de redução existentes, as quais são muito benéficas
para a modelagem e análise de falhas. Com expressões na álgebra temporal de falhas
nós permitimos a verificação de propriedades de segurança (safety) baseadas em Árvores
de Falhas Estáticas, Temporais ou Dinâmicas. A lógica criada neste trabalho pode ser
usada com outras álgebras além das apresentadas. Sendo usada em conjunto com a álgebra
temporal de falhas, tem a intenção de ajudar os analistas a considerar todas as possíveis
situações em expressões complexas com operadores relacionados ao ordenamento das
falhas, evitando esquecer combinações de falhas sutis (porém relevantes). Além disso, nossa
álgebra temporal de falhas trata operadores NOT, que têm sido deixados de fora em outros
trabalhos. Nós ilustramos nosso trabalho com alguns estudos de caso simples, mas reais,
fornecidos pelo nosso parceiro industrial, a EMBRAER.
Isabelle/HOL foi utilizado para a mecanização das provas dos teoremas da álgebra temporal
de falhas.",TESE,An Algebra of Temporal Faults,5012422,1
"Within the context of this dissertation, a modeling language is any artificial language that, from
a set of consistent constructors and rules of association between them, can be used to diagram
a domain and, consequently, generate interpretable or executable code. The use of modeling
language to diagram procedural programs in relational database is still a limited subject within
boundaries, mostly: unexpressive (or inexisting) metamodels and meticulous and overloaded
graphic notation. Aiming at overcoming limits, this dissertation proposes Crystal, a modeling
simplified language to support procedural program specification for relational databases. Crystal
is strongly based in Model Driven Development (MDD) - Software Engineering paradigm that
generates interpretable/executable code from existing models. That is, in MDD, models are more
than documentation assets, because such models correspond to executable objects. In order to
show the feasibility of the proposed language, the CrystalCASE tool is developed as proof of
concept. Beyond that, in order to present evidence that the proposed model advances the state of
the art, test scenarios are specified exploring the main Procedural Language/Structured Query
Language (PLSQL) constructors and, from these scenarios, the proposed model is compared
with most relevant related work. As a result of the comparative analysis, it can be seen that
the proposed model presents the following advantages: more expressive metamodel and more
simplified and representational graphic notation.",,BANCO DE DADOS,CRYSTAL DE MENEZES SANTOS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,17/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Model Driven Development. Domain-Specific Modeling Language. Visual Programming Language;CASE tool. Model transformation.',BANCO DE DADOS,ROBSON DO NASCIMENTO FIDALGO,108,Desenvolvimento Dirigido por Modelo.;Linguagem de Modelagem de Domínio Específico. Linguagem Visual de Programação. Ferramenta CASE. Transformação de Modelo. Desenvolvimento Dirigido a Modelo.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"No contexto deste trabalho, uma linguagem de modelagem é qualquer linguagem artificial que, a
partir de um conjunto consistente de construtores e de regras de associação entre estes, pode ser
usada para diagramar um domínio e, consequentemente, gerar código interpretável ou executável.
O uso de linguagem de modelagem para diagramar programas procedurais em Banco de Dados
Relacionais ainda é um tema que tem limitações, principalmente: metamodelos pouco expressivos
(ou inexistentes) e notação gráfica sobrecarregada e pouco abrangente. Visando superar
estas limitações, esta dissertação propõe Crystal, uma linguagem de modelagem simplificada
para apoiar a especificação de programas procedurais para Banco de Dados Relacionais. Crystal
é fortemente baseada em Model Driven Development (MDD) – um paradigma da Engenharia de
So f tware que gera código interpretável/executável a partir de modelos. Isto é, em MDD, modelos
são mais do que artefatos de documentação, pois estes correspondem a objetos executáveis.
Visando mostrar a viabilidade da linguagem proposta, a ferramenta CrystalCASE é desenvolvida
como prova de conceito. Além disso, de modo a apresentar evidências que o trabalho proposto
avança o estado da arte, são especificados cenários de testes que exploram os principais construtores
de Procedural Language/Structured Query Language (PLSQL) e, a partir desses cenários,
o trabalho proposto é comparado com os principais trabalhos relacionados. Como resultado das
análises comparativas, pode-se constatar que o trabalho proposto tem as seguintes vantagens:
metamodelo mais expressivo e notação gráfica mais simplificada e representativa.",DISSERTAÇÃO,CRYSTAL: UMA LINGUAGEM DE MODELAGEM SIMPLIFICADA PARA APOIAR A ESPECIFICAÇÃO DE PROGRAMAS PROCEDURAIS EM BANCO DE DADOS RELACIONAIS,5012425,1
"Network Access Control (NAC) management is a critical task. Misconfigurations may
result in vulnerabilities that may compromise the overall network security. Traditional access
control setups rely on firewalls, IEEE 802.1x, VLAN, ACL, and LDAP. These approaches work
well for stable and small networks and are hard to integrate and configure. Besides, they are
inflexible and require per-device and vendor-specific configurations, being error-prone.
The Software-Defined Networking (SDN) paradigm overcomes architectural problems of
traditional networks, simplifies the network design and operation, and offers new opportunities
(programmability, flexibility, dynamicity, and standardization) to manage these issues. Furthermore,
SDN reduces the human intervention, which in turn also reduce operational costs and
misconfigurations. Despite this, access control management remains a challenge, once managing
security policies involves dealing with a large set of access control rules; detection of conflicting
policies; defining priorities; delegating rights; reacting to dynamic network states and events.
This dissertation explores the use of SDN to mitigate these problems. We present
HACFlow, a novel SDN framework for network access control management based on the OrBAC
model. HACFlow aims to simplify and automate the NAC management. It allows network
operators to govern rights of network entities by defining dynamic, fine-grained, and high-level
access control policies.
To illustrate the operation of HACFlow we present through a step by step how the main
management tasks are executed. Our study case is a Smart City network environment. We
conducted many experiments to analyze the scalability and performance of HACFlow, and the
results show that it requires a time in the order of milliseconds to execute all the management
tasks, even managing many policies. Besides, we compare HACFlow against related approaches.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,DANIEL ROSENDO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Software-defined Networks,;Internet of Things networks, Security management, Policy-based management, Autonomic and cognitive management.'",MÍDIA E INTERAÇÃO,JUDITH KELNER,76,"Redes Definidas por Software,;Internet das Coisas, Gerenciamento de segurança, Gerenciamento baseado em políticas, Gerenciamento autônomo e cognitivo.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Gerenciar o controle de acesso entre recursos (usuários, máquinas, serviços, etc.) em
uma rede é uma tarefa crítica. Erros de configuração podem resultar em vulnerabilidades que
podem comprometer a segurança da rede como um todo. Em redes tradicionais, esse controle
de acesso é implementado através de firewalls, IEEE 802.1x, VLAN, ACL, and LDAP. Estas
abordagens funcionam bem em redes menores e estáveis, e são difíceis de configurar e integrar.
Além disso, são inflexíveis e requerem configurações individuais e específicas de cada fabricante,
sendo propensa à erros.
O paradigma de Redes Definidas por Software (SDN) supera os problemas arquiteturais
das redes tradicionais, simplifica o projeto e operação da rede, e proporciona novas oportunidades
(programabilidade, flexibilidade, dinamicidade, e padronização) para lidar com os problemas
enfrentados em redes tradicionais. Apesar das vantagens do SDN, o gerenciamento de políticas
de controle de acesso na rede continua sendo uma tarefa difícil. Uma vez que, gerenciar tais
políticas envolve lidar com uma grande quantidade de regras; detectar e resolver conflitos; definir
prioridades; delegar papéis; e adaptar tais regras de acordo com eventos e mudanças de estado da
rede.
Esta dissertação explora o paradigma SDN a fim de mitigar tais problemas. Neste
trabalho, apresentamos o HACFlow, um framework SDN para gerenciamento de políticas de
controle de acesso na rede baseado no modelo OrBAC. HACFlow tem como principal objetivo
simplificar e automatizar tal gerenciamento. HACFlow permite que operadores da rede governe
os privilégios das entidades da rede através da definição de políticas de controle de acesso
dinâmicas, em alto nível, e com alta granularidade.
Para ilustrar o funcionamento do HACFlow apresentamos um passo a passo de como
as principais tarefas de genrenciamento de controle de acesso são realizadas. Nosso estudo de
caso é um ambiente de rede de uma cidade inteligente. Vários experimentos foram realizados
a fim de analisar a escalabilidade e performance do HACFlow. Os resultados mostram que o
HACFlow requer um tempo na ondem de milissegundos para executar cada uma das tarefas
de gerenciamento, mesmo lidando com uma grande quantidade de regras. Além disso, nós
comparamos HACFlow com propostas relacionadas existentes na literatura.",DISSERTAÇÃO,A HIGH-LEVEL AUTHORIZATION FRAMEWORK FOR SOFTWARE-DEFINED NETWORKS,5016452,
"Opinion Mining (OM), also known as sentiment analysis, is the field of study that analyzes
people’s sentiments, evaluations, attitudes, and emotions about different entities expressed
in textual input. This is accomplished through the classification of an opinion into categories,
such as positive, negative, or neutral. Supervised machine learning (ML) and lexicon-based are
the most frequent approaches for OM. However, these approaches require considerable effort for
preparing training data and to build the opinion lexicon, respectively.
In order to address the drawbacks of these approaches, this Thesis proposes the use of
unsupervised clustering approach for the OM task which is able to produce accurate results for
several domains without manually labeled data for the training step or tools which are language
dependent. Three swarm algorithms based on Particle Swarm Optimization (PSO) and Cuckoo
Search (CS) are proposed: the DPSOMUT which is based on a discrete PSO binary version, the
IDPSOMUT that is based on an Improved Self-Adaptive PSO algorithm with detection function,
and the IDPSOMUT/CS that is a hybrid version of IDPSOMUT and CS.
Several experiments were conducted with different corpora types, domains, text language,
class balancing, fitness function, and pre-processing techniques. The effectiveness of the
clustering algorithms was evaluated with external measures such as accuracy, precision, recall,
and F- score.
From the statistical analysis, it was possible to observe that the swarm-based algorithms,
especially the PSO ones, were able to find better solutions than conventional grouping techniques,
such as K-means and Agglomerative. The PSO-based algorithms achieved better accuracy using
a word bigram pre-processing and the Global Silhouette as fitness function. The OBCC corpus
is also another contribution of this Thesis and contains a gold collection with 2,940 tweets in
Brazilian Portuguese with opinions of consumers about products and services.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,ELLEN POLLIANA RAMOS SOUZA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,22/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Swarm Optimization, Text Clustering, Opinion Mining, Opinion Clustering;Twitter'",ADMINISTRAÇÃO E INTEGRAÇÃO DE SISTEMAS,ADRIANO LORENA INACIO DE OLIVEIRA,122,"Otimização de Enxame;Agrupamento de Texto, Mineração de Opinião, Agrupamento de Opinião, Twitter",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A mineração de opinião, também conhecida como análise de sentimento, é um campo de estudo que analisa os sentimentos, opiniões, atitudes e emoções das pessoas sobre diferentes entidades, expressos de forma textual. Tal análise é obtida através da classificação das opiniões em categorizas tais como positiva, negativa ou neutra. As abordagens de aprendizado supervisionado e baseadas em léxico são mais comumente utilizadas na mineração de opinião. No entanto, tais abordagens requerem um esforço considerável para preparação da base de dados de treinamento e para construção dos léxicos de opinião, respectivamente. A fim de resolver as desvantagens das abordagens apresentadas, esta Tese propõe o uso de uma abordagem de agrupamento não supervisionada para a tarefa de mineração de opinião, a qual é capaz de produzir resultados precisos para diversos domínios sem a necessidade de participação humana, conhecimento linguístico ou tempo para treinamento. Três algoritmos de inteligência coletiva baseados em otimização de partícula de enxame (Particle Swarm Optimization - PSO) são propostos: o DPSOMUT que é baseado em versão discreta do PSO, o IDPSO que é baseado em uma versão melhorada e autoadaptativa do PSO com função de detecção e o IDPSO/CS que é uma versão híbrida do IDPSO com o Cuckoo Search (CS). Diversos experimentos foram conduzidos com diferentes tipos de corpus, domínios, língua do texto, balanceamento de classes, função de otimização e técnicas de pré-processamento. A eficácia dos algoritmos de agrupamento foi avaliada com medidas externas como a acurácia, a precisão, a revocação e f-medida. Melhores resultados foram obtidos pelo IDPSO e o corpus de opinião de consumidores brasileiros (Opinion of Brazilian Portuguese corpus - OBCC), utilizando o pré-processamento baseado em word bigram e Global Silhouette como função de otimização.  O corpus OBCC é também uma contribuição desta Tese e contem uma coleção dourada com 2.940 tweets com opiniões de consumidores sobre produtos e serviços em Português brasileiro.",TESE,SWARM OPTIMIZATION CLUSTERING METHODS FOR OPINION MINING,5016892,1
"Blind image deconvolution (blind image deblurring or blind image restoration) is a classical
inverse problem of Image Processing and Computer Vision. In a blind deconvolution
process, the original uncorrupted image and and the Point Spread Function (PSF) of the
degradation are unknown. Images of optic endoscopy are commonly affected by blur due
camera movements during the image capture process or due involuntary movements of
the gastrointestinal tissue. In this work, we propose a new blind deconvolution method
of optical endoscopy blurred images. We explore the use of specular reflections that are
a very common degradation in this type of images. These reflections are used as a prior
information to the PSF estimation process. The proposed method can be divided into
two stages. The first stage consists of a new method to perform a precise segmentation of
specular reflections. The second step consists of a new estimation method of the PSF. This
PSF is inferred by grouping and fusion of specular reflections previously segmented. We
observed that the proposed method of fusion of theses reflections meets the prerequisites
that the estimated PSF must be sparse, positive and have a small size when compared with
the blurred image size. Also, since we know that these reflections may contain relevant
information about the degradation, we show that when fused, they may be used in the
estimation of the PSF producing satisfactory results where, in many cases, the quality of
the images restored by the proposed method is superior or equal to the images restored by
other state-of-the-art methods.",,COMPUTAÇÃO INTELIGENTE,FABIANE DA SILVA QUEIROZ,UNIVERSIDADE FEDERAL DE PERNAMBUCO,10/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Image Processing;Specular Reflections Segmentation. Space Invariant Deblurring. Image Restoration. Blind image deconvolution.',PROCESSAMENTO DE IMAGENS,TSANG ING REN,120,Processamento de Imagens;Segmentação de Reflexos Especulares. Borramento Invariante no Espaço. Restauração de Imagens. Deconvolução Cega.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A restauração cega de imagens corrompidas por borramento é um problema inverso clássico
da área de Processamento de Imagens e Visão Computacional. Um processo de restauração
de imagens é definido como cego, quando desconhecemos tanto a imagem original não
corrompida quanto a Função de Espalhamento de Ponto da degradação (PSF - Point
Spread Function) e o ruído envolvido neste processo. Imagens de endoscopia óptica são
comumente afetadas por borramento devido à movimentação da câmera no processo de
captura ou a movimentos involuntários da superfície do tecido do trato gastrointestinal.
Nesta tese propomos um novo método de restauração cega de imagens de endoscopia
óptica. Para tanto, nós exploramos o uso de reflexos especulares, que constituem outro tipo
de degradação muito comum nesse tipo de imagem, e que por sua vez, são utilizados como
informação a priori no processo de estimativa da PSF do borramento. Sendo assim, o
método aqui proposto pode ser dividido em duas etapas. A primeira etapa consiste de um
novo método de segmentação precisa de reflexos especulares que podem, por sua vez, conter
ricas informações sobre a degradação sofrida pela imagem. A segunda etapa consiste de
um novo método de estimação da PSF da degradação, no qual a mesma é inferida através
do agrupamento e fusão dos reflexos segmentados previamente. Observamos que a fusão
de reflexos especulares não fere os pré-requisitos de que a PSF estimada deve ser esparsa,
positiva e com um tamanho pequeno quando comparado ao tamanho da imagem sendo
restaurada. Além disso, uma vez que sabemos que tais reflexos podem conter informações
relevantes do processo de borramento, mostramos que quando fundidos, estes podem ser
utilizados na estimação da PSF do borramento, produzindo assim resultados satisfatórios
de imagens restauradas quando estas são comparadas à imagens restauradas por métodos
do estado da arte, sendo a qualidade das imagens restauradas pelo método aqui proposto
muitas vezes superior ou equivalente aos trabalhos comparados.",TESE,RESTAURAÇÃO DE IMAGENS ATRAVÉS DE REFLEXOS ESPECULARES,5017947,1
"A common scenario when automating tests begins with a test engineer writing test cases with no
formal specification or software supervision. Then, there is a developer that tries to understand
what an ambiguous test means in order to transform it into code. Finally, there is an experienced
tester that is responsible to supervise the execution and to verify whether the failures are indeed
bugs, or a mistake from the developer or test engineer, or just a matter of change in the requirements
or user interface. To reduce these recurring problems in test automation, we propose
a unified process, to write test cases using a controlled natural language to check consistency
and dependencies automatically or search for similar test descriptions written in free natural
language. The proposed process is applied to a reusable capture & replay strategy, so anyone
can automate tests even without previous knowledge about coding, besides mitigating scalability/
maintainability issues by reusing actions with granularity ranging from simple commands,
such as open an application, to entire test cases or even test suites. The actions are represented
by an abstract, framework-free notation. Besides, the implementation covers plugins, including
voice recording and image processing; and a proactive traceability for elements in the user interface.
The strategy was consolidated through its implementation in a tool, in the context of
a partnership with Motorola Mobility. It has been adopted in practice for different test suites,
achieving a reuse ratio up to 71% for test cases written with no standard whatsoever, yet presenting
time gains similar to traditional C&R approaches when compared to coding. Furthermore,
through standardization, it was possible to define a consistency notion and to capture an association
graph (whose edges represent dependency and cancellation relations) among test actions.
From these associations, it was possible to establish a mechanism to ensure that each test action
is preceded by actions on which it depends to be granted to execute. An editor (Kaki) was implemented
to mechanize both adherence to the CNL and the dependencies between tests. From the
associations informed within the user interface, Kaki generates an Alloy model automatically
and uses Alloy Analyzer to verify consistency, besides possibly suggesting the insertion of test
actions to satisfy the dependencies, automatically.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,FILIPE MARQUES CHAVES DE ARRUDA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Test Automation. Capture & Replay. Reuse. Natural Language Processing. Alloy. Model Finding.',ENGENHARIA DE SOFTWARE,AUGUSTO CESAR ALVES SAMPAIO,95,Automação de Testes. Capture & Replay. Reuso. Processamento de Linguagem Natural. Alloy. Verificação de Modelos.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Um cenário comum na automação de testes inicia-se a partir da escrita dos casos de testes por
um engenheiro de testes, sem alguma especificação formal ou supervisão por software. Então,
um desenvolvedor tenta interpretar o que o teste potencialmente ambíguo significa antes de
transformá-lo em código executável. Finalmente, um testador é responsável por verificar se as
falhas são realmente bugs, ou erros cometidos pelo desenvolvedor, pelo engenheiro de teste ou
apenas uma mudança nos requisitos ou na interface de usuário. Para reduzir estes problemas
recorrentes na automação de testes, nós propomos um processo unificado de escrita de casos de
testes usando linguagem natural controlada que permite a verificação automática de consistência
e dependências na escrita de ações de teste, ou usando uma busca por descrições similares de
ações quando escritas em linguagem natural livre. O processo proposto é aplicado em uma estratégia
de capture & replay reusável, assim permitindo a automação por pessoas sem background
em programação, além de mitigar problemas intrínsecos de escalabilidade e manutenabilidade
através do reuso de ações de teste com granularidade que pode variar desde comandos simples,
como abrir um aplicativo, um caso de teste inteiro, ou mesmo uma suite de testes. As ações
são representadas através de uma notação abstrata e framework-agnóstica. Além disso, a implementação
ainda abrange plugins que incluem captura de voz e processamento de imagem;
e rastreabilidade proativa para os artefatos. A estratégia foi concretizada através de sua implementação
em uma ferramenta (Zygon) e avaliada no contexto de uma parceria com a Motorola
Mobility, tendo sido adotada na prática em diferentes suítes de testes usando a linguagem natural
livre, atingindo um reuso de até 71% com ganho de tempo similar a abordagens de capture &
replay quando comparadas à programação. Além disso, através da padronização da linguagem
natural controlada, foi possível definir uma noção de consistência e capturar um grafo de associações
(dependências e cancelamentos) entre ações de teste. A partir destas associações, foi
possível estabelecer mecanismos para garantir que cada ação de um teste devesse ser precedida
por ações das quais a mesma depende para ser executada. Um editor (Kaki) foi implementado
para mecanizar tanto aderência à CNL, como a dependência entre ações de teste. A partir das
associações informadas na interface gráfica, Kaki cria automaticamente um modelo Alloy e
usa o Alloy Analyzer para fazer a verificação de consistência, além de possivelmente sugerir a
inserção de ações de teste para satisfazer as relações de dependências, automaticamente.",DISSERTAÇÃO,TEST AUTOMATION FROM NATURAL LANGUAGE WITH REUSABLE CAPTURE & REPLAY AND CONSISTENCY ANALYSIS,5017969,1
"In this work, a robotic agent, Angela, was designed and simulated with two
Freedom, capable of performing search and visual exploration in a virtual environment. Its architecture
Allows easy replacement and incorporation of modules that increase
Functionality. On the farm, the visual attention module detects protruding objects and
Promotes learning; In the search, the object proposal generation module can
Detection results. The sorting module, which determines whether the displayed object
Corresponds to the target, is composed of the classifier oiSGNG (Online Incremental
Supervised Growing Neural Gas), which was conceived and implemented in this dissertation. O
OiSGNG resulted in an article accepted for publication in IJCNN 2017 (International Joint
Conference on Neural Networks). The image segmentation module was another contribution
Of this project: StochGrow, an image-based
Growth of regions, provides a solid solution to the commitment of speed and
Quality of real-time targeting. The system allows future ramifications for
Various robotics problems.",,COMPUTAÇÃO INTELIGENTE,FELIPE DUQUE BELFORT DE OLIVEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,13/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'visual search;Visual attention;Robotics;Online learning.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,ALUIZIO FAUSTO RIBEIRO ARAUJO,83,atenção visual;robótica;aprendizado online.;busca visual,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Neste trabalho, foi concebido e simulado um agente robótico, Angela, com dois graus de
liberdade, capaz de realizar busca e exploração visual num ambiente virtual. Sua arquitetura
modular permite a fácil substituição e incorporação de módulos que incrementem
sua funcionalidade. Na exploração, o módulo de atenção visual detecta objetos salientes e
promove o aprendizado; na busca, o módulo de geração de propostas de objeto consegue
sólidos resultados de detecção. O módulo de classificação, que determina se o objeto visualizado
corresponde ao alvo, é composto pelo classificador oiSGNG (Online Incremental
Supervised Growing Neural Gas), que foi concebido e implementado nesta dissertação. O
oiSGNG resultou num artigo aceito para publicação no IJCNN 2017 (International Joint
Conference on Neural Networks). O módulo de segmentação de imagem foi outra contribuição
deste projeto: o StochGrow, algoritmo de segmentação de imagens baseado em
crescimento de regiões, fornece uma sólida solução para o compromisso de velocidade e
qualidade de segmentação em tempo real. O sistema permite ramificações futuras para
atender diversos problemas de robótica.",DISSERTAÇÃO,Busca e Exploração Visual Robótica em Ambiente Simulado,5017988,1
"Software Product Lines (LPS) are set of reusable software products that share functionality
or behavior. Reusing a specific set of products can improve productivity and product quality
offered by a company in the sense that new products can be created by systematically
combining existing artifacts. But SPL maintenance is not simple, since a single change
on asset can impact several products. In many situations, it is desirable to provide some
assurance that we can safely change a SPL in the sense that the behaviour of existing
products is preserved after the change. Developers can rely on previously proposed safe
evolution notions and by means of transformation templates to ensure safe evolution.
However, the existing templates focus only in scenarios where a SPL is expanded with the
development of new features, and have not been evaluated in the context of extracting
features from existing code. Therefore, to find out more templates that fit situations not
foreseen in previous studies of the safe evolution of SPL, we conducted a study using an
industrial system developed in Java, with roughly 400 KLOC, with demand for features
extraction and transform into an SPL.
This study revealed the need for new templates to address feature extraction scenarios, as
well as improving the existing templates notation to address more expressive mappings
(Configuration Knowledge) between feature expressions and code assets. As a result of
this study, we propose new templates that can not be derived from existing ones, we
successfully extracted a SPL from this existing system using the proposed templates,
and also found evidence that the new templates can help to prevent defects during SPL
evolution.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,FERNANDO CHAVES BENBASSAT,UNIVERSIDADE FEDERAL DE PERNAMBUCO,20/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Software Product Lines. Product Line Evolution. Safe Evolution. Refinement.',ENGENHARIA DE SOFTWARE,PAULO HENRIQUE MONTEIRO BORBA,67,Linhas de Produtos de Software. Evolução de Linhas de Produtos. Evolução Segura. Refinamento.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Linhas de Produtos de Software (LPS) são conjuntos de produtos de software reutilizáveis
que compartilham funcionalidades ou comportamento. Reusar um conjunto específico de
produtos pode melhorar a produtividade e qualidade dos produtos oferecidos por uma
empresa, no sentido de que novos produtos podem ser criados combinando de forma
sistemática os artefatos existentes. Porém manter LPS não é tão simples, uma vez que
uma única mudança em um artefato pode afetar vários produtos. Em muitas situações, é
desejável proporcionar algum tipo de garantia para alterar uma LPS de forma segura, no
sentido de que o comportamento dos produtos existentes é preservado após a alteração.
Os desenvolvedores podem contar com noções de evolução segura propostas anteriormente
e por meio de templates de transformação para assegurar a evolução segura.
No entanto, os templates existentes focam apenas em situações em que LPS é expandida
com o desenvolvimento de novas features, e não foram avaliadas no contexto da extração
de features a partir do código existente. Por isso, para descobrir mais templates que se
adequam à situações não previstas em estudos anteriores de evolução segura de Linhas
de Produtos de Software (LPS), foi realizado um estudo utilizando um sistema industrial
desenvolvido em Java, com aproximadamente 400 KLOC, com demanda para extração de
features e transformação em LPS.
Esse estudo revelou a necessidade de novos templates para lidar com cenários de extração
de features, bem como melhorar a notação de templates existentes para tratar mapeamentos
(Configuration Knowledge) mais expressivos entre expressões de features e artefatos de
código. Como resultado deste estudo, nós propomos templates novos e que não podem ser
derivados dos existentes, extraímos com sucesso LPS a partir do sistema existente usando
os templates propostos, e também encontramos evidência de que os novos templates podem
ajudar a prevenir defeitos durante a evolução de uma LPS.",DISSERTAÇÃO,Evolução Segura de Linhas de Produtos de Software: Cenários de Extração de Features,5017994,1
"Software product lines (SPLs) are sets of related systems that are built based on reusable
artifacts. They have three elements: a variability model, that has feature declarations and
dependencies among them; implementation artifacts and a configuration knowledge, that
maps features to their implementation. SPLs provide several advantages, like software
quality and reuse improvements, productivity gains and the capacity to customize a system
depending on customers needs.
There are several challenges in the SPL development context. To build customizable
software and meet all customer needs, SPLs tend to increase over time. The larger a SPL
becomes, the higher is the complexity to evolve it. Therefore, it is not trivial to predict
which products are affected by a change, specially in large SPLs. One might need to check
if products had their behaviour preserved to avoid inadvertently affecting existing users in
an evolution scenario.
In refactoring and conservative extension scenarios, we can avoid this problem by checking
for behavior preservation, either by testing the generated products or by using formal
theories. Product line refinement theories support that by requiring behavior preservation
for all existing products. This happens in a number of situations, such as code refinements.
For instance, in function renaming transformations, all existing products behave exactly
as before the change, so we can say that this transformation is safe. Another example of
SPL refinement would be changing a feature type from mandatory to optional. In this
case, we increase variability, but preserving all products from the original SPL.
Although several evolution scenarios are safe (or technically refinement), in many others,
such as bug fixes or feature removals, there is a high chance that only some of the products
are refined. In these scenarios, the existing theories would give no support, since we can
not assume behaviour preservation holds for all products.
To support developers in these and other non refinement situations, we define partially
safe evolution for product lines, that is formalised through a theory of partial refinement
that helps to precisely understand which products should not be affected by an evolution
scenario. This provides a kind of impact analysis that could, for example, reduce test
effort, since products not affected do not need to be tested. Additionally, we formally
derive a catalog of partial refinement templates that capture evolution scenarios, and
associated preconditions, not covered before. Finally, we evaluate the proposed templates
by analyzing commits from two product line systems (Linux and Soletta) and we found
evidence that those templates could cover a number of practical evolution scenarios.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,GABRIELA CUNHA SAMPAIO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,27/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Product line evolution, product line maintenance, product line refinement'",ENGENHARIA DE SOFTWARE,PAULO HENRIQUE MONTEIRO BORBA,108,"Evolução de linhas de produto, manutenção de linhas de produto, refinamento de linhas de produto",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Linhas de produto de software (LPSs) são conjuntos de sistemas relacionados desenvolvidos
a partir de artefatos reusáveis. Há diversas vantagens de se trabalhar com LPS, como
melhorias na qualidade do código e o aumento de reuso, e também ganhos em produtividade
e uma maior customização do software, que se torna configurável para atender aos critérios
dos clientes. Porém, há também muitos desafios. Os sistemas tendem a crescer com o
tempo, o que aumenta a complexidade de evoluir a LPS. Então, a tarefa de descobrir
o conjunto de produtos afetados em uma mudança se torna não trivial, principalmente
em LPS maiores. Os desenvolvedores eventualmente precisam verificar se os produtos
existentes preservaram comportamento para evitar afetar usuários inadvertidamente.
Em cenários de refatoração e extensão conservadora, nós podemos evitar esse problema
checando se o comportamento dos produtos foi preservado através da realização de testes
nos produtos gerados, ou ainda com o uso de teorias formais. De fato, isso acontece em
várias situações. Por exemplo, em cenários de refinamentos de código, como renomeações
de funções, todos os produtos continuam se comportando exatamente da mesma forma,
então nós dizemos que esta evolução é segura. Outro exemplo de refinamento de LPS
seria alterar o tipo de uma feature mandatória para opcional. Neste caso, nós estamos
aumentando variabilidade, mas preservando todos os produtos da LPS original.
Apesar de haver um grande número de cenários de evolução segura (o que tecnicamente, é
sinônimo de refinamento), em outros, como correções de defeitos ou remoções de features,
existe uma chance razoável de apenas alguns produtos serem refinados. Nestes cenários,
as teorias existentes não seriam capazes de dar suporte, já que nem todos os produtos
preservam comportamento.
Para dar suporte aos desenvolvedores nestes e em outros cenários de não refinamento, nós
definimos o conceito de evolução parcialmente segura de linhas de produto de software,
que é formalizado através de uma teoria de refinamento parcial, que ajuda a entender
precisamente que produtos não devem ser afetados num cenário de evolução. Com isto, nós
provemos uma espécie de análise de impacto que poderia, por exemplo, reduzir o esforço
envolvido no desenvolvimento de testes, dado que produtos não afetados não precisariam
ser testados. Adicionalmente, nós derivamos formalmente um catálogo de templates de
refinamento parcial que capturam cenários de evolução, e pré-condições associadas, não
cobertos anteriormente. Finalmente, nós avaliamos os templates propostos através de uma
análise de commits de duas LPS (Linux e Soletta) e encontramos evidência de que os
templates poderiam cobrir uma série de cenários práticos de evolução.",DISSERTAÇÃO,Partially Safe Evolution of Software Product Lines,5017997,1
"Facial expressions provide information on the emotional response and play an essential
role in human interaction and as a form of non-verbal communication. However, the recognition
of expressions is still something considered complex for the computer. In this work,
it is proposed a novel feature extractor that uses motion estimation for Facial Expression
Recognition (FER). In this approach, the facial movement between two expressions is coded
using an estimation of the region displacements between two images, which may be of
the same face or the like. The facial expression image is compared to another more similar
image in each facial expression of the training base, the best match is obtained using the
Structural Similarity Index (SSIM). After identifying the most similar images in the training
base, the motion vectors are calculated between the reference image and the other more similar
in one of the expressions of the base. To calculate the motion vectors is proposed the
MARSA (Modified Adaptive Reduction of the Search Area) algorithm. All motion vectors
are compared to the coordinates with the highest occurrences of all motion vectors obtained
during the training phase, from this comparison the feature vectors are generated that
serve as input data for a SVM (Support Vector Machine), which will perform the classification
of facial expression. Several databases of images and videos of faces reproducing
facial expressions were used for the experiments, the adopted criteria for selection of the
bases was the frequency which they are used in the state of the art, then were chosen: Cohn-
Kanade (CK), Extended Cohn-Kanade (CK+), Japanese Female Facial Expression (JAFFE), MMI,
and CMU Pose, Illumination, and Expression (CMU-PIE). The experimental results demonstrate
that the recognition rates of facial expressions are compatible to recent literatureworks
proving the efficiency of the presented method.",,COMPUTAÇÃO INTELIGENTE,HEMIR DA CUNHA SANTIAGO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,07/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Feature extraction, block marriage, motion estimation, recognition of facial expressions.'",INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,TSANG ING REN,139,"Extração de características, casamento de bloco, estimação de movimento, reconhecimento de expressões faciais.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"As expressões faciais fornecem informações sobre a resposta emocional e exercem um papel
fundamental na interação humana e como forma de comunicação não-verbal. Contudo,
o reconhecimento das expressões ainda é algo considerado complexo para o computador.
Neste trabalho, propomos um novo extrator de características que utiliza a estimação de
movimento para o reconhecimento de expressões faciais. Nesta abordagem, o movimento
facial entre duas expressões é codificado usando uma estimação dos deslocamentos de regiões
entre duas imagens, que podem ser da mesma face ou de faces similares. A imagem
da expressão facial é comparada a outra imagem mais similar em cada expressão facial da
base de treinamento, amaior similaridade é obtida usando a medida de Similaridade Estrutural
(SSIM - Structural Similarity Index). Após a identificação das imagens mais similares
na base de treinamento, são calculados os vetores de movimento entre a imagem cuja expressão
facial será reconhecida e a outra mais similar em uma das expressões da base. Para
calcular os vetores de movimento é proposto o algoritmoMARSA (Modified Adaptive Reduction
of the Search Area). Todos os vetores demovimento são comparados às coordenadas com
asmaiores ocorrências dentre todos os vetores demovimento obtidos durante a fase de treinamento,
a partir dessa comparação são gerados os vetores de características que servem
de dados de entrada para uma SVM (Support VectorMachine), que irá realizar a classificação
da expressão facial. Diversas bases de imagens e vídeos de faces, reproduzindo expressões
faciais, foram utilizadas para os experimentos. O critério adotado para a escolha das bases
foi a frequência com que são utilizadas em outros trabalhos do estado da arte, portanto
foram escolhidas: Cohn-Kanade (CK), Extended Cohn-Kanade (CK+), Japanese Female Facial Expression
(JAFFE), MMI e CMU Pose, Illumination, and Expression (CMU-PIE). Os resultados
experimentais demostram taxas de reconhecimento das expressões faciais compatíveis a
outros trabalhos recentes da literatura, comprovando a eficiência do método apresentado.",TESE,RECONHECIMENTO DE EXPRESSÕES FACIAIS UTILIZANDO ESTIMAÇÃO DE MOVIMENTO,5017998,1
"The computer’s evolution has shown over the years these devices have evolved in several
features such as new technologies in use, size reduction, cost reduction, increased performance,
and reduced energy consumption. Among these improvements we highlight as fundamental
to embedded system design, the improvements in performance and energy consumption. In
a microprocessor-based system, the major contributor to the energy consumption is the cache
hierarchy, which can account for up to 50% of the energy consumed by the entire system.
This work introduces the AbcDE, a cache design space exploration approach to applicationspecific
MPSoC platforms. The AbcDE uses the algorithm ABC (Artificial Bee Colony) in
multi-objective mode (improvement of performance and energy consumption simultaneuosly)
and using DoE (Design of Experiments) techniques to improve the efficiency of algorithm global
search, reducing the execution time. In the experiments we evaluated the AbcDE approach to
some applications of Splash2 benchmark (fft, radix and matrix multiplication) and ParMiBench
benchmark (Djikstra) and was obtained a L1 cache configurations set into the Pareto front
with a reduction of 42.3% in the exploration time. The mean number of platform executions
is 40.4% lower when compared with the original multi-objective ABC algorithm. All results
were obtained for a NoC-based MPSoC platform using four processors. We also evaluated
the AbcDE approach by executing the previously cited benchmark applications in conjunction
with the benchmark applications Sha, Stringsearch and Basicmath (ParMibench benchmark)
for multilevel cache hierarchy (L1 and L2). Cache configurations within Pareto Front were
obtained and it was obtained a mean number of MPSoC platform simulations at about 37,14 %
smaller than the ABCMOP algorithm, and about 37,10 % smaller than the MOPSO algorithm
(Considering all applications of the experiments). Although obtaining a significant improvement
in efficiency terms, compared to the ABCMOP and MOPSO algorithms, the AbcDE algorithm
did not degrade its accuracy. The AbcDE algorithm, in terms of hypervolume metric, obtained on
average less than the ABCMOP algorithm by only 0.91%, and obtained on average superior to
the MOPSO algorithm by only 0.66%. The AbcDE algorithm was able to achieve optimal results
for multi-level cache configurations efficiently and without degrading its accuracy, simulating
only about 0.13 % of the total design space of the cache hierarchy.",,ENGENHARIA DA COMPUTAÇÃO,MARCUS VINICIUS DUARTE DOS SANTOS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,09/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Design Space Exploration. Swarm Intelligence. Metaheuristic. MPSoC. ABC Algorithm. Design of Experiments.',ENGENHARIA DA COMPUTAÇÃO,EDNA NATIVIDADE DA SILVA BARROS,134,Exploração de Espaço de Projeto. Inteligência de Enxame. Metaheurística. MPSoC. Algoritmo ABC. Projeto de Experimentos.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A evolução dos computadores tem nos mostrado que, com o passar dos anos, esses
equipamentos têm evoluído em diversas características, como novas tecnologias em uso, redução
de tamanho, redução de custo, aumento do desempenho, e redução do consumo de energia.
Entre essas melhorias destacamos como fundamentais para projetos de sistemas embarcados as
melhorias em desempenho de aplicação específica e a melhoria em consumo. Em um sistema
microprocessado, um dos principais responsáveis pelo consumo de energia é a hierarquia de
memória cache, que pode ser responsável por até 50% da energia consumida pelo sistema
completo. Nesse trabalho é apresentada uma abordagem para exploração do espaço de projeto
de memórias cache em plataformas MPSoCs de aplicação específica que utiliza como base
o algoritmo ABCs (Colônia Artificial de Abelhas) adaptado para multi-objetivo (melhoria de
desempenho e de consumo de energia) e utilizando técnicas de DoE (Design of Experiments)
para tornar a busca global do algoritmo mais eficiente, reduzindo seu tempo total de execução. O
algoritmo ABC modificado foi denominado de algoritmo AbcDE. Nos experimentos avaliamos
a abordagem AbcDE executando algumas aplicações dos benchmark Splash2 (fft, radix e
matrix) e o ParMibench (Djikstra) para um nível de cache (L1) e foi obtido um conjunto de
configurações da cache L1 dentro do Pareto front reduzindo o tempo de exploração em uma
média de 42,3%. O número de simulações da plataforma MPSoC foi reduzida em 40,4% quando
comparado com o uso do algoritmo ABC original em multi-objetivo. Os resultados foram obtidos
para uma plataforma MPSoC baseada em NoC com 4 processadores. Também avaliamos a
abordagem AbcDE executando as aplicações dos benchmarks previamente citados em conjunto
com as aplicações do benchmark ParMibench (Sha, Stringsearch e Basicmath) para hierarquia
de cache em multinível (L1 e L2). Foram obtidas configurações de cache dentro do Pareto
Front apresentando uma quantidade média de execuções da plataforma MPSoC em cerca de
37,14% menor que o algoritmo ABCMOP, e em cerca de 37,10 % menor que o algoritmo
MOPSO (considerando todas as aplicações dos experimentos). Mesmo obtendo uma melhoria
significativa em termos eficiência, comparado aos algoritmos ABCMOP e MOPSO, o algoritmo
AbcDE não degradou sua precisão. O algoritmo AbcDE, em termos de hipervolume, foi em
média inferior ao algoritmo ABCMOP em apenas 0,91%, e foi em média superior ao algoritmo
MOPSO em apenas 0,66%. Verificamos que o algoritmo AbcDE conseguiu obter resultados
ótimos para configurações de cache multi-nível com eficiência e sem degradar sua precisão,
simulando apenas cerca de 0,13% do espaço do projeto total da hierarquia de cache.",TESE,UMA ABORDAGEM BASEADA EM METAHEURÍSTICAS PARA EXPLORAÇÃO DO ESPAÇO DE PROJETO DE MEMÓRIAS CACHE MULTINÍVEL EM PLATAFORMAS MULTI-CORES PARA APLICAÇÃO ESPECÍFICA,5018003,1
"In this research, it aims to verify a conceptual model of tools of authoring and execution of Units of learning (UoLs) increases efficiency, effectiveness and satisfaction of users with little skill with technology in the teaching activities of planning, implementation, editing, use and reuse of these UoLs. The motivation of this survey occurred from the gaps exposed by Griffiths and Liber (2009) and Goddard, Griffiths and Mi, (2015) which present, respectively, the opportunities and problems using the standard Instructional Management System-Learning Design (IMS-LD) and its tools, as well as the capabilities of representing resources and reuse of these with IMS-LD. The research method was guided by the Design Science Research (DSR) Which enabled the presentation of useful artifacts to solve the problem in evidence. Furthermore, the systematic revision of the literature (SRL) identified the examples and features of authoring tools and players. The survey was characterized as a qualitative and quantitative nature and used questionnaires, interviews and non-participant observation, such as data collection instruments (FLICK, 2013a). The statistical methods of grouping based on the Euclidean and Gower distances, and the bootstrap resampling technique supported data analyses. The Attrakdiff questionnaire was used in the analysis and comparison of the expectations and satisfaction of the use of the interactive and online educational resource sharing repository (RECREIO) by the sample participants, that was formed by teachers, instructional designers, researchers, educational content developers and educational software developers. The search results culminated in the Recreio conceptual model, which incorporated repository, authoring tools and UAS players, and plug-in integration into learning management system (LMS). The results of the Attrakdiff technique demonstrated good satisfaction from participants by surpassing expectations for the use of Recreio and their duties. While analyses from statistical data confirmed that the proposed model has enabled the reuse of UoLs by stakeholders and is in line with the needs of users with simple and lean semantics, which facilitated and assured interoperability, changes, adjustments, maintenance and reuse of the UoLs.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,FRANCISCO KELSEN DE OLIVEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,09/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'IMS Learning Design;Reuse;Interoperability;Units of learning;Authoring tool.',MÍDIA E INTERAÇÃO,ALEX SANDRO GOMES,190,IMS Learning Design;Reuso;Interoperabilidade;Unidades de aprendizagem;ferramenta de autoria.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Na presente pesquisa, objetivou-se verificar um modelo conceitual de ferramentas de autoria e de execução de Unidades de Aprendizagem (UA) aumenta a eficiência, a eficácia e a satisfação de usuários com pouca habilidade com tecnologia nas atividades docentes de planejamento, implementação, edição, uso e reuso dessas UAs. A motivação dessa pesquisa ocorreu a partir das lacunas expostas por Griffiths e Liber (2009) e por Goddard, Griffiths e Mi, (2015) que apresentam, respectivamente, as oportunidades e os problemas de uso do padrão de especificação Instructional Management System - Learning Design (IMS-LD) e de suas ferramentas, bem como as potencialidades de representação dos recursos e reuso desses com IMS-LD. O método da pesquisa foi norteado pelo Design Science Research (DSR), que possibilitou a apresentação de artefatos úteis para solução do problema em evidência. Além disso, a revisão sistemática da literatura (RSL) identificou os exemplos e as características de ferramentas de autoria e players. A pesquisa foi caracterizada como de natureza quali-quantitativa e utilizou questionários, entrevistas e observação não participante, como instrumentos de coletas de dados (FLICK, 2013a). Os métodos estatísticos de agrupamento baseados nas distâncias Euclidiana e de Gower, e a técnica de reamostragem Bootstrap deram suporte às análises dos dados. O questionário Attrakdiff foi utilizado na análise e comparação das expectativas e das satisfações de uso do Repositório de Compartilhamento de Recursos Educacionais Interativos e Online (RECREIO) pelos participantes da amostra, que foi formada por professores, designers instrucionais, pesquisadores, desenvolvedores de conteúdos educacionais e desenvolvedores de softwares educativos. Os resultados da pesquisa culminaram no modelo conceitual Recreio, que integrou repositório, ferramentas de autoria e players de UAs, e plug-in de integração aos ambientes virtuais de aprendizagem (AVA). Os resultados da técnica Attrakdiff demonstraram boa satisfação dos participantes ao superarem as expectativas quanto ao uso do Recreio e de suas funções. Enquanto as análises a partir dos dados estatísticos confirmaram que modelo proposto possibilitou o reuso das UAs pelos interessados e está em consonância às necessidades dos usuários com uma semântica simples e enxuta, que facilitou e garantiu interoperabilidade, alterações, adequações, manutenções e reutilizações das UAs.",TESE,UM MODELO CONCEITUAL DE REUSO DE UNIDADES DE APRENDIZAGEM PARA MÚLTIPLAS PLATAFORMAS,5018340,1
"A time series is a collection of observations measured sequentially in time. Several realworld
dynamic processes can be modeled as time series. One of the main problems of time
series analysis is the forecasting of future values. As a special kind of data stream, a time
series may present concept drifts, which are changes in the underlying data generation
process from time to time. The concept drift phenomenon affects negatively the forecasting
methods which are based on observing past behaviors of the time series to forecast future
values. Despite the fact that concept drift is not a new research area, the effects of concept
drifts in time series are not widely studied. Some approaches proposed in the literature to
handle concept drift in time series are passive methods that successive update the learned
model to the observations that arrive from the data stream. These methods present no
transparency to the user and present a potential waste of computational resources. Other
approaches are active methods that implement a detect-and-adapt scheme, in which the
learned model is adapted just after the explicit detection of a concept drift. By using explicit
detection, the learned model is updated or retrained just in the presence of drifts, which
can reduce the space and computational complexity of the learning system. These methods
are generally based on monitoring the residuals of a fitted model or on monitoring the
raw time series observations directly. However, these two sources of information (residuals
and raw observations) may not be so reliable for a concept drift detection method applied
to time series. Residuals of a fitted model may be influenced by problems in training.
Raw observations may present some variations that do not represent significant changes
in the time series data stream. The main contribution of this work is an active adaptive
learning system which is able to handle concept drift in time series. The proposed method,
called Feature Extraction and Weighting for Explicit Concept Drift Detection (FW-FEDD)
considers a set of time series features to detect concept drifts in time series in a more
reliable way, being trustworthy and transparent to users. The features considered are
weighted according to their importance to define concept drifts at each instant. A concept
drift test is then used to detect drifts in a more reliable way. FW-FEDD also implements
a forecasting module composed by a pool of forecasting models in which each model is
specialized in a different time series concept. Several computational experiments on both
artificial and real-world time series showed that the proposed method is able to improve
the concept drift detection accuracy compared to methods based on monitoring raw time
series observations and residual-based methods. Results also showed the superiority of
FW-FEDD compared to other passive and active adaptive learning systems in terms of
forecasting performance.",,COMPUTAÇÃO INTELIGENTE,RODOLFO CARNEIRO CAVALCANTE,UNIVERSIDADE FEDERAL DE PERNAMBUCO,13/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Adaptive learning systems. Data streams. Concept drift. Time series. Forecasting.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,ADRIANO LORENA INACIO DE OLIVEIRA,151,Sistemas de aprendizado adaptativo. Fluxos de Dados. Mudança de Conceito. Séries Temporais. Previsão.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Uma série temporal é uma coleção de observações medidas sequencialmente no tempo.
Diversos processos dinâmicos reais podem ser modelados como uma série temporal. Um
dos principais problemas no contexto de séries temporais é a previsão de valores futuros.
Sendo um tipo especial de fluxo de dados, uma série temporal pode apresentar mudança
de conceito, que é a mudança no processo gerador dos dados. O fenômeno da mudança
de conceito afeta negativamente os métodos de previsão baseados na observação do
comportamento passado da série para prever valores futuros. Apesar de que mudança de
conceito não é uma nova área, os efeitos da mudança de conceito em séries temporais ainda
não foram amplamente estudados. Algumas abordagens propostas na literatura para tratar
esse problema em séries temporais são métodos passivos que atualizam sucessivamente o
modelo aprendido com novas observações que chegam do fluxo de dados. Estes métodos
não são transparentes para o usuário e apresentam um potencial consumo de recursos
computacionais. Outras abordagens são métodos ativos que implementam um esquema
de detectar-e-adaptar, no qual o modelo aprendido é adaptado somente após a detecção
explícita de uma mudança. Utilizando detecção explícita, o modelo aprendido é atualizado
ou retreinado somente na presença de mudanças, reduzindo a complexidade computacional e
de espaço do sistema de aprendizado. Estes método são geralmente baseados na monitoração
dos resíduos de um modelo ajustado ou na monitoração dos dados da série diretamente. No
entanto, estas duas fontes de informação (resíduos e dados crus) podem não ser tão confiáveis
para um método de detecção de mudanças. Resíduos de um modelo ajustado podem ser
influenciados por problemas no treinamento. Observações cruas podem apresentar variaçõs
que não representam mudanças significativas no fluxo de dados. A principal contribuição
deste trabalho é um sistema de aprendizado adaptativo ativo capaz de tratar mudanças
de conceito em séries temporais. O método proposto, chamado de Feature Extraction and
Weighting for Explicit Concept Drift Detection (FW-FEDD) considera um conjunto de
características da série temporal para detectar mudança de conceito de uma forma mais
confiável, sendo transparente ao usuário. As características consideradas são ponderadas
de acordo com sua importância para a definição das mudanças em cada instante. Um teste
de mudança de conceito é utilizado para detectar as mudanças de forma mais confiável.
FW-FEDD também implementa um módulo de previsão composto por um conjunto
de modelos de previsão onde cada modelo é especializado em um conceito diferente.
Diversos experimentos computacionais usando séries reais e artificiais mostram que o
método proposto é capaz de melhorar a detecção de mudança de conceito comparado com
métodos baseados na monitoração de dados crus da série e métodos baseados em resíduos.
Resultados também mostraram a superioridade do FW-FEDD comparado com outros
métodos de aprendizado adaptativo ativos e passivos em termos de acurácia de predição.",TESE,AN ADAPTIVE LEARNING SYSTEM FOR TIME SERIES FORECASTING IN THE PRESENCE OF CONCEPT DRIFT,5018357,1
"Text Classification is a problem in which a natural language document is assigned to
one of the pre-established classes. Text Classification, with feature vectors generated by Bagof-
Words, has two notable difficulties: high dimensionality and sparse data matrix. Feature
selection reduces these difficulties, but discards information in the process. An alternative is
to perform transformations over the features, because by altering the features it is possible to
work without discarding information, allowing improvement of recognition rates and, in some
cases, reduction of dimensionality and sparseness. Among these transformations, two underused
in literature are: Dissimilarity Representation (DR), where each document is represented by a
vector composed of distances calculated relative to a set of reference documents; and Dichotomy
Transformation (DT), where the original problem is transformed into a binary problem and
each document is transformed into several vectors with features obtained by the absolute value
of the difference for the documents of a subset of the original set. The use of DR can reduce
both the high dimensionality and sparseness. Whereas the use of DT, despite not reducing
dimensionality or sparseness, improves the recognition rates of the classifier, since it works
with a larger amount of documents on a problem transformed into two classes. In this work,
two multiple classifiers systems for Text Classification are proposed: Combined Dissimilarity
Spaces (CoDiS) and Combined Dichotomy Transformations (CoDiT), each one based on the
transformations mentioned above. The multiple classifiers benefits from the need to find a set
for the transformations, because using different sets allows the creation of a diverse and robust
system. Experiments were performed comparing the proposed architectures with literature
methods using up to 47 public databases and the results show that the proposals achieve superior
performance in most cases.",,COMPUTAÇÃO INTELIGENTE,ROBERTO HUGO WANDERLEY PINHEIRO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,17/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Text Classification. Dissimilarity Representation. Dichotomy Transformation. Multiple Classifiers System.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,GEORGE DARMITON DA CUNHA CAVALCANTI,116,Classificação de Documentos. Dissimilarity Representation. Dichotomy Transformation. Combinação de Classificadores.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Classificação de Documentos é um problema no qual um documento em linguagem
natural deve ser designado como pertencente à uma das classes pré-estabelecidas. A Classificação
de Documentos, com vetores de características gerados pela Bag-of-Words, possui duas
dificuldades notáveis: alta dimensionalidade e matriz de dados esparsa. Seleção de características
reduzem essas dificuldades, mas descarta informação no processo. Uma alternativa é realizar
transformações sobre as características, pois ao alterar as características é possível trabalhar
sem descartar informações, possibilitando uma melhoria nas taxas de reconhecimento e, em
alguns casos, redução da dimensionalidade e esparsidade. Dentre essas transformações, duas
pouco utilizadas na literatura são: Dissimilarity Representation (DR), no qual cada documento
é representado por um vetor composto de distâncias calculadas com relação a um conjunto
de documentos referência; e Dichotomy Transformation (DT), no qual o problema original é
transformado em um problema binário e cada documento é transformado em vários vetores com
características obtidas pelo valor absoluto da diferença para os documentos de um subconjunto
do conjunto original. A utilização da DR pode reduzir tanto a alta dimensionalidade quanto a
esparsidade. Enquanto que a utilização da DT, apesar de não reduzir a dimensionalidade ou
esparsidade, melhora as taxas de reconhecimento do classificador, pois trabalha com uma quantidade
maior de documentos sobre um problema transformado para duas classes. Neste trabalho,
são propostos dois sistemas de múltiplos classificadores para Classificação de Documentos:
Combined Dissimilarity Spaces (CoDiS) e Combined Dichotomy Transformations (CoDiT),
cada um baseado em uma das transformações citadas acima. Os múltiplos classificadores se
beneficiam da necessidade de encontrar um conjunto para as transformações, pois utilizando
diferentes conjuntos possibilita a criação de um sistema diverso e robusto. Experimentos foram
realizados comparando as arquiteturas propostas com métodos da literatura usando até 47 bancos
de dados públicos e os resultados mostram que as propostas atingem desempenho superior na
maioria dos casos.",TESE,COMBINAÇÃO DE CLASSIFICADORES EM DIFERENTES ESPAÇOS DE CARACTERÍSTICAS PARA CLASSIFICAÇÃO DE DOCUMENTOS,5018386,1
"In a collaborative software development environment, developers often implement their
contributions (or tasks) independently using local versions of the files of a system. However,
contributions from different developers need to be integrated (merged) to a central version of
the system, which may lead to different types of conflicts such as syntactic, static semantic
or even dynamic semantic conflicts. The first two types are more easily identifiable as
they lead to syntactically incorrect programs and to programs with compilations problems,
respectively. On the other hand, dynamic semantic conflicts, which may be caused by
subtle dependencies between contributions, may not be noticed during the integration
process. This type of conflict alters the expected behaviour of a system, leading to bugs.
Thus, failing to detect dynamic semantic conflicts may affect a system’s quality.
Hence, this work’s main goal is to understand if Information Flow Control (IFC), a security
technique used for discovering leaks in software, could be used to indicate the presence of
dynamic semantic conflicts between developers contributions in merge scenarios. However,
as defining if a dynamic semantic conflict exists involves understanding the expected
behaviour of a system, and as such behavioural specifications are often hard to capture,
formalize and reason about, we instead try to detect a code level adaptation of the notion
of interference from Goguen and Meseguer. Actually, we limit our scope to interference
caused by developers contributions on the same method. More specifically, we want to
answer if the existence of information flow between developers same-method contributions
of a merge scenario can be used to estimate the existence of interference.
Therefore, we conduct an evaluation to understand if information flow may be used to
estimate interference. In particular, we use Java Object-sensitive ANAlysis (JOANA)
to do the IFC for Java programs. JOANA does the IFC of Java programs by using a
System Dependence Graph (SDG), a directed graph representing the information flow
through a program. As JOANA accepts different options of SDG, we first establish which
of these SDG options (instance based without exceptions) is the most appropriate to
our context. Additionally, we bring evidence that information flow between developers
same-method contributions occurred for around 64% of the scenarios we evaluated. Finally,
we conducted a manual analysis, on 35 scenarios with information flow between developers
same-method contributions, to understand the limitations of using information flow to
estimate interference between same-method contributions. From the 35 analysed scenarios,
for only 15 we considered that an interference in fact existed. We found three different
major reasons for detecting information flow and no interference: cases related to the
nature of changes, to excessive annotation from our strategy and to the conservativeness of
the flows identified by JOANA. We conclude that information flow may be used to estimate
interference, but, ideally, the number of false positives should be reduced. In particular,
we envisage room for solving around three quarters of the obtained false positives.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,ROBERTO SOUTO MAIOR DE BARROS FILHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Software merging. Dynamic semantic conflict. Interference. Information flow. System Dependence Graph (SDG)',ENGENHARIA DE SOFTWARE,PAULO HENRIQUE MONTEIRO BORBA,127,Integração de software. Conflito de semântica dinâmica. Interferência. Fluxo de informação. System Dependence Graph (SDG),CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Em um ambiente de desenvolvimento colaborativo, desenvolvedores frequentemente implementam
suas contribuições (ou tarefas) independentemente usando versões locais dos
arquivos de um sistema. No entanto, contribuições de diferente desenvolvedores precisam
ser integradas a uma versão central do sistema, o que pode levar a diferentes tipos de
conflitos de integração como conflitos sintáticos, de semântica estática ou até de semântica
dinâmica. Os dois primeiros tipos são mais fáceis de identificar dado que eles levam a
programas sintaticamente incorretos e a erros de compilação, respectivamente. Por outro
lado, conflitos de semântica dinâmica, que são em geral causados por dependências sutis
entre as contribuições, podem não ser percebidos durante o processo de integração. Esse
tipo de conflito altera o comportamento esperado de um sistema, o que leva a bugs.
Portanto, falhar em detectar estes conflitos pode afetar negativamente a qualidade de um
sistema.
Tendo isso em mente, o principal objetivo deste trabalho é entender se Information Flow
Control (IFC), uma técnica de segurança utilizada para descobrir vazamentos de segurança
em software, pode ser utilizado para indicar a presença de conflitos de semântica dinâmica
entre contribuições de cenários de integração. Porém, como a definição da existência de
um conflito de semântica dinâmica envolve o entendimento do comportamento esperado
de um sistema, e como especificações desse tipo de comportamento são geralmente difíceis
de capturar, formalizar e entender, nós na realidade utilizamos uma adaptação a nível de
código da noção de interferência de Goguen e Meseguer. Na realidade, nós limitamos o nosso
escopo a interferência causada por contribuições de desenvolvedores nos mesmos métodos.
Mais especificamente, nós desejamos responder se a existência de fluxo de informação entre
duas contribuições (de dois desenvolvedores) no mesmo método pode ser utilizada para
estimar a existência de interferência.
Portanto, nós realizamos uma avaliação com o intuito de entender se fluxo de informação
pode ser usado para estimar interferência. Em particular, nós utilizamos o Java Objectsensitive
ANAlysis (JOANA) para fazer o IFC de programas Java. O JOANA faz o IFC
de programas Java usando uma estrutura chamada System Dependence Graph (SDG),
um grafo direcionado representando o fluxo de informação em um programa. Como o
JOANA aceita diferentes opções de SDG, primeiro nós estabelecemos qual destas opções
é a mais apropriada para o nosso contexto. Adicionalmente, trazemos evidência que fluxo
de informação entre contribuições de desenvolvedores no mesmo método aconteceram para
cerca de 64% dos cenários que avaliamos. Finalmente, realizamos uma análise manual, em
35 cenários de integração com fluxo de informação entre contribuições de desenvolvedores
no mesmo método, para entender as limitações de utilizar fluxo de informação para
estimar interferência entre contribuições no mesmo método. Dos 35 cenários analisados,
para apenas 15 consideramos que uma interferência existia de fato. Nós achamos três
razões principais para fluxo de informação ser detectado e não existir interferência: casos
relacionados a natureza das mudanças, a limitações da nossa estratégia de anotação e a
natureza conservadora dos fluxos identificados pelo JOANA. Nós concluímos que fluxo de
informação pode ser utilizado para estimar interferência, mas, idealmente, o número de
falsos positivos precisa ser reduzido. Em particular, nós enxergamos espaço para reduzir
até três quartos dos falsos positivos.",DISSERTAÇÃO,Using information flow to estimate interference between developers same-method contributions,5018395,1
"Competency questions have an important role in the development of ontologies. Usually,
they are used as ontology requirements, however many ontology development methodologies
do not detail how to check the requirements or only suggest checking the questions manually.
Thus, there is a lack of tools to check competency questions automatically, including using
reasoners, that could make the development faster and could improve the quality of ontologies.
Furthermore, requirement traceability for ontology engineering is rarely explored, even though
it is studied and used by software engineers for years. In this work, we introduce an iterative
method to expand ontologies using competency questions, and a tool that implements this method.
Many novel approaches are presented: a component that translates natural language competency
questions to description logics to check them automatically using reasoners; a component that
generates competency questions to guide engineers; a component that writes OWL code using
questions and answers; a tracker that monitors the relations among requirements and code and
vice-versa; and a method that integrates all previous components to create an iterative way to
expand ontologies using questions and answers similar to a controlled dialogue. To evaluate the
method and its implementation we ran tests using the SNOMED CT ontology to analyze the
behavior of the developed components. Also, we did two case studies, thus users could evaluate
the tool. The tests showed the capacity of the method to check and add knowledge to an ontology.
We could create questions to check all chosen axioms of SNOMED CT, and the implementation
was able to suggest questions to add knowledge in 69,1% of cases. The case studies exposed
the strength and weakness of the implementation. They showed the implementation potential
to improve the ontology development, because it is simple to interact using natural language to
check and to add axioms, even for non-experts users. Also, the requirement traceability stores
and presents important information for the ontology engineers.",,COMPUTAÇÃO INTELIGENTE,YURI DE ALMEIDA MALHEIROS BARBOSA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,23/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Ontology. Ontology engineering. Competency questions. Description logics.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,FREDERICO LUIZ GONCALVES DE FREITAS,145,Ontologias. Engenharia de ontologias. Questões de competência. Lógicas de descrição.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Questões de competência possuem um papel importante no ciclo de desenvolvimento
de ontologias. Elas são amplamente usadas para definir requisitos de ontologias, entretanto a
maioria das metodologias de desenvolvimento não especifica como a checagem dos requisitos
deve ser feita ou apenas sugere que as questões sejam verificadas manualmente. Assim, faltam
ferramentas que suportem checagem automática, inclusive utilizando raciocinadores, que podem
agilizar o desenvolvimento e melhorar a qualidade das ontologias produzidas. Além disso,
a rastreabilidade de requisitos raramente é explorada, mesmo essa atividade sendo estudada
e utilizada por anos na engenharia de software. Nesse trabalho são introduzidos um método
iterativo para expansão de ontologias usando questões de competência e uma ferramenta que
implementa o método. Várias inovações são apresentadas: um componente que traduz questões
de competência em linguagem natural para lógica de descrição para efetuar checagem automática
usando raciocinadores; um gerador de questões de competência para guiar engenheiros durante o
desenvolvimento; um componente que escreve código OWL de acordo com perguntas e respostas;
um rastreador que monitora as relações entre requisitos e código e vice-versa; e um método
que integra todos os pontos anteriores, criando uma maneira iterativa de expandir ontologias
através de perguntas e respostas semelhante a um diálogo controlado. Para avaliar o método
proposto e sua implementação foram executados testes com a ontologia SNOMED CT para
analisar o comportamento dos componentes criados. Também foram realizados dois estudos de
caso para avaliar o uso da ferramenta por usuários. Os testes mostraram a capacidade do método
em checar e adicionar conhecimento a uma ontologia. Foi possível criar perguntas para checar
todos os axiomas escolhidos da SNOMED CT e a implementação conseguiu sugerir perguntas
para adicionar conhecimento à ontologia em 69,1% dos casos. Os estudos de caso levantaram os
pontos fortes e fracos da implementação, mostrando o potencial da implementação em melhorar
o desenvolvimento de ontologias, pois a interação através de linguagem natural é simples tanto
para checagem quanto para adição de axiomas, mesmo para usuários leigos, e a rastreabilidade
de requisitos grava e apresenta informações importantes para o engenheiro de ontologias.",TESE,UM MÉTODO DE EXPANSÃO DE ONTOLOGIAS BASEADO EM QUESTÕES DE COMPETÊNCIA COM RASTREABILIDADE AUTOMÁTICA,5018398,1
"The increasing inclusion of mobile robots in people’s daily lives, sharing space as agents
in a range of different activities, has given rise to the creation of a series of new composite technologies.
In this context, it appears the Semantic Mapping, which aims to create an abstraction
or representation of space in which a robot navigates in order to provide a means for common
understanding and communication between these robots and humans. This abstraction is created
in the form of a map that aggregates semantic information (i.e., information that makes sense
to a human in terms of communication) about the environment in which the robot is. In this
way, this dissertation introduces an incremental semantic mapping approach, with online and
unsupervised learning, based on self-organizing maps (SOMs) with time-varying topology. The
approach is divided into the mapping module, which incrementally creates topological maps
of environments, enriched with recognized objects as determinant semantic information, and
in the module of places categorization, endowed with an incremental, unsupervised learning
method with online training, based on SOM. In order to evaluate the viability of the approach, it
was tested in experiments with real world data, which demonstrated a promising capability for
the incremental acquisition of topological maps enriched with semantic information and for the
categorization of places based on this information.",,COMPUTAÇÃO INTELIGENTE,YGOR CESAR NOGUEIRA SOUSA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,20/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Semantic Mapping. Self-Organizing Maps. Unsupervised Machine Learning. Incremental Learning. Mobile Robotics.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,HANSENCLEVER DE FRANCA BASSANI,81,Mapeamento Semântico. Mapas Auto-organizáveis. Aprendizagem de Máquina Não-supervisionada. Aprendizagem Incremental. Robótica Móvel.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A crescente inclusão de robôs móveis na vida cotidiana das pessoas, compartilhando
espaço como agentes em diferentes atividades, tem dado impulso à criação de uma série de novas
tecnologias compostas. Neste contexto, aparece o Mapeamento Semântico, que visa criar uma
abstração ou representação do espaço em que um robô navega, a fim de proporcionar um meio
para comum entendimento e comunicação entre estes robôs e seres humanos. Essa abstração
é criada sob a forma de um mapa que agrega informações semânticas (isto é, informações
que façam sentido para um ser humano em termos de comunicação) sobre o ambiente no qual
o robô se encontra. Desta forma, esta dissertação introduz uma abordagem de Mapeamento
Semântico incremental, com aprendizagem online e não-supervisionada, baseada em Mapas
Auto-organizáveis (SOMs) de topologia variante no tempo. A abordagem se divide no módulo de
mapeamento, o qual cria mapas topológicos de ambientes incrementalmente, enriquecidos com
objetos reconhecidos como informação semântica determinante, e no módulo de categorização
de lugares, dotado de um método de aprendizagem incremental, não-supervisionado, com treinamento
online, baseado em SOM. Com o intuito de avaliar a viabilidade da abordagem, a mesma
foi testada a partir de experimentos realizados com uma base dados reais, os quais demonstraram
de forma promissora sua capacidade na aquisição incremental de mapas topológicos enriquecidos
com informações semânticas e na categorização de lugares mapeados a partir destas informações.",DISSERTAÇÃO,MAPEAMENTO SEMÂNTICO INCREMENTAL COM APRENDIZAGEM ONLINE E NÃO-SUPERVISIONADA,5018401,1
"It is perceibed there is some concern to find solutions to support learning facing the challenges of introducing new technologies in the school context. These studies seek to define and combine new teaching standards, mobile technologies and software tools to contribute to learning (MILRAD; WONG; SHARPLES; HWANG; LOOI and OGATA, 2013). Considering the above, the review of the literature on individual student monitoring indicates that despite the existence of various assessment tools and student's monitoring, there is a gap on the communication between teacher-student, with regard to social regulation and the individual monitoring. One should note that social regulation is often used to explain the phenomenon of participation in learning activities, its dynamics and relational nature (MICHINOV; BRUNOT; LEBOHEC; JUHEL and DELAVAL, 2010). Thus, the problem addressed in this research refers to the difficulty of teachers in performing effectively the individual monitoring of students, at a later stage from the presentation of an epistemic object as well as the impact of monitoring the learning phenomenon. Thus, the main goal of this thesis is to check whether and how the phenomenon of social regulation in computational tool supported by an individual monitoring student approach can contribute to the process of individual monitoring and interaction between teacher and student. Therefore, the approach to individual monitoring of students was drawn from the literature review and implemented in Cadmo tool. Thus, the research method adopted to evaluate the acceptance of this approach as well as the implemented tool is a case study, whose procedures followed the process of Grounded Theory. Two studies were carried out, including the modalities of face-to-face and distance education in high education. The analysis of the results was performed using a qualitative approach, based on both Activity Theory and Grounded Theory. The results indicate that Cadmo computational tool for monitoring individual was well accepted by teachers and students participating in the experiment, and can stimulate social regulation between teachers and students, reducing the difficulty of monitoring the student.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,ROSANGELA SARAIVA CARVALHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Feedback. Individual Monitoring of Student. Interaction.',MÍDIA E INTERAÇÃO,ALEX SANDRO GOMES,368,Feedback. Acompanhamento Individual do Discente. Interação.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Nota-se certa preocupação em encontrar soluções que apoiem a aprendizagem mediante os desafios da introdução das novas tecnologias no contexto escolar. Tais pesquisas buscam definir e combinar novos padrões de ensino, tecnologias móveis e ferramentas de software de modo a contribuir para a aprendizagem. Considerando o exposto, a revisão da literatura acerca do acompanhamento individual do discente indica que apesar da existência de diversas ferramentas de avaliação e de acompanhamento do discente, observa-se uma lacuna quanto à comunicação docente-discente no que tange à regulação social e ao acompanhamento individual. Note-se que a regulação social geralmente é usada para explicar o fenômeno da participação em atividades de aprendizagem, sua dinâmica e natureza relacional. Assim, o problema tratado nesta pesquisa refere-se à dificuldade de o docente realizar de forma efetiva o acompanhamento individual do discente em etapa posterior à apresentação do objeto epistêmico, bem como o impacto desse acompanhamento sobre o fenômeno da aprendizagem. Desta forma, o objetivo geral desta tese é verificar se o fenômeno da regulação social em ferramenta computacional apoiado por uma abordagem de acompanhamento Individual do discente torna efetivo o processo de acompanhamento individual e de interação entre o docente e o discente. Para tanto, a abordagem de acompanhamento individual do discente foi elaborada a partir da revisão da literatura e implementada na ferramenta Cadmo. Assim, o método de pesquisa adotado para avaliar a aceitação da referida abordagem, bem como da ferramenta implementada, é um estudo de caso, cujos procedimentos seguiram o processo da Grounded Theory. Foram realizados 2 estudos de caso que incluiram as modalidades de ensino presencial e a distância no ensino superior. A análise dos resultados obtidos foi realizada por meio de uma abordagem qualitativa, fundamentada na Teoria da Atividade e na Grounded Theory. Os resultados indicam que a ferramenta computacional Cadmo foi bem aceita pelos docentes e discentes participantes, podendo estimular a regulação social entre o docente e o discente, reduzindo a dificuldade de acompanhamento do discente.",TESE,UMA FERRAMENTA COMPUTACIONAL PARA REGULAÇÃO SOCIAL DO DISCENTE,5018680,1
"Time series forecasting is an important research field in machine learning. Since the

literature shows several techniques for the solution of this problem, combining outputs of

different models is a simple and robust strategy. However, even when using combiners, the

experimenter may face the following dilemma: which technique should one use to combine

the individual predictors? This work presents a framework for dynamic selection of forecast

combiners. The dynamic selection process can be summarized in three steps. The first one

is responsible for the generation of the base experts set, and this set can be formed by

models of the same kind or heterogeneous ones. The diversity of the experts is important

in both cases. The second phase (selection) is carried out by estimating the competence of

the available models in the set generated in the first phase, with respect to local regions of

the feature space. In the case of dynamic selection, the model selection is performed for

each test pattern, instead of using the same selection for all of them (static selection). The

third phase is the integration of the selected models. In the proposed method, predictors

from statistics (linear and nonlinear) and machine learning were used. As combiners, we

chose techniques that use extra data and some others that do not require an independent

dataset for determining the weights of the linear combination. Two dynamic selection

algorithms were proposed, based on accuracy and behavior. For each of them, variations

were implemented with respect to the use of all or the best predictors and combiners of

the pool. To test the proposed method, ten chaotic time series were used: Mackey- Glass,

Lorenz, Rossler, Henon, Periodic, Quasi-Periodic, Laser and three time series produced

from electroencephalogram exams. The prediction of chaotic series is important for many

areas of human activity such as astronomy and signal processing, and those that were

tested also are used as benchmark in several works. The best variations of the proposed

dynamic selection algorithms have achieved satisfactory results in all databases. After

performing statistical tests, it was verified that the methods were superior to the best

combiners and predictors based on most scenarios, for short and long term forecasting.",,COMPUTAÇÃO INTELIGENTE,ANDERSON TENORIO SERGIO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,17/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Time series forecasting. Dynamic selection. Chaotic time series. Time series  forecasting ensemble. Time series forecasting combining.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,TERESA BERNARDA LUDERMIR,129,Previsão de séries temporais.Seleção dinâmica.Séries temporais caóticas. Comitês de previsão de séries temporais.Combinação de previsão de séries temporais.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A previsão de séries temporais é um importante campo de estudo em aprendizagem de máquinas. Já que a literatura mostra diversas técnicas para a solução desse problema, combinar saídas de diferentes modelos é uma estratégia simples e robusta. Entretanto, mesmo quando usando tais combinadores, o experimentador pode encarar o seguinte dilema: qual técnica deve ser usada para combinar os preditores individuais? Este trabalho apresenta um arcabouço para seleção dinâmica de combinadores de previsão de séries temporais.
O processo de seleção dinâmica pode ser resumido em três fases. A primeira delas é responsável pela geração do conjunto de especialistas base, sendo que esse conjunto pode ser formado por modelos de mesma natureza ou heterogêneos. A diversidade dos especialistas é importante em ambas as situações. A segunda fase, de seleção, é realizada através da estimação da competência dos modelos disponíveis no conjunto gerado na primeira fase, em respeito a regiões locais do espaço de características.  No caso da seleção dinâmica, a escolha dos modelos é realizada para cada padrão de teste, ao invés de utilizar a mesma seleção para todos eles (seleção estática). A terceira fase é a integração dos modelos selecionados. No método proposto, foram utilizados como preditores individuais modelos estatísticos (lineares e não-lineares) e de aprendizagem de máquinas. Como combinadores, técnicas que usam e que não necessitam de uma base de dados independentes para determinação dos pesos da combinação linear. Foram propostos dois algoritmos de seleção dinâmica, baseados em acurácia e comportamento. Para cada um deles, foram implementadas variações no que diz respeito ao uso de todos ou dos melhores preditores e combinadores do comitê.
Para testar o método proposto, dez séries temporais caóticas foram utilizadas: Mackey-Glass, Lorenz, Rossler, Henon, Periodic, Quasi-Periodic, Laser e três séries produzidas a partir de exames de eletroencefalograma. A previsão de séries caóticas tem importância para várias áreas de atuação humana como astronomia e processamento de sinais, sendo que algumas das que foram testadas também funcionam como benchmark em diversas pesquisas. As melhores variações dos algoritmos de seleção dinâmica propostos alcançaram resultados satisfatórios em todas as bases de dados. Após a realização de testes estatísticos, comprovou-se que os métodos foram superiores aos melhores combinadores e preditores base na maioria dos cenários, para previsão de curto e longo alcance.",TESE,SELEÇÃO DINÂMICA DE COMBINADORES DE PREVISÃO DE SÉRIES TEMPORAIS,5020394,1
"Many-Objective Optimization Problems (MaOPs) are an especial class of multiobjective
problems in which four or more objectives are optimized simultaneously. Currently, these
problems have attracted attention of the researchers due the two reasons: (i) many
real-world applications are naturally many-objective problems and (ii) population-based
heuristics presents great difficulties for solving these problems. For example, Paretodominance
based algorithms such as Multiobjective Evolutionary Algorithms (MOEAs)
and Multiobjective Particle Swarm Optimization (MOPSO) algorithms are ineffective in
these problems because almost all solutions in the population become non-dominated
solutions, resulting in loss of convergence pressure for the Pareto front. Because of this,
researchers have proposed new strategies for dealing with this problems, mainly for MOEAs.
However, very little has been done to make the MOPSOs effective in these scenarios.
Regardless of this, Particle Swarm Optimization (PSO) algorithms are known by fast
speed of convergence in single-objective problems and they seem be suitable for MaOPs.
Moreover, to create algorithms that are capable of balancing both convergence and diversity
is a research challange. Therefore, there are a necessity to develop PSO-based algorithms
for dealing with MaOPs. Thus, this thesis proposes a new algorithm based on PSO whose
aim is to promote a better balancing between convergence toward the Pareto front and
diversity of solutions. To achieve this aim, the proposed algorithm uses a set of reference
points to impose a selection pressure to Pareto front while it allows a better control
of the diversity. Furthermore, our algorithm use an external archive where it maintains
non-dominated solutions and from which the social leaders are picked in according to two
measures proposed for us, namely, the convergence measure and the density measure. The
objective of these measures is to promote the convergence toward Pareto Front and to
promote the diversity along it simultaneously. To evaluate our algorithm we used three
well-known metrics in the literature, namely, generational distance, inverted generational
distance, and hypervolume; and six benchmark problems of the DTLZ family with two,
three, five, seven and ten objectives. Moreover, the algorithm was compared to two PSObased
algorithms (SMPSO and CDAS-SMPSO) and three evolutionary algorithms (MDFA,
CEGA and NSGA-III). The results showed that our algorithm was sucessful in balancing
the requirements of convergence and diversity in the test problems compared to other
algorithms in the literature. Besides, our algorithm was applied in a real-world problem
involving the physical topology design of optical network in which it obtained good results.",,COMPUTAÇÃO INTELIGENTE,ELLIACKIN MESSIAS DO NASCIMENTO FIGUEIREDO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,17/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Many-Objective Problems;Evolutionary Algorithm. Particle Swarm Optimization. Swarm Intelligence.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,TERESA BERNARDA LUDERMIR,150,Otimização de Muitos Objetivos;Algoritmos Evolucionários. Enxame de Partículas. Inteligência de Enxames.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Many-Objective Optimization Problems (MaOPs) são uma classe especial de problemas
multiobjetivos que apresentam quatro ou mais objetivos. Algoritmos evolucionários ou
de enxame de partículas tradicionais falham ao tentarem resolver MaOPs pois eles se
tornam ineficazes ou ineficientes nestes problemas. Em virtude disso, alguns pesquisadores
propuseram diferentes estratégias para contornar as dificuldades impostas por MaOPs,
sobretudo para Multiobjective Evolutionary Algorithms (MOEAs) tradicionais para esses
problemas. Em contrapartida, muito pouco tem sido feito no sentido de adaptar algoritmos
de Multiobjective Particle Swarm Optimizations (MOPSOs) para MaOPs. Não obstante,
algoritmos baseados em enxames de partículas no geral são reconhecidos pela rápida
velocidade de convergência em problemas com um único objetivo e por isso parecem ser
também adequados para problemas multiobjetivos e com muitos objetivos. Desta forma,
existe a necessidade de se desenvolver MOPSOs para lidar com MaOPs. Sendo assim,
o objetivo desta tese foi desenvolver um MOPSO para resolver MaOPs no sentido de
promover um maior balanceamento entre convergência para Frente de Pareto e diversidade
de soluções nesses problemas. Para isso, o algoritmo proposto nesta tese usa um conjunto
de pontos de referência para impor uma pressão de convergência para a Frente de Pareto
enquanto permite um maior gerenciamento da diversidade. Além disso, a abordagem
proposta usa um arquivo externo em que são armazenadas soluções não-dominadas e do
qual são retirados os líderes sociais das partículas de acordo com duas medidas que foram
propostas nesta tese, a saber, a medida de convergência e a medida de densidade. O objetivo
dessas medidas é promover a convergência para a Frente de Pareto e promover a diversidade
ao longo dela simultaneamente. A proposta foi avaliada usando seis problemas da família
DTLZ com dois, três, cinco, sete e dez objetivos e usando métricas bem estabelecidas na
literatura (distância geracional, distância geracional invertida e hipervolume) para medir a
convergência e diversidade do conjunto solução obtido pelo algoritmo; e ele foi comparado
com duas abordagens baseadas em enxames (SMPSO e CDAS-SMPSO) e três abordagens
evolucionárias (CEGA, MDFA, e NSGA-III) afim de mostrar suas vantagens e pontos
de melhorias frente a outros algoritmos bem estabelecidos na literatura. Os resultados
mostraram que o algoritmo proposto foi bem sucedido em equilibrar convergência e
diversidade nos problemas testados apresentando resultados equivalentes ou superiores ao
NSGA-III que é uma das propostas mais bem sucedidas até o momento para lidar com
problemas com muitos objetivos. Além disso, o algoritmo proposto foi aplicado em um
problema real de projeto de redes ópticas de alta capacidade com o intuito de avaliar sua
utilidade prática.",TESE,UMA NOVA ABORDAGEM BASEADA EM ENXAMES DE PARTÍCULAS PARA OTIMIZAÇÃO DE MUITOS OBJETIVOS,5026838,1
"The importance of structural variants as a source of phenotypic variation has grown in recent
years. At the same time, the number of tools that detect structural variations using Next-
Generation Sequencing (NGS) has increased considerably with the dramatic drop in the cost of
sequencing in last ten years. Then evaluating properly the detected structural variants has been
featured prominently due to the uncertainty of such alterations, bringing important implications
for researchers and clinicians on scrutinizing thoroughly the human genome. These trends have
raised interest about careful procedures for assessing the outcomes from variant calling tools.
Here, we characterize the relevant technical details of the detection of structural variants, which
can affect the accuracy of detection methods and also we discuss the most important caveats
related to the tool evaluation process. This study emphasizes common assumptions, a variety
of possible limitations, and valuable insights extracted from the state-of-the-art in CNV (Copy
Number Variation) detection tools. Among such points, a frequently mentioned and extremely
important is the lack of a gold standard of structural variants, and its impact on the evaluation
of existing detection tools. Next, this document describes a biclustering-based methodology to
screen a collection of structural variants and provide a set of reliable events, based on a defined
equivalence criterion, that is supported by different studies. Finally, we carry out experiments
with the proposed methodology using as input data the Database of Genomic Variants (DGV).
We found relevant groups of equivalent variants across different studies. In summary, this thesis
shows that there is an alternative approach to solving the open problem of the lack of gold
standard for evaluating structural variants.",,TEORIA DA COMPUTAÇÃO,FRANCISCO DO NASCIMENTO JUNIOR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,17/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'DNA Copy Number Variations. Variant detection methods;Next-generation sequencing. Biases analysis. Evaluation of variants',ALGORITMOS E COMPLEXIDADE,KATIA SILVA GUIMARAES,100,Variações no número de cópias;Métodos de detecção de variações estruturais. Sequenciamento de nova-geração. Avaliação de variações estruturais,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A importância das variantes estruturais como fonte de variação fenotípica tem se proliferado nos
últimos anos. Ao mesmo tempo, o número de ferramentas que detectam variações estruturais
usando Next-Generation Sequencing (NGS) aumentou consideravelmente com a dramática
queda no custo de seqüenciamento nos últimos dez anos. Neste cenário, avaliar corretamente
as variantes estruturais detectadas tem recebido destaque proeminente devido à incerteza de
tais alterações, trazendo implicações importantes para os pesquisadores e clínicos no exame
minucioso do genoma humano. Essas tendências têm impulsionado o interesse em procedimentos
criteriosos para avaliar os variantes identificados. Inicialmente, caracterizamos os detalhes
técnicos relevantes em torno da detecção de variantes estruturais, os quais podem afetar a
precisão. Além disso, apresentamos advertências fundamentais relacionadas ao processo de
avaliação de uma ferramenta. Desta forma, este estudo enfatiza questões como suposições
comuns à maioria das ferramentas, juntamente com limitações e vantagens extraídas do estadoda-
arte em ferramentas de detecção de variantes estruturais. Entre esses pontos, há uma muito
questão bastante citada que é a falta de um gold standard de variantes estruturais, e como
sua ausência impacta na avaliação das ferramentas de detecção existentes. Em seguida, este
documento descreve uma metodologia baseada em biclustering para pesquisar uma coleção de
variantes estruturais e fornecer um conjunto de eventos confiáveis, com base em um critério de
equivalência definido e apoiado por diferentes estudos. Finalmente, realizamos experimentos
com essa metodologia usando o Database of Genomic Variants (DGV) como dados de entrada e
encontramos grupos relevantes de variantes equivalentes em diferentes estudos. Desta forma,
esta tese mostra que existe uma abordagem alternativa para o problema em aberto da falta de
gold standard para avaliar variantes estruturais.",TESE,SCREENVAR - A BICLUSTERING-BASED METHODOLOGY FOR EVALUATING STRUCTURAL VARIANTS,5026882,1
"Service-Oriented Architecture (SOA) is a software design style based on the notion of
Service-Oriented Computing (SOC) that facilitates the interoperability among computer systems
of possibly different businesses. Cost is one of the most challenging factors influencing SOA
adoption in organisations and a significant factor in SOA project success. Nevertheless, many
institutions across the world have adopted SOA to interconnect their computing infrastructures
(Business-to-Business) and offer interfaces to their customers (Business-to-Customer). For these
companies, SOA can address their needs to access the market more quickly, respond to changes
in a business environment, improve business processes, improve customers’ services and even
reduce costs. In SOA, service composition has emerged as an important strategy to enable
collaboration of applications provided by different companies (Business-to-Business). With the
increasing number of Web services having similar functionality but different pricing schemes,
choosing the most appropriate set of services with the lowest cost has been a challenge in service
compositions. Several techniques to compute and analyse the cost of service compositions
already exist. However, there is still no approach to developing cost management systems
able to assist in the planning, definition, scheduling, execution, monitoring and adaptation
of compositions taking into account all classes of cost behaviour and all type of cost drivers.
Thus, the principal objective of this thesis is to present a framework to manage cost throughout
the service composition life-cycle in an integrated way taking into account cost properties of
services and service compositions. Therefore, we provide a metamodel to specify complex cost
behaviours. Also, we propose algorithms to compute service costs and select the best services
that meet the cost requirements of the service composition according to the cost behaviour of
each service. Moreover, we propose an architecture for developing engines able to manage the
cost throughout the service composition life-cycle. Finally, we implemented a prototype based
on the proposed architecture and executed experiments that show the effectiveness and efficiency
of our approach to managing the cost of service composition.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,ROBSON WAGNER ALBUQUERQUE DE MEDEIROS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,07/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Service-Oriented Architecture;Service Composition. Cost. Cost Management.',SISTEMAS DISTRIBUÍDOS,NELSON SOUTO ROSA,139,Arquitetura Orientada a Serviços;Composição de Serviço. Custo. Gerenciamento de Custo.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A Arquitetura Orientada a Serviços (do inglês SOA) é um estilo de projeto de software
baseado na Computação Orientada a Serviço (do inglês SOC) que facilita a interoperabilidade
entre os sistemas de computadores de empresas possivelmente diferentes. Custo é um dos fatores
mais desafiadores que influenciam a adoção de SOA nas organizações e um fator significativo no
sucesso do projeto SOA. No entanto, muitas organizações em todo o mundo adotaram SOA para
interconectar suas infra-estruturas de computação (Business-to-Business) e oferecer interfaces
para seus clientes (Business-to-Customer). Para essas empresas, SOA pode atender às suas
necessidades de acesso ao mercado mais rapidamente, responder às mudanças em um ambiente
de negócio, melhorar os processos de negócios, melhorar os serviços dos clientes e até mesmo
reduzir os custos. Em SOA, a composição de serviço emergiu como uma estratégia importante
para permitir a colaboração de aplicações fornecidas por diferentes empresas (Business-to-
Business). Com o crescente número de serviços Web com funcionalidades semelhantes, mas
com modelos de precificação diferentes, a escolha do conjunto de serviços com o menor custo
tem sido um desafio nas composições de serviços. Na literatura existem técnicas para calcular
e analisar o custo de composições de serviços. No entanto, ainda não há uma abordagem
para desenvolver sistemas de gerenciamento de custos capazes de auxiliar no planejamento,
definição, escalonamento, execução, monitoramento e adaptação de composições levando em
conta todas as classes de comportamento de custo e os diferentes tipo de fatores de custo adotados
pelos provedores de serviços. O objetivo principal desta tese é apresentar um framework para
gerenciar composições de serviços ao longo de todas as fases do seu ciclo de vida de uma
forma integrada levando em consideração comportamentos complexos de custo de serviços.
Sendo assim, fornecemos um metamodelo para especificar comportamentos complexos de custo.
Além disso, propomos algoritmos para calcular os custos e selecionar os melhores serviços que
atendam aos requisitos de custo da composição de acordo com o modelo de comportamento de
custo de cada serviço. Além disso, propomos uma arquitetura para o desenvolvimento de engines
de execução capazes de gerenciar o custo ao longo do ciclo de vida da composição. Finalmente,
desenvolvemos um protótipo baseado na arquitetura proposta e executamos experimentos que
mostram a eficácia e a eficiência de nossa abordagem para gerenciar custo de composições de
serviço.",TESE,COST MANAGEMENT OF SERVICE COMPOSITION,5028750,1
"Video traffic occupies most of the volume of transmitted data over the Internet, mainly
because of the ability of users to record and share their content. However, there is a high
heterogeneity of devices, software, and network for viewing these media, requiring these videos
to be transcoded to formats compatible with most video viewers. Transcoding videos is a
computationally expensive and highly variable demand activity so that it can benefit from
the distributed and elastic ability of cloud computing. Although, identifying a configuration
of the automatic elastic mechanisms (auto-scaling), which meets the minimum performance
requirements required by the SLA at the lowest possible cost is not a simple task. It needs
adjusting several parameters, such as the time to create and remove VMs, the type of VM that
will be added, taking into account the transcoding and instantiation time of each type. Besides,
in public infrastructures, contracts with the cloud provider may incur different costs for the
same type of hired VM, where the appropriate option is related to auto-scaling and expected
workload. In private cloud infrastructures, configuration complexity will also bring aspects of
the proper dynamic dimensioning of the physical infrastructure to reduce electrical consumption
while complying with the SLA. To assist in the choice of these parameters, this dissertation
proposes Stochastic Petri Nets models to compute the throughput, the mean response time,
and the cost in public clouds and electrical consumption in private clouds. These metrics are
evaluated from the input of configuration parameters and requisition rate expected for the system.
The proposed stochastic models were also integrated with the GRASP optimization algorithm,
in order to find the configurations that should be adopted in the cloud to comply with the SLA
and minimize the cost of owning the system. Case studies demonstrate that the combination of
models with optimization mechanisms is useful to guide administrators in choosing the values of
configuration parameters to deploy and tune systems while respecting performance requirements
and minimizing cost. The application of this approach also allowed to identify the behavior of
the cost of a system in relation to the SLA, in one of the presented case studies, the reduction of
30 to 15 seconds in the minimum response time represented a 299% increase in cost, while a
reduction of 45 to 30 seconds only an increase of 6%. This behavior is especially useful when
negotiating new SLAs.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,IURE DE SOUSA FE,UNIVERSIDADE FEDERAL DE PERNAMBUCO,03/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Video Transcoding;Cloud Computing. Auto-Scaling. Performance. Stochastic Modeling. Optimization',SISTEMAS DISTRIBUÍDOS,PAULO ROMERO MARTINS MACIEL,123,Transcodificação de Vídeo;Computação em Nuvem. Auto-Scaling. Desempenho. Modelagem Estocástica. Otimização.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O tráfego de vídeo ocupa a maior parte do volume de dados que são transmitidos pela
Internet, principalmente devido à capacidade de usuários comuns gravarem e compartilharem
seus próprios conteúdos. Entretanto, há uma grande heterogeneidade de dispositivos, softwares e
rede para visualização dessas mídias, requerendo que esses vídeos sejam transcodificados para
formatos compatíveis com a maioria dos visualizadores de vídeo. Transcodificar vídeos é uma
atividade computacionalmente cara e com demanda altamente variável, portanto, pode beneficiarse
da capacidade distribuída e elástica da computação em nuvem. No entanto, identificar
uma configuração dos mecanismos automáticos de elasticidade (auto-scaling), que cumpra os
requisitos mínimos de desempenho requeridos pelo SLA ao menor custo possível não é uma
tarefa simples, requer ajustar diversos parâmetros, como o momento de criar e retirar VMs, o
tipo de VM que será adicionada, levando em conta o tempo de transcodificação e instanciação
de cada tipo. Além disso, em infraestruturas públicas, os contratos firmados com o provedor de
nuvem podem apresentar custos diferentes para um mesmo tipo de VM alugada, onde a opção
apropriada é relacionada com o auto-scaling e carga de trabalho esperada. Já em infraestruturas
de nuvem privadas, a complexidade de configuração trará também aspectos do dimensionamento
dinâmico adequado da infraestrutura física para reduzir o consumo elétrico enquanto cumpre o
SLA. Com objetivo de auxiliar na escolha desses parâmetros, esta dissertação propõe modelos
em Redes de Petri Estocástica para computar a vazão, o tempo médio de resposta, o custo
em nuvens públicas e o consumo elétrico em nuvens privadas. Essas métricas são avaliadas a
partir da entrada dos parâmetros de configuração e taxa de requisições esperada para o sistema.
Os modelos estocásticos propostos também foram integrados com o algoritmo de otimização
GRASP, com objetivo de encontrar as configurações que devem ser adotadas na nuvem para
cumprir o SLA e minimizar o custo de manter o sistema. Os estudos de caso demonstram que a
combinação dos modelos com mecanismos de otimização é útil para orientar os administradores
nas escolhas dos valores dos parâmetros de configuração para implantar e ajustar sistemas,
respeitando os requisitos de desempenho e minimizando o custo. A aplicação dessa abordagem
também permitiu identificar o comportamento do custo de um sistema em relação ao SLA, em
um dos estudos de caso apresentados, a redução de 30 para 15 segundos no tempo de resposta
mínimo representou um aumento de 299% no custo, já uma redução de 45 para 30 segundos
apenas um aumento de 6%. Este comportamento é especialmente útil na negociação de novos
SLAs.",DISSERTAÇÃO,PLANEJAMENTO DE TRANSCODIFICAÇÃO DE VÍDEO EM NUVEM ELÁSTICA,5028796,1
"Multiple Predictor Systems (MPS) consists in development a group of models for forecast
same variable. According to some works in the literature, MPS can improve in time series forecast.
For that, building a group with several models for same forecasting variable and the final forecast
is to get through a combination or selection model.The selection of forecasts consists in finding
the better model inside of a group that can improve efficiency and performance. The motivation
for selection decreases the cost and to use a specific model for each pattern of the time series.
Beyond the approach to get the final forecast of Ensemble, it is necessary that exist diversity
among models forecasting. For that, one approach is to split the time series in different samples
and train a model for each sample, thus making it possible to build off experts’ models. In the
knowledge that time series forecast can be better through MPS, it still exists the necessity for new
research to find answers to some questions. For instance, in which circumstances are combination
approaches better than selection? Which are the better approaches to combining? How important
is it to train the models through different partitions of the series? For that, in present theses are
developed with diverse approaches of combination: Mean Simple, Median, Linear Regression,
Neural Networks and Support Vector Machine, and an approach to forecasting selection that
use a distance measure between patterns for select the model. To verify the feasibility between
different approaches, four series are used: Mackey-Glass, Laser, Microsoft, and Goldman Sachs.
For each time series, the performances of the architectures were evaluated through several metrics.
Based on the results obtained, it is possible to infer that on average, the performances presented
by better combination approaches: Neural Network, Linear Regression, and Simple Mean are
better than selection approach. Moreover, the relationship between accuracy and diversity for
combination approach vary according to approach used.",,COMPUTAÇÃO INTELIGENTE,ERAYLSON GALDINO DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Analysis and Forecasting of Time Series;Combination of predictors;Ensemble of Predictors, Selection of Predictors;Machine Learning.'",REDES NEURAIS,PAULO SALGADO GOMES DE MATTOS NETO,115,"Análise e Previsão de Séries Temporais;Combinação de Preditores;Ensemble de Preditores, Seleção de Preditores;Aprendizado de Máquina.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Sistema de Múltiplos Preditores (SMP) consiste no desenvolvimento de um conjunto
de modelos para prever a mesma variável. Na literatura é possível encontrar trabalhos que
mostram que a sua utilização em previsão de séries temporais pode melhorar a acurácia. Para tal,
é construído um conjunto com diferentes preditores e a previsão final é obtida através da seleção
ou combinação. A seleção de preditores consiste em encontrar um preditor dentro do conjunto
que possa retornar a melhor previsão, podendo reduzir o custo computacional e aumentar a
acurácia do sistema de previsão. Além da abordagem para retornar a previsão final é necessário
que exista diversidade entre os preditores, a qual, consiste em desenvolver um conjunto de
preditores que apresentem previsões diferentes para a mesma variável. Uma das técnicas para
criar modelos diversos é particionar a série em diferentes amostras e treinar um modelo para
cada amostra, possibilitando o desenvolvimento de preditores especialistas em padrões diferentes
da série. Sabendo que a utilização de SMP pode aumentar a acurácia de sistemas de previsão de
séries temporais, ainda existe a necessidade de pesquisas que apresentem em que circunstâncias
a combinação é melhor que a seleção de preditores, além disso, quais as melhores formas de
combinação e qual a influência de treinar os preditores através de partições diferentes da série.
Para tal, esta dissertação realiza uma comparação entre diferentes abordagens de combinação:
Média Simples, Mediana, Regressão Linear, Redes Neurais e Maquina de Vetor de Suporte; e
uma abordagem de seleção de preditores que utiliza medida de distância entre os padrões para
selecionar o melhor modelo, com o intuito de responder esses questionamentos. Para avaliar o
desempenho das diferentes abordagens, foram utilizadas quatro séries: Mackey Glass, Laser,
Microsoft e Goldman Sachs. Em cada série os desempenhos das arquiteturas foram avaliados
através de um conjunto de métricas. Com base nos resultados obtidos, é possível inferir que
em média os desempenhos apresentados pelas melhores abordagens de combinação: Redes
Neurais, Regressão Linear e Média Simples, são melhores que a abordagem de seleção utilizada.
Além disso, a relação entre a acurácia e a diversidade dos modelos pode variar de acordo com a
abordagem de combinação utilizada.",DISSERTAÇÃO,PREVISÃO DE SÉRIES TEMPORAIS USANDO SISTEMAS DE MÚLTIPLOS PREDITORES,5045122,1
"Stereo matching approaches that generate dense, accurate, robust, and real-time disparity
maps are quite appealing to many applications such as 3D reconstruction and
autonomous navigation. Among the best approaches is the Semi Global Correspondence
(SGM) technique. This technique, combined with local similarity metrics, robustly supports
the various challenges present in the stereo camera system such as noise, low texture
and occlusions. The great quality of the SGM technique is due to the fact that this algorithm
performs an optimization throughout the image, propagating smaller costs from
several independent one-dimensional paths through the image. However, irregular access
to data, large amount of computational operations, and high bandwidth to store intermediate
results poses challenges for parallel implementation of the SGM technique on
FPGAs. In this way, in the seek for solving such challenges, this work proposes a scalable
array architecture based on systolic array, fully pipeline. The architecture is based on
a combination of multilevel parallelism such as image line processing (two-dimensional
processing) and disparity. For the implementation of the SGM technique it was proposed
to combine it with the gradient filter as a preprocessing step and absolute differences as
a method of local similarity. This combination proved to be a robust approach for both
high-quality images available in the Middlebury benchmark (22.7 % of wrong pixels) and
for low-quality images provided from the camera system built into our research group.
In addition, the hardware L/R check step was also developed, which allowed the detection
of noisy and occlusion regions. This whole stereo matching system was implemented,
simulated and validated on the Cyclone IV FPGA platform, generating disparate image
maps in HD resolution (1024x768 pixel), with a range of 128 disparity levels and using 4
path directions for the SGM method. With this configuration an operating frequency of
100 Mhz was obtained, providing images at a rate of 127 frames per second, using 70%
of its resource in logical elements for processing and 63% of memory for intermediate
data storage. In addition, this stereo matching approach was validated in a real context
of a complete stereo system in which a stereo camera system was built and the steps of
calibration, rectification were implemented. For this validation, the DE2i-150 hardware/-
software platform was used with the calibration and rectification steps implemented in
the processor and the proposed SGM architecture implemented in FPGA and both the
platforms communicating itself through the PCI-Express bus using the RIFFA 2.2 framework.
This complete stereo vision system has achieved a speedup of 30x over the SGM
processor approach offered by the open source computer vision library (OpenCV) and
with a power dissipation of 2 watts.",,ARQUITETURA DE COMPUTADORES E SISTEMAS DIGITAIS,LUCAS FERNANDO DA SILVA CAMBUIM,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Stereo Matching;Embedded System. High Performance. High Quality. FPGA.',ARQUITETURA DE COMPUTADORES,EDNA NATIVIDADE DA SILVA BARROS,140,"Correspondência Estéreo;Paralelismo, Pipeline, Alto Desempenho, FPGA",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Sistemas de correspondência estéreo que geram mapas de disparidade densos, precisos, robustos e em tempo real são bastante atraentes para uma variedade de aplicações tais como reconstrução de imagem 3D, segmentação e navegação autônoma (ex: veículos autônomos). A maioria das abordagens que atendem satisfatoriamente a todas as características mencionadas são baseadas na técnica de Semi Global Matching (SGM). Esta técnica, que está entre as melhores técnicas atuais, pode ser combinada com diferentes tipos de métricas de similaridades locais e suporta de forma robusta os vários desafios presentes no sistema de câmeras estéreo tais como ruídos, baixa textura e oclusões. A ótima qualidade da técnica SGM deve-se ao fato de que este algoritmo realiza uma otimização ao longo de toda a imagem, propagando custos menores a partir de vários caminhos unidimensionais independentes através da imagem. Contudo, o acesso irregular aos dados, a grande quantidade de operações computacionais e altas larguras de banda para armazenar os resultados intermediários impõe desafios para implementação paralela da técnica SGM em plataformas Field Programmable Gate Array (FPGAs). Desta forma, na busca por resolver tais desafios, este trabalho propõe uma arquitetura escalável em hardware baseado em array sistólico, totalmente pipeline. A arquitetura é baseada em uma combinação de paralelismo em vários níveis tais como no processamento das  linhas de imagem (processamento em duas dimensões) e disparidade. Para a implementação da técnica SGM foi proposto a sua combinação com o filtro de gradiente sobel como uma etapa de pré-processamento e diferenças absoluta como um método de similaridade local. Esta combinação mostrou-se uma abordagem robusta tanto para imagens de alta qualidade disponíveis no banco de imagens Middlebury (22.7\% de pixels errados)  como para imagens de baixa qualidade fornecido pelo sistema de câmeras construído no nosso grupo de pesquisa. Além disso, também foi desenvolvido a etapa de checagem L/R em hardware, que permitiu detectar regiões ruidosas e regiões de oclusão. Todo este sistema de correspondência estéreo foi implementado, simulado e validado na plataforma FPGA Cyclone IV gerando mapas de imagens de disparidade em resolução HD (1024x768 pixel), com range de 128 níveis de disparidades e usando 4 direções de caminhos para o método SGM. Com essa configuração obteve-se uma frequência de operação de 100Mhz, fornecendo imagens em uma taxa de 127 frames por segundo, utilizando 70% de seu recurso em elementos lógicos para processamento e 63% de memória para armazenamento de dados intermediários. Além disso, esta abordagem de correspondência estéreo foi validada em um contexto real de um sistema estéreo completo no qual foi montado um sistema de câmeras estéreo e implementado as etapas de calibração, retificação. Para tal validação, foi utilizada a plataforma hardware/software DE2i-150 com as etapas de calibração e retificação implementadas em processador Central Processing Unit (CPU) e a arquitetura proposta do SGM implementado em FPGA e ambos os processamentos se comunicando através do barramento PCI-Express usando o framework RIFFA 2.2. Este sistema de visão estéreo completo permitiu obter frames de mapas de disparidade a uma taxa de 60 fps com resolução de 320x240 e com espaço de disparidade de 64 valores tendo um ganho de velocidade de 30x em relação a abordagem SGM oferecida pela biblioteca de código aberto de visão computacional OpenCV.",DISSERTAÇÃO,Um módulo de Hardware de Tempo Real de Correspondência Semi Global para um Sistema de Visão Estéreo,5050098,1
"Improvements in computational systems may be constrained by the efficiency of storage
drives. Therefore, replacing hard disk drives (HDD) with solid-state drives (SSD) may
also be an effective way to improve system performance, both in personal computers and data
centers. However, the higher cost per gigabyte and reduced lifetime of SSDs hinder a thorough
replacement. To mitigate these issues, several architectures and storage policies have been conceived
based on hybrid storage systems, but performance and availability models have not been
proposed to better assess such different architectures. This dissertation presents an approach
based on stochastic models for performance and availability modeling of hybrid storage systems,
using stochastic Petri nets (SPN) and reliability block diagrams (RBD). The proposed
models can represent write, read and mixed operations, and they may estimate response time,
throughput and availability. Initially, statistical analyses (paired t-test) validate SPN models
conceived, for a scenario with four different storage devices (HDDs) and five different object
sizes, with the DiskSim support, which is a storage system simulation tool. Next, a design
of experiment (DoE) defines the relevant factors for a storage architecture based on the cloud
computing platform adopted (OpenStack Swift). Hereafter, performance and availability experiments,
considering this cloud computing platform, provide a comparison between traditional
storage technologies (HDD and SSD) and different hybrid storage solutions. Also, this work
proposes a joint evaluation of both performance (response time), availability (for calculating
downtime), and cost, in order to provide an analysis of different technologies and storage architectures
as well as read/write policies. The results show the feasibility of the proposed approach
as well as the improvements by adopting hybrid storage devices associated with an adequate
storage policy.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,ERIC RODRIGUES BORBA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Performance Evaluation. Availability;Reliability Block Diagrams. Hybrid Storage. Stochastic Petri Nets. Cloud Computing.',REDES DE COMPUTADORES,EDUARDO ANTONIO GUIMARAES TAVARES,106,Avaliação de Desempenho;Disponibilidade. Diagrama de Blocos de Confiabilidade. Armazenamento Híbrido. Redes de Petri Estocásticas. Computação em Nuvem.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Avaliação de Dependabilidade de smartphones,"O aperfeiçoamento de sistemas computacionais pode vir a ser limitado de acordo com
a eficiência dos dispositivos de armazenamento. Deste modo, a substituição de dispositivos
de disco rígido (hard disk drives) por dispositivos de estado sólido (solid-state drives) pode ser
uma forma efetiva para melhorar o desempenho dos sistemas, tanto para computadores pessoais,
quanto para data centers. Entretanto, o alto custo por gigabyte e um reduzido tempo de vida,
dificulta a substituição por completo dos hard disk drives (HDDs) por solid-state drives (SSDs).
Para mitigar estas questões, diversas arquiteturas e políticas de armazenamento têm sido concebidas
baseadas em sistemas de armazenamento híbridos, todavia, não propõem modelos de
desempenho e disponibilidade para melhor avaliar diferentes arquiteturas. Esta dissertação apresenta
uma abordagem baseada em modelos estocásticos para a modelagem de desempenho e disponibilidade
de sistemas de armazenamento híbridos, usando redes de Petri estocásticas (SPN)
e diagrama de blocos de confiabilidade (RBD). Os modelos propostos possibilitam representar
operações de escrita, leitura e mixed (escrita e leitura), e eles podem estimar tempo médio
de resposta, vazão e disponibilidade. Inicialmente, análises estatísticas (teste t emparelhado)
validam os modelos SPN concebidos, para um cenário com quatro diferentes dispositivos de armazenamento
(HDDs) e cinco tamanhos diferentes de objetos, com o auxílio do DiskSim, que é
uma ferramenta de simulação de sistemas de armazenamento. Em seguida, um planejamento de
experimento (DoE) define os fatores relevantes para uma arquitetura baseada na plataforma de
computação em nuvem adotada (OpenStack Swift). Posteriormente, experimentos de desempenho
e disponibilidade, considerando esta plataforma de computação em nuvem, proporcionam
uma comparação entre tecnologias de armazenamento tradicionais (HDD e SSD) e diferentes
soluções de armazenamento híbrido. Além disso, este trabalho propõe a avaliação conjunta do
desempenho (tempo médio de resposta), disponibilidade (para o cálculo do downtime) e custo,
com o intuito de fornecer uma análise a respeito de diferentes tecnologias e arquiteturas de armazenamento,
bem como políticas de leitura e escrita. Os resultados demonstram a viabilidade
da abordagem proposta, bem como os benefícios por adotar dispositivos de armazenamento
híbridos associados a uma política de armazenamento adequada.",DISSERTAÇÃO,MODELAGEM DE DESEMPENHO E DISPONIBILIDADE PARA SISTEMAS DE ARMAZENAMENTO HÍBRIDOS,5076889,1
"In literature, Cyber-Physical Systems (CPS) represent a combination of discrete and continuous dynamics, often found in systems where a digital controller unit (discrete) is connected with a physical system (continuous). In this context, Model-based testing  is a relatively recent subject in literature and still being actively researched and developed.

In this work, we propose a process for sound conformance testing of Cyber-Physical Systems. The goal behind this process is to develop a practical approach that provides a semi-automatic sound solution for our testing strategy. Furthermore, a prototype tool was developed combining aspects from different subject areas such as computer science, physics and control theory.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,HUGO LEONARDO DA SILVA ARAUJO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,17/08/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Formal methods;software testing, conformance testing, hybrid systems, cyber-physical systems'",ENGENHARIA DE SOFTWARE,AUGUSTO CESAR ALVES SAMPAIO,64,"Sistemas Ciber-Físicos;Teste de Conformidade;Teste Baseado em Modelo, Sistemas Híbridos",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),"Modelagem, Verificação e Teste Composicional de Sistemas com Aplicações na Indústria Aeronáutica","Na literatura moderna, o termo Sistema Híbrido é usado para descrever sistemas que combinam elementos contínuos e discretos. Um exemplo particular, é um sistema em que um controlador digital (elemento discreto) está conectado à um sistema físico (elemento contínuo). Sistemas dese tipo, que envolvem componentes físicos e digitais altamente integrados e que lidam com métricas temporais e espaciais, são conhecidos como sistemas ciber-físicos (SCF).
Nesse contexto, o uso de técnicas que aplicam o método de Teste Baseado em Modelo (do inglês, Model Based Testing - MBT) em sistemas ciber-físicos é um assunto relativamente recente e está sendo ativamente pesquisado e desenvolvido. A análise de SCFs é de alta complexidade devido à multi-disciplinaridade de tais sistemas, que combina aspectos de diversas áreas como ciência da computação, física e sistemas de controle.
Nesse trabalho, nós propomos um processo para teste de conformidade de sistemas ciber-físicos que seja considerado sound. O objetivo desse processo é oferecer uma abordagem prática que provê uma solução semi-automática para a nossa estratégia de teste. Certas etapas do processo foram mecanizadas através do uso de um protótipo de ferramenta que desenvolvemos. O processo é composto por 5 etapas, que vai da análise dos modelos de entrada até o ajuste dos resultados obtidos.",DISSERTAÇÃO,A Process for Sound Conformance Testing of Cyber-Physical Systems,5076932,1
"A data stream is an ordered sequence of instances that arrive at a rate that does not
allow them to be permanently stored in memory. Such data are potentially unlimited in
size, rendering it impossible to be processed by most traditional data mining approaches.
These events require new requirements of the learning algorithms due to the specifics
of dynamic environments. Most of them learn decision models that evolve continuously
over time, making it evident that non-stationarity hinders the learning process, where
changes occur in the distribution of probability of the data (Concept Drift). An important
issue, not yet adequately addressed, is the experimental work project to evaluate
and compare decision models that evolve over time. The Prequential methodology is an
approach used to evaluate the performance of classifiers in data streams with stationary
and non-stationary distributions. It is based on the premise that the goal of statistical
inference is to make sequential probability forecasts for future observations, instead of expressing
information about the past predictions accuracy. This work makes an empirical
evaluation of the methodology, considering the three strategies used to update the prediction
model, namely Basic Window, Sliding Window, and Fading Factors. Specifically,
it seeks to identify which of the variations is most appropriate for the experimental evaluation
of the results in scenarios where concept drifts occur, with greater interest in the
accuracy observed within the total data flow. The metrics adopted for the evaluation are
Prequential accuracy of the approaches and the actual accuracy obtained in the learning
process of each data stream. The results of the carried out experiments suggest that the
use of Prequential in the variation Sliding Window is the best alternative.",,BANCO DE DADOS,JUAN ISIDRO GONZALEZ HIDALGO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,04/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Prequential Methodology;Concept Drift;Data Stream.',BANCO DE DADOS,ROBERTO SOUTO MAIOR DE BARROS,94,Avaliação Prequential;Mudança de Conceito;Fluxo de dados.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Fluxo de dados (Data Stream) é uma sequência ordenada de instâncias que chegam a
uma velocidade que não permite que sejam armazenadas permanentemente na memória.
Tais dados são potencialmente ilimitados no tamanho, tornando-os impossíveis de
serem processados pela maioria das abordagens tradicionais de mineração de dados. Estes
acontecimentos impõem novas exigências aos algoritmos de aprendizagem devido às
especificidades dos ambientes dinâmicos. A maioria deles aprende modelos de decisão que
evoluem continuamente ao longo do tempo, tornando evidente que a não estacionaridade
dificulta o processo de aprendizagem, onde ocorrem mudanças na distribuição de probabilidade
dos dados – Mudança de Conceito (Concept Drift). Uma questão importante,
ainda não convenientemente abordada, é o projeto de trabalho experimental para avaliar e
comparar modelos de decisão que evoluem ao longo do tempo. A metodologia Prequential
é uma abordagem utilizada para a avaliação de desempenho de classificadores em fluxos
de dados com distribuições estacionárias e não estacionárias. Ela é baseada na premissa de
que o objetivo da inferência estatística é fazer previsões de probabilidade sequencial para
observações futuras, em vez de expressar informações sobre a acurácia passada alcançada.
Este trabalho realiza uma avaliação empírica da metodologia abordada considerando as
três estratégias utilizadas para atualizar o modelo de predição, a saber Basic Window
(Janela Básica), Sliding Window (Janela Deslizante), e Fading Factors (Fator de Desvanecimento).
Especificamente, procura-se identificar qual das variações é a mais adequada
para a avaliação experimental dos resultados em cenários onde acontecem mudanças de
conceitos, com maior interesse nas observações passadas dentro do fluxo total de dados.
As métricas adotadas para a avaliação são acurácia Prequential dos enfoques e a acurácia
real obtida no processo de aprendizagem de cada fluxo de dados. Os resultados dos experimentos
realizados sugerem que a utilização de Prequential na variação Sliding Window
seja a melhor alternativa.",DISSERTAÇÃO,Experiências com Variações Prequential para Avaliação da Aprendizagem em Fluxo de Dados,5091292,1
"Cloud computing is a computational paradigm that has been used over the last few
years because of its resource provisioning characteristics in a scalable way, where their
users pay only for what they consume. This computational model enables several services
to be offered from its Infrastructure as a Service. However, the failure of components of
cloud resources is something quite common and that directly affects the availability of
the services that use them. Thus, interest in the field of academic research has arisen
in studying and evaluating this environment in order to guarantee high availability of
services in the cloud. To assist in the evaluation of these services, researchers develop
tools, however, most software requires constant updating to adapt to the environment in
which the user is led to rework. Therefore, this work aims to develop a framework that
helps the researcher in the study of the availability of cloud computing services. This
framework uses SPN as a fault injection mechanism, which allows the user to evaluate
several models of computational clouds because the framework will not be modified to suit
the computational environment that will be evaluated. Moreover, the proposed solution
monitors the environment and informs the user of the failure times and system repair.
Our results showed that the framework was efficient and effective in the result of the
availability of the models evaluated in the case studies.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,ALINE SANTANA OLIVEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,25/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Stochastic Petri net;Fault Injection. Cloud Computing. Monitoring. Dependability',REDES DE COMPUTADORES,PAULO ROMERO MARTINS MACIEL,100,Rede de Petri Estocástica;Injeção de Falha. Computação em nuvem. Monitoramento. Dependabilidade,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A computação em nuvem é um paradigma computacional que vem sendo utilizado
ao longo dos últimos anos devido as suas características de provisionamento de recursos
de forma escalável, onde seus usuários pagam apenas por aquilo que consomem. Esse
modelo computacional possibilita que diversos serviços sejam ofertados a partir da sua
Infraestrutura como Serviço. Porém, a falha de componentes dos recursos da nuvem é algo
bastante comum e que afeta diretamente a disponibilidade dos serviços que os utilizam.
Dessa forma, surgiu o interesse na área da pesquisa acadêmica em estudar e avaliar esse
ambiente a fim de garantir alta disponibilidade em serviços na nuvem. Para auxiliar na
avaliação desses serviços, os pesquisadores desenvolvem ferramentas, entretanto a maioria
dos softwares precisam de atualizações constantes para que se adaptarem ao ambiente no
qual foi desenvolvido o que leva o usuário ao retrabalho. Sendo assim, este trabalho tem
como proposta desenvolver um framework que auxilie o pesquisador no estudo de disponibilidade
de serviços de nuvem computacional. Esse framework utiliza SPN (Stochastic
Petri Nets) como um mecanismo de injeção de falhas, que permite que o usuário avalie
vários modelos de nuvens computacionais pois o framework não sofrerá modificação para
se adequar ao ambiente computacional que será avaliado. Além disso a solução proposta
monitora o ambiente e informa ao usuário os tempos de falha e reparo do sistema. Nossos
resultados mostraram que o framework foi eficiente e eficaz no resultado da disponibilidade
dos modelos avaliados no estudos de caso.",DISSERTAÇÃO,SIMF: UM FRAMEWORK DE INJEÇÃO E MONITORAMENTO DE FALHAS DE NUVENS COMPUTACIONAIS UTILIZANDO SPN,5105645,1
"Data clustering techniques generally work with objects that can be described by either
feature or relational data. In relational data only the information pertaining the relationship
degree between pairs of objects is available. The most usual case of relational data is when
there is a dissimilarity matrix (NxN) between N objects and each cell of said matrix contains the
relationship degree between a given pair of objects.
These relational data may be (and generally are) complex, such as multimedia objects,
which may cause the relationship between those objects to be described by multiple (dis)similarity
matrices. Each matrix is called view and data described in that way are said to be multi-view.
There are three main approaches to manage multi-view data in cluster analysis in the the
state of the art: concatenation, distributed and centralized. In the centralized approach the views
are considered simultaneously in order to find hidden patterns in the data. On one hand, this
poses a great challenge as it requires a profound change in the clustering process. On the other
hand, this approach generally offers results with superior quality in comparison with the other
two approaches.
Clustering is a hard task, specially when it concerns complex relational high-dimension
multi-view data. To facilitate the process it is not unusual to use the object labels, although
labeled data are generally scarce. Therefore the use of parcial supervision is common, which
requires only some of the objects are labeled in a given dataset.
This work introduces the SS-MVFCVSMdd (Semi-Supervised Multi-View Fuzzy Clustering
Vector Set-Medoids) algorithm, based on the MVFCVSMdd and functions in a similar way
as the SS-MVFCSMdd. The SS-MVFCVSMdd is a semi-supervised multi-view fuzzy c-medoids
vectors partitional algorithm, which utilizes pairwise constraints (must-link and cannot-link)
between objects as partial supervision and infers prototypes and relevance weights for each view.
Experiments performed using several datasets comparing the performance of the proposed
algorithm with algorithms that have similar characteristics as the proposed algorithm. The results
indicate that the SS-MVFCVSMdd had a similar or superior quality than the other algorithms.",,INTELIGÊNCIA COMPUTACIONAL,DIOGO PHILIPPINI PONTUAL BRANCO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,21/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Fuzzy Partitioning;Multi-view;Relational Data;Semi-supervised',ANÁLISE DE DADOS SIMBÓLICOS E/OU NUMÉRICOS E MÉTODOS AFINS,FRANCISCO DE ASSIS TENORIO DE CARVALHO,65,"Agrupamento Fuzzy,;Visão Múltipla, Dados Relacionais, Semi-supervisão",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Métodos Não Supervisionados de Classificação para Dados Quantitativos e Simbólicos,"Técnicas de agrupamento de dados geralmente operam em objetos que podem estar descritos
pelos seus atributos (feature data) ou por dados relacionais. Em dados relacionais apenas
a informação que representa o grau de relacionamento entre os pares de objetos está disponível.
O caso mais comum de dados relacionais é quando se tem uma matriz de dissimilaridade (NxN)
entre N objetos e cada célula da matriz tem a informação do grau de relacionamento entre um
par de objetos.
Esses dados relacionais podem ser (e geralmente são) complexos, tais como objetos
multimídia, o que faz com que o relacionamento entre objetos possa ser descrito por múltiplas
matrizes de (dis)similaridade. Cada matriz é chamada de visão e dados descritos desta forma são
ditos multi-view.
Há três principais abordagens para administrar dados multi-view em análise de agrupamento
no estado da arte: abordagem de concatenação (fusão de dados), abordagem distribuída
e abordagem centralizada. Na abordagem centralizada, se utiliza as múltiplas visões de forma
simultânea para encontrar padrões escondidos nos dados; representa um desafio importante
pois requer uma modificação profunda do processo de particionamento. Em compensação, essa
abordagem geralmente tem uma qualidade dos resultados superior em relação às outras duas
abordagens.
Agrupamento de dados é uma tarefa difícil, especialmente quando se trata de dados
complexos, relacionais, de alta dimensionalidade e com múltiplas visões. Para facilitar o
processo, não é incomum utilizar os rótulos dos objetos, contudo, dados rotulados geralmente
são escassos; por isso é comum o uso de supervisão parcial, que necessita apenas o rótulo de
alguns objetos de um dado conjunto.
Este trabalho introduz o algoritmo SS-MVFCVSMdd (Semi-Supervised Multi-View
Fuzzy Clustering Vector Set-Medoids), baseado no MVFCVSMdd e com funcionamento parecido
com o SS-MVFCSMdd. O SS-MVFCVSMdd é um algoritmo particional do tipo fuzzy cmedoids
vectors semi-supervisionado de dados relacionais representados por múltiplas matrizes
de dissimilaridade. O SS-MVFCVSMdd utiliza restrições par-a-par (must-link e cannot-link)
entre objetos como supervisão parcial e é capaz de inferir representantes e pesos de relevância
para cada visão.
Experimentos são realizados em vários conjuntos de dados comparando seu desempenho
com algoritmos de características similares ao SS-MVFCVSMdd. Os resultados apontam que o
SS-MVFCVSMdd teve uma qualidade similar ou superior em relação aos outros algoritmos.",DISSERTAÇÃO,AGRUPAMENTO FUZZY C-MEDOIDS SEMI-SUPERVISIONADO DE DADOS RELACIONAIS REPRESENTADOS POR MÚLTIPLAS MATRIZES DE DISSIMILARIDADE,5135058,01
"Many pattern recognition algorithms are probabilistic in their structure and as such, they
use statistical inference to determine the best label for a given instance to be classified. The
statistical inference is based generally on Bayes theory which strongly uses the average vectors,
i, and covariance matrices, i, of existing classes in the training data. These parameters are
unknown and estimates are made by following various algorithms. However, the estimates
made exclusively from the training data are still the most used. Because they are estimates, the
parameters ^i and ^
i are perturbed when a new vector is inserted into the class which they
belong to. Evaluating the perturbations that occurred in all classes simulating a possible inclusion
of the instance to be classified in the same one, we defined in this work a new decision rule
which assigns the test instance to the class in which occurs the slightest perturbation in ^i and
^
i parameters or the combination of both. In this area, several approaches are possible, it’s
worth mentioning the decision trees, neural networks, instance-based learning and the support
vector machine (SVM). However, until the moment of the writing of this text, was not found
in the literature, approaches that use parameters perturbations to pattern’s classification. In
tests performed initially on synthetic data and later on 21 real databases available in the UCI
Repository Learning, was verified that perturbation-based classifier, which was denominated
PerC (Perturbation Classifier), presented performance significantly superior to the versions of the
SVM with polinomial kernels of degrees 2 and 3 and roughly equivalent to k-Nearest Neighboor
with k = 3 and k = 5, Naïve Bayes, SVM with Gaussian kernel, CART and MLP neural networks,
having the PerC the highest ranking according to the Friedman statistical test. The results
demonstrated that the perturbation-based approach is therefore useful to pattern classification.",,COMPUTAÇÃO INTELIGENTE,EDSON LEITE ARAUJO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,10/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Patterns Recognition. Perturbations. Covariance Matrice and Mean Vector. Bayes Classifier.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,GEORGE DARMITON DA CUNHA CAVALCANTI,104,Reconhecimento de padrões. Perturbações. Matriz de Covariância e Vetor Médio. Classificador de Bayes.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Muitos algoritmos de reconhecimento de padrões são probabilísticos em sua construção
e como tal, usam a inferência estatística para determinar o melhor rótulo para uma dada instância
a ser classificada. A inferência estatística baseia-se em geral, na teoria de Bayes que por sua vez,
utiliza fortemente dos vetores médios, i, e matrizes de covariância, i, de classes existentes
nos dados de treinamento. Estes parâmetros são desconhecidos e estimativas são realizadas
seguindo vários algoritmos. Entretanto, as estimativas feitas exclusivamente a partir dos dados
de treinamento são ainda as mais utilizadas. Por se tratarem de estimativas, os parâmetros ^i e
^
i sofrem perturbações quando se insere um novo vetor na classe à qual pertencem. Avaliando
as perturbações ocorridas em todas as classes simulando uma possível inserção da instância a ser
classificada nas mesmas, definimos neste trabalho uma nova regra de decisão a qual atribui a
instância de teste à classe em que ocorrer a menor perturbação nos parâmetros ^i e ^
i ou numa
combinação de ambos. Nesta área, várias abordagens são possíveis, entre elas merecem destaque
as árvores de decisão, as redes neurais, o aprendizado baseado em instâncias e a máquina de
vetores de suporte(SVM). Entretanto, até o momento da escrita deste texto, não foi encontrado
na literatura, abordagens que utilizem as perturbações de parâmetros para a classificação de
padrões. Em testes realizados inicialmente em dados sintéticos e posteriormente em 21 bancos
de dados reais disponíveis no UCI Repository Learning, verificou-se que o classificador baseado
em perturbações, o qual foi denominado PerC (Perturbation Classifier), apresentou performance
significativamente superior às versões do SVM com kernels polinomiais de graus 2 e 3, e
praticamente equivalente aos k-Nearest Neighboor com k=3 e k=5, Naïve Bayes, SVM com
kernel gaussiano, CART e as redes neurais MLP, tendo o PerC o maior ranking segundo o teste
estatístico de Friedman. Os resultados demonstraram que a abordagem baseada em perturbações
são, portanto, úteis para a classificação de padrões.",TESE,UM CLASSIFICADOR BASEADO EM PERTURBAÇÕES,5135156,1
"A última década testemunhou um aumento considerável de artefatos físicos, interativos e programáveis. Sensores, atuadores, dispositivos eletrônicos, plataformas e frameworks de desenvolvimento tornaram-se mais acessíveis, fomentados pela proliferação de tecnologias móveis, pelo crescimento das comunidades de DIY e Maker e pela disseminação de filosofias open source e open hardware. Mais pessoas estão experimentando com estas ferramentas para programar o mundo físico além da tela. Os dispositivos interativos para a expressão artística apresentam desafios de concepção e desenvolvimento, uma vez que a essência da interação se baseia em habilidades que são difíceis de serem obtidas e precisam de tempo para adquiri-las. Abordagens artísticas interativas podem ensinar valiosas lições aplicáveis a outros níveis de interação humano-computador. Uma classe de objetos artísticos interativos físicos é o instrumento musical digital (DMI). Os DMIs são artefatos em que o controle gestual e a produção sonora são fisicamente desacoplados, mas digitalmente mapeados. Isto fornece mais liberdade para um designer DMI, se comparado com instrumentos acústicos, mas aumenta a complexidade do espaço de design. Além disso, a literatura não estabelece métodos ou diretrizes para a exploração deste espaço de possibilidades. Para esta questão, a prototipação parece ser uma abordagem promissora, pois protótipos não são apenas uma ferramenta para testar e comunicar ideias, mas também para gerar ideias. Como um DMI é um meio para produzir música, seu protótipo deve fornecer feedback sonoro em tempo real aos gestos de controle. Por esse motivo, no contexto da DMI, protótipos não funcionais não são inteiramente adequados. Por outro lado, o desenvolvimento de protótipos funcionais exige mais tempo e esforço e, consequentemente, pode ser um gargalo do processo iterativo. Como fornecer caminhos estruturados e exploratórios para gerar idéias de DMI? Como diminuir o tempo e o esforço de construir protótipos DMI funcionais? Para lidar com essas questões, propomos o conceito de herança instrumental, ou seja, a aplicação de estruturas e gestos de instrumentos existentes para gerar ideias de novos instrumentos. Como suporte para análise e combinação, utilizamos um método de design tradicional, caixa morfológica, na qual os artefatos existentes são divididos em partes, apresentados de forma visual e, em seguida, recombinados para produzir novas ideias. Finalmente, integrando todas essas ideias em um objeto concreto, desenvolvemos um toolkit de prototipação física para a construção de protótipos funcionais de DMI: Probatio, um sistema modular de blocos e suportes para protótipos de instrumentos baseados em certas posturas e controles gestuais para interação musical. A avaliação do ambiente mostrou que o uso do artefato pode reduzir o tempo necessário para se obter um protótipo funcional e também há indícios de que pode influenciar o aumento do número de ciclos entre a geração de ideias e a avaliação. Além disso, os usuários relataram envolvimento mais musical com Probatio em comparação com um toolkit de sensores genéricos.",,COMPUTAÇÃO INTELIGENTE,FILIPE CARLOS DE ALBUQUERQUE CALEGARIO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Novas interfaces para expresso musical, Instrumentos Musicais Digitais, Gerao de Ideias, Prototipao, Toolkit para Prototipao'",INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,GEBER LISBOA RAMALHO,161,"New interfaces for musical expression, Digital Musical Instruments, Idea Generation, Prototyping, Prototyping Toolkit",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Last decade has likely witnessed a considerable rise in physical programmable interactive artifacts. Sensors, devices, platforms, and frameworks have become more accessible, fostered by the proliferation of mobile technologies, the growth of the DIY and Maker communities, and the spread of open source and open hardware philosophies.  More people are experimenting with sensors and actuators to program the physical world beyond the screen. Interactive devices for artistic expression present challenges that are worth investigating, since the essence of the interaction is based on skills which are hard to be obtained. Interactive artistic approaches can teach valuable lessons applicable to other levels of interaction design and human-computer interaction. One class of artistic physical interactive objects is the digital musical instrument (DMI). DMIs are artifacts in which gestural control and sound production are physically decoupled but digitally mapped. It provides freedom for a DMI designer but increases the complexity of the design space. Besides, guidelines have not yet been established. To address this issue, prototyping seems to be a promising approach as they are not only a tool for testing and communicating ideas, but also for generating them. As a DMI is a means to produce music, its prototype should provide real-time sound feedback for control gestures. For that reason, in DMI context, non-functional prototypes are not entirely suitable. On the other hand, the development of functional prototypes demands more time and effort, and consequently, can be a bottleneck of iterative design. How to provide initial explorative paths to generate DMI ideas? How to decrease time and effort of building functional DMI prototypes? To deal with those questions and inspired by early exploration actions, we propose the concept of instrumental inheritance, that is the application of gestures and postures of existing instruments to generate ideas of new instruments. As support for analysis and combination, we leverage a traditional design method, the morphological chart, in which existing artifacts are split into parts, presented in a visual form and then recombined to produce new ideas. Finally, integrating all these ideas in a concrete object, we developed a physical prototyping toolkit for building functional DMI prototypes: Probatio, a modular system of blocks and supports to prototype instruments based on certain postures and gestural controls for musical interaction. The evaluation the environment showed that it contributed to reducing the time to achieve a functional prototype, and also influencing the increase in the number of cycles between idea generation and evaluation. Besides, the users reported more musical engagement with Probatio in comparison to a generic sensor toolkit.",TESE,METHOD AND TOOLKIT FOR DESIGNING DIGITAL MUSICAL INSTRUMENTS: GENERATING IDEAS AND PROTOTYPES,5150569,1
"Template matching is a well known computer vision problem. Its solutions can be
aplied in object recognition, detection and tracking applications. An algorithm to solve template
matching problem consists in looking for areas of an image that more closely resemble a smaller
image of reference (template). Its operation is based on calculating the similarity or dissimilarity
between the template and each region of image that it can overlay (image window).
The metric Zero Mean Normalized Cross correlation (ZNCC) is a measure of similarity
widely used in template matching problems due to its robustness to linear variations of brightness
and contrast. The main disadvantage of the template matching technique, especially using ZNCC
metric is the high computational cost of the calculation.
Some applications, such as screening of multiple independent objects or multiple poses
of the same subject require finding the best matching positions with multiple templates, which
increases the computational cost of operation. A real time solution for multiple template matching
is hard to be obtained.
This paper proposes a coprocessor prototyped in FPGA that explores concepts of parallelism
and pipeline to speed up the calculation of ZNCC between an image and multiple templates.
Experimental results shows a 3x speedup comparing FPGA performance to implementations on
GPU and CPU. Furthermore, the proposed accelerator achieves real-time performance (32.13FPS)
for processing templates up to 10 (Image 432x432 and template 72x144) (ALBUQUERQUE
et al., 2016).",,ENGENHARIA DA COMPUTAÇÃO,ERIKA SPENCER DE ALBUQUERQUE,UNIVERSIDADE FEDERAL DE PERNAMBUCO,30/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'FPGA, Object tracking, Computer vision, template matching, embedded systems'",ENGENHARIA DA COMPUTAÇÃO,EDNA NATIVIDADE DA SILVA BARROS,107,"FPGA, Rastreamento de objetos, ZNCC, Template matching, visão computacional",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Template matching ou casamento de padrões é um problema clássico de visão computacional,
soluções para esse problema se aplicam a reconhecimento, detecção e rastreamento de
objetos. O casamento de padrões consiste em buscar regiões de uma imagem fonte que mais se
assemelham a uma imagem menor de referência (template). Uma abordagem para realizar essa
busca baseia-se em comparar, através de uma medida de similaridade, a imagem de referencia
com cada janela de mesma dimensão da imagem fonte.
A métrica Correlação Cruzada Normalizada de Média Zero (ZNCC) é uma medida de
similaridade amplamente utilizada em problemas de casamento de padrões devido a sua robustez
a variações lineares de brilho e contraste. O principal desafio para o casamento de padrões,
especialmente usando a métrica ZNCC é o alto custo computacional de calcular os valores de
ZNCC referentes a cada janela de imagem.
Há ainda, aplicações que requerem o casamento de padrões para múltiplos padrões
(templates), como por exemplo, o rastreio de múltiplos objetos independentes ou de múltiplas
poses do mesmo objeto , isso multiplica o custo computacional da operação, tornando difícil a
obtenção de uma solução em tempo real.
Esse trabalho propõe uma arquitetura de módulo em hardware com prototipação em
FPGA que explora conceitos de paralelismo e pipeline para acelerar o cálculo da ZNCC entre
uma imagem e múltiplos padrões. Resultados experimentais mostram que o módulo proposto
chega a acelerar em 3x o tempo de processamento comparado às implementações em GPU e
CPU. Além disso, o acelerador proposto alcança um dempenho de tempo real (32.13FPS) para o
processamento de até 10 templates (Imagem 432x432 e template 72x144) (ALBUQUERQUE
et al., 2016).",DISSERTAÇÃO,DESENVOLVIMENTO DE UM MÓDULO PARA TEMPLATE MATCHING BASEADO EM ZNCC COM PROTOTIPAÇÃO EM FPGA.,5150575,1
"The increasing cases of attacks on devices connected to Internet of Things (IoT) networks,
where attackers take advantage of the low processing power and the fragile protection of such
equipments, is a determining factor for giving greater attention to IoT security. The use of
Smart Cards (SCs) to protect these systems is pertinent, as they are physical attacks resistant,
portable and inexpensive. Nonetheless, their processing and memory are still not sufficient
to meet the high demand of the devices connected to IoT. For these reasons, using multiple
clustered SCs may be interesting to protect such networks. However, a platform that does not
degrade the performance of the read and write process, is needed to control many SCs. The Field
Programmable Gate Arrays (FPGAs) fit this criterion, since they are reconfigurable devices with
the ability to parallelize tasks and code replication. By connecting these devices to the network,
it would be possible to add security to other systems and ensure communication integrity for the
IoT. These factors led to the development of the Smart Card Cluster (SCC), which is a platform
for IoT security. It can be used in a variety of ways to protect the IoT layers or applications. In
this work, it is used to add protection to messages that are exchanged between gateways and
middleware IoT, ensuring message authenticity and integrity through secure hardware signing.
With three SCs the SCC allows to sign 26.18 messages per minute. The system uses FPGA to
efficiently manage SCs. It is modular, portable, scalable and cost-effective to user’s needs. This
enables the user to avoid unnecessary costs, since the embedded devices, processing power, and
the amount of SCs are adapted to the application scenario. Thus, the platform can be distributed
close to the things. In this work it is shown that the SCC becomes 69% more efficient than
another commercial device used for clustering SCs, the SIM Array, moreover the cost of using
each SC in the SCC is US$ 263.54 lower.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,GIBSON BELARMINO NUNES BARBOSA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Digital Signature. FPGA. Grid of Secure Elements. Internet of Things. Security. Smart Cards.',REDES DE COMPUTADORES,DJAMEL FAWZI HADJ SADOK,86,Assinatura Digital. FPGA. Grid de Elementos Seguros. Internet das Coisas. Segurança. Smart Cards.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),IMPRESS - Intelligent System Development Platform for Intelligent and Sustainable Society,"Os crescentes casos de ataques a dispositivos conectados às redes de Internet das Coisas
(do inglês Internet of Things - IoT), com invasores se aproveitando do baixo poder de processamento
e segurança frágil desses equipamentos, são fatores determinantes para as atenções serem
voltadas à segurança em IoT. A utilização de Cartões Inteligentes (do inglês Smart Cards - SCs)
para proteger esses sistemas é pertinente, pois eles são resistentes a ataques físicos, portáteis e
baratos. Porém, o processamento e a memória deles ainda não são suficientes para atender a alta
demanda dos dispositivo conectados à IoT. Por estas razões, utilizar vários SCs agrupados pode
ser interessante para proteger tais redes de dispositivos IoT. Contudo, o gerenciamento de muitos
SCs requer uma plataforma que não degrade o desempenho do processo de leitura e escrita.
Os Arranjos de Portas Programáveis em Campo (do inglês Field Programmable Gate Arrays
- FPGAs) se encaixam nesse critério, pois são dispositivos reconfiguráveis com capacidade de
paralelização de tarefas e replicação de código. Associando esses dispositivos, e conectando-os à
rede, seria possível agregar segurança a outros sistemas e garantir a integridade da comunicação
na IoT. Estes fatores motivaram o presente trabalho a propor e desenvolver o Smart Card Cluster
(SCC), que é uma plataforma de segurança para IoT. Ele pode ser empregado de diversas
formas para oferecer proteção às camadas de rede ou de aplicação da IoT. Contudo, neste
trabalho ele será utilizado para agregar proteção às mensagens que são trocadas entre gateways
e o middleware IoT, assegurando a autenticidade e integridade das mensagens por meio de
assinatura feita em hardware seguro. Com três SCs o SCC permite assinar 26,18 mensagens por
minuto. O sistema utiliza FPGA para fazer o controle eficiente de SCs. Através de experimentos
concluiu-se que ele é modular, portátil, escalável e com custo adequado ao cenário de IoT ao qual
é aplicado. Isto possibilita que o utilizador evite gastos, pois os dispositivos embarcados, o poder
de processamento, e a quantidade de SCs podem ser ajustados de acordo com a necessidade.
Além do que, a plataforma pode ser distribuída para perto dos dispositivos na ponta da rede. No
trabalho é mostrado que o SCC consegue ser 69% mais eficiente que outro dispositivo comercial
utilizado para agrupamento de SCs, que é conhecido como SIM Array, além disso o custo da
utilização de cada SC no SCC chega a ser US$263,54 menor.",DISSERTAÇÃO,SISTEMA DE SEGURANÇA PARA IOT BASEADO EM AGRUPAMENTO DE SMART CARDS GERENCIADOS POR FPGA,5157353,1
"The presence of different background noise in speech signal, has been a challenging
to define a model for automatic speech recognition system. Missing-feature reconstruction is
a compensation method to improve the noise robustness. A conventional models for missingfeature
reconstruction is based on acoustic feature and statistical method to improve speech
recognition. Nevertheless, these models degrade performance when different background noise
is present in the signal. In this work, we propose a new adaptive speech model for speech
recognition with missing-feature reconstruction, using unsupervised learning, for online (realtime)
and offline systems, that automatically modifies as appropriate. For this, a new approach
using Self-Organizing Map (SOM), to identify and extract articulatory features, and neural
network with time-varying structure (LARFSOM), were used. In this work, an adaptive model
for speech recognition with missing-feature reconstruction was proposed. For this, a new
approach to identify the articulatory features, through the pitch and the Self-Organizing Map
(SOM), and a neural network with time-varying structure (LARFSOM) for missing-feature
reconstruction, were used. The purpose of this model is speech recognition in online (real-time)
and offline systems, that automatically modifies as appropriate. Thus, it is expected that the
model is robust for speaker variation. For evaluation purposes, Aurora 2 and TIMIT databases
were used. As a result, we obtain a Word Error Rate average of 4.46% on Aurora 2 and 6.96% on
TIMIT. Experimental results indicate that, even without prior knowledge (oracle) of the signal,
the model is robust to noise, speaker variation, type of speech, and speech size.",,COMPUTAÇÃO INTELIGENTE,HESDRAS OLIVEIRA VIANA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,08/05/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Robust Speech Recognition. Missing-feature Reconstruction. Articulatory and Acoustic Features. Unsupervised Learning. SOM. LARFSOM.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,ALUIZIO FAUSTO RIBEIRO ARAUJO,90,Reconhecimento da Fala. Reconstrução de Características Ausentes. Características Acústicas e Articulatórias. Aprendizagem não-supervisionada. SOM. LARFSOM.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A presença de diferentes tipos e intensidades de ruídos nos sinais da fala, têm sido
um desafio para definir um modelo para o reconhecimento automático da fala. Neste sentido,
estuda-se a “reconstrução de características ausentes”, que é um método de compensação, cujo
objetivo é melhorar a robustez dos algoritmos de reconhecimento da fala em relação aos ruídos.
Um modelo convencional para reconstrução de características ausentes utiliza características
acústicas e métodos estatísticos para melhorar o reconhecimento da fala. No entanto, para
este modelo, a taxa de acerto diminui quando o ruído presente no sinal é diferente do que foi
utilizado no treinamento. Neste trabalho, um modelo adaptativo para reconhecimento da fala
com reconstrução de características ausentes foi proposto. Para isso, foi utilizada uma nova
abordagem para identificar as características articulatórias, através do pitch e do Mapa Auto-
Organizável, e uma rede neural com topologia variante no tempo (LARFSOM) para reconstruir
as características ausentes. O objetivo desse modelo é reconhecer a fala em sistemas online
(tempo real) e offline que possam se modificar automaticamente sempre que for necessário.
Assim, espera-se que o modelo seja independente de locutor. Para avaliar o modelo proposto,
utilizamos as bases TIMIT e Aurora 2. Como resultados, foram obtidas uma taxa de erro médio
de reconhecimento da fala de 6,96% para a base TIMIT e 4,46% para a base Aurora 2. Os
experimentos realizados mostram que, mesmo sem utilizar um conhecimento prévio do sinal
(oráculo), o modelo apresentou estabilidade (em relação a taxa de erro médio) quando existe
presença ou ausência de ruído no sinal, bem como, na existência de locutores com diferentes
gêneros e sotaques pronunciando frases com diferentes tamanhos.",TESE,MODELO ADAPTATIVO PARA RECONHECIMENTO DA FALA COM RECONSTRUÇÃO DE CARACTERÍSTICAS AUSENTES,5157414,1
"Congenital heart disease (CHD) figure as the most common birth defects group in the world.
The incidence of CHD is estimated at about 90 for each 10000 born alive. Heart diseases are
serious diseases and lack of early diagnosis can lead to death or serious complications, some of
which are disabling.
Among several techniques used for the detection of CC, the measurement of blood saturation
through pulse oximetry (PO) has been established as a cheap and non-invasive screening method
for newborns. However, studies have shown that the pulse oximetry test may be ineffective in
detecting some congenital heart defects.
One of the CHD whose PO has poor detection performance is Coarctation of Aorta (CoA). CoA
is characterized by the narrowing of a portion of the descending aortic artery and, if undiagnosed
and treated early, can lead to death or serious complications such as hypertension, coronary
artery disease, congestive heart failure, recoarctation, aortic aneurysm, aortic rupture and stroke.
One of the symptoms of CoAo is a delay or reduction of the femoral pulse (lower extremity) in
relation to the brachial (upper extremity) pulse, so that techniques that analyze variations in the
blood pulse are promising in the detection of this cardiopathy.
Photoplethysmography (PPG), for example, is an optoelectronic method used to measure the
volume of blood in the microvascular tissue of the body parts tissue. Studies have analyzed
several indices extracted from the photoplethysmographic signal, related to the amplitude of the
signal and its behavior in time as well, which have presented direct relations with the health of
the cardiovascular system. This fact indicates that the comparison of captured PPG signals at the
upper and lower extremities of the body is a potential indicator of the presence of cardiopathies.
This work aimed at the development of a hardware and software system for capturing and
analyzing photoplethysmographic signals at two sites of the body simultaneously. The system,
which receives signals through commercial sensors widely used by professionals of health,
conditions and extracts characteristics of the pulses obtained (systolic and diastolic peaks, and
dichrotic notch) of the two curves captured and calculates indices from such characteristics.
This project focused on the development of the system and analysis of the various characteristics
of the signals captured, besides the calculation of the indices. In the future, the system should be
used in a clinical setting to assess its specificity and sensitivity in the detection of congenital
heart diseases, including CoAo.",,ENGENHARIA DA COMPUTAÇÃO,JOAO ERIK DE ANDRADE MELO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,11/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Embeded System, Photoplethysmography, simultaneous photoplethysmography, Coarctation of Aorta, CoA, congenital heart disease, pulse wave analysis.'",ENGENHARIA DA COMPUTAÇÃO,MANOEL EUSEBIO DE LIMA,94,"Sistema embarcado, fotopletismografia, fotopletismografia simultânea, cardiopatia congênita crítica, análise de sinais",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"As cardiopatias congênitas (CC) figuram como o grupo de defeitos de nascimento mais comuns
e graves no mundo. A incidência de CC é estimada em cerca de 90 para cada 10000 nascidos
vivos. As cardiopatias são doenças graves e a falta de diagnóstico precoce pode levar a óbito ou
a complicações sérias, algumas incapacitantes.
Entre várias técnicas utilizadas para a detecção de CC, a medição da saturação sanguínea feita
através da oximetria de pulso (OP), vem sendo estabelecida como método barato e não invasivo,
de triagem de recém nascidos. No entanto, estudos têm mostrado que o teste com oximetria de
pulso pode ser pouco efetivo na detecção algumas cardiopatias congênitas.
Uma das CC cujo a OP apresenta baixo desempenho na detecção é a Coarctação da Aorta
(CoAo). A CoAo é caracterizada pelo estreitamento de uma porção da artéria aorta descendente
e se não diagnosticada e tratada precocemente pode levar a óbito ou a complicações sérias como
hipertensão, doença arterial coronária, falha congestiva do coração, recoarctação, aneurisma da
aorta, ruptura da aorta e acidentes cerebrovascular.
Uma dos sintomas da CoAo é um atraso ou redução do pulso femoral (extremidade inferior) em
relação ao pulso braquial (extremidade superior), de modo que técnicas que analisem variações
no pulso sanguíneo são promissoras na detecção desta cardiopatia.
A fotopletismografia (PPG), por exemplo, é um método optoeletrônico utilizado para medir o
volume de sangue no leito microvascular do tecido de partes do corpo. Estudos têm analisado
diversos índices extraídos do sinal fotopletismográfico, tanto relacionados à amplitude do sinal,
quanto ao seu comportamento no tempo, os quais têm apresentado relações diretas com a saúde
do sistema cardiovascular. Este fato indica que a comparação de sinais de PPG capturados nas
extremidades superior e inferior do corpo é um potencial indicador da presença de cardiopatias.
Este trabalho visou o desenvolvimento de um sistema de hardware e software para captura e
análise de sinais fotopletismográficos em dois locais do corpo simultaneamente. O sistema,
que recebe os sinais através de sensores comerciais amplamente utilizados pelos profissionais
de saúde, os condiciona e extrai características dos pulsos obtidos (picos sistólico e diastólico,
início do pulso e dicrotic notch) das duas curvas captadas e calcula índices a partir de tais
características.
Este projeto focou no desenvolvimento do sistema e análise das várias características dos
sinais captados, além do cálculo dos índices. No futuro, o sistema deverá ser utilizado em um
cenário clínico para avaliação de sua especificidade e sensibilidade na detecção de cardiopatias
congênitas, entre elas a CoAo.",DISSERTAÇÃO,DESENVOLVIMENTO DE UM EQUIPAMENTO PARA CAPTURA E ANÁLISE DE SINAIS FOTOPLETISMOGRÁFICOS DE DUPLO CANAL SIMULTÂNEO,5157436,1
"We have observed the growth of the use of natural interaction in the most diverse
computational environments due to the ease of learning and the naturalness of interaction offered
to the user. Devices capable of recognizing human gestures are used in conjunction with gesture
recognition techniques to achieve this purpose. Gesture recognition most often involves machine
learning or advanced recognition algorithms adding more complexity to the development of an
application based on natural interaction. Our work aims to construct a Domain Specific Language
(DSL) capable of describing hand gestures, considerably reducing development efforts. For the
development of this DLS, we used the programming language C#, a widely used general purpose
language, the Unity3D gaming development platform and Leap Motion, a sensor capable of
detecting hand and finger movements and using them as input. The main objective of this work
is to provide a DSL that helps the development of applications that use natural hand gestures as
interaction. The aim of this DSL is to facilitate the specification of gestures through a notation
capable of abstracting such complexities.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,JOSE THIAGO PEREIRA DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,28/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Humancomputer interaction, Natural Interaction, Leap Motion, Gesture recognition, Domain-Specific Language'",ENGENHARIA DE SOFTWARE,ANDRE LUIS DE MEDEIROS SANTOS,78,"Interação Humano-Computador, Interação Natural, Leap Motion, Reconhecimento de Gestos, Linguagens Específicas de Domínio",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),INVESTIGANDO LINGUAGENS DE DOMÍNIO ESPECÍFICO E FABRICAS DE SOFTWARE PARA O DESENVOLVIMENTO DE JOGOS,"Temos observado o crescimento da utilização da interação natural nos mais diversos ambientes
computacionais devido a facilidade de aprendizado e naturalidade de interação oferecidas
ao usuário. Dispositivos capazes de reconhecer os gestos humanos são utilizados juntamente com
técnicas de reconhecimento de gestos para alcançar tal finalidade. O reconhecimento de gestos
na maioria das vezes envolve aprendizagem de má-quina ou algoritmos avançados de reconhecimento
adicionando mais uma complexidade no desenvolvimento de uma aplicação baseada em
interação natural. O nosso trabalho tem por objetivo a construção de uma Linguagem Específica
de Domínio (DSL) capaz de descrever os gestos das mãos, reduzindo consideravelmente os
esforços no desenvolvi-mento. Para o desenvolvimento da DSL, foi utilizada a linguagem de
programação C#, uma linguagem de propósito geral amplamente utilizada, a plataforma de
desenvolvimento de jogos Unity3D e o Leap Motion, um sensor capaz de detectar movimentos
das mãos e dedos e os utilizar como comandos de entrada. O principal objetivo desta dissertação
é disponibilizar uma DSL que auxilie o desenvolvimento de aplicações que utilizem gestos
naturais das mãos como interação. A proposta da DSL é tornar a especificação de gestos mais
simples através de uma notação capaz de abstrair tais complexidades.",DISSERTAÇÃO,UMA LINGUAGEM DE DOMÍNIO ESPECÍFICO PARA AUXILIAR O DESENVOLVIMENTO DE APLICAÇÕES BASEADAS EM GESTOS DAS MÃOS,5157484,1
"As a result of the complexity increase in current industrial processes, requests for treatment of nonlinear systems have been overburdening modern control techniques. The modern control theory is based on a model to represent these processes, however complex models can result in a complicated and difficult controller to maintain. Data-Driven Control techniques are getting featured in areas where the system complexity, or the absence of a model, can be overcame by a lot of available data, which can be used to calculate the control signal directly. Among Data-Driven Control methods, the Model Free Adaptive Control (MFAC) technique stands out for characteristics, such as being on-line, using just input and output data from the plant and reference signal, as well as having formulations for systems with varying degrees of non-linearity. However, the MFAC is still making unanswered questions, such as the choice of controller parameters. The tuning of these parameters can be transformed into an optimization problem, nevertheless, a control project usually involves multiple objectives to be attended. Therefore, this work will define strategies and a multi-objective evolutionary algorithm, based on differential evolution and directed immigrants, to adjust the MFAC controller parameters. Several cases will be evaluated and two adaptive strategies for these parameters will be implemented: An off-line strategy, at which the parameters are optimized in all acting period, and another on-line, where the controller uses the optimized parameters obtained in the previous strategy performing optimizations at smaller intervals when some situations are detected. The results obtained through simulations suggest that the controller with optimized parameters off-line is better than parameters found in the literature. In addition, the proposed on-line strategy has been able to improve or at least maintain the benefits of off-line optimization.",,COMPUTAÇÃO INTELIGENTE,JUDAS TADEU GOMES DE SOUSA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,12/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Adaptive Control. Data Driven Control. Multiobjective Optimization. Differential Evolution.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,ALUIZIO FAUSTO RIBEIRO ARAUJO,173,Controle Adaptativo. Controle Direcionado a Dados. Otimização Multiobjetivo. Evolução Diferencial.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A demanda pelo tratamento de sistemas não-lineares, resultado do aumento da complexidade dos processos industriais recentes, tem dificultado o uso de técnicas de controle moderno. A teoria de controle moderno é baseada na existência de modelo para representar o processo, no entanto, o uso de modelos complexos pode resultar num controlador complexo e difícil de manter. Técnicas de controle direcionadas por dados estão ganhando destaque em áreas onde a complexidade do sistema, ou mesmo a inexistência de um modelo, podem ser superadas pela disponibilidade de dados do processo, os quais podem ser capturados e usados para calcular diretamente o sinal de controle. Dentre os métodos de controle direcionados por dados, a técnica de Controle Adaptativo Livre de Modelo (MFAC – Model Free Adaptive Control) se destaca por características como: ser on-line, depender apenas dos dados de entrada e saída medidos da planta e do sinal de referência e por possuir formulações que atendem sistemas com vários graus de não-linearidade. Porém, o MFAC ainda possui questões em aberto, por exemplo, a escolha dos parâmetros do controlador. O ajuste desses parâmetros pode ser transformado num problema de otimização, no entanto, um projeto de controle costuma envolver múltiplos objetivos a serem atendidos. Portanto, neste trabalho serão definidas estratégias e um algoritmo evolucionário multi-objetivo, baseado em evolução diferencial e em imigrantes direcionados, para sintonia dos parâmetros do controlador MFAC. Vários casos de estudos serão testados e duas estratégias de ajustes para os parâmetros serão implementadas: uma estratégia off-line, na qual os parâmetros são otimizados em todo intervalo de operação, e outra on-line, onde o controlador usa os parâmetros otimizados na estratégia anterior, mas também realiza otimizações em intervalos menores, enquanto o controle atua, quando algumas situações são detectadas. Os resultados obtidos mediante simulações, sugerem que o controlador usando parâmetros otimizados off-line tem melhor desempenho do que um com parâmetros encontrados na literatura. Além disso, a estratégia de otimização on-line proposta, conseguiu melhorar ou pelo menos manter os benefícios obtidos com a otimização off-line.",TESE,OTIMIZAÇÃO MULTI-OBJETIVO ON-LINE DOS PARÂMETROS DE UM CONTROLADOR MFAC APLICADO A SISTEMAS NÃO-LINEARES MEDIANTE ALGORITMO DE EVOLUÇÃO DIFERENCIAL COM IMIGRANTES DIRECIONADOS,5157487,1
"The number of Internet users has grown dramatically in the last years. Consequently, the complexity of the Internet has increased and several challenges have arisen because of its expansion. As an example, the phenomenon known as ossification of the Internet architecture makes harder to create new solutions in the network layer. One of the key ways to overcome barriers and make networks more flexible and efficient is Network Virtualization (NV). In this context, virtualization technologies such as Software Defined Network (SDN) and Network Functions Virtualization (NFV) allow better network management and greater capability to handle different demands. However, even with the several innovations brought by these virtualization technologies, it is important to note that it is necessary to apply efficient management techniques. In addition to that, there is a need to allocate resources to keep the system functional and performing satisfactorily. Thus, there are three problems related to network virtualization: (1) allocation of virtual machines; (2) allocation of SDN controllers and (3) issues of dependability and optimization of resources. This work proposes an algorithm based on GRASP (Greedy Random Adaptive Search Procedure) metaheuristic algorithm for the allocation of virtual networks whose objective is to obtain an allocation with the highest possible availability considering a set of constraints for the correct functioning of the network. Also, considering the recent challenges introduced by Software Defined Networks (SDN), a model was proposed based on Integer Linear Programming (ILP) for the SDN Controller Placement Problem. The objective of the proposed model is to minimize the deployment cost while taking into account several constraints and investigating features of real network topologies that affect the SDN controller placement cost. Finally, in a complementary way, algorithms are developed and validated to perform the following: (1) Load balancing between SDN controllers; (2) Path creation between switches and controllers; (3) Repositioning SDN controllers in order to minimize the number of disconnections in the event of a link failure.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,MARCELO ANDERSON BATISTA DOS SANTOS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,10/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Network Virtualization;Software Defined Network (SDN);Dependability;GRASP and Linear Program',REDES DE COMPUTADORES,STENIO FLAVIO DE LACERDA FERNANDES,129,Virtualização de redes;Redes Definidas por Software (SDN);Dependabilidade;GRASP e Programação Linear,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),SLA4Cloud: Measurement and SLA Management of Heterogeneous Cloud Infrastructures,"Devido ao rápido crescimento da Internet, em poucos anos, houve o aumento da complexidade da rede e diversos desafios surgiram para sua expansão. Como exemplo, pode-se citar o fenômeno conhecido como a ossificação da Internet. Em meio a esse cenário, a virtualização de redes (Network Virtualization - NV) surgiu como uma das principais formas para superar obstáculos e tornar a rede mais flexível e eficiente. Tecnologias de virtualização como Software Defined Network (SDN) e Network Functions Virtualization (NFV) buscam, por exemplo, dar um maior poder de gerenciamento à rede, aumentando o seu grau de adaptação à novas demandas. No entanto, mesmo com as inúmeras inovações trazidas por essas tecnologias de virtualização, é importante notar que embora haja um maior poder de manipulação da rede, faz-se necessário empregar técnicas eficientes de gerenciamento e alocação de recursos capazes de manter o sistema funcional e com desempenho satisfatório. Assim, como parte desse cenário, destacamos nessa tese três problemas relacionados a virtualização de redes: (1) alocação de máquinas virtuais; (2) alocação de controladores SDN e (3) questões de dependabilidade e otimização do uso dos recursos disponíveis. Desta forma, esta tese propõe uma heurística baseada na meta-heurística GRASP (Greedy Random Adaptive Search Procedure) para alocação de redes virtuais cujo objetivo é obter uma alocação com a maior disponibilidade possível considerando um conjunto de restrições para o correto funcionamento da rede. Além disso, contemplando os recentes desafios introduzidos por Redes Definidas por Software (SDN) é proposto um modelo através de Programação Linear Inteira para o problema de alocação de controladores SDN. Considera-se controladores com diferentes capacidades e custos onde a função objetivo deste modelo busca uma solução de cobertura da rede de forma que o custo total da implantação de controladores SDN seja minimizado. Adicionalmente busca-se identificar quais características de uma rede de backbone tem influência no custo final de alocação destes controladores SDN. Por fim, de forma complementar, são desenvolvidos e validados algoritmos para realizar: (1) Balanceamento de carga entre controladores SDN; (2) Criação de caminhos entre Switches e Controladores; (3) Reposicionamento de controladores SDN de forma a minimizar o número de desconexões em caso de falha de enlaces.",TESE,"ALOCAÇÃO DE REDES VIRTUAIS E CONTROLADORES EM REDES DEFINIDAS POR SOFTWARE: UMA ANÁLISE DE CUSTO, REDE E DEPENDABILIDADE",5157543,1
"The connection method earned good reputation in the field of automated theorem proving for
around three decades, thanks to its simplicity, clarity, eciency and parsimonious use of memory.
It has recently been applied in automatic provers that reason over ontologies written in the
description logics ALC. However, its proofs are not very readable, consisting of a set of pairs
of connections that are formed by complementary atomic formulas found in each path through
a matrix. The readability is largely lost by the gain of performance and transformations applied
to the formula to be proved. This work presents a conversion method to translate ALC. connection
proofs into ALC. sequent proofs. With the translation into sequent, a more readable and
intelligible representation is obtained. The conversion method proposed here receives the ALC.
formula and its corresponding connection proof in non-clausal form. A tree representation of
the ALC. formula is built and serves as a guide in the conversion process. As the connection
proof is traversed, the pairs of complementary literals that form the connections are searched in
the formula tree; in parallel to this process, a sequent proof is being built. Finally, the algorithm
that implements the process is presented, of which the complexity suggests the viability of the
method.",,COMPUTAÇÃO INTELIGENTE,EUNICE PALMEIRA DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,15/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Description Logics. Attributive Concept Language with Complements (). Connection Method. Sequent Calculus.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,FREDERICO LUIZ GONCALVES DE FREITAS,128,"Lógica de Descrições, Attributive Concept Language with Complements (ALC), Método de Conexões, Cálculo de Sequentes.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O método de conexões ganhou boa reputação na área de prova automática de teoremas por cerca de três décadas, devido à sua simplicidade, clareza, eficiência e uso racional de memória. Este método recentemente tem sido aplicado em provadores automáticos que raciocinam sobre ontologias escritas em lógica de descrições ALC. No entanto, as provas geradas por esse método são de difícil compreensão, consistindo em um conjunto de pares de conexões que são formados por fórmulas atômicas complementares encontradas ao longo de cada caminho de uma matriz. A legibilidade das provas é em grande parte perdida pelo ganho de performance e transformações aplicadas à fórmula a ser provada. Esse trabalho apresenta um método de conversão das provas em ALC geradas pelo método de conexões para um sistema de sequentes ALC. Com a transformação para sequentes, obtém-se uma representação mais legível e inteligível. O método de conversão proposto aqui recebe a fórmula ALC e sua correspondente prova de conexões em formato não-clausal. Uma representação em árvore da fórmula ALC é construída e serve como guia no processo de conversão. À medida em que a prova em conexões é percorrida, busca-se na árvore da fórmula os pares de literais complementares que formam as conexões; paralelamente a este processo, uma prova em sequentes vai sendo construída. Por fim, é apresentado o algoritmo que implementa o processo, cuja complexidade sugere a viabilidade do método.",TESE,Conversão de Provas em Lógica de Descrições ALC Geradas pelo Método de Conexões para Sequentes,5168849,1
"Threats to information security can have great impact on business finances and company’s reputation. Traditional methodologies for evaluating the maturity of data centers investigate security parameters to determine the compliance of data centers and international security norms. This work proposes two innovative evaluation procedures to capture other security perspectives on data center environments: (1) weighted analysis - it weights higher security controls simultaneously present in a higher number of norms; (2) contextual analysis - it is sensitive to the importance level that the organization assigns to each security control. Through the proposed methodology, security engineers can identify security issues, characterize the security maturity, and suggest new policies improve security configurations of data centers. This work also includes a case study to evaluate the benefits of the methodology in real-world scenarios. Results demonstrated that the proposed methodology evaluates higher the security elements more relevant for the company, where as traditional approaches consider all security aspects to be equally important.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,MILTON VINICIUS MORAIS DE LIMA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,20/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Data Center, Security Information, Maturity Evaluation, Security Metrics, ISO/IEC 27002, NIST 800-53.'",REDES DE COMPUTADORES,RICARDO MASSA FERREIRA LIMA,145,Data Center;Segurança da Informação;Avaliação da Maturidade;Métricas de Segurança;ISO/IEC 27002;NIST SP 800-53.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"As ameaças à segurança da informação podem ter um grande impacto nas finanças e na reputação da empresa. As metodologias tradicionais para avaliar a maturidade dos data centers investigam os parâmetros de segurança para determinar a conformidade dos data centers e as normas internacionais de segurança. Este trabalho propõe dois procedimentos de avaliação para capturar outras perspectivas de segurança em ambientes de data centers: (1) análise ponderada – pondera os controles de segurança, simultaneamente presentes em um número maior de normas; (2) análise contextual - é sensível ao nível de importância que a organização atribui a cada controle de segurança. Através da metodologia proposta, os engenheiros de segurança podem identificar problemas de segurança, caracterizar a maturidade da segurança e sugerir novas políticas para melhorar as configurações de segurança dos data centers. Este trabalho também inclui um estudo de caso para avaliar os benefícios da metodologia em cenários do mundo real. Os resultados demonstraram que a metodologia proposta avalia os elementos de segurança mais relevantes para a empresa, onde as abordagens tradicionais consideram todos os aspectos de segurança como igualmente importantes.",DISSERTAÇÃO,Uma Metodologia para Apoiar Políticas de Segurança em Ambientes de Data Center: Uma Estrutura Sistemática com Multiperspectiva,5169039,1
"Depression is one of the most frequent psychological disorders in the world. This disorder
has a major impact on the individual’s quality of life, compromises social and family
relationships and can lead to suicide. In adolescence, depression is one of the most usual
psychological illnesses. Several emotional problems start and peak during this phase of
life. In Brazil, for instance, suicide is the third leading cause of death among youngsters.
Cognitive-Behavioural Therapy (CBT) has been shown to be effective in treating depression.
This type of therapy can also be performed through the computer, without the
accompaniment of a therapist. Studies have identified important advantages in computermediated
communication compared to face-to-face human interaction, such as promoting
anonymity and increasing self-disclosure. However, most current CBT-based systems have
not shown a satisfactory degree of interactivity. In this scenario, chatbots are an alternative
to minor this deficiency of computer based therapy systems. Chatbots are systems
created to simulate a real dialogue with users, being able to analyze and influence their
behavior. This type of system interacts with the user through the use of natural language.
This research project developed a chatbot based on the principles of CBT, whose aim is
to dialogue with adolescents who suffer from depression. To build the chatbot Beck, we
deployed ChatScript, a computer language tailored for the development of this type of
system, which has won several awards. We evaluated the performance and usefulness of
our chatbot Beck through a conversation test and a survey conducted with adolescents.
The results obtained are very satisfactory: 85.94% of the adolescents answered that they
were satisfied with Beck’s performance, and 92.19% of the adolescents agreed that the
chatbot was useful for them.",,COMPUTAÇÃO INTELIGENTE,OBERDAN ALVES DE ALMEIDA JUNIOR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,04/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Chatbots. Natural Language Processing. Depression. Cognitive-Behavioural Therapy. ChatScript.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,FLAVIA DE ALMEIDA BARROS,165,Chatbots. Processamento de Linguagem Natural. Depressão. Terapia Cognitivo-Comportamental. ChatScript.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Desenvolvimento de Chatterbots Agentes Conversacionais Incorporados em Jogos Sérios,"A depressão é um dos distúrbios psicológicos mais frequentes no mundo. Esse distúrbio causa um grande impacto na qualidade de vida do indivíduo, compromete as relações sociais e familiares, e pode levar ao suicídio. Na adolescência, a depressão é uma das doenças psicológicas mais comuns. Muitos dos problemas emocionais têm início e pico durante esta fase da vida. Entre os jovens, o suicídio representa a terceira principal causa de morte. A Terapia Cognitivo-Comportamental (TCC) tem se mostrado bastante eficaz no tratamento da depressão. Este tipo de terapia também pode ser realizado através do computador, sem o acompanhamento de um terapeuta. Estudos identificaram vantagens importantes na comunicação mediada por computador em comparação com a interação humana face a face, como na promoção do sentimento de anonimato e no aumento da auto-revelação. Contudo, a maioria dos sistemas atuais baseados em TCC não apresenta um grau satisfatório de interatividade. Neste cenário, os chatbots são uma alternativa para diminuir essa deficiência dos sistemas de terapia via computador. Chatbots são sistemas criados para simular um diálogo real com usuário, podendo ter a capacidade de analisar e influenciar seus comportamentos. Esse tipo de sistema interage com o usuário através do uso da linguagem natural. Este projeto de mestrado desenvolveu um chatbot capaz de conversar com adolescentes que sofrem depressão baseado nos princípios da TCC. Para construir o chatbot, utilizamos o ChatScript, uma linguagem de desenvolvimento desse tipo de sistema, que foi vencedora de vários prêmios.",DISSERTAÇÃO,Beck: Um Chatbot Baseado na Terapia Cognitivo-Comportamental para Apoiar Adolescentes com Depressão,5169098,1
"Different terms such as ""the real-time enterprise"", ""software infrastructures"",
""service oriented architectures"" and ""composite software applications"" have gained
importance in industry. It brings us the need of information systems that support crossapplication
integration, cross-company transactions and end-user access through a
range of channels, including the Internet. In this context, Software Product Line (SPL)
Engineering has gained importance by product oriented companies, as a strategy to
cope with the increasing demand of large-scale product customization, providing an effective
and efficient ways of improving productivity, software quality, and time-to-market.
These benefits combined with the need of most applications interact with other applications,
and the internet access makes critical assets vulnerable to many threats. For most
of the product oriented companies, security requirements are likely to be as varied as for
any other quality. Thus, it is important to supply variants of the same product to satisfy
different needs. Owing to its variability management capabilities, software product line
architectures can satisfy these requirements if carefully designed the resulting system
has a better chance of meeting its expectations. All these requirements should be
achieved at early design phases. Otherwise the cost to design a secure architecture
will increase, which could worsen in SPL context, due to its complexity. In this context,
this thesis evaluates different techniques to implement security tactics for the purpose
of assessing conditional compilation and aspect-oriented programming as variability
mechanisms concerning maintainability by accessing code size, separation of concerns,
coupling and cohesion from software architects in the context of Software Product Lines
projects. Hence, to better support SPL architects during design decisions, a family of
experiments using three different testbeds was performed to analyze different security
techniques regarding to maintainability. We have found that for most of the techniques
conditional compilation had a smaller amount of lines of code when compared with Aspect
Oriented Programming. The separation of concerns attribute had the low impact on
maintainability when implemented with aspect-oriented programming. The analysis also
showed that detect attack techniques are less costly than resist attack techniques. The
results are useful for both researchers and practitioners. On the one hand, researchers
can identify useful research directions and get guidance on how the security techniques
impact on maintainability. On the other hand, practitioners can benefit from this thesis
by identifying the less costly variability implementation mechanism, as well as, learning
concrete techniques to implement security tactics at the code level.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,PAULO ANSELMO DA MOTA SILVEIRA NETO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,02/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Software Engineering. Software Security. Tactics. Design Patterns. Non functional properties.',ENGENHARIA DE SOFTWARE,VINICIUS CARDOSO GARCIA,177,Linha de produto de software. segurança de software. variabilidade. táticas de segurança. técnicas de segurança. compilação condicional. programação orientada a aspectos.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),CRIAÇÃO DE UM INSTITUTO NACIONAL DE ENGENHARIA DE SOFTWARE EM PERNAMBUCO,"Diferentes termos como “empresa em tempo real”, “infraestrutura de software”,
“arquiteturas orientadas a serviço” e “aplicações de software” tem ganhado importância
na indústria. Isso requer sistemas de informação que suportem a integração com outras
aplicações, transações entre empresas e acesso ao usuário final por uma variedade de
canais, incluindo internet. Nesse contexto, Linha de Produto de Software (LPS) tem
ganhado importância por empresas orientadas a produtos de software, como uma estratégia
para lidar com a crescente demanda de personalização de produtos em grande
escala, proporcionando uma forma eficaz e eficiente de melhorar a produtividade, a
qualidade do software e o tempo de lançamento para o mercado. Esses benefícios
combinados com a necessidade da maioria dos aplicativos precisarem interagir com
outras aplicações e o acesso à Internet tornam essas aplicações vulneráveis a muitas
ameaças. Para a maioria das empresas orientadas à produto, os requisitos de
segurança podem variar assim como outro atributo de qualidade do software. Assim, é
importante fornecer variantes do mesmo produto para satisfazer diferentes necessidades.
Devido às suas capacidades de gerenciamento de variabilidade, arquiteturas de
linha de produtos têm a capacidade de satisfazer esses requisitos, se cuidadosamente
projetada o sistema resultante terá uma melhor chance de satisfazer as expectativas.
Todos esses requisitos devem ser alcançados nas primeiras fases do projeto, caso
contrário, o custo para projetar uma arquitetura segura aumentará, o que poderia
piorar no contexto SPL, devido à sua natureza complexa. Assim, para melhor apoiar
os arquitetos durante as decisões de projeto. Uma família de experimentos utilizando
três SPLs distintas foram utilizadas para analisar diferentes técnicas de segurança,
implementadas usando compilação condicional (CC) e programação orientada a aspectos
(AOP). Essa avaliação teve como objetivo analisar as técnicas e mecanismos
em relação a: tamanho, “separation of concerns”, coesão e acoplamento. O resultado
nos mostra que para a maioria das técnicas quando implementadas com compilação
condicional apresentavam uma menor quantidade de código quando comparadas com
AOP. O atributo de “separation of concerns” teve menor impacto na manutenção quando
implementado com programação orientada a aspectos. A análise também mostrou que
técnicas de detecção de ataque são menos onerosas do que técnicas para resistir
a ataque. Os resultados são úteis para pesquisadores e profissionais. Por um lado,
os pesquisadores podem identificar direções de pesquisa e obter orientação sobre
como as técnicas de segurança impactam na manutenção. Por outro lado, os profissionais
podem se beneficiar deste estudo, identificando o mecanismo de implementação
da variabilidade menos dispendioso, bem como aprendendo técnicas concretas para
implementar táticas de segurança a nível de código.",TESE,ASSESSING SECURITY IN SOFTWARE PRODUCT LINES: A MAINTENANCE ANALYSIS,5177637,1
"How to verify if a classical proof is also intuitionistic? Gentzen’s natural deduction only
requires no occurrence of the law of the excluded middle or the elimination of double negation
rule in an intuitionistic derivation. His sequent calculus achieves the same result by restricting the
number of formulas on the right-hand side to at most one, which removes its multiple-conclusion
feature that is important for the calculus’ symmetry. There are approaches today that take this
into account and present solutions for multiple-conclusion sequent calculi for intuitionistic logic,
but while giving us some useful insights on what constitutes an intuitionistic system, they inherit
the bureaucracy from Gentzen’s formalism. Here we separate intuitionistic logic from classical
in non-sequential derivations by adopting a geometric perspective in our approach. We propose
an intuitionistic version for two multiple-conclusion systems initially defined for propositional
classical logic: N-Graphs, which was presented by de Oliveira (2001) as a symmetric natural
deduction; and Robinson’s proof-nets (2003), that were inspired in sequent calculus.",,TEORIA DA COMPUTAÇÃO,RUAN VASCONCELOS BEZERRA CARVALHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,03/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'intuitionistic logic, proof graphs, multiple-conclusion, proof theory, proof-nets'",LÓGICA E SISTEMAS DEDUTIVOS,ANJOLINA GRISI DE OLIVEIRA,144,"lógica intuicionista, grafos de prova, múltipla conclusão, teoria da prova, proof-nets",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),"N-grafos: normalização, ciclos, intuicionismo, definição de um assistente de provas, redefinição do critérios de corretude","Como verificar se uma prova clássica também é intuicionista? Em dedução natural
basta não haver ocorrência da lei do terceiro excluído ou da eliminação da dupla negação,
conforme proposto por Gentzen. No seu cálculo de sequentes o mesmo resultado é alcançado
restringindo o número de fórmulas no lado direito a no máximo um. Assim não há múltiplaconclusão,
embora esta seja importante para a simetria. Hoje já existem abordagens que levam
isso em conta e propõem cálculos de sequentes para lógica intuicionista com várias fórmulas
no consequente. Mas ainda que elas nos forneçam compreensões do que diferencia a lógica
intuicionista da clássica, há o problema da burocracia inerente ao formalismo de Gentzen.
Aqui separamos a lógica intuicionista da clássica em derivações não-sequenciais adotando uma
abordagem geométrica. Propomos uma versão intuicionista para dois sistemas de múltipla
conclusão inicialmente definidos apenas para a lógica clássica proposicional: os N-Grafos,
apresentados por de Oliveira (2001) e baseado em dedução natural; e as proof-nets de Robinson
(2003), inspiradas no cálculo de sequentes.",TESE,CÁLCULOS DE MÚLTIPLA CONCLUSÃO PARA A LÓGICA INTUICIONISTA SOB UMA PERSPECTIVA GEOMÉTRICA,5177777,1
"This work aims to secure the communication between devices connected to the Internet
of Things (IoT) by integrating microcontrollers and Smart Cards (SCs), plastic cards in which
are embedded cryptographic tamper-resistant chips, currently used in applications that require
a high level of security (e.g., banks). An architecture, which involves hardware and software
projects, is proposed for a system that establishes an encrypted and authenticated communication,
based on Transport Layer Security (TLS) Protocol, between IoT devices and a server, focusing
on low-cost development boards. Tests were performed initially on Arduino UNO boards and
the final device has an ESP8266 microcontroller (specifically, an ESP-12E module), which
has integrated Wi-Fi capabilities and is simple to program. Additionally, the SC used is Java
Card-based, which simplifies the development and installation of programs (known as applets) on
the card. It contains a modified version of IsoApplet, an open source program under development
that allows the realization of cryptographic tasks, implemented according to ISO7816 standards.
Thus, the execution of essential operations in the implementation of a security infrastructure such
as key generation, encryption and decryption (in both symmetric and asymmetric cryptography),
digital signature and secure data storage (e.g., secret keys, certificates) is delegated by the
microcontroller to the card, which has specialized hardware. The microcontroller, in turn, can
be connected to sensors and connects in an authenticated way to a server, sending encrypted
data. Finally, it is shown that it is possible to build a device connected to the Internet of Things,
which is able to send messages safely, by integrating low-cost microcontrollers and Smart Cards.
A cost analysis of the device shows that it can have a market-compatible price if produced on
a large scale. A second analysis, regarding the power consumption of the board, shows that,
depending on the type of application, the device can run on battery for days. The contributions
of this work, in addition to the manufacture of the IoT device itself, include the development of
libraries that enable communication between microcontrollers (compatible with Arduino) and
Smart Cards and the expansion of open-source software for Java Cards by adding cryptographic
functions associated with TLS.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,THIAGO DE OLIVEIRA CAVALCANTE,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Internet of Things. Security. Smart Card. Java Card. Microcontrollers',REDES DE COMPUTADORES,DJAMEL FAWZI HADJ SADOK,105,Internet das Coisas. Segurança. Smart Card. Java Card. Microcontroladores,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),IMPRESS - Intelligent System Development Platform for Intelligent and Sustainable Society,"Este trabalho tem como objetivo proteger a comunicação entre dispositivos conectados à
Internet das Coisas, do inglês Internet of Things (IoT), através da integração entre microcontroladores
e Smart Cards (SCs), cartões de plástico nos quais estão embutidos chips criptográficos
invioláveis, atualmente utilizados em aplicações que exigem um alto nível de segurança (e.g.,
bancos). É proposta uma arquitetura, a qual envolve projetos de hardware e software, para um
sistema que estabelece uma comunicação autenticada e criptografada, baseada no Protocolo
Transport Layer Security (TLS), entre dispositivos IoT e um servidor. O foco do trabalho está em
placas de desenvolvimento de baixo custo. Testes foram realizados inicialmente no Arduino UNO
e o dispositivo final possui o microcontrolador ESP8266 (em específico, o módulo ESP-12E),
que possui Wi-Fi integrado, o que facilita a sua inclusão na IoT, e é simples de programar.
Adicionalmente, é utilizado um SC com a tecnologia Java Card, que torna mais simples o
desenvolvimento e a instalação de programas (conhecidos como applets) no cartão. Nele está
instalada uma versão modificada do IsoApplet, um programa open source em desenvolvimento
que permite a execução de tarefas criptográficas, implementado de acordo com os padrões
ISO7816. Assim, a execução de operações essenciais na implementação de uma infra-estrutura
de segurança como geração de chaves, cifragem e decifragem (em ambas criptografias simétrica
e assimétrica), assinatura digital e armazenamento seguro de dados (e.g., chaves secretas, certificados)
é delegada pelo microcontrolador ao cartão, que possui hardware especializado. O
microcontrolador, por sua vez, pode ser ligado a sensores e se conectar de forma autenticada
com um servidor, enviando informações criptografadas. Por fim, demonstra-se que é possível
construir um dispositivo conectado à Internet das Coisas, capaz de enviar mensagens de forma
segura, a partir da integração entre microcontroladores de baixo custo e Smart Cards. Uma
análise de custo do dispositivo construído, mostra que o mesmo pode ter um preço compátivel
com o mercado, se produzido em larga escala. Uma segunda análise, relativa ao consumo de
energia da placa, mostra que, a depender do tipo de aplicação, o dispositivo pode funcionar com
bateria por dias. As contribuições deste trabalho, além da fabricação do próprio dispositivo IoT,
incluem o desenvolvimento de bibliotecas que habilitam a comunicação entre microcontroladores
(compatíveis com Arduino) e Smart Cards e a expansão de um software open-source para Java
Cards com funções criptográficas associadas ao TLS.",DISSERTAÇÃO,SISTEMA DE COMUNICAÇÃO SEGURA PARA DISPOSITIVOS CONECTADOS À INTERNET DAS COISAS COM UTILIZAÇÃO DE SMART CARDS,5177943,1
"According to the World Health Organization, breast cancer is the most common form of cancer among adult women worldwide, being one of the most fatal types of cancer. Studies show that providing early diagnosis can contribute to a reduction in mortality rates and increase as treatment options. Computer Assisted Diagnostics are being proposed and used in the health area. Mathematical Morphology is a nonlinear theory widely used in the processing of digital images. It is a based on the mathematical theory of intersection and union of sets. Extreme Learning Machines (ELMs) are learning machines composed of at least one hidden layer, with nodes with configurable kernels and random weights, and an output layer composed of nodes with linear kernels whose weights are adjusted in a non-iterative way, by means of the Moore-Penrose's pseudoinverse. In this work are proposed as Morphological Machines of Extreme Learning (mELMs), that is, ELMs with neurons in the hidden layer based on the basic non-linear morphological operators of erosion and dilation. The proposed method was evaluated using 2,796 mammography images from the IRMA database (Image Recovery in Medical Applications). Through experiments carried out with verified that as mELMs of erosion and dilatation, we presented performance of equivalent or superior classification, when compared with the results obtained by classifiers classic of the state of the art. An IRMA database is divided into four tissue types (adipose, fibrous, heterogeneously dense and dense extremity). Experiments were performed with each type of tissue and with all database. The attributes were extracted using moments from Haralick and Wavelets.",,ARQUITETURA DE COMPUTADORES E SISTEMAS DIGITAIS,WASHINGTON WAGNER AZEVEDO DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,15/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Breast Cancer. Digital Mammography. Extreme Learning Machines (ELM). Mathematical Morphology. Morphological Extreme Learning Machines (mELM).',SISTEMAS DIGITAIS,ABEL GUILHERMINO DA SILVA FILHO,161,Câncer de mama. Mamografia digital. Extreme Learning Machines (ELM). Morfologia Matemática.Máquinas Morfológicas de Aprendizado Extremo (mELM).,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"De acordo com a Organização Mundial de Saúde, o câncer de mama é a forma mais comum de câncer entre as mulheres adultas em todo o mundo, sendo um dos tipos de câncer mais fatal. Estudos mostram que a disponibilização de diagnóstico precoce pode contribuir para a redução das taxas de mortalidade e aumentar as opções de tratamento. Diagnósticos assistidos por computador (Computer Aided Detection/Diagnosis) estão sendo propostos e usados visando auxiliar os profissionais da área de saúde. Morfologia Matemática é uma teoria de processamento não linear amplamente utilizada no processamento de imagens digitais. É baseada na teoria matemática da intersecção e união de conjuntos. Máquinas de Aprendizado Extremo (Extreme Learning Machines, ELMs) são máquinas de aprendizado compostas por pelo menos uma camada escondida, com nodos com kernels configuráveis e pesos aleatórios, e uma camada de saída, composta por nodos com kernels lineares cujos pesos são ajustados de forma não iterativa, por meio da pseudoinversa de Moore-Penrose. Neste trabalho são propostas as Máquinas Morfológicas de Aprendizado Extremo (Morphological Extreme Learning Machines, mELMs), ou seja, ELMs com neurônios na camada escondida baseados nos operadores morfológicos não lineares básicos de erosão e dilatação. O método proposto foi avaliado uilizando 2.796 imagens de mamografias da base de dados IRMA (Image Retrieval in Medical Applications). Através dos experimentos realizados foi verificado que as mELMs de erosão e dilatação, no que concerne à acurácia e ao índice kappa, apresentaram desempenho de classificação equivalente ou superior, quando comparados com os resultados obtidos pelos classificadores clássicos do estado da arte. A base de dados IRMA é dividida em quatro tipos de tecidos (adiposo, fibroso, heterogeneamente denso e extremamente denso). Foram realizados experimentos com cada tipo de tecido e com toda a base de dados. Os atributos foram extraídos usando momentos de Haralick e Wavelets.",TESE,MÁQUINAS MORFOLÓGICAS DE APRENDIZADO EXTREMO APLICADAS À DETECÇÃO E CLASSIFICAÇÃO DE LESÕES EM MAMOGRAFIAS,5178088,1
"In Brazilian semi-arid region, rainfall is characterized by long periods of drought, which
only in 2016 caused the expenditure of more than one billion reais (Brazilian currency,
about USD 330 millions) in water distribution programs for the poor population of this
region, of which about 14% were spent on surveillance. The current form of surveillance
does not contemplate any way of measuring the quantity or quality of water received, and
various fraud reports published by the local media outlets show that such surveillance
is an expensive and error-prone process. So the problem is how to improve the monitoring
and control of water distribution, increasing the transparency and efficiency of
the water distribution programs. Internet of Things (IoT) Systems are capable of transmitting
information about water volume and quality, this information can be used by
the decision-making systems for management purposes. However, the limitations of IoT
devices require a technology like Cloud Computing to complement their applications,
because Cloud Computing has virtually unlimited capabilities in terms of storage and
processing power. Motivated by the importance of the water distribution program for the
Brazilian semi-arid region and by the need to define a framework to allow the integration
of partial solutions for monitoring these types of programs, this master’s dissertation
presents an IoT and Cloud Computing-based architecture that provides Web services to
orchestrate the distribution of water in the Brazilian semi-arid region. In order to reach
the objectives of this work, a mapping of the main technologies used in Water Resources
Management Systems that apply IoT approach was carried out and also was realized analyses
of the use of Information Technology as an instrument to support the surveillance
of Brazilian water distribution program. From this knowledge was specified, designed
and implemented a reference architecture. Finally, from an evaluation, it was possible to
measure how able is the Proposed Architecture to be implemented in a real context.",,ENGENHARIA DA COMPUTAÇÃO,WILSON ALVES DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,29/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Internet of Things. Cloud Computing. Evidence-Based Software Engineering. Software Architecture. Water Distribution. Drought.',ENGENHARIA DA COMPUTAÇÃO,VINICIUS CARDOSO GARCIA,117,Internet das Coisas. Computação em Nuvem. Engenharia de Software Baseada em Evidência. Arquitetura de Software. Distribuição de Água. Seca.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Na região Semiárida brasileira o regime de chuvas é caracterizado por períodos longos
de estiagem com secas prolongadas, fazendo com que no ano de 2016 fossem gastos mais
de um bilhão de Reais em programas de distribuição de água à população carente dessa
região, dos quais cerca de 14% desse recurso foi gasto em fiscalização. A forma de fiscalização
atual não contempla nenhuma maneira de mensurar a quantidade ou a qualidade
da água recebida e várias notícias de fraudes publicadas pela mídia em geral indicam que
essa fiscalização é um processo caro e propício a erro. O problema então é como melhorar
o monitoramento e fiscalização na distribuição de água, ampliando a transparência e a
eficiência de programas federais de distribuição deste recurso. Sistemas de Internet das
Coisas (IoT), são capazes de transmitir informações sobre volume e qualidade da água,
estas informações podem ser usadas de forma integrada com os sistemas de tomada de
decisão para fins de gestão. No entanto, as limitações dos dispositivos IoT requerem uma
tecnologia como Computação em Nuvem para complementar suas aplicações, dado que a
Computação em Nuvem possui capacidades praticamente ilimitadas em termos de armazenamento
e processamento. Motivado pela importância do programa de distribuição de
água para o Semiárido brasileiro e pela necessidade de definição de um framework para
permitir a integração de soluções parciais para fiscalização destes tipos de programas,
este trabalho apresenta uma arquitetura baseada em IoT e Computação em Nuvem que
disponibiliza serviços para orquestrar a distribuição de água no Semiárido brasileiro. Para
atingir os objetivos deste trabalho foi realizado um mapeamento das principais tecnologias
empregadas em Sistemas de Gerenciamento de Recursos Hídricos que utilizam IoT e
foi realizada uma análise do emprego da Tecnologia da Informação como instrumento de
apoio à fiscalização do programa de distribuição de água. Apoiado nestes conhecimentos
foi especificada, projetada e implementada uma arquitetura de referência. Por fim, a partir
de uma avaliação pôde-se mensurar o quão a Arquitetura Proposta está apta para ser
implantada em um contexto real.",DISSERTAÇÃO,Uma Arquitetura para Orquestração da Distribuição de Água no Semiárido Brasileiro Baseada em Internet das Coisas e Computação em Nuvem,5178159,1
"Software testing is one of the activities performed during the software development cycle. It
is primarily responsible for ensuring the quality of the product under development. Testing
is present in the various types of software products developed, from applications developed
for desktop and web up to mobile platforms. The technology present in mobile devices
has increasingly facilitated the life of the end user. For instacne, the ways of interaction
have become more natural through new types of interfaces like gestures and voice. More
traditional interactions such as touch screen are no longer considered challenging, since they
are widely supported by the test automation frameworks available in the market. Differently,
interactions through the human voice, which do not have tools with such support, are
still a challenge. Given this scenario, this kind of test is executed manually, where the
tester needs to interact directly with the device through her voice, or to manipulate
real time tools to play audio files containing the commands that will exercise the voice
feature. This work introduces FREVoz – A Framework for Automation of Voice Testing.
FREVoz extends the framework FREVO by adding a new layer of communication that
allows the development of test cases with support for audio manipulation. Through this
extension, test developer can automatically program and execute voice test cases.FREVoz
is presented as a viable alternative to the development of test automation scripts based
on UI Automator, although it may be possible to apply its concepts to other types of
test automation technologies. A case study was carried out through the automation of
a suite of voice tests with the objective of comparing the total time spent in execution
through the two approaches, manual and automatic. An experiment was also carried out
with 3,840 test cases, making it possible to exercise the speech recognition application
available on mobile devices on the Android platform, Google Voice Search, for 16 valid
commands containing 3 variations of audios with different quality and intonation. Each of
these variations was performed 10 times in 8 supported languages in order to evaluate the
Google Voice Search application behavior in recognizing of these commands, as well as to
perform an evaluation of the quality of the audio resources used. Thus, we have simulated
some of the voice tests made within the context of the cooperation project between the
Center of Informatics of Federal University of Pernambuco and Motorola.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,ALEX ANTONIO CANDIDO SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,07/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Software Engineering. Software Testing. Test Automation. Voice Testing. UI Automator. FREVO.',ENGENHARIA DE SOFTWARE,JULIANO MANABU IYODA,112,"Engenharia de Software;Teste de Software, Automação de Testes, Teste de Voz, Reconhecimento de Voz, Android, UI Automator, FREVO.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Teste de software é uma das atividades desempenhadas durante o ciclo de desenvolvimento de um software. Ele é o principal responsável por garantir a qualidade do produto sob desenvolvimento. Essa atividade está presente nos diversos tipos de produtos de software desenvolvidos, desde aplicações desenvolvidas em plataformas para desktop, web e dispositivos móveis. A tecnologia presente nos dispositivos móveis tem facilitado cada vez mais a vida do usuário final enquanto as possibilidades de interação tem se tornado mais natural através de novos tipos de interfaces de interação por gestos e voz. As interações mais tradicionais como o toque na tela não são mais consideradas desafiadoras, uma vez que são amplamente suportadas pelos frameworks de automação de teste disponíveis no mercado, diferentemente das interações através da voz humana, que não possuem ferramentas com tal suporte.  Diante desse cenário, esse tipo de teste acaba sendo executado de forma manual, onde o testador precisa interagir diretamente com o dispositivo através da voz, ou então manipular em tempo real ferramentas que possibilitem a execução de arquivos de áudio com gravações dos comandos de voz. O trabalho proposto, FREVoz - Um Framework para Automação de Testes de Voz, estende o framework FREVO adicionando-o uma nova camada de comunicação que permite o desenvolvimento de casos de teste com suporte a manipulação de áudios. Através dessa extensão, o desenvolvedor de testes automáticos consegue desenvolver e executar casos de teste de voz de maneira automática. O principal trabalho relacionado a este é o Zygon, que é uma ferramenta que permite a gravação (capture) de passos baseados na interação com um dispositivo móvel e depois sua execução (replay), além de permitir que o usuário grave comandos de áudio em tempo real e crie testes de voz sem a necessidade de programá-los. Outros trabalhos relacionados estão centrados na área de reconhecimento de voz na área médica e educacional. FREVoz apresenta-se como uma alternativa viável para o desenvolvimento de scripts de automação de testes baseado em UI Automator. Através de um experimento onde foram realizadas 3.840 execuções de casos de teste, foi possível exercitar a aplicação de reconhecimento de voz disponível nos dispositivos móveis na plataforma Android, o Google Voice Search, para 16 comandos válidos contendo 3 variações de áudios com qualidade e entonação diferentes. Cada uma dessas variações foi executada 10 vezes em 8 idiomas suportados, com o propósito de avaliarmos o comportamento da aplicação Google Voice Search perante o reconhecimento desses comandos, bem como realizarmos uma avaliação da qualidade dos recursos de áudio utilizados. Neste experimento, simulamos assim alguns dos testes de voz realizados dentro do contexto do projeto de cooperação entre o Centro de Informática da Universidade Federal de Pernambuco e a Motorola.",DISSERTAÇÃO,FREVoz – Um Framework para Automação de Testes de Voz,5427942,1
"Context: This doctorate research is inserted in the context of the Software Engineering
(SE) teaching, and it has as motivation the need to develop technical skills in students
who intend to work in this area. Typically, software industry professionals graduate in
Computing undergraduate courses in order to prepare themselves to work in the Software
Engineering area. In spite of it, the software industry is not satisfied with the preparation
level of the new graduate professionals. This deficiency in the professional formation may
be the result of inadequate education. The fact that practical approaches are best suited
for teaching Software Engineering is a consensus among researchers in the area.
However, most courses do not offer to the students the opportunity to perform practical
activities. There is a predominance of traditional approaches to the teaching of SE, such
as lectures, which prove to be inefficient because they focus only on the teacher.
Objective: Given this background, the main objective of this research is to support the
adoption of practical approaches in the teaching-learning process of SE in order to the
students to develop certain technical skills at the application level. With this goal, we
have defined an iterative model that integrates the main approaches focused on the student
applied in the SE teaching. As a differential, we incorporated in this model the training
practices adopted by the software industry, and adapted them to the academic context.
Methods: A specialist’s panel composed of three Ph.D. professors and researchers in the
SE teaching area took care of the evaluation of the documentation and the usability of this
model. Subsequently, we carried out a controlled experiment in two undergraduate
courses in order to compare the use of this iterative model and the traditional teaching
approaches in relation to the development of Software Engineering competencies. The
data were collected from structured questionnaires and analyzed using ANOVA.
Results: The specialist’s considered the model's documentation complete and consistent.
As for the controlled experiment, the results obtained in the model instantiation in the
teaching of the Software Project Management knowledge unit were more effective than
the obtained in the traditional approaches application. However, these results were not
observed in the experiment related to the Requirements Engineering knowledge unit.
Conclusions: This research found evidence that the adoption of student-focused
approaches and industry training practices allow the development of certain SE technical
skills more effectively than the traditional teaching approach. However, it is observed that
students' motivation and commitment have a direct influence on these results.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,CARLOS DOS SANTOS PORTELA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,30/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Software Engineering Teaching;Competencies Development, Teaching Model, Student-Focused Approaches, Industry Training Practices.'",ENGENHARIA DE SOFTWARE,ALEXANDRE MARCOS LINS DE VASCONCELOS,290,"Ensino de Engenharia de Software;Desenvolvimento de Competências, Modelo de Ensino, Abordagens Focadas no Aluno, Práticas de Capacitação da Indústria.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Contexto: Esta pesquisa está inserida no contexto do ensino de Engenharia de Software (ES), tendo como motivação a necessidade de desenvolver competências técnicas nos alunos que pretendem atuar nessa área. Tipicamente, os profissionais da indústria de software cursam graduação na área de Computação a fim de se prepararem para atuar na área de ES. No entanto, a indústria de software se queixa de que os cursos de graduação não ensinam aos estudantes as competências necessárias para que eles possam começar a executar o seu trabalho com eficiência. Essa carência na formação de profissionais pode ser resultado de uma educação inadequada. Existe um consenso entre os pesquisadores da área de que abordagens práticas são as mais indicadas para o ensino de ES. No entanto, nem todos os cursos oferecem essa oportunidade aos alunos de realizarem atividades práticas. Muitos professores adotam abordagens tradicionais para ensinar ES, como aulas expositivas, que acabam sendo pouco eficientes, pois são centradas apenas no professor. Objetivo: Nesse contexto, o principal objetivo dessa pesquisa é apoiar a adoção de abordagens práticas no processo de ensino-aprendizagem de ES a fim de que os alunos desenvolvam determinadas competências técnicas a nível de aplicação. Para tal, definiu-se um modelo iterativo que integra as principais abordagens focadas no aluno que são aplicadas no ensino de ES. Como diferencial, incorporou-se nesse modelo práticas de capacitação adotadas pela indústria de software adaptadas para o contexto acadêmico. Métodos: A avaliação da documentação e usabilidade desse modelo foi feita através de um painel de especialistas composto por 3 professores doutores que atuam como pesquisadores na área de ensino de ES. Posteriormente, realizaram-se experimentos controlados em 2 turmas de graduação a fim de comparar o uso do modelo e abordagens tradicionais de ensino em relação ao desenvolvimento de competências em ES. Os dados foram coletados através de questionários estruturados e analisados usando ANOVA. Resultados: O painel de especialistas considerou a documentação do modelo completa e consistente. Quanto aos experimentos, os resultados obtidos pelo modelo foram mais efetivos que os obtidos pelas abordagens tradicionais com a média do valor de P = 0,005. Conclusões: Os resultados obtidos reforçam que abordagens práticas tendem a ser mais adequadas para o ensino de ES. Adicionalmente, os experimentos controlados mostraram evidências de que abordagens focadas no aluno e práticas de capacitação da indústria permitem desenvolver competências técnicas em ES de maneira mais efetiva do que a abordagem tradicional de ensino. No entanto, a motivação dos alunos possui influência direta sob esses resultados.",TESE,UM MODELO ITERATIVO PARA O ENSINO DE ENGENHARIA DE SOFTWARE BASEADO EM ABORDAGENS FOCADAS NO ALUNO E PRÁTICAS DE CAPACITAÇÃO DA INDÚSTRIA,5428010,1
"[Context] Dynamic Software Product Lines (DSPLs) are SPLs in which the configuration occurs at runtime. DSPL approaches provide means for modelling variability and a configura-tion process for binding variability according to runtime context and/or non-functional re-quirements (NFRs). However, taking various contexts and NFRs into account may result in multiple, and sometimes conflicting, possible configurations. Most of DSPL approaches do not provide means for prioritizing possible configurations in order to select one of them. [Ob-jective] In this work, we propose a Requirements Engineering (RE) approach for DSPL, ConG4DaS (Contextual Goal models For Dynamic Software product lines), which provides: (i) models for capturing variability with goals, NFRs, contexts and the relationship between them; and (ii) a configuration process that takes contexts, NFRs and their priority and interac-tions into account. [Method] We have used simulation based assessment to compare ConG4DaS with another approach with respect to the satisfaction level of the highest priority softgoal. We simulated several different contexts of two DSPL examples and compared the configurations generated by both approaches. We also performed a survey, using an online questionnaire, with RE and DSPL researchers to evaluate ConG4DaS perceived usefulness. [Results] In the simulation based assessment, in the configurations selected by ConG4DaS the number of positive contributions and the difference between the numbers of positive and negative contributions to the highest priority softgoal are greater than in the other approach, in most cases. In the survey, both groups of researchers (RE and DSPL) perceived ConG4DaS as useful for modelling and configuring DSPL variability. But the RE group gave more positive answers than the DSPL group.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,GABRIELA GUEDES DE SOUZA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/09/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Requirements Engineering, Dynamic Software Product Lines, Goal Models, Adaptive Systems'",ENGENHARIA DE SOFTWARE,CARLA TACIANA LIMA LOURENCO SILVA SCHUENEMANN,228,"Engenharia de Requisitos, Linhas de Produto de Software Dinâmicas, Modelos de Objetivos, Sistemas Adaptativos.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Desenvolvimento de Sistemas Adaptativos de Qualidade,"[Contexto] Linhas de Produto de Software Dinâmicas (LPSDs) são LPSs em que a configuração ocorre em tempo de execução. Abordagens para LPSD proveem meios para modelar variabilidade, além de um processo de configuração para amarrar a variabilidade de acordo com mudanças no contexto e/ou requisitos não-funcionais (RNFs) em tempo de execução. Contudo, levar em conta contextos e RNFs pode resultar em múltiplas configurações possíveis, às vezes conflitantes. A maioria das abordagens para DSPL não possuem uma maneira de priorizar essas possíveis configurações para poder selecionar uma delas, e aquelas que possuem não levam em conta a prioridade dos RNFs para o contexto atual. [Objetivo] Neste trabalho, propõe-se uma abordagem de Engenharia de Requisitos (ER) para LPSDs, chamada ConG4DaS (do inglês, Contextual Goal models For Dynamic Software product lines), que provê: (i) modelos para capturar variabilidade usando objetivos, RNFs, contextos e seus relacionamentos; e (ii) um processo de configuração que leva em conta contextos, RNFs e suas prioridades e interações. [Método] Foi feita uma avaliação baseada em simulações para comparar ConG4DaS com outra abordagem, com respeito ao nível de satisfação do softgoal prioritário. Foram simulados vários contextos diferentes de dois exemplos de LPSD e comparou-se as configurações geradas pelas duas abordagens. Também foi realizada uma pesquisa, usando questionário online, com pesquisadores de ER e LPSD para avaliar a utilidade percebida de ConG4DaS. [Resultados] Na avaliação baseada em simulações, o número de contribuições positivas para o softgoal prioritário e a diferença entre o número de contribuições positivas e negativas para o mesmo softgoal eram maiores nas configurações escolhidas por ConG4DaS do que nas configurações da outra abordagem, na maioria dos casos. No questionário, os dois grupos de pesquisadores (tanto de ER, como de LPSD) perceberam ConG4DaS como útil para modelagem e configuração da variabilidade de LPSD. Entretanto, no grupo de ER houve mais respostas positivas do que no grupo de LPSD.",TESE,Contextual Goal Models for Dynamic Software Product Lines,5473643,1
"The mulsemedia applications are those capable of engage three or more human senses, promoting the enrichment of multimedia traditional content with new media objects (olfactory, haptic, etc.) and, consequently, increasing immersion and improving the Quality of Experience (QoE). The mulsemedia content is represented by the MPEG-V standard. Observing previous research, we find in the literature efforts related to the authorship of sensorial effects through tools, as well as research aimed at reproduction and rendering of sensory effects. However, it is currently possible to identify a gap in the definition of processes, methods and tools to support the systematic development of mulsemedia applications in accordance with the MPEG-V standard. The main objective of this work is to propose a Model-driven approach (MDD) to integrate media, software and sensory effects projects. In this research, the thesis is argued that MDD can increase the productivity of the development of mulsemedia applications, in particular those with such strong integration requirement with complex programming logic. In particular, the structuring of the requirements of a applications family is performed; configuration of the common and variables parts of each category of applications through a Features Model; use of domain-specific languages for modeling views that integrate media design, software design, and sensory effects; and use of meta-programming techniques for automatic generation of application code. Finally, the experimental project is presented involving quantitative and qualitative empirical studies carried out with the purpose of determining the viability of the approach, as well as the benefits achieved through its use. The results show that the approach contributes to the development of software development for mulsemedia applications: on average the development time is 60% faster than the development without the use of MDD, and in general 87,5% of the participants classified that the approach as being extremely useful or useful in the development of mulsemedia applications.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,MARCELO FERNANDES DE SOUSA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,11/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Model-driven Development. Generative Development. Domain-specific Language. Mulsemedia. Sensory Effects.',ENGENHARIA DE SOFTWARE,CARLOS ANDRE GUIMARAES FERRAZ,158,"Desenvolvimento dirigido por modelos, desenvolvimento generativo, linguagem de domínio específico, desenvolvimento de aplicações mulsemedia.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"As aplicações mulsemedia são aquelas que envolvem três ou mais dos sentidos humanos, promovendo o enriquecimento do conteúdo multimídia tradicional com novos objetos de mídia (olfativos, hápticos, etc.) e, consequentemente, o aumento da imersão e a melhoria da Qualidade de Experiência (QoE) do usuário, sendo normatizadas pelo padrão MPEG-V. Observando pesquisas anteriores para melhorar o suporte ao desenvolvimento de aplicações mulsemedia, encontra-se na literatura esforços relacionados à autoria de efeitos sensoriais por meio de ferramentas, bem como pesquisas voltadas a reprodução e renderização de efeitos sensoriais. Contudo, atualmente é possível identificar lacunas relacionadas ao sincronismo entre os objetos de mídia (áudio, vídeo, imagem, texto e efeitos sensoriais) que compõem uma aplicação mulsemedia; integração entre projetos de mídias, software e efeitos sensoriais; facilidades na integração de efeitos sensoriais com lógica imperativa e abstração das complexidades relacionadas a plataformas específicas de domínio de aplicação. Além disso, também é possível identificar uma lacuna na definição de processos, métodos e ferramentas que auxiliem o desenvolvimento sistemático de aplicações mulsemedia em conformidade com o padrão MPEG-V. O objetivo principal deste trabalho é propor uma abordagem de desenvolvimento orientado a modelos que integre de modo sistemático as diferentes disciplinas envolvidas no desenvolvimento de aplicações mulsemedia contemplando soluções para as lacunas existentes.   Nesta pesquisa, defende-se a tese de que o desenvolvimento orientado a modelos pode reduzir a complexidade e diminuir o tempo de desenvolvimento de aplicações de mulsemedia, em especial, as que possuem como requisito forte integração com lógica de programação complexa. Para tanto, é realizada uma estruturação dos requisitos de famílias de aplicações; configuração das variabilidades de uma família por meio de um Modelo de Features; emprego de linguagens específicas de domínio para modelagem de visões que integram os projetos de mídia, software e os efeitos sensoriais; além da utilização de técnicas de metaprogramação para geração automática do código das aplicações em uma plataforma específica. Para demonstrar esta tese, é descrita uma abordagem de desenvolvimento orientado a modelos no domínio específico de mulsemedia e exemplos de uso. Por fim, são apresentados os resultados de uma avaliação envolvendo estudos empíricos quantitativos e qualitativos realizados com o intuito de determinar a viabilidade da abordagem, assim como os benefícios alcançados por meio da sua utilização.",TESE,UMA ABORDAGEM DE DESENVOLVIMENTO ORIENTADO A MODELOS PARA O DOMÍNIO DE APLICAÇÕES MULSEMEDIA,5494220,1
"While the literature on enterprise architecture(EA) models, frameworks, and methodologies for EA implementation has many exemplars, the field is still missing mechanisms of EA analysis. EA analysis is the process which uses any technique or method to extract information from EA models about a particular concern, in order to support EA management by the experts or inform stakeholders. In this thesis, we model the EA as a complex network, a concept discussed in network theory, to analyze EA structural aspects. During our exploratory study about EA network analysis (EANA), it was clear that the field was still lacking foundational aspects such as 1- no common language shared by researchers is available; 2- no clarity about what concerns can be analyzed with network analysis initiatives and 3-techniques and methods´ implementation are not clear in the papers. We solve those gaps in order to describe how to perform analysis of EA components and their relationships supported by network measures. The research approach comprehends qualitative methods such as systematic literature review, thematic analysis and design science research method. The research is conducted in three complementary and interrelated phases, aiming at first, to collect and synthesize the available knowledge about the analysis approaches existent in the literature. In a second moment, we aim to trace a comprehensive understanding of the main concepts involved in EANA such as their analysis concerns, modeling decisions, inputs required and steps necessary to perform it. Altogether, this resulted in a set of six proposed artifacts: EANA meta-model, EANA library, EANA process, EANA data derivation strategy. Thirdly, we investigate the use of those artifacts, evaluating them empirically through their instantiations and/ or with the help of EA experts of three German multinational companies. The evaluation results were positive regarding, among other criteria, the efficacy and utility of the proposed artifacts in their respective contexts. As contributions, we claim the definition of the conceptual foundations of the EANA research field. Complementary, the study is not limited to the theoretical findings since it advances the understanding of empirical network analysis, whereas it offers a library of analysis initiatives, methods to derive EA data and guidelines to help experts through the analysis process (EANA process). Finally, we also add to the EANA knowledge base two new EANA methods which were also empirically evaluated. We expect that results can enhance the awareness researchers and practitioners about the EA network-based analysis´ efficacy and utility, a step necessary to develop more rationally grounded methods and tools to support the EA management considering structural aspects.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,ALIXANDRE THIAGO FERREIRA SANTANA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,15/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Enterprise architecture, network, structural analysis, design science.'",ENGENHARIA DE SOFTWARE,HERMANO PERRELLI DE MOURA,371,"Arquitetura empresarial, análise de redes, estrutural, design science.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Enquanto a literatura sobre modelos, frameworks e metodologias de implementação de arquitetura empresarial - AE (do inglês, enterprise architecture, EA) é representativa, a pesquisa em AE ainda carece de mecanismos específicos para sua análise. Análise de AE é o processo que usa técnicas ou métodos para extrair informações arquiteturais sobre um aspecto de interesse sobre a AE, a partir de modelos, e com o objetivo de dar suporte aos especialistas no gerenciamento da AE ou ainda pra informar seus stakeholders. Nesta tese, a AE é modelada como uma rede complexa, um conceito originário da teoria de redes, com o objetivo de analisar aspectos estruturais de AE. Durante o estudo exploratório sobre a análise de redes aplicada ao contexto de AE, constatamos a ausência de fundamentos conceituais básicos como, por exemplo, 1- ausência de uma linguagem comum aos autores dos trabalhos, 2- a desconhecimento sobre a abrangência dos estudos de análise estrutural no contexto de AE e 3- carência de informação acerca do processo de analise estrutural realizado nos trabalhos. Nosso objetivo principal na tese é investigar como as métricas e métodos de análise de redes podem sem aplicados no contexto de análise de AE. Métodos qualitativos de pesquisa como revisão sistemática de literatura, análise temática e design Science research foram utilizados em três fases complementares e inter-relacionadas. Primeiramente, para coletar e consolidar o conhecimento sobre abordagens de análise de AE existentes na literatura. Numa segunda etapa, o objetivo foi traçar um entendimento abrangente sobre os principais conceitos envolvidos na análise estrutural de AE, mapeando seus aspectos de análise, modelos, métodos e técnicas utilizados, culminando com o design de quatro artefatos propostos: um meta-modelo para análise de redes no contexto de AE; uma biblioteca reunindo as iniciativas de análise extraídas dos artigos; um processo de análise de redes para AE e uma estratégia para derivação de dados. Na terceira e última etapa, investigou-se o uso dos artefatos avaliando-os empiricamente por meio de suas instanciações e da opinião de especialistas em AE de três organizações multinacionais alemãs. Os resultados foram positivos considerando, dentre outros critérios, a eficácia e utilidade dos artefatos propostos nos seus respectivos contextos. Como contribuições, esta pesquisa define os conceitos fundamentais para análise de redes em AE, além de avançar no entendimento acerca da análise empírica de redes naquele contexto, uma vez que apresenta um catálogo de métricas e métodos, métodos para derivação de dados, além de um processo para auxiliar os especialistas ao longo da execução da análise. Finalmente, a pesquisa também contribui para a base de conhecimento com dois métodos de análise validados também empiricamente. Com base nos resultados, espera-se corroborar o potencial da análise de AE baseada em redes, sua eficácia e utilidade para pesquisadores e práticos, além de estimular a adoção e desenvolvimento de ferramental para suportar o gerenciamento de AE, considerando seus aspectos estruturais.",TESE,ENTERPRISE ARCHITECTURE ANALYSIS BASED ON NETWORK PARADIGM: A Framework Proposal and Empirical Evaluation,5517175,1
"Context: The quality of a software product can be directly influenced by the quality of its development process. Therefore, immature or ad-hoc test processes are means that are unsuited for introducing systematic test automation, and should not be used to support improving the quality of software. Objective: In order to conduct this research, the benefits and limitations of and gaps in automating software testing had to be assessed in order to identify the best practices and to propose a strategy for systematically introducing test automation into software development processes. Method: To conduct this research, an exploratory bibliographical survey was undertaken so as to underpin the search by theory and the recent literature. Additionally, empirical interviews were conducted so as to gather practical information about how test automation strategies are introduced and practised in development organizations and to collect practical experiences from automated testing specialists working in the software industry. After defining the proposal, two case studies were conducted so as to analyze the proposal in a real world environment. Results: A Framework for Automating Software Testing – FAST, is a theoretical framework consisting of a hierarchical structure to introduce test automation into practices that can instantiated in line with the specific and distinct needs of any project. Based on the proposal, FAST was introduced into two distinct case studies, from which rich qualitative and quantitative data were collected. Conclusion: The findings of this research showed that the absence of systematic processes is one of the factors that hinders the introduction of test automation. Based on the results of the case studies, FAST can be considered as a satisfactory alternative that lies within the scope of introducing and maintaining test automation in software development.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,ANA PAULA CARVALHO CAVALCANTI FURTADO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,13/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Software testing. Test automation. Software process improvement',ENGENHARIA DE SOFTWARE,SILVIO ROMERO DE LEMOS MEIRA,178,"Teste de software, Automação de teste, Melhoria de processo de software.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Contexto: A qualidade de um sistema ou de um produto pode ser diretamente influenciada pela qualidade do processo utilizado para desenvolver e mantê-lo. Nesse cenário, os processos de teste imaturos ou ad-hoc não são considerados como ambiente propício para a introdução sistemática da automação de teste, que pode ser usada como uma forma de apoiar a melhoria da qualidade do software. Objetivo: Para a realização desta pesquisa, foi necessário, incialmente, analisar os benefícios e limitações da implantação de automação de teste de software. Além disso, analisar os fatores de insucesso da implantação de automação de teste de software nas organizações. A partir de então, propor uma estratégia para introdução sistemática de práticas de automação de teste no contexto de projeto de desenvolvimento de software. Método: Para a realização desta pesquisa, foi realizada uma revisão bibliográfica exploratória, para buscar a fundamentação teórica, embasamento da pesquisa e análise de trabalhos relacionados. Além disso, entrevistas empíricas foram conduzidas para coletar informações práticas sobre como as estratégias de automação de teste são introduzidas e praticadas nas organizações; e coletar experiências práticas de profissionais especialistas em automação de teste no ambiente de trabalho. Após definição da proposta, 2 estudos de caso foram executados com intuito de avaliar a proposta. Resultados: Proposta de uma estratégia para introdução da automação de teste consolidada através do Framework for Automating Software Testing – FAST. A proposta consiste em framework teórico que contempla uma estrutura hierárquica para a implantação de automação de teste a partir de práticas que podem ser instanciadas de acordo com as necessidades específicas e distintas de cada contexto de projeto. A partir da proposta, o FAST foi implantado e analisado em 2 contextos distintos de estudo de caso, onde dados quantitativos e qualitativos foram coletados. Conclusão: Baseado na pesquisa, pode-se observar que a ausência de processos sistemáticos é um dos fatores que dificulta a introdução da automação de teste. A proposta do FAST, analisada a partir do estudo de caso, pode ser considerada como uma alternativa satisfatória para a introdução e manutenção da automação de teste no escopo do projeto de desenvolvimento de software.",TESE,FAST: Um Framework para Automação de Teste,5536775,1
"The new Information and Communication Technologies (ICT) favor a wide range
of possibilities for the realization of professional, social and personal activities, yet still
generate a great difficulty of use to people with special needs that have severe motor,
perception and intellectuals, imposing barriers to interaction and increasing their digital
exclusion. To ease these difficulties in the use of ICT, new devices and interfaces are
available that replace conventional devices such mouse, keyboard, tactile surfaces and
others. Non-invasive brain computer interface (BCI), high definition cameras for reading
facial expressions, infrared eye tracking devices, body movement readers, and other
devices are available for use by everyone at a reasonable cost. Among the humancomputer
interaction solutions for ICT use, Literature highlights the use of Multimodality
for offering benefits through ease of use, where the weak point of one interface can be
overcome with the execution of a strong point of another, this enables the user to perform
the operation that best fits their particular situation, increasing the rate of completed
tasks, reducing the time and effort to complete the task, providing greater satisfaction for
the prevention and rapid recovery of errors in an intuitive way. For the adequate use of
this solution it is necessary to have an intermediate layer of software that performs the
treatment of the signals coming from the various devices and also a use model that
harmonizes and synchronizes this simultaneity in the application of these interfaces, in
particular its adaptation to people with special needs. This work presents a structured
method called M ^ 2ALL, an environment-promoting process for adaptive multimodal
interaction for people with special needs that favors the use of this medium for people
with disabilities. This method suggests the reduction of the constructive effort by its
organization, maintaining the possibility of elaboration of customized multimodal
environments for the use of people with special needs. The paper presents the practical
use of the model in a remote educational environment located on the web and performs
the evaluation of usability by disabled people. This multimodal environment was
configured according to the proposed method indicating its potential as an assistive
technology resource.",,BANCO DE DADOS,ANTULIO DE OLIVEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,04/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Human Interface Computer. Multimodality. Assistive Technology. Accessibility. Usability.',BANCO DE DADOS,FERNANDO DA FONSECA DE SOUZA,226,Interface Humano Computador;Multimodalidade;Tecnologia Assistiva;Acessibilidade;Usabilidade,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"As novas Tecnologias da Informação e Comunicação (TIC) favorecem um amplo campo de possiblidades para a realização de atividades profissionais, sociais e pessoais, no entanto ainda geram uma grande dificuldade de uso às pessoas com necessidades especiais que possuem limitações severas motoras, de percepção e intelectuais, impondo a elas barreiras de interação e aumentando a sua exclusão digital. Para amenizar essas dificuldades no uso das TIC, encontram-se disponíveis novos dispositivos e interfaces que substituem os dispositivos convencionais como mouse, teclado, superfícies táteis entre outros. Dispositivos como interface cérebro computador não invasivas (BCI), câmeras de alta definição para leitura de expressões faciais, dispositivos infravermelhos de rastreamento ocular, leitores de movimento corporal, entre outros dispositivos, encontram-se disponíveis para uso por todos à custos módicos. Dentre as soluções de interação humano-computador para emprego das TIC à Literatura destaca o emprego da Multimodalidade por oferecer benefícios pela facilidade de uso, onde o ponto fraco de uma interface pode ser superado com a execução de um ponto forte de outra, isso possibilita ao usuário realizar a operação que melhor se adapta a sua situação particular, aumentando a taxa de tarefas completadas, reduzindo o tempo e o esforço para completar a tarefa, proporcionando uma maior satisfação pela prevenção e recuperação rápida de erros de maneira intuitiva.
Para o uso adequado dessa solução é necessário a existência de uma camada intermediária de software que realize o tratamento dos sinais provenientes dos vários dispositivos e também de um modelo de uso que harmonize e sincronize essa simultaneidade na aplicação dessas interfaces, em particular a sua adaptação para as pessoas com necessidades especiais.
Este trabalho apresenta um método estruturado denominado de M^2ALL, um processo fomentador de ambiente para interação multimodal adaptativo a pessoas com necessidades especiais que favorece o uso deste meio para as pessoas com deficiência. Este método sugere a redução do esforço construtivo pela sua organização, mantendo a possibilidade de elaboração de ambientes multimodais personalizados para uso de pessoas com necessidades especiais. O trabalho apresenta o emprego prático do modelo em um ambiente educacional a distância situado na web e realiza a avaliação da usabilidade por pessoas deficientes deste ambiente multimodal configurado segundo o método proposto indicando a sua potencialidade como um recurso de tecnologia assistiva.",TESE,M^2ALL – Multimodal Methodo Acessibility Layer – Um Método para Uso de Camada de Software Multimodal Adaptativo a Portadores de Necessidades Especiais,5536853,1
"The practice of classifying objects according to the observed similarities and properties
is an important activity for many branches of science. Its importance is due to the fact that
the organization of data into groups is a fundamental mode to understand and learn about
ones. In Biology, for example, there is concern divide the different animals or plants into
groups for better understanding of biological functions. In many problems, besides informing
the group which a particular object belongs, it is necessary to understand how this object is
similar for all groups due to of the vagueness or uncertainty of the data, emerging, so the fuzzy
clustering. The primary method of fuzzy clustering is the Fuzzy C-Means (FCM), which has
some disadvantages as considering that all groups have spherical shapes. Another disadvantage
is that there is not the possibility to analyze which variable (or a subset of them) was more
important to set the final value of the degree of membership. This work presents different
clustering methods using fuzzy approach present in the current literature and introduces fuzzy
clustering methods where the degrees of membership are multivariate. Thus, given an object,
it is possible to calculate the degree it belongs to a group according to a given variable. From
this type of degree multivariate relevance, two advantages can be pointed out: 1 - ability to
interpret the relevance of each object for a given group according to each variable; 2 - getting
more information from the data leading to a better quality of clustering. The objective of this
work is to propose two types of methods: the first one is based on the Fuzzy C-Means and the
second one is based on the Possibilistic Fuzzy C-Means. Moreover, interpretation indices are
also proposed for assessing the quality of the clustering according to each cluster and variable
from a fuzzy partition obtained by each proposed method. Aiming to evaluate the performance
of the methods, a comparative study with respect to fuzzy clustering using the Monte Carlo
experiment is carried out. Experiments with synthetic and real data and a validation index is used
to evaluate the methods were planned. Furthermore, application with biological data is presented
showing the usefulness of the proposed methods. The results showed that multivariate methods
are preferable when the variables are independent and have different intra-class variabilities.",,INTELIGÊNCIA COMPUTACIONAL,BRUNO ALMEIDA PIMENTEL,UNIVERSIDADE FEDERAL DE PERNAMBUCO,08/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Fuzzy Clustering. Multivariate Degree. Fuzzy C-Means. Possibilistic Fuzzy C-Means. Application.',ANÁLISE DE DADOS SIMBÓLICOS E/OU NUMÉRICOS E MÉTODOS AFINS,RENATA MARIA MENDES CARDOSO,144,"Análise Estatística de Formas, Métodos de Agrupamento Particionais, Procedimento Bagging.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Algoritmos de agrupamento por particionamento encontram uma partição que maximiza ou minimiza algum critério numérico. A análise estatística de forma é usada para tomar decisões observando a forma de objetos. A forma de um objeto é a informação restante quando os efeitos de locação, escala e rotação são removidos com a utilização de operações matemáticas adequadas. Esta pesquisa apresenta algoritmos de agrupamento baseados em centro e busca adaptados para os dados de análise estatística de formas. Esses algoritmos são novas versões dos algoritmos de agrupamento Subida da Encosta, Busca Tabu, K-médias e KI-médias apropriados para o tratamento de dados de formas bidimensionais. Para avaliar a formação dos agrupamentos para os algoritmos propostos, novos critérios de agrupamento foram gerados para dados de formas a partir de estatísticas de testes de hipóteses e critérios usuais já existentes na literatura. A fim de melhorar a qualidade dos resultados de agrupamento, os algoritmos propostos foram também analisados utilizando o método Bagging que faz reamostragem com repetição para os dados de entrada e gera grupos através de uma votação majoritária. Estudos de simulação foram realizados para validar esses métodos propostos e três conjuntos de dados reais disponíveis na literatura também foram considerados. A qualidade dos experimentos foi avaliada pelo índice Rand corrigido e os resultados mostraram que os algoritmos propostos para formas planas são eficientes para os conjuntos de dados analisados.",TESE,MÉTODOS DE AGRUPAMENTO DIFUSO MULTIVARIADO BASEADOS NO FUZZY C-MEANS,5536915,1
"The universal use of mobile computing devices, especially smartphones, is undeniable
and an irreversible process. This fact encourages the organizations using information
systems to adapt them to provide an adequate access through this computational tool,
providing a good experience of use besides taking advantage of new possibilities inherent to
these devices. Legacy systems, however, can make this adaptation difficult, either because
of its technology, inappropriate code coupling or architecture, since the technology at
the time of its development lags behind with time, demanding the modernization of its
architecture. In this context, the microservices architecture has been emerging. This work
proposes a modernization process of legacy systems to a microservice-based architecture,
distributing the system into several small independent services, each focused on a single
task and communicating through messages. This distribution and independence will leave
each service independent to use any technology, breaking the technological constraints of
the legacy system in addition to facilitating future evolutions. However, this separation
of the system, can be time-consuming because of the need to understand the business
rules implemented and the necessary refactorings. In order to prioritize the availability
of mobile access to these systems, the proposed process provides an intermediate step of
modernization using the REST Wrapping technique.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,BRUNO CHAVES DE FREITAS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,23/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'mobile. mobile devices. legacy systems. microservices. REST Wrapping',ENGENHARIA DE SOFTWARE,ROBERTO SOUTO MAIOR DE BARROS,101,mobile. dispositivos móveis. sistemas legados. microservices. REST Wrapping.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O uso universal de dispositivos móveis computacionais, especialmente dos smartphones, é incontestável e um processo sem volta. Este fato impulsiona as organizações possuidoras de sistemas de informação a adaptá-los para um adequado acesso através deste veículo computacional, proporcionando uma boa experiência de uso além de aproveitar novas possibilidades inerentes a estes dispositivos. Os sistemas legados, no entanto, podem dificultar esta adaptação, seja por sua tecnologia, acoplamento de código ou arquitetura inapropriados, uma vez que a tecnologia à época de seu desenvolvimento fica defasada com o passar do tempo, demandando uma modernização de sua arquitetura. Neste contexto, a arquitetura de microservices tem se destacado. Este trabalho propõe um processo de modernização de sistemas legados para uma arquitetura baseada em microservices, distribuindo o sistema em diversos serviços pequenos, independentes entre si, focados
cada um em uma única tarefa e comunicando-se por mensagens. Esta distribuição e independência deixarão cada serviço livre para utilizar qualquer tecnologia, quebrando as amarras tecnológicas do sistema legado, além de facilitar futuras evoluções. Esta “quebra”
do sistema, no entanto, pode ser demorada, em virtude da necessidade de entendimento das regras de negócio implementadas e dos refatoramentos necessários. Em virtude disto, para priorizar a disponibilização do acesso mobile a estes sistemas, o processo proposto prevê uma etapa intermediária de modernização utilizando a técnica de REST Wrapping.",DISSERTAÇÃO,Modernização de Sistemas Legados para Disponibilização em Dispositivos Móveis com Arquitetura Baseada em Microservices,5616956,1
"The technological convergence between the Internet of Things (IoT), mobile devices
and wearable devices has, last few years, resulted in the considerable increase by the interest
in services mobile health care applications (mHealth). The mobile devices are still quite
resource-constrained and Mobile Cloud Computing (MCC) intends to mitigate mobile computing
challenges. MCC is a paradigm that offers cloud computing for extending the capacity of mobile
devices. The architecture of MCC includes a variety of components, such as: wearable devices,
mobile devices, cloud and network interfaces. The interaction between these components brings
challenges to guarantee desired levels of availability.
In this way, this research proposes perform the availability evaluation of an IoT environments,
using MCC-based technology, for the provision of the a mHealth service, with the goal of
assist in mHealth planning. For that aim, this work proposes hierarchical models to evaluate the
availability of this service. Initially, a Basic Architecture is specified with no redundancy. The
Basic Architecture was modeled with a hierarchical model composed by Reliability Diagram Models
(RBD) and Continuous Time Markov Chains (CTMC). Additionally, the Basic Architecture
was validated by a fault injection approach. The Basic Architecture is composed of a smartwatch,
a smartphone and a cloud infrastructure. Based on the Basic Architecture model, a sensitivity
analysis was performed to find availability bottlenecks. Based on the sensitivity analysis we
proposed two extended versions of the Basic Architecture: an architecture with multiple network
interfaces and a Cloudlet-based Architecture. These two extensions were evaluated in terms of
availability and annual downtime. Finally, we evaluated the reliability and availability of mobile
devices, as well as investigating the effects of using different connection interfaces on battery
autonomy and its relation to service availability.
These results have indicated that Cloudlet-based Architecture increased the service
availability compared to the Basic Architecture. By varying the interfaces the experiments
have indicated one configuration with superior availability. The configuration was composed
by a Bluetooth connection, active Broadband and a Mobile Network connectivity. The main
contribution of this work is the proposed model for evaluating availability of IoT environments,
used as input for the planning of infrastructure mHealth systems.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,CAMILA GONZAGA DE ARAUJO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,06/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Hierarchical Models. Availability Analysis. Mobile Cloud Computing. Wearable Device. Internet of Things.',REDES DE COMPUTADORES,PAULO ROMERO MARTINS MACIEL,113,Modelos Hierárquicos. Análise de Disponibilidade. Computação Móvel em Nuvem. Dispositivos Vestíveis,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O rápido desenvolvimento dos dispositivos móveis e vestíveis (wearable) resultou, nos últimos anos, no considerável aumento pelo interesse em serviços e aplicações de saúde móvel (Mobile Health - mHealth). Estes dispositivos ainda apresentam recursos limitados e uma das soluções adotadas para superar estas limitações tecnológicas é recorrer ao poder da Mobile Cloud Computing (MCC). A MCC é um paradigma que tem como característica prover recursos computacionais da nuvem para estender a capacidade dos dispositivos móveis. Neste contexto, é preciso destacar que a variedade de componentes (dispositivos wearable, dispositivos móveis, nuvem e interfaces de rede) e a interação entre estes trazem desaﬁos para assegurar níveis desejados de disponibilidade.
Dessa forma, esse trabalho tem como proposta realizar a avaliação de disponibilidade de um ambiente MCC para o provimento do serviço mHealth, com a ﬁnalidade de auxiliar o planejamento de infraestruturas mHealth dentro do contexto MCC. Para isto, o trabalho propõe modelos hierárquicos para avaliar a disponibilidade desse serviço. Inicialmente, deﬁniu-se a Arquitetura Base, sem mecanismos de redundância, a qual foi modelada a partir de um modelo hierárquico composto de Reliability Block Diagram (RBD) e de Continuous Time Markov Chain (CTMC), e validado através de um testbed de injeção de falhas e reparos. A Arquitetura Base composta por um smartwatch, um smartphone e infraestrutura de nuvem. Com base no modelo da Arquitetura Base, aplicamos a técnica de análise de sensibilidade para identiﬁcar gargalos de disponibilidade. A partir dos resultados obtidos, por intermédio da análise de sensibilidade, apresentamos duas adaptações da Arquitetura Base: uma arquitetura com múltiplas interfaces de rede e uma arquitetura baseada em Cloudlet. As duas adaptações são avaliadas em termos de disponibilidade e downtime anual. Por ﬁm, avaliamos a conﬁabilidade e disponibilidade dos dispositivos móveis, bem como investigamos os efeitos do uso de diferentes interfaces de conexão sobre o consumo energético e sua relação com a disponibilidade do serviço.
Os referidos resultados expressam que a Arquitetura Cloudlet proporcionou aumento da disponibilidade do serviço, em comparação com a Arquitetura Base. Também podemos inferir que, dentre as variações de diferentes interfaces de conexões, quando o usuário possui conexão Bluetooth e conectividade com Banda Larga e Rede Móvel ativas, a disponibilidade do serviço apresenta melhores resultados. A principal contribuição desta pesquisa é a proposição de modelos para a avaliação de disponibilidade de um ambiente MCC, os quais poderão ser utilizados como subsídio para o planejamento de infraestruturas de serviços mHealth.",DISSERTAÇÃO,Modelos para Avaliação de Disponibilidade em Ambiente Mobile Cloud Computing: um Estudo Aplicado em Serviços Mhealth Utilizando Dispositivo Wearable,5617120,1
"Many real-world problems can be formulated as optimization problems in continuous
domains. In the last years, bio-inspired algorithms, whice are based on the behavior of natural
phenomena, have been increasingly employed to solve such problems. In this work, 8 (eight)
algorithms inspired by nature are investigated: genetic algorithms (GA), ant colony optimization
(ACO), particle swarm optimization (PSO), artificial bee colony (ABC), firefly algorithm (FA),
cuckoo search algorithm (CS), bat algorithm (BAT) and self-adaptive cuckoo search algorithm
(SACS). These algorithms are analyzed in three different types of problems, which comprise
(1) benchmark functions commonly studied in optimization problems, (2) prediction of wind
energy from wind speed with real data collected from two wind farms, and clustering patterns,
required in solving unsupervised problems. The experiments performed with the different
algorithms investigated the main advantages and disadvantages of the algorithms concerning (1)
the quality of the solutions obtained according to specific metrics for each problem, (2) algorithm
execution time and (3) convergence time for the best solution. A bio-inspired technique of
automatic parameter tuning was developed and employed in all problems and algorithms in order
to determine optimal values for each method and to allow a consistent comparison of the results.
The performed experiments showed that the cuckoo search algorithm works efficiently, robustly
and superior to the other investigated methods for most of the experiments, and the long tail
property of the Lévy Flight distribution, explored in this work, is the main responsible for the
efficiency of this algorithm.",,COMPUTAÇÃO INTELIGENTE,CARLOS EDUARDO MARTINS BARBOSA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,09/05/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Bio-inspired algorithms. Swarm Intelligence. Optimization Problems. Cuckoo Search. Lvy Flight.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,GERMANO CRISPIM VASCONCELOS,194,Algoritmos Bio-inspirados. Inteligência de Enxames. Problemas de Otimização. Busca do Pássaro Cuco. Voo de Lévy.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Muitos problemas do mundo real podem ser formulados como problemas de otimização em domínios contínuos. Nos últimos anos, algoritmos bio-inspirados, que fundamentam-se no comportamento dos fenômenos naturais, têm sido cada vez mais empregados para resolver tais problemas. Neste trabalho, são investigados 8 (oito) algoritmos inspirados na natureza: algoritmos genéticos (GA), otimização por colônia de formigas (ACO), otimização por enxame de partículas (PSO), colônia de abelhas artificiais (ABC), algoritmo do vaga-lume (FA), algoritmo de busca do pássaro cuco (CS), algoritmo do morcego (BAT) e algoritmo de busca autoadaptativa do pássaro cuco (SACS). Estes algoritmos são analisados em três tipos de problemas distintos, que compreendem (1) funções de benchmark estudadas comumente em problemas de otimização, (2) previsão da energia eólica a partir da velocidade do vento com dados reais coletados de dois parques eólicos, e (3) clusterização de padrões, necessária na solução de problemas não-supervisionados. Os experimentos realizados com os diferentes algoritmos analisaram as principais vantagens e deficiências dos algoritmos em relação à (1) qualidade das soluções obtidas segundo métricas de desempenho específicas para cada problema, (2) tempo de execução do algoritmo e (3) tempo de convergência para a melhor solução. Uma técnica de ajuste automático dos parâmetros, também bio-inspirada, foi desenvolvida e empregada em todos os problemas e algoritmos, para se determinar os valores ótimos para cada método e permitir uma comparação consistente dos resultados. Os experimentos realizados evidenciaram que o algoritmo do pássaro cuco funciona de forma eficiente, robusta e superior aos outros métodos investigados para a maioria dos experimentos realizados, e que a propriedade de cauda longa da distribuição com voos de Lévy, explorada neste trabalho, é a principal responsável pela eficiência deste algoritmo.",DISSERTAÇÃO,Algoritmos Bio-inspirados para Solução de Problemas de Otimização,5617251,1
"Chatterbots are applications that aim to simulate a real conversation with a to be human,
so that they also behave like humans. The main idea in issue is to make the two sides of the
dialogue talk about a given domain of knowledge, so that the talk revolves intelligently around
this domain. From a dialogue, a variety of information is issued that can, and should be, relevant
to the domain in question. Thus, this information knowledge and learning through the parties
involved in the dialogue. This is quite common in chat conversations, making them widely
used as a source of knowledge. According to Thomas Gruber (1993), ontology is an explicit
specification of a conceptualization. It is a very useful resource for representing knowledge.
The ontology is at such a high level of abstraction that it establishes a terminology common
and unambiguous for the domain in question. This makes it a strongly suitable for its use as a
data model, since the representation of the concepts of the proposed domain ""teaches""chatterbot,
making it possible to make inferences on the objects, and makes it apt to talk to the user as
naturally as possible. There is a great challenge regarding the extraction of knowledge from
natural language, since there is a variability in the way people write and speak. This makes it
difficult to obtain knowledge through chatterbots from natural language. The general objective of
this project is to demonstrate that the construction and knowledge arising from dialogues between
people and chatterbots is a viable solution for the process of acquisition of an ontology-based
domain model, modeled on description.",,COMPUTAÇÃO INTELIGENTE,CARLOS EDUARDO TEIXEIRA LIMA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,07/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Artificial Intelligence. Chatterbots. Knowledge Representation. Ontologies',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,FREDERICO LUIZ GONCALVES DE FREITAS,102,"Chatterbot, AIML, Lógica de Descrições, Representação de Conhecimento, Aprendizado de Ontologias, Processamento de Linguagem Natural, Raciocínio Automático.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Chatterbots são robôs destinados a conversação e interação direta com seres humanos, cujo principal objetivo é dialogar com os mesmos em linguagem natural [1]. Os chatterbots podem apresentar características típicas de seres humanos, e são divididos em três gerações quanto as técnicas envolvidas na construção dos mesmos: a primeira geração caracteriza-se pelo uso de técnicas de casamento de padrão e regras gramaticais; a segunda geração utiliza técnicas de Inteligência Artificial – IA e a terceira e mais utilizada atualmente, inclui técnicas de casamento de padrões mais complexos, baseadas em linguagens de marcação, como: SGML (Standard Guaranteed Markup Language) e XML (eXtensible Markup Language). A maioria dos chatterbots desta última geração possuem bases de conhecimento implementadas em AIML (Artificial Intelligence Markup Language). São inúmeras as aplicações e impactos nas mais diversas áreas do conhecimento com a utilização de chatterbots, desde aplicações com Inteligência Artificial, Educação, Aprendizado Automático, EBL, ILP até Extração da Informação. Com o avanço das TDICs-Tecnologias Digitais da Informação e Comunicação e a popularização dos computadores pessoais bem como da Internet, os chatterbots são inseridos em um cenário promissor e são utilizados em interações com seres humanos, possuindo assim, um papel educativo já que muitos possuem diversos conhecimentos sobre conteúdos acadêmicos distintos. O foco da pesquisa foi desenvolver uma arquitetura de um chatterbot baseado em ontologias, com conceitos genéricos e abstratos de diversos domínios de conhecimento, no qual possui como principal objetivo criar automaticamente conhecimento utilizando Ontologias e suas Lógicas de Descrição – DL associadas, além de retirar ambigüidades das ontologias geradas automaticamente a partir dos diálogos em linguagem natural com seres humanos, apoiando-os na fixação, compreensão e re(construção) de conhecimento e habilidades. Serão utilizadas técnicas de aprendizado de ontologias, Processamento de Linguagem Natural - PLN, ILP e técnicas não empregadas em chatterbots correntes.",DISSERTAÇÃO,Um Chatterbot para Criação e Desenvolvimento de Ontologias com Lógica de Descrição,5617430,1
"Recently, deep learning has caused a significant impact on computer vision, speech
recognition, and natural language understanding. In spite of the remarkable advances, deep
learning recent performance gains have been modest and usually rely on increasing the depth
of the models, which often requires more computational resources such as processing time and
memory usage. To tackle this problem, we turned our attention to the interworking between
the activation functions and the batch normalization, which is virtually mandatory currently.
In this work, we propose the activation function Displaced Rectifier Linear Unit (DReLU)
by conjecturing that extending the identity function of ReLU to the third quadrant enhances
compatibility with batch normalization. Moreover, we used statistical tests to compare the
impact of using distinct activation functions (ReLU, LReLU, PReLU, ELU, and DReLU) on the
learning speed and test accuracy performance of VGG and Residual Networks state-of-the-art
models. These convolutional neural networks were trained on CIFAR-10 and CIFAR-100, the
most commonly used deep learning computer vision datasets. The results showed DReLU
speeded up learning in all models and datasets. Besides, statistical significant performance
assessments (p<0:05) showed DReLU enhanced the test accuracy obtained by ReLU in all
scenarios. Furthermore, DReLU showed better test accuracy than any other tested activation
function in all experiments with one exception, in which case it presented the second best
performance. Therefore, this work shows that it is possible to increase the performance replacing
ReLU by an enhanced activation function.",,COMPUTAÇÃO INTELIGENTE,DAVID LOPES DE MACEDO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Activation Functions. Batch Normalization. ReLU. DReLU. Deep Learning. Convolutional Neural Networks.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,TERESA BERNARDA LUDERMIR,91,"Aprendizado de Máquina, Redes Neurais Deep Learning",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Recentemente, a aprendizagem profunda tem causado um impacto significante em visão computacional, reconhecimento de voz e compreensão de linguagem natural. Ademais, novas aplicações em medicina, negócios, financias e agricultura foram criadas ou melhoradas. Apesar de avanços significativos, recentemente os ganhos em desempenho tem sido modestos e usualmente dependem do incremento da profundidade dos modelos, o que normalmente
requer mais recursos computacionais como tempo de processamento e uso de memória. Para abordar este problema, nós voltamos nossa atenção para o interfuncionamento entre as funções de ativações e a normalização em batch, o qual é praticamente obrigatório atualmente. Neste trabalho, nós propomos a função de ativação Displaced Rectifier Linear Unit (DReLU) a partir da conjectura que estender a função identidade da ReLU para o terceiro quadrante aprimora a compatibilidade com a normalização em batch. Ademais, nós usamos testes estatísticos para comparar o impacto de usar funções de ativação distintas (ReLU, LReLU, PReLU, ELU, and DReLU) na performance da velocidade de treinamento e na acurácia dos testes de modelos estado da arte VGG e Redes Residuais. Estas redes neurais convolucionais foram treinadas no CIFAR-10 e CIFAR-100, as mais comumente utilizadas base de dados utilizadas em visão computacional para aprendizagem profunda. Os resultados mostraram que DReLU aumentou a velocidade de aprendizagem em todos os modelos e bases de dados. Ademais, DReLU melhorou a acurácia dos testes apresentados pela ReLU em todos os cenários. Além disso, DReLU apresentou melhor acurácia de testes que qualquer outra função de ativação testada em todos os cenários com uma exceção, no qual esta apresentou a segunda melhor performance.",DISSERTAÇÃO,ENHANCING DEEP LEARNING PERFORMANCE USING DISPLACED RECTIFIER LINEAR UNIT,5617560,1
"This research presents a qualitative analysis on the phenomenon of engagement in the
use of a computer-supported educational social network for collaborative teaching-learning on
Glucose-6-Phosphate Dehydrogenase (G6PD) deficiency. Openredu.org, an open source
social education network, was used as mediator of virtual interactions among students,
teachers, health professionals, parents and family members. The main objective is to make a
qualitative analysis of the effect of engagement in an educational social network with adult
volunteers in collaborative teaching processes with emphasis on the exchange of experiences
and content elaboration on Glucose-6-Phosphate Dehydrogenase (G6PD). An empirical
qualitative research with exploratory research techniques was conducted through semistructured
interviews and questionnaires with the focus of understanding the context and soon
after an application of an action research whose objective was to perform an improvement in
the practice. According to the evaluation of the engagement in the educational social network
and user satisfaction, the results obtained initially demonstrate a symbolic engagement and a
satisfaction of the users who participated in the demonstration environment. The engagement
and contributions of network participants in the learning environment even in the testing
phase, will provide future improvements of the same.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,CLOVES ALVES DA ROCHA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,07/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Network Engagement. Community. Collaborative learning. Educational social network. Learning project. Deficiency in Glucose-6-Phosphate Dehydrogenase.',ENGENHARIA DE SOFTWARE,SILVIO ROMERO DE LEMOS MEIRA,112,"Engajamento em uma comunidade CSCL, Aprendizagem colaborativa suportada por computador, Rede social educacional, Projeto de aprendizagem, Doenças e deficiências genéticas.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Contexto: Esta pesquisa apresenta uma análise qualitativa sobre o fenômeno do engajamento no uso de uma rede social educacional apoiada por computador para o ensino-aprendizagem colaborativa sobre doenças genéticas. Como mediador das interações virtuais entre os alunos, professores, profissionais de saúde, pais e os familiares, utilizou-se o programa openredu.org, uma rede social educacional open source. Objetivo: O principal objetivo é fazer uma análise qualitativa do efeito do engajamento em uma rede social educacional com voluntários adultos em processos de ensino colaborativo com ênfase na troca de experiências sobre doenças genéticas. Método: Foi realizado uma pesquisa qualitativa empírica com técnicas de pesquisa exploratória por meio de entrevistas semiestruturadas e questionários com o foco de compreender o contexto e logo após a aplicação de uma pesquisa-ação cujo o objetivo foi realizar uma melhoria na prática em rede social educacional. Resultados: Conforme avaliação do engajamento na rede social educativa e da satisfação dos usuários, os resultados obtidos demonstram, inicialmente, um simbólico engajamento e uma satisfação dos usuários que participaram do ambiente beta ou de testes. Conclusões: O engajamento e as contribuições dos participantes da rede no ambiente de aprendizagem mesmo que na fase de testes, irá proporcionar futuras melhorias da mesma.",DISSERTAÇÃO,ANÁLISE QUALITATIVA SOBRE ENGAJAMENTO NO USO DE UMA REDE SOCIAL EDUCACIONAL: APRENDIZAGEM COLABORATIVA SOBRE DOENÇAS GENÉTICAS,5618106,1
"Aspect-based opinion mining can be applied to extract relevant information expressed by patients in drug reviews (e.g., adverse reactions, efficacy of a drug, symptoms and conditions of patients). This new domain of application presents challenges as well as opportunities for research in opinion mining. Nevertheless, the literature is still scarce of methods to extract multiple relevant aspects present in drug reviews. In this thesis we propose a new method to extract and classify aspects in drug reviews. The proposed solution has two main steps. In the aspect extraction, a new method based on syntactic dependency paths is proposed to extract opinion pairs in drug reviews, composed by an aspect term associated to opinion term. In the aspect classification, a supervised classifier is proposed based on domain and linguistics resources to classify the opinion pairs by aspect type (e.g., condition, adverse reaction, dosage and effectiveness). In order to evaluate the proposed method we conducted experiments with datasets related to three different diseases: ADHD, AIDS and Anxiety. For the extraction problem, a comparative evaluation was performed with two other methods, the proposed method obtained competitive results, obtained an accuracy of 78% for ADHD, 75.2% for AIDS and 78.7% for Anxiety. For the classification problem, promising results were obtained in the experiments and various issues were identified and discussed.",,COMPUTAÇÃO INTELIGENTE,DIANA CABRAL CAVALCANTI,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Opinion mining. Aspect extraction. Aspect classification. Natural language processing. Machine learning. Drugs reviews.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,RICARDO BASTOS CAVALCANTE PRUDENCIO,169,"Mineração de Opinião;Extração de Aspectos;Classificação de Aspectos;Processamento de Linguagem Natural;Aprendizagem de máquina, comentários sobre medicamento.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Mineração de Opinião baseada em Aspectos pode ser aplicada para extrair informações relevantes expressas por pacientes em comentários textuais sobre medicamentos (por exemplo, Reações Adversas, Eficácia quanto ao uso de um determinado remédio, sintomas e condições do paciente antes usar o medicamento). Este novo domínio de aplicação apresenta desafios, bem como oportunidades de pesquisa em Mineração de Opinião. No entanto, a literatura ainda é escassa sobre métodos para extrair múltiplos aspectos relevantes presentes em análises de fármacos. Neste estudo foi proposto um novo método para extrair e classificar aspectos em comentários opinativos sobre medicamentos. A solução proposta tem duas etapas principais. Na extração de aspectos, um novo método baseado em caminhos de dependência sintática é proposto para extrair pares de opiniões em revisões de medicamento, um par de opinião é composto por um termo de aspecto associado a um termo opinativo. Na classificação de aspectos, propõe-se um classificador supervisionado baseado em recursos de domínio e de linguística para classificar pares de opinião por tipo de aspecto (por exemplo, Condição clínica, Reação Adversa, Dosagem e Eficácia). Para avaliar o método proposto, foi realizado experimentos em conjuntos de dados relacionados a três diferentes condições clínicas: ADHD, AIDS e Ansiedade. Resultados promissores foram obtidos nos experimentos e várias questões foram identificadas e discutidas. Para o problema de extração o método proposto atingiu precisão de 78% para ADHD, 75,2% para AIDS e 78,7% para Ansiedade. Enquanto para o problema de classificação o modelo proposto atingiu uma taxa de acerto de 76,86% para ADHD, 78,16% para AIDS e 76,78% para Ansiedade.",TESE,MINERAÇÃO DE OPINIÕES BASEADA EM ASPECTOS PARA REVISÕES DE MEDICAMENTOS,5620402,1
"Resource scarcity is a major obstacle for many mobile applications, since devices have limited battery and processing power. As an example, there are applications that seamlessly augment human cognition and typically require resources that far outstrip mobile hardware’s capabilities, such as language translation, speech recognition, and face recognition. The use of cloud computing has been shown to be a feasible alternative to process demanding mobile devices workloads, leading to the research field called mobile cloud computing (MCC). By using the cloud, mobile devices may offload computation to resourceful servers. Many issues related to such a process have been investigated in
the past decade, but those related to offloading process still remain. This PhD research has developed a smart MCC offloading strategy for mobile applications. The approach have considered an innovative balanced infrastructure parameters strategy. Another MCC challenge is related to the process of infrastructure evaluation and planning. Evaluating the MCC infrastructure in a deep level of detail may provide to software engineers precise information, guiding their decisions. Instead of evaluating the MCC infrastructure as a black-box, this work proposes to analyze the application at source-code level. This PhD research proposes providing a way for representing method-calls and evaluating mobile cloud applications by using stochastic petri nets (SPNs). The SPNs in this work
allow software engineers to understand their applications through a statistic report. Case studies have showed that the proposed techniques are helpful for guiding cloud systems designers and administrators in the decision-making process.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,FRANCISCO AIRTON PEREIRA DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Mobile Cloud Computing, Stochastic Petri Nets, Offloading, Scheduling, Performance Evaluation, Energy'",REDES DE COMPUTADORES,PAULO ROMERO MARTINS MACIEL,117,"Computação em nuvem móvel, redes de Petri estocásticas, Deslocamento, agendamento, avaliação de desempenho, Energia",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A escassez de recursos é um grande obstáculo para muitas aplicações móveis, uma vez
que os dispositivos têm bateria e poder de processamento limitados. Como exemplo,
há aplicativos que aumentam a cognição humana, aplicativos para tradução de idiomas
e reconhecimento de fala. Estes aplicativos normalmente requerem recursos que ultrapassam as capacidades de hardware. O uso da computação em nuvem tem se mostrado
uma alternativa viável para processar cargas de trabalho de dispositivos móveis limitados. Com o objetivo de mitigar este problema nasceu o campo de pesquisa chamado
computação em nuvem móvel (MCC). Ao usar a nuvem, os dispositivos móveis podem
transferir seu processamento para servidores potentes. Muitas questões relacionadas a
esse processo têm sido investigadas na última década, mas as relacionadas com o processo
de execução remota ainda permanecem. Esta pesquisa de doutorado desenvolveu uma
estratégia de execução remota de aplicativos móveis na nuvem. O algoritmo desenvolvido
considerou uma estratégia inovadora de balanceamento de parâmetros coletados do estado
da infraestrutura. Outro desafio do MCC está relacionado ao processo de avaliação e
planejamento da infraestrutura tecnológica adotada. Uma avaliação detalhada do desempenho de diferentes configurações de infraestrutura pode fornecer aos engenheiros de
software informações precisas, guiando suas decisões. Ao invés de avaliar a infraestrutura
como uma caixa-preta, este trabalho propõe analisar a aplicação em nível de código-fonte,
mais precisamente chamadas de método. O trabalho utiliza redes de Petri estocásticas
(SPNs) para representar e avaliar desempenho e gasto de bateria de dispositivos móveis.
As SPNs neste trabalho permitem aos engenheiros de software entender suas aplicações
através de um relatório estatístico. Estudos de caso mostraram que as técnicas propostas
nesta pesquisa são úteis para orientar designers e administradores de sistemas de nuvem
no processo de tomada de decisão.",TESE,Improving Mobile Cloud Performance using Offloading Techniques and Stochastic Models,5620521,1
"One of the major challenges faced by managers in large urban centers is mobility. In 2012, the National Policy on Urban Mobility was published by the Ministry of Cities, which established an Urban Mobility Plan for municipalities with more than 20 thousand inhabitants. Based on this policy, the plan was complemented by the Recife City Hall, which among its aspects we can mention: ""development of the necessary tool for analysis of the current transportation system and prognosis of displacements and future projects."" Considering these facts, this dissertation proposes the implementation of an Intelligent Transportation System (ITS) in the main road belonging to the north-south corridor of the Pernambuco state capital. This system consists of a hybrid application with functions for traffic and safety monitoring using heterogeneous inter-vehicular communication (IVC) technology composed of the IEEE 802.11p standard and LTE cellular networks. In order to analyze the impact caused by the implementation of the ITS, simulations were modeled in the Veins (Vehicles in Network Simulation) framework, which is composed of INET Framework integration, for network functions, in the OMNET ++ simulation environment and by the SUMO mobility simulator. The scenario for simulation was modeled from real data provided by the Transit and Urban Transport Company of Recife. It is analyzed the mean time of travel of the vehicles, from the point of view of mobility, and the parameters of latencies (delay) and packet delivery rate (PDR).",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,DIOCLECIANO DANTAS NETO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,15/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Vehicular Networks, Interveular Communication, IEEE 802.11p, WAVE, Veins, LTE, Intelligent Transport System.'",ENGENHARIA DE SOFTWARE,DANIEL CARVALHO DA CUNHA,66,"Redes Veiculares, Comunicação Interveicular, IEEE 802.11p, WAVE, Veins, LTE, Sistema Inteligente de Transporte.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Um dos grandes desafios enfrentados pelos gestores dos grandes centros urbanos é a mobilidade. Em 2012, foi publicada a Política Nacional de Mobilidade Urbana pelo Ministério das Cidades, que estabeleceu a obrigatoriedade de um Plano de Mobilidade Urbana para municípios com mais de 20 mil habitantes. Com base nessa política, foi realizada a complementação do referido plano pela Prefeitura do Recife, que entre os seus aspectos pode-se citar: “desenvolvimento do ferramental necessário para análise do sistema de transporte atual e prognóstico dos deslocamentos e projetos futuros”. Tendo em vista tais fatos, esta dissertação propõe a implantação de um Sistema Inteligente de Transporte (ITS – Intelligent Transportation System) na principal via pertencente ao corredor norte-sul da capital pernambucana. Tal sistema é composto por uma aplicação híbrida com funções para monitoramento de tráfego e segurança (safety) utilizando-se tecnologia de comunicação inter-veicular (IVC – Inter-Vehicular Comunication) heterogênea composta pelo padrão IEEE 802.11p e redes de celular LTE. Para análise do impacto causado pela implantação do ITS foram realizadas simulações modeladas no framework Veins (Vehicles in Network Simulation) que é composto pela integração do  INET Framework, para funções de redes, no ambiente de simulação OMNET++ e pelo simulador de mobilidade SUMO. O cenário para simulação foi modelado a partir de dados reais disponibilizados pela Companhia de Trânsito e Transporte Urbano do Recife. É analisado o tempo médio de trajeto dos veículos, na ótica da mobilidade, e os parâmetros de redes latência (delay) e taxa de perda de pacotes (PDR – packet delivery rate).",DISSERTAÇÃO,Uma Análise da Implantação de Sistemas Inteligentes de Transporte na Região Metropolitana do Recife: Enfoque em Arquitetura de Redes Veiculares,5620671,1
"The Information Centric Network (ICN) paradigm proposes that network operations focus on the content retrieval, regardless of its physical location. Such architecture differs from the original Internet project, i.e., host-centric and IP-address-dependent. Storing content on routers rather than on the content source, as well as the intrinsic support for the consumer’s mobility, are some features that make ICN one of the prominent solutions to deal with the boost in both video and mobile traffic. New standards such as those for Vehicular Ad Hoc Networks (VANETs) created the need to address new requirements such as efficient content delivery while moving between wireless access points. For instance, smart vehicles may share data so as to report bad weather, accidents or even transmit multimedia content. Despite the mobility support given for the consumer, there are open issues for operators in the ICN deployment, such as transparent support for provider mobility. This thesis investigates and proposes separate solutions for two problems: (a) network equipment storing decision (in-network caching) and (b) transparent mobility for content providers. It is proposed the synergy between mobility and caching, aiming to solve the problem of provider mobility in end-to-end scenarios. The solution to (a) consisted of a model and cache ranking algorithm that is based on the number of hits. The adopted simulation tool was the Omnet++ simulator and our results showed gains of 10% to 30% compared to some of the most cited strategies, using hit rate, number of jumps and average packet delay as the main performance metrics. The solution to (b) aimed at fast route update (after the handover events) and was based on a scheme for special package forwarding. An analytical model to analyze the proposal handover cost was developed and validated through simulation. In terms of flow, the proposal gains range from 3%, with the provider moving at 1m/s, and 244%, with the provider moving at 30m /s. In addition, it has been demonstrated a relationship between mobility and caching in ICN networks and that it is possible to further improve services if the solution to problems (a) and (b) are tackled simultaneously. MobCache, a framework for addressing the provider mobility in CCNs (Content Centric Network) was proposed to improve QoS (Quality of Service) and QoE (Quality of Experience) for video traffic, taking into account both wired and wireless network. The results showed the proposal’s superiority, with gains of up to 58% and considering QoS and QoE, MobCache achieved 58% in terms of flow rate and 16% to 80% in video quality evaluation, in particular PSNR (Peak Signal to Noise Ratio). In addition, the proposal was also evaluated through a mathematical model, presenting gains that ranged from 150% to 300% in this scenario. The model was validated by means of software simulation, resulting in a similar output for a single input data, in both methods.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,EDSON ADRIANO MARAVALHO AVELAR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,03/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'ICN, CCN, In-network Caching, Provider Mobility'",REDES DE COMPUTADORES,KELVIN LOPES DIAS,117,"CCN, ICN, Mobilidade, Caching, NDN, RankCache, MobCache",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),"Mecanismos, Protocolos e Arquiteturas para a Internet do Futuro (Bolsa de Produtividade em Pesquisa - PQ2)","O paradigma ICN (Information Centric Network) propõe que as operações da rede tenham o foco na recuperação do conteúdo ou informação, não importando sua localização física, Esta arquitetura difere substancialmente do projeto original da Internet, em vigor, centrado no host e dependente do endereçamento IP.  O armazenamento de conteúdo nos roteadores da rede e não apenas no provedor/fonte de conteúdo, além do suporte intrínseco à mobilidade do consumidor de conteúdo, são algumas das características que tornam o ICN uma das soluções proeminentes para lidar com a explosão tanto do tráfego de vídeo na Internet quanto do acesso via dispositivos móveis.  Tradicionalmente, os provedores ou fontes de conteúdo, estão localizados em servidores em nuvens computacionais e CDNs (Content Distribution Networks), por exemplo, para serviços como os fornecidos pelo Youtube e Netflix, ou mesmo, podem ser smartphones fornecendo informações sobre sua localização, streaming de vídeo ou mensagens em redes sociais.  Com o advento da Internet das Coisas (IoT - Internet of Things) e Redes Veiculares (VANETs – Vehicular Ad Hoc Networks), os provedores trazem novos  requisitos, como a continuidade no fornecimento de conteúdo enquanto se deslocam entre pontos de acesso sem fio. Sensores em robôs ou drones podem prover informações de temperatura, umidade, estado de objetos, vídeos para fins de monitoramento e segurança; veículos pertencentes a VANETs podem fornecer dados sobre a vias, acidentes, etc. Apesar do suporte à mobilidade de consumidor no ICN, há questões em aberto para que as operadoras invistam na implantação do ICN em suas redes , como o suporte transparente à mobilidade do provedor. Nesse contexto, esta tese investiga e propõe soluções para dois problemas tratados isoladamente pela literatura da área de redes centradas na informação: (a) decisão de armazenamento nos elementos da rede (in-network caching) e (b) mobilidade transparente de provedor de conteúdo. Dessa forma, propõe-se a sinergia entre mobilidade e caching com o objetivo de solucionar o problema de mobilidade de provedor em cenários fim-a-fim. Primeiramente, foi desenvolvida uma solução para cada problema. A solução para (a) consistiu em um modelo binomial e um algoritmo para ranqueamento de cache baseado no número de acertos; a avaliação, utilizando o simulador Omnet++, mostrou ganhos de 10% a 30% em comparação com algumas das estratégias mais citadas na literatura usando métricas de taxa de acerto, número de saltos e atraso médio de pacotes. A solução para (b) foi baseada em um esquema de envio de pacotes especiais para atualização das rotas de forma rápida e direta, após o handover. Um modelo analítico para analisar o custo do handover da proposta foi desenvolvido e validado via simulação. Diversas soluções da literatura foram comparadas em termos de métricas de QoS. Em relação à vazão, os ganhos da proposta variam de 3%, com o provedor movendo-se a 1m/s, e 244%, com o provedor movendo-se a 30m/s. Além disso, foi demonstrado na tese que há relação entre mobilidade e caching em redes ICN e que é possível melhorar ainda mais os serviços se a solução para os problemas (a) e (b) for desenvolvida de forma conjunta. Com isso, foi proposto o MobCache, um arcabouço para tratar a mobilidade de provedor nas CCNs visando melhorar a QoS e QoE para tráfego de vídeo, levando-se em consideração tanto a parte cabeada quanto a sem fio da rede. Os resultados de QoS e QoE mostraram a superioridade da proposta em relação ao estado da arte, com ganhos de até 58% em relação à vazão e de 16% a 80% em relação à avaliação de qualidade de vídeo, em particular o PSNR.",TESE,MobCache: MOBILIDADE DE PROVEDOR E ARMAZENAMENTO EFICIENTE EM REDES SEM FIO ORIENTADAS A CONTEÚDO,5620895,1
"Internet of Things (IoT) is considered as another step in the evolution of the Internet we have today and it has gained significant attention from academia and industry. Researches related to RFID, NFC (Near Field Communication) and other pervasive and IoT technologies is increasing, providing ways to make real world connections to the virtual world. Related to IoT, Web of Things (WoT) aims to create things representation on web through HTTP. Real world objects are being tagged with QR Code, NFC tags, and other technologies to provide a web presence of that object with specific URLs. This work raise scenarios where it is also possible to add web presence for non computational things through the usage of general purpose devices (like smartphones or wearables), improving the way users interact with their surroundings. With the proposed mechanism is possible, for example, to tag things virtually on a non intrusive way, without the need to change real environment, recognizing images (like paintings) and locations to associate an URL with detailed information about the identified thing, providing virtual experiences from physical world.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,EDWIN CARLO RIBEIRO MARINHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,29/08/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Internet of Things;Web of Things;Web Presence for Things',ENGENHARIA DE SOFTWARE,KIEV SANTOS DA GAMA,91,"Internet das Coisas, Web das Coisas, Web Presence para Coisas",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Internet das Coisas (IoT) é considerada como um outro passo na evolução da Internet e tem ganho atenção significante da academia e industria.  Pesquisas relacionadas a RFID, NFC (Near Field Communication) e outras tecnologias pervasivas e de IoT estão expandindo, provendo meios de realizar conexões entre o mundo real e o mundo virtual. Relacionada com a Iot, a Web das Coisas (WoT) visa criar representação para as coisas na web utilizando HTTP. Objetos do mundo real estão sendo marcados com QR Code, tags NFC e outras tecnologias para prover presença na web através de URLs para um determinado objeto. O presente trabalho aponta cenários onde é possível adicionar presença na web para coisas não computacionais através do uso de dispositivos de propósito geral (como smartphones ou wearables), melhorando a forma que usuários interagem com as coisas ao redor. Com o mecanismo proposto é possível, por exemplo, marcar coisas virtualmente de forma não intrusiva, sem necessidade de alterações no ambiente real, convertendo imagens (como pinturas) e localizações em URLs relacionadas. Tais URLs direcionam para informações detalhadas da coisa identificada, provendo experiência virtual a partir do mundo físico.",DISSERTAÇÃO,Web Presence for Things Through Physical Mobile Interaction Techniques,5621042,1
"The ability of embedding software that demands a greater processing power in smaller
circuits encourages the emergence of smarter devices in a variety of areas. The spread of this
technology shows signs that it is feasible to actually have smart homes in a few years. However,
there are still challenges, and it is necessary to develop solutions that deal with difficulties related
to interoperability between heterogeneous devices; With the complexity of hardware abstraction;
With the difficulty of managing of big data that can be produced in the environment, besides being
able to interact without using complex controls. However, if these challenges were solved, it
would still be necessary to develop tools capable of managing the events triggered by the users or
by the environment itself, considering the support to the natural interaction, from a perspective of
contextual learning. Therefore, the main goal of this work is to propose and validate an Operating
System architecture for Internet of Things with a focus on the Smart Homes, where natural
interaction is a requirement. The proposed architecture aims to abstract the hardware layer,
providing tools to guide the development of applications in the residential scope. An Operating
System prototype was developed to validate the proposed architecture, aiming to investigate the
conformity between the components of the architecture and to contemplate the requirements
identified in the state of the art. For this purpose, case studies were developed, where it was
possible to observe in which way the requirements were considered and, subsequently, each
requirement was quantitatively measured, with the goal of establishing metrics that can be used
as reference models in future work. Therefore, the main contribution of this research was the
architecture of Operating System for Internet of Things, as well as high-fidelity prototypes of
an Operating System for Smart Homes, called VoxarHomeOS. Through the case studies, it was
possible to analyze the behavior of the architecture components, highlighting evidence indicating
that the proposed architecture suited the requirements of interoperability, scalability, extensibility,
network discovery, context sensitivity, data analysis and, natural interaction. Considering these
requirements and comparing them with the related works, we can indicate that the present work
presents itself as an evolution of the state of the art, Whereas in addition to complying with the
requirements, it provides users with mechanisms to define gestures, voice commands to interact
with the environment, as well as learn from the data of the residential environment.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,FABIANO AMORIM VAZ,UNIVERSIDADE FEDERAL DE PERNAMBUCO,06/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Internet of Things. Smart Home. Natural Interaction.',MÍDIA E INTERAÇÃO,VERONICA TEICHRIEB,145,Internet das Coisas. Residências Inteligentes. Interação Natural,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A possibilidade de embarcar um software, que demanda um poder de processamento maior, em circuitos com tamanho bem reduzido, fomenta o surgimento de dispositivos mais inteligentes em uma diversidade de áreas. A popularização desta tecnologia mostra indícios de que é factível possuirmos residências verdadeiramente inteligentes em alguns anos. Contudo, existem alguns aspectos desafiadores, onde é preciso lidar com a dificuldade de interoperabilidade entre os dispositivos heterogêneos; com a complexidade de abstração de hardware; com a dificuldade de gerir uma massa de dados que pode ser produzida no ambiente, bem como interagir utilizando controles complexos. Entretanto, ainda que os aspectos supracitados fossem solucionados, seria necessário desenvolver ferramentas capazes de gerenciar os eventos disparados tanto pelos usuários quanto pelo próprio ambiente, provendo suporte à interação natural, sob uma perspectiva de aprendizado contextual. Deste modo, o objetivo principal é propor e validar uma arquitetura de Sistema Operacional para Internet das Coisas com enfoque no ambiente residencial, onde a interação natural é um requisito fundamental. A proposta tem como finalidade abstrair a camada de hardware, provendo subsídios para auxiliar o desenvolvimento de aplicações no âmbito residencial. Como forma de validar a arquitetura proposta, foi desenvolvido um protótipo de Sistema Operacional, no intuito de averiguar a harmonia entre os componentes e principalmente o atendimento aos requisitos levantados através da literatura. Para tanto, foram elaborados estudos de casos, onde foi possível observar de qual modo os requisitos foram tratados e, posteriormente, foi mensurado quantitativamente cada requisito, a fim de estabelecer métricas que podem ser utilizadas como referências em futuras implantações. Portanto, a principal contribuição desta pesquisa é a arquitetura de Sistema Operacional para Internet das Coisas, além desta, destaca-se ainda o protótipo funcional de Sistema Operacional, que denominamos de VoxarHomeOS.",TESE,Sistema Operacional para Residência Inteligentes Baseado em Análise de Comportamental com Suporte à Interação Natural,5621226,1
"Internet of Things (IoT) is a paradigm that interconnects several devices acting like sensors and actuators. This interconnectivity of different types of devices generates challenges, such as management and safety. One of the major challenges is to develop mechanisms that ensure the security of data sent by devices with limited resources in untrusted networks. Therefore, this work aims to specify and design a Software Architecture, which provides security to the data transmitted in IoT systems using the MQTT protocol in unreliable networks. Generally, a solution that uses Hash of data generated by the device sends these data (Message + Hash) together. In this process, the data can be intercepted and changed, so the consumer would not identify that it has been changed. For this, we develop a technique using cryptographic Hash functions, which aims to guarantee integrity, confidentiality and authenticity requirements of the data transmitted by these devices. Our technique generates the Hash based on (value + salt + counter) and, in this way, only the Hash is sent. The consumer application has mechanisms to translate this Hash to the original value. To evaluate our technique we make some tests using 8 tools to try to decipher the Hash values, which aim to verify the effectiveness of this technique regarding integrity, confidentiality and authenticity of the data transmitted in untrusted networks. We also evaluate the resources consumed by each device using our technique. The results show that it is not possible to decipher the Hash values generated using our technique. This way, we conclude that our solution achieves its objectives by guaranteeing the security of the data transmitted by devices with limited resources.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,FLAVIO DA SILVA NEVES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,06/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Internet of Things. Software Architecture. Security. Hash. MQTT.',ENGENHARIA DE SOFTWARE,SILVIO ROMERO DE LEMOS MEIRA,89,"Internet das Coisas, Arquitetura de Software, Segurança, Hash, MQTT.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Internet das Coisas (IoT) é um paradigma em que vários dispositivos (sensores e atuadores) estão interligados. Esta interconectividade de diferentes tipos de dispositivos gera desafios, tais como: gerenciamento e segurança. Um dos grandes desafios consiste em desenvolver mecanismos que garantam segurança dos dados enviados por dispositivos com recursos limitados, em redes não confiáveis. Portanto, este trabalho tem por objetivo especificar e projetar uma Arquitetura de Software, que forneça segurança aos dados transmitidos em sistemas IoT usando o protocolo MQTT em redes não confiáveis. Geralmente, uma solução que usa Hash dos dados gerados pelo dispositivo, envia esses dados (Mensagem + Hash) juntos. Nesse processo, os dados podem ser interceptados e alterados. Desta forma, o consumidor não identificará que houve uma alteração. Para isso, foi desenvolvida uma técnica usando funções de Hash criptográficas, que visa garantir requisitos de integridade, confidencialidade e autenticidade dos dados transmitidos pelos dispositivos. Esta técnica proposta gera o Hash do (valor + salt + contador), dessa maneira, é enviado apenas o Hash, a aplicação consumidora terá mecanismos para traduzi-lo para o valor original. Para avaliar a técnica projetada neste trabalho foram feitos alguns testes usando 8 ferramentas para tentar decifrar os valores Hash, que tiveram por objetivo verificar a eficácia da técnica no que diz respeito a garantia de integridade, confidencialidade e autenticidade dos dados transmitidos em redes não confiáveis. Outra métrica avaliada é, o quanto de recursos do dispositivo é consumido usando esta solução. Nos testes feitos não foi possível decifrar os valores Hash gerados usando a técnica desenvolvida neste trabalho. Tendo em vista as avaliações realizadas conclui-se que a solução aqui projetada atinge seus objetivos que é garantir a segurança dos dados transmitidos por dispositivos com recursos limitados.",DISSERTAÇÃO,Uma Abordagem de Segurança para os Dados Transmitidos por Dispositivos em Internet das Coisas,5625088,1
"In the last years, mobile devices (mainly smartphones and tablets) have become quite
popular, being responsible for a significant part of the Internet traffic. In these devices, multimedia
services, such as video streaming over HTTP, are commonly adopted and the TCP protocol
is the standard for assure reliable transmission of data, regardless of network transmission
rate, delay, duplication, or segment reordering. Nevertheless, the applications transmission
performance needs, as well as improvements in network technologies, have resulted in higher
transmission speeds, far beyond the purpose for which TCP was originally designed. To reduce
this problem, several changes have been made on this protocol - called TCP extensions for high
performance – with the intention of improving the bandwidth utilization offered by high-speed
network technologies, such as Wi-Fi, WCDMA (3G) and LTE (4G). Energy consumption on
mobile device is another major concern because of the low capacity of the batteries to provide
energy to these devices, mainly applications of streaming, that are responsible for a great part
of this consumption. Thus, changes in network technologies and TCP parameters can solve
performance problems, as well as have an effect on energy consumption. This research evaluates
the influence of configurable TCP/IP parameters on performance and energy consumption on
Android smartphone, by adopting video streaming over HTTP as workload. In order to perform,
the following factors are evaluated: (i) network technology; (ii) network bandwidth; and (iii)
representative TCP parameters (including extension for high performance of this protocol). The
results are obtained from the realization of three experiments – projected by using Design of
Experiments (DoE) – in which the first one serves to select, among the initially listed factors,
which are critical related to the performance. The second evaluates the factors selected from
the previous experiment, considering performance and energy consumption. The third and last
experiment considers the same factors, levels and metrics of the second, but it covers the injection
of packet loss and network delay with the purpose of evaluating the influence of configurable
TCP/IP parameters in networks with these characteristics. Experimental results show that
bandwidth is highly influential in the performance of mobile devices. Network technology has
great impact on performance and energy consumption, and some TCP parameters can influence
these metrics.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,JONAS DA CONCEICAO NASCIMENTO PONTES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,20/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Smartphones. TCP extensions for high performance. Performance evalution. Energy consumption.',REDES DE COMPUTADORES,EDUARDO ANTONIO GUIMARAES TAVARES,70,Smartphones. Extensões TCP para Alto Desempenho. Avaliação de Desempenho. Consumo de Energia,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Nos últimos anos, os dispositivos móveis (principalmente smartphones e tablets) tornaram-se muito populares sendo responsáveis por uma parte significativa do tráfego da Internet. Nesses dispositivos, os serviços multimídia, tal como streaming de vídeo sobre HTTP, são comumente adotados e o protocolo TCP é o padrão para garantir a transmissão confiável de dados, independentemente da taxa de transmissão da rede, atraso, duplicação ou reordenamento de segmento. Não obstante, as necessidades de desempenho de transmissão das aplicações, bem como os avanços nas tecnologias de rede, resultaram em velocidades de transmissão cada vez maiores, muito além do propósito para o qual o TCP foi originalmente projetado. Para atenuar esse problema, várias modificações foram feitas nesse protocolo – chamadas extensões para alto desempenho – com a finalidade de melhorar a utilização da largura de banda oferecida pelas tecnologias de rede de alta velocidade como, por exemplo, Wi-Fi, WCDMA (3G) e LTE (4G). O consumo de energia dos dispositivos móveis é outra grande preocupação devido à baixa capacidade das baterias em prover energia para esses dispositivos, com destaque para aplicações de streaming que são responsáveis por grande parte desse consumo. Assim, modificações em tecnologias de rede e parâmetros do TCP podem resolver problemas de desempenho, bem como influenciar no consumo de energia. Esta dissertação avalia a influência de parâmetros TCP/IP configuráveis no desempenho e no consumo de energia em smartphone Android, adotando o serviço de streaming de vídeo sobre HTTP como carga de trabalho. Para isso, os seguintes fatores são avaliados: (i) tecnologia de rede; (ii) a largura de banda da rede; e (iii) importantes parâmetros do protocolo TCP (incluindo extensões para alto desempenho deste protocolo). Os resultados são obtidos a partir da realização de três experimentos (planejados utilizando DoE) em que o primeiro serve para selecionar, dentre os fatores inicialmente elencados, quais são críticos no tocante ao desempenho. O segundo avalia os fatores selecionados a partir do experimento anterior, considerando desempenho e consumo de energia. Já o terceiro e último experimento considera os mesmos fatores, níveis e métricas do segundo, mas abrange a injeção de perda de pacote e atraso na rede com a intenção de avaliar a influência dos parâmetros TCP/IP configuráveis em redes com essas características. Os resultados experimentais mostram que a largura de banda é altamente influente no desempenho de dispositivos móveis. A tecnologia de rede tem grande atuação no desempenho e no consumo de energia e alguns parâmetros TCP podem influenciar nessas métricas.",DISSERTAÇÃO,Análise de Desempenho e Consumo de Energia de parâmetros TCP/IP em dispositivos móveis,5625401,1
"Context: Although Agile Software Development (ASD) has grown in recent years, research evidence points out several limitations concerning its Requirements Engi-neering activities. Improving the quality of Software Requirements Specifications (SRS) may help gaining a competitive advantage in the software industry. In this context, inadequate specification acts as a catalyst to others problems, such as low productivity of the team and difficulty in maintaining software. Goal: The goal of this study is to investigate the phenomenon of the requirements specification activity in ASD, discuss relevant findings to industrial practice, and propose how more effec-tive SRS can be written in an ASD context. Method: A mixed method was used to analyze the phenomenon. First, a systematic mapping study was conducted to characterize the landscape of requirements engineering in ASD. Then six industrial case studies investigated the phenomenon in practice. A quality model emerged from the cross-case analysis of the studies. Next, based on findings of the investiga-tions, an approach that uses design practices was elaborated to address some of the quality factors pointed out in the model. Finally, empirical studies were conducted in the industry to evaluate how the approach works in practice. Results: The simplicity and objectivity are essential factors in SRS in ASD. The main factors that affect the quality of SRS are due to the customer-driven nature of SRS in ASD. Customer-driven SRS are prolix, hindering the understanding of the developer, as they are, at the same time, insufficient supporting coding, testing and maintenance tasks. Con-clusion: The design practices used in the proposed approach have the potential to reduce the gap between the problem and the solution domains, producing an objec-tive SRS, team-driven, and closer to that will be implemented.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,JULIANA DANTAS RIBEIRO VIANA DE MEDEIROS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,13/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Agile Software Development. Software Requirements Specification. Empirical Soft-ware Engineering',ADMINISTRAÇÃO E INTEGRAÇÃO DE SISTEMAS,ALEXANDRE MARCOS LINS DE VASCONCELOS,180,Desenvolvimento Ágil de Software. Especificação de Requisitos de Software. Engenharia de Software Empírica.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Contexto: Embora o Desenvolvimento Ágil de Software (DAS) tenha crescido nos últimos anos, estudos empíricos apontam vários problemas relacionados com as atividades de engenharia de requisitos. Observou-se que a especificação inadequada age como um catalizador para outros problemas, como por exemplo, baixa produtividade da equipe e dificuldades na manutenção do software. Melhorar a qualidade da Especificação de Requisitos de Software (ERS) pode ajudar a ganhar uma vantagem competitiva na indústria de software. Objetivo: O objetivo deste estudo é investigar o fenômeno da especificação de requisitos no DAS, discutir relevantes implicações desse fenômeno para a indústria, e propor práticas para escrever ERS voltadas para a equipe de desenvolvimento. Método: Primeiro, um Mapeamento Sistemático (MS) foi realizado para caracterizar o panorama da engenharia de requisitos no DAS. O método de síntese temática foi utilizado para codificar e sintetizar os dados coletados a partir dos estudos primários selecionados. Em seguida, alguns dos desafios apontados no MS foram investigados com mais profundidade em seis estudos de caso industriais. Os dados coletados a partir de documentos, observações e entrevistas com engenheiros de software foram triangulados, analisados e sintetizados usando técnicas de teoria fundamentada e meta-etnografia. Resultados: A análise e síntese cruzada dos estudos de caso resultaram em um modelo de qualidade que define a simplicidade e objetividade como fatores essenciais na ERS no DAS. Os principais fatores que afetam a qualidade estão relacionados à natureza orientada para o cliente que tende a deixar a ERS prolixa, dificultando a compreensão do desenvolvedor, ao mesmo tempo que é insuficiente para a codificação, testes e manutenção. Uma abordagem foi proposta para fornecer uma especificação de requisitos mais próxima das necessidades de desenvolvimento, atendendo alguns dos fatores de qualidade do modelo. Conclusão: Os estudos empíricos que avaliaram a abordagem demonstram que as práticas de design utilizadas pela abordagem tem o potencial de reduzir a distância entre o domínio do problema e o da solução, produzindo uma ERS objetiva, voltada para o desenvolvedor, e próxima do que vai ser implementado.",TESE,An approach to support the Requirements Specification in Agile Software Development,5625473,1
"The study of genomes was boosted by advancements in biotechnology that took place
since the second half of 20th century. In particular, the development of new high-throughput
sequencing platforms induced the proliferation of nucleic sequences raw data. Although, DNA
assembly, i.e., reconstitution of original DNA sequence from its fragments, is still one of the most
computational challenging steps. Traditional approach to this problem concerns the solution
of intractable problems over graphs that are built over the fragments, as the determination of
Hamiltonian paths. More recently, new solutions based in the so called de Bruijn graphs, also
built over the sequenced fragments, have been adopted. In this case, the assembly problem relates
to finding Eulerian paths, for what polynomial solutions are known. However, those solutions,
in spite of having a smaller computational cost, still demand a huge computational power in
practice, given the big amount of data involved. For example, the representation employed by
some assembly tools for a gdB of human genome may reach hundreds of gigabytes. Therefore,
it is necessary to apply algorithmic techniques to efficiently manipulate data in internal and
external memory. In modern computer architectures, memory is organized in hierarchical layers:
cache, RAM, disc, network, etc. As the level grows, the storage capacity is also bigger, as is the
access time (latency). That is, the speed of access is smaller. The aim is to keep information
limited as much as possible in the highest levels of memory and reduce the need for block
exchange between adjacent levels. For that, an approach are cache-oblivious algorithms, that
try to reduce the the exchange of blocks between cache and main memory without knowing
explicitly the physical parameters of the cache. Another alternative is the use of succinct data
structures, that store an amount of data in space close to the minimum information-theoretical.
In this work, three representations of the de Bruijn graph were implemented, aiming to assess
their performances in terms of cache memory efficiency. The first implementation is based in a
traditional traversal algorithm and representation for the de Bruijn graph using adjacency lists
and is used as a reference. The second implementation is based in cache-oblivious algorithms
originally described for traversal in general graphs. The third implementation is based in a
succinct representation of the de Bruijn graph, with optimization for cache memory usage. Those
implementations were assessed in terms of number of accesses to cache memory in the execution
of two algorithms, namely depth-first search (DFS) and Eulerian tour. Experimental results
indicate that traditional and generic cache-oblivious representations show, in this order, the least
absolute values in terms of number of cache misses and least times for small amount of data.
However, the succinct representation shows a better performance in relative terms, when the
proportion between number of cache misses and total number of access to memory is taken
into account. This suggests that this representation could reach better performances in case of
extreme usage of memory.",,TEORIA DA COMPUTAÇÃO,JAMERSON FELIPE PEREIRA LIMA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,20/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'De Bruijn graphs. Cache-oblivious algorithms. Succinct data structures. Fragment assembly. Eulerian tour.',ALGORITMOS E COMPLEXIDADE,PAULO GUSTAVO SOARES DA FONSECA,94,"Grafos de de Bruijn, Algoritmos cache-oblivious, Estruturas de dados sucintas, montagem de fragmentos, tour euleriano",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O estudo dos genomas dos seres vivos têm sido impulsionado pelos avanços na biotecnologia ocorridos a partir da segunda metade do Séc. XX. Em particular, o desenvolvimento de novas plataformas de sequenciamento de alto desempenho tem ocasionado a proliferação de dados brutos de fragmentos de sequências nucleicas. Todavia, a montagem do DNA, ou seja, a reconstituição da sequência original a partir dos fragmentos continua a ser uma das etapas computacionais mais desafiadoras. A abordagem tradicional desse problema envolve a solução de problemas intratáveis sobre grafos obtidos a partir dos fragmentos, como por exemplo a determinação de caminhos hamiltonianos.
Mais recentemente, novas soluções baseadas nos chamados grafos de de Bruijn (gdB), também obtidos a partir dos fragmentos sequenciados, têm sido adotadas. Nesse caso, o problema da montagem relaciona-se com o de encontrar caminhos eulerianos, para o qual soluções polinomiais são conhecidas. Todavia, essas soluções, apesar de apresentarem um custo computacional teórico mais baixo, continuam a demandar, na prática, um consequente poder computacional, face ao volume de dados envolvido. Por exemplo, a representação empregada por algumas ferramentas para o gdB do genoma humano pode alcançar centenas de gigabytes. Assim sendo, faz-se necessário o emprego de técnicas algorítmicas para manipulação eficiente de dados em memória interna e externa.
Nas arquiteturas computacionais modernas, a memória é organizada de forma hierárquica em camadas:  cache, memória RAM, disco, rede, etc. À medida que o nível aumenta, cresce a capacidade de armazenagem, porém também o tempo de acesso (latência), ou seja, diminui a velocidade. Portanto, o objetivo é manter a informação limitada o mais possível aos níveis inferiores e diminuir a troca de dados entre níveis adjacentes. Para tal, uma das abordagens são os chamados algoritmos cache-oblivious, que têm por objetivo reduzir o número de trocas de dados entre a memória cache e a memória principal sem que seja necessário para tanto introduzir parâmetros relativos à configuração da memória ou instruções para a movimentação explícita de blocos de memória. Uma outra alternativa que vêm ganhando ímpeto nos últimos anos é o emprego de estruturas de dados ditas sucintas, ou seja, estruturas que usam uma quantidade ótima de bits do ponto de vista da teoria da informação.
Neste trabalho, foram implementadas três representações para os gdB, com objetivo de avaliar os seus desempenhos em termos da utilização eficiente da memória cache. A primeira corresponde a uma implementação tradicional com listas de adjacências, usada como referência, a segunda é baseada em estruturas de dados cache-oblivious originalmente descritas para percursos em grafos genéricos, e a terceira corresponde a uma representação sucinta específica para os gdB, com otimizações voltadas ao melhor uso da cache. O comportamento dessas representações foi avaliado quanto à quantidade de acessos à memória em dois algoritmos, nomeadamente, o percurso em profundidade (DFS) e o tour euleriano.
Os resultados experimentais indicam que a versões tradicional e cache-oblivious genérica apresentam, nessa ordem, os menores números absolutos de cache misses e menores tempos de execução para dados pouco volumosos.  Entretanto, a  versão sucinta apresenta um melhor desempenho em termos relativos, considerando-se a proporção entre o número de cache misses e a quantidade de acessos à memória, o que sugere que um melhor desempenho geral em situações mais extremas de utilização de memória.",DISSERTAÇÃO,Representações Cache Eficientes para Montagem de Fragmentos Baseada em Grafos de de Bruijn de Sequências Biológicas,5652686,1
"RINA — Recursive InterNetwork Architecture is an architectural proposition for the
Future Internet built on the concept that network communication is just Inter-process Communication
(IPC). It has a stack with a variable number of layers in a recursive model, in which
mechanisms containing a fixed set of functionalities are connected and have their operation
defined by a set of one or more policies. This programmability allows the evolution of the
network over time, without the need to undergo modifications in its structure, thus allowing the
emergence of new services and protocols. Congestion is the state where demand for resources is
greater than the network capacity. Congestion control is the application of measures to prevent
or eliminate congestion in computer networks. This work aims to study the congestion control
in this new architecture, based on the problems faced by the current Internet architecture. An
investigation was made on the classical Internet congestion control algorithms, their problems,
and proposed solutions. The study was conducted by simulation of algorithms proposed for
the current architecture, taking into account the characteristics and functionalities available in
the RINA architecture. Goodput, delay, and fairness in bandwidth allocation were computed
between competing flows. The results of the experiments showed that the FAST mechanism
performed better in the goodput metric. The factors that most impacted in goodput were the
PDU size (32.1%), the combined effect of the congestion control mechanism and propagation
delay (12.5%), and only the selection of the control mechanism congestion (12.3%). For the
delay, the CUBIC mechanism performed better, presenting lower values for the metric when
compared to the FAST mechanism. The factors that most influenced the variation observed were
the propagation delay (81.5%) and the congestion control mechanism (6.5%). The influence of
the congestion control mechanism and the propagation delay (32.3%) and only the propagation
delay (26.5%) were the most influential factors for the adjustment of the bandwidth among
competing flows. The other factors and their combinations had no significant influence. The
FAST presented a better justice index in the moments whose propagation delay was greater.
Despite having achieved a better result than CUBIC, problems related to fairness in the division
of the bandwith between competing flows were also evident in the FAST mechanism, especially
when new flows are initiated in an already congested channel. Based on the results obtained in
the experiments carried out in this work, we conclude that the congestion control mechanisms
designed for the Internet are applicable in the RINA architecture.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,KLEBER ALVES LEAL,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'CUBIC. FAST. Future Internet.',REDES DE COMPUTADORES,JOSE AUGUSTO SURUAGY MONTEIRO,82,RINA;controle de congestionamento;Internet do Futuro.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Ambiente de Monitoração para Redes para Experimentação e Novas Arquiteturas de Rede,"RINA –Recursive InterNetwork Architectureé uma proposta de arquitetura para a Inter-net do Futuro construída sobre o conceito de que comunicação de rede é apenas comunicaçãoentre processos. Ela possui uma pilha com número variável em um modelo recursivo, no qualmecanismos contendo um conjunto fixo de funcionalidades são conectados e têm o seu funcio-namento definido por um conjunto de uma ou mais políticas. Esta programabilidade permite aevolução da rede ao longo do tempo, sem a necessidade de sofrer modificações na sua estrutura,possibilitando assim o surgimento de novos serviços e protocolos. Congestionamento é o estadoonde a demanda por recursos é maior que a capacidade rede. O controle do congestionamentoé a aplicação de medidas que visam prevenir ou eliminar o congestionamento em redes decomputadores. Este trabalho tem como objetivo estudar o controle de congestionamento nestanova arquitetura, tomando como base os problemas enfrentados pela arquitetura atual da Internet.Foi feita uma investigação sobre os algoritmos clássicos de controle de congestionamento daInternet, seus problemas e soluções propostas. O estudo foi conduzido por meio de simulaçãode algoritmos propostos para a arquitetura atual, levando-se em consideração as característicase funcionalidades disponíveis na arquitetura RINA. Foram analisadas as métricas degoodput,atraso e a justiça na distribuição da banda entre os fluxos concorrentes. Os resultados mostraramque os algoritmos clássicos de controle de congestionamento e suas evoluções, com poucasadaptações em virtude das particularidades da arquitetura RINA, obtêm resultados satisfatórios.",DISSERTAÇÃO,Um estudo sobre o controle de congestionamento na Arquitetura RINA,5652816,1
"Information Technology (IT) services have proliferated around the world because of
society's emerging demand to use technological resources as a means of solving problems
and performing tasks. Among IT services, we offer products and solutions that require
suppliers, fast deliveries and increasingly challenging quality levels. In this scenario, the
provision of IT services has become an increasingly critical area, requiring companies to
diversify strategies and align with their corporate governance. Several standards, models,
standards and practices have been used over the years to help structure the activities of IT
service delivery so that customer needs are met. In this same context, the Brazilian Federal
Public Administration (APF) is one of the largest consumers of IT services and solutions in
Brazil. Each year, APF plans the specific budget for IT acquisitions, based on the IT
contracting process set forth in Normative Instruction IN/SLTI/MPOG 04/2014, which
is mandatory for all entities linked to the System of Administration of Resources of
Information Technology (SISP) of the Executive Branch. The process of hiring IT
solutions, provided by IN/SLTI/MPOG 04/2014 of APF, considers Brazilian Legislation
as the main element to subsidize IT acquisitions in Brazil. This process regulates and guides
IT hiring regardless of its type. On the other hand, the Brazilian IT industry is guided by
best practices worldwide, such as the ITIL Framework, ISO/IEC 20000 and CMMI-SVC.
Therefore, there is a lack of alignment between the APF and the Brazilian IT Industry. This
work aims to establish an Approach to Provision of IT Services by Brazilian Companies
adopting good practices relevant to the domain from standards, norms and models directed
to the provision of IT services. For this, a documentary analysis was carried out based on
official documents of the Court of Auditors of the Union (TCU) and semi-structured
interviews were conducted in companies that provide IT services to APF. Based on the
results, an Approach to IT Service Provision was proposed to APF (APSTI). The approach
focuses on the IT Contracts Management phase of the process of hiring IT solutions, since
it is at this stage that companies are affected. In addition, the approach was implemented in
a company providing IT services to the APF, using the Research-Action methodology, for
its evaluation and evolution. Finally, feedbacks were collected from IT service managers to
identify the perceptions of managers who have used the approach, evaluating their impacts,
improvements and recording knowledge. The results showed that it is feasible to use an
approach aimed at providing IT services to APF. Finally, a knowledge base was created
throughout the research, based on the findings and findings of the study.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,LUIZ SERGIO PLACIDO DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,25/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Provision of Information Technology Services, Contracting of Information Technology Services, Brazilian Federal Public Administration.'",ENGENHARIA DE SOFTWARE,ALEXANDRE MARCOS LINS DE VASCONCELOS,278,"Prestação de Serviços de Tecnologia da Informação, Contratação de Serviços de Tecnologia da Informação, Administração Pública Federal Brasileira.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Os serviços de Tecnologia da Informação (TI) têm se proliferado em todo o mundo pela emergente demanda da sociedade em utilizar os recursos tecnológicos como meio de solucionar problemas e executar tarefas que atendam às suas necessidades. Dentre os serviços de TI, tem-se a oferta de produtos e soluções, que requerem dos provedores entregas rápidas e em níveis de qualidade cada vez mais desafiadores. Neste cenário, a prestação de serviços de TI tem se tornado uma área cada vez mais crítica, uma vez que, exige das empresas estratégias diversificadas e alinhadas à sua governança corporativa. Diversas normas, modelos, padrões e práticas têm sido utilizados ao longo dos anos com o objetivo de auxiliar a estruturação das atividades da prestação de serviços de TI, de forma que as necessidades dos clientes sejam atendidas. Neste mesmo contexto, a Administração Pública Federal Brasileira (APF) surge como a maior consumidora de serviços e soluções de TI no Brasil. Anualmente, a APF planeja o orçamento específico destinado às aquisições de TI, utilizando como base o processo de contratação previsto na Instrução Normativa IN/SLTI/MPOG 04/2014, que é obrigatório para todos os órgãos ligados ao Sistema de Administração dos Recursos de Tecnologia da Informação (SISP) do Poder Executivo. O processo de contratações de soluções de TI, previsto pela IN/SLTI/MPOG 04/2014 da APF, considera a Legislação Brasileira como principal elemento para subsidiar as aquisições de TI no Brasil. Este processo regulamenta e norteia todas as contratações de TI, independente do seu tipo. Por outro lado, a indústria brasileira de serviços de TI está orientada por melhores práticas mundialmente disseminadas como por exemplo Framework ITIL, Norma ISO/IEC 20000, Modelos CMMI-SVC e MR-MPS-SV. Logo, observa-se a falta de alinhamento existente entre o processo de contratação de soluções de TI previsto na IN/SLTI/MPOG 04/2014 e a Indústria Brasileira de TI. Este trabalho tem como objetivos (i) verificar a utilização do processo de contratação de soluções de TI previsto pela IN/SLTI/MPOG 04/2014 pelos órgãos ligados à APF e (ii) identificar a percepção das empresas brasileiras que fornecem produtos e serviços de TI à APF. Para isto, foi realizada uma análise documental com base em relatórios oficiais do Tribunal de Contas da União (TCU) e foram conduzidas entrevistas semiestruturadas em empresas que prestam serviços de TI à APF. Com base nos resultados, foi proposto uma Abordagem para Prestação de Serviços de TI à APF (APSTI).. A Abordagem foca na fase de Gestão dos Contratos de TI, do processo de contratação de soluções de TI, uma vez que é nessa fase que as empresas são afetadas. Adicionalmente, a Abordagem foi implementada em uma empresa de Prestação de Serviços de TI à APF, utilizando a técnica de Pesquisa-Ação na Indústria, para sua avaliação e evolução. Finalmente, foram coletados feedbacks de gestores de serviços de TI para identificar as percepções dos gestores que fizeram uso da Abordagem, avaliando seus impactos, melhorias e realizando o registro do conhecimento.",TESE,APSTI: Uma Abordagem para Prestação de Serviços de Tecnologia da Informação à Administração Pública Federal por Empresas Brasileiras,5655121,1
"Background: In highly competitive business environments, it is critical that software industry continuously innovates and the actions of individuals are fundamental to this improvement. With this, more and more organizations are investing and worrying about the innovative behavior of their employees. On the other hand, the customer also has a key role in building this innovative environment, whose participation can reduce uncertainties and improve the suitability of the product or service in the market. Therefore, the customer’s relationship with the individual's innovative behavior can define the direction of the organization's success in its projects.
Goal: The aim of this study is to establish a relationship of customer archetypes with the attitudes towards the individual innovative behavior in the software industry. As a result, the IBMSW-c was constructed, a refinement of the innovative behavior model of software professionals (IBMSW).
Method: Based on an inductive approach, three case studies were performed. After these steps, a cross-case analysis was performed and the results lifted. Finally, the result was verified through member checking with software engineers and project managers.
Results: The IBMSW-c is a refinement of the IBMSW model, identifying the impact of different customer archetypes on the innovative behavior professionals. Good practices have also been raised so that the customer and team relationship is beneficial to the emergence of new ideas. Finally, new antecedents to the individual's innovative behavior were also raised and composed the IBMSW-c.
Conclusion: The result of this study will provide software companies a guide to the best relationship between development team and customers, providing parameters to diagnose the relationship between the parties during the development of a project.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,MARCOS JOSE DE MENEZES CARDOSO JUNIOR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,05/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'innovative behavior, customers, software engineering.'",ENGENHARIA DE SOFTWARE,FABIO QUEDA BUENO DA SILVA,243,"comportamento inovador, clientes, engenharia de software.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Background: Em ambientes de negócios altamente competitivos é fundamental que a indústria de software inove continuamente e as ações dos indivíduos são de importância fundamental para essa melhoria. Com isso, cada vez mais as organizações estão investindo e se preocupando com o comportamento inovador dos seus funcionários. Por outro lado, o cliente também possui um papel fundamental na construção desse ambiente inovador, cuja participação pode reduzir incertezas e melhorar a adequação do produto ou serviço no mercado. Por conseguinte, a relação do cliente com o
comportamento inovador do indivíduo pode definir os rumos do sucesso da organização em seus projetos.

Objetivos: O objetivo principal desse estudo é estabelecer uma relação de arquétipos de clientes com as atitudes voltadas para o comportamento inovador do indivíduo na indústria de software. Como resultado final, será construído o IBMSW-c, modelo da relação do cliente com o comportamento inovador do engenheiro de software, um refinamento do modelo para o comportamento inovador do engenheiro de software (IBMSW).

Método: Baseado em uma abordagem indutiva, serão utilizados três estudos de caso, uma observação não participante para coletar e analisar os dados afim de responder à questão central de pesquisa. Após esses passos, uma síntese cruzada de dados será realizada para cruzar os achados dos estudos de caso e levantar os resultados. O resultado final será verificado através do
member checking com engenheiros de software e gerentes de projeto.

Resultados esperados: O modelo IBMSW-c deve identificar o impacto causado pelos diferentes arquétipos de clientes no comportamento inovador dos engenheiros de software. Também serão levantadas boas práticas a serem realizadas para que a relação cliente e equipe de desenvolvimento seja benéfica ao aparecimento de novas ideias. Por fim, novos antecedentes ao modelo IBMSW também serão levantados.",TESE,Incorporação de arquétipos em relacionamentos do cliente no modelo de comportamneto inovador do engenheiro de software,5655213,1
"Context - Previous research emphasizes that Motivation and Satisfaction are key success factors for software projects and have a direct impact, not only on productivity, but also on the quality of the artifacts produced. The academy has been developing research on these topics, however, software managers continue to develop actions based on their technical and personal experiences, without scientific rigor. In practice, they develop managerial actions and reflect on their interventions without theoretical bases on motivation and satisfaction. Objective - The main goal of this thesis was to construct, verify and validate a method for managers to apply to the development of software projects, based on the Theory of Motivation and Satisfaction of Software Engineers (TMS-SE), to evaluate the motivation and satisfaction of their engineers and implement managerial decisions to improve them. Method - Development of semi-structured interviews with the participating managers and engineers in order to know the context and promote reflections on motivation and satisfaction. Then the action research development, in conjunction with managers and software engineers, for the construction and validation of the method for managing the motivation and satisfaction of software engineers, based on the TMS-SE theory, and real time development of software design. Results - This study produced a method for management of motivation and satisfaction of software engineers - M3S-SE, based on the TMS-SE theory, developed and validated by project managers, as well as contributions to update the TMS-SE theory. Further, as the main empirical results, an increase on managers' knowledge about software engineers in their projects and an increase on the self-knowledge of the participating engineers. Finally, the study also produced the planning and implementation of new managerial practices and changes in the motivation and satisfaction of the engineers. Conclusion - The set of scientific methods that were applied proved adequate to the nature of the managerial activities and the set of problems understood by these actors. It was demonstrated that it is possible, through action research, to operationalize the processes in 2 (two) different industrial environments, based on the TMS-SE theory, starting from a theory and producing a normative process.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,MARCOS SUASSUNA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Motivation and Satisfaction. Action Research. Method for Management.',ENGENHARIA DE SOFTWARE,FABIO QUEDA BUENO DA SILVA,296,Motivação e Satisfação. Pesquisa Ação. Método para Gerenciamento.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Contexto – Pesquisas anteriores ressaltam que Motivação e Satisfação são fatores chave de sucesso para os projetos de software e têm impacto direto, não só na produtividade, mas também na qualidade dos artefatos produzidos. A academia vem desenvolvendo pesquisas nesses temas, no entanto, gerentes de software continuam desenvolvendo ações baseadas em suas experiências técnicas e pessoais, sem rigor científico. Na prática, desenvolvem ações gerenciais e refletem sobre suas intervenções sem bases teóricas sobre motivação e satisfação. Objetivo – O principal objetivo desta tese foi construir, verificar e validar um método para que gerentes, ao aplicá-lo ao longo do desenvolvimento de projetos de software, à luz da teoria Theory of Motivation and Satisfaction of Software Engineers - TMS-SE, possam avaliar a motivação e satisfação dos seus engenheiros e implementar decisões gerenciais para melhorá-las. Método – Desenvolvimento de entrevistas semiestruturadas com os gerentes e engenheiros participantes com o objetivo de conhecer o contexto e promover reflexões sobre motivação e satisfação. Em seguida, o desenvolvimento de pesquisa ação, em conjunto com gerentes e engenheiros de software, para a construção, verificação e validação do método para o gerenciamento da motivação e satisfação de engenheiros de software, à base da teoria TMS-SE, e em tempo real do desenvolvimento de projeto de software. Resultados – Este estudo produziu um método para gerenciamento da motivação e satisfação de engenheiros de software – M3S-SE, baseado na teoria TMS-SE, desenvolvido, verificado e validado por gerentes de projetos, assim como contribuições para atualização da teoria TMS-SE. Como principais resultados empíricos, verificou-se o aumento do conhecimento dos gerentes sobre os engenheiros de software em seus projetos e o aumento do autoconhecimento dos engenheiros participantes. Finalmente, produziu-se o planejamento e implementação de novas práticas gerenciais e mudanças na motivação e satisfação dos engenheiros. Conclusão - O conjunto de métodos científicos que foram aplicados mostrou-se adequado à natureza das atividades gerenciais e do conjunto de problemas entendidos por estes atores e demonstrou-se que é possível, através de pesquisa ação, com a operacionalização dos processos em 2 (dois) ambientes industriais diferentes, à luz da teoria TMS-SE, sair de uma teoria e montar uma normatização.",TESE,Método para Gerenciamento da Motivação e Satisfação de Engenheiros de Software no Desenvolvimento de Projetos – M3S-SE,5670535,1
"Monitoring is an essential task for the management of any network and several network
functions dependent on it. In Software Defined Networking (SDN) is no different, even with
all the benefits provided by it. Network monitoring helps to understand patterns in network
traffic, allowing to capture its state and enhance its configuration. Furthermore, it aids the
infrastructure management, the discovery of bottlenecks, the location of problems related to
software and hardware, and enforces SLAs (Service Level Agreement). Cloud services are
spread around the globe and are used constantly by several companies. However, ensure the
safety and quality of these services is a difficult and non-stop chore. Network monitoring has
a major part in assuring that the services are executed without problem. Ideally, it is expected
that the monitoring solution does not overload the network, scales together with the network
causing minimal impact, has a controlled resource usage and works for every network device
independent of the vendor. The vendor lock-in problem was solved by the OpenFlow protocol,
the main and default SDN protocol. However, the others aforementioned problems are dependent
on the monitoring solution implementation and used strategies. Furthermore, to the best of
our knowledge, no SDN monitoring solution solved these problems ensuring all the needed
network monitoring capabilities. In order to address this gap, we proposed SHAMon, a network
monitoring solution for Software Defined Networking, created to provide fine-grained and
precise information, generating minimal network and resources overhead. We propose the use
of the binomial algorithms used for congestion control by the TCP protocol to control our
statistics request mechanism. We use all the OpenFlow features to aid in getting timely and
refined statistics. Additionally, we use a proxy-server architecture to allow our solution to scale
with the network infrastructure. To validate the use of binomial algorithms, we defined a set of
experiments. The first set of experiments was intended on analyze two functions of our solution,
the former being how variations of parameters of the congestion control algorithms are related
to how much the polling time vary. The latter was how variations of the thresholds that limit
the execution of the congestion control algorithms are related to the decision to vary the polling
time. In addition, we did a comparison experiment with three other solutions, Payless, periodic
polling, and OpenNetMon in the same network scenario. The major objective of this experiment
was to highlight some of our qualities. The results from both experiments were good, the former
showed us the behavior of the binomial algorithms and our solution. Our solution had a low
error in network measurement, varying from 26% to 15%. The latter showed that our monitoring
overhead was three times lower than the others, validating that our solution is both accurate and
not network consuming.",,COMPUTAÇÃO INTELIGENTE,MARCOS VINICIOS DA SILVA MACHADO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,11/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Software Defined Networking. Networking Monitoring. Network Management',PROCESSAMENTO DE IMAGENS,JUDITH KELNER,100,"Software Defined Networking, Network Monitoring, Network Management",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Redes Definidas por Software (SDN) simplificam o gerenciamento da rede devido a sua arquitetura, a qual separa o plano de dados do plano de controle, permitindo um controle centralizado. OpenFlow (OF) é o protocolo padrão nestas redes, permitindo comunicação entre controlador e dispositivos da rede. Devido a estas e outras diferenças entre redes tradicionais e redes definidas por software, a atividade de monitoramento da rede é feita de maneira diferente, na qual estas variações precisam e devem ser levadas em consideração. Por exemplo, o próprio protocolo OpenFlow provê mecanismos de coleta de estatísticas dos dispositivos da rede. Esta, e outras singularidades, requerem diferentes abordagens, ou, ainda, adaptações de abordagens antigas, para permitir a execução da função de monitoramento da rede. Além destes, os problemas comuns de monitoramento em redes tradicionais ainda existem, como evitar sobrecarga na rede, escalabilidade e precisão. Para atender estas demandas, nós apresentamos SHAMon, uma solução de monitoramento para redes definidas por software, criada para prover informações de rede refinadas e precisas, gerando mínima sobrecarga na rede.",DISSERTAÇÃO,SHAMon: A Scalable High Accurate Monitoring Solution,5670580,1
"Self-Adaptive Systems (SAS) can adapt their own behavior in response to context information or changes in the environment and also in response to their own behavior. The interest in requirements engineering for SAS has grown in recent years, but despite this, the work involving requirements specification of these systems does not guide requirements elicitation. Goal-oriented requirements engineering (GORE) modeling languages are widely used to specify requirements for SAS. There are GORE modeling languages specifically proposed for the SAS domain and each of them presents a fixed and small set of concepts. Ontologies can be used to overcome this limitation, since they can help in the representation of concepts within a domain, as well as in the communication and specification of requirements. The purpose of this thesis is to provide a richer set of SAS concepts to guide the elicitation and specification of requirements for such systems. An ontology for SAS is proposed, as well as a process to guide the use of the ontology for eliciting and specifying requirements for SAS. The unique core ontology for requirements for SAS in literature does not cover all main concepts that SAS involves, like the modeling dimensions and a feedback loop. In order to achieve the objective, firstly, two systematic literature reviews (SLRs) were performed to analyze the work involving knowledge representation for SAS and context-aware systems. A total of twenty-three studies were selected in both. Then, three GORE modeling languages for SAS were analyzed - Tropos4AS, AdaptiveRML and Design Goal Model - to identify the concepts that these languages are able to represent. It was observed that the analyzed languages do not represent most of the concepts involved in the SAS domain. With the results of both SLRs and the analysis of the GORE modeling languages, an ontology was proposed to aid the requirements engineer to perform the elicitation and specification of SAS. To create the ontology, three methodologies were used: Uschold and Gruninger’s, METHONTOLOGY and SABiO. The proposed ontology covers the main concepts of self-adaptive systems, such as the feedback loop concepts, context, the modeling dimensions for SAS, and goal-oriented requirements. The ontology evaluation had seven criteria: completeness, verification, validation, usability, utility, easiness of use and correctness. The ontology is complete in comparison to the related works selected in both SLRs, because it covers all the concepts present in such works. The ontology was verified and validated by instantiating a multimedia news system. The usability, usefulness and easiness of use of both the ontology and the process were evaluated by requirements engineers. In this case study, the participants used the process to instantiate an ambulance dispatch system and then answered a survey about the ontology and the process. This evaluation found that the ontology is usable and useful, although the process isn’t easy to use. Another survey was answered by SAS specialists to evaluate the correctness of the ontology, who agreed the ontology is correct.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,MONIQUE CONCEICAO SOARES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,11/09/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Ontology, specification, elicitation, modelling, self-adaptive systems, requirements engineering, goal-oriented.'",ADMINISTRAÇÃO E INTEGRAÇÃO DE SISTEMAS,JAELSON FREIRE BRELAZ DE CASTRO,245,"Ontologia, especificação, elicitação, modelagem, sistemas auto-adaptativos, engenharia de requisitos, orientação a objetivos.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Sistemas auto-adaptativos (Self-Adaptive Systems - SAS) conseguem adaptar o próprio comportamento em resposta a informações de contexto ou mudanças no ambiente e também em resposta ao próprio comportamento. O interesse em engenharia de requisitos para SAS tem crescido nos últimos anos, mas apesar disto, os trabalhos que envolvem especificação de requisitos desses sistemas não guiam a elicitação de requisitos. Linguagens de modelagem de engenharia de requisitos orientados a objetivos (GORE) são muito utilizadas para especificar requisitos para SAS. Existem linguagens de modelagem GORE que foram propostas especificamente para o domínio de SAS e cada uma apresenta um conjunto fixo e pequeno de conceitos. Ontologias podem ser utilizadas para superar essa limitação, já que elas ajudam na representação de conceitos dentro de um domínio, bem como na comunicação e especificação de requisitos. O objetivo desta tese é fornecer um conjunto mais rico de conceitos para SAS para orientar a elicitação e a especificação de requisitos para tais sistemas. Uma ontologia para SAS é proposta, bem como um processo para orientar o uso da ontologia para elicitação e especificação de requisitos para SAS. A única ontologia core para requisitos para SAS na literatura não abrange todos os principais conceitos que SAS envolve, como as dimensões de modelagem e um feedback loop. Para atingir o objetivo, em primeiro lugar, foram realizadas duas revisões sistemáticas de literatura (RSLs) para analisar o trabalho que envolve a representação do conhecimento para SAS e sistemas sensíveis ao contexto. Um total de vinte e três estudos foram selecionados em ambos. Então, três linguagens de modelagem GORE para SAS foram analisadas - Tropos4AS, AdaptiveRML e Design Goal Model - para identificar os conceitos que essas linguagens podem representar. Observou-se que as linguagens analisadas não representam a maioria dos conceitos envolvidos no domínio SAS. Com os resultados das SLRs e da análise das linguagens de modelagem GORE, foi proposta uma ontologia para ajudar o engenheiro de requisitos a realizar a elicitação e a especificação de SAS. Para criar a ontologia, foram utilizadas três metodologias: Uschold e Gruninger, METHONTOLOGY e SABiO. A ontologia proposta abrange os principais conceitos de sistemas auto-adaptativos, como os conceitos de feedback loop, contexto, dimensões de modelagem para SAS e requisitos orientados a objetivos. Também foi proposto um processo para o uso da ontologia. A avaliação da ontologia foi baseada em sete critérios: completude, verificação, validação, usabilidade, utilidade, facilidade de uso e corretude. Nossa ontologia é completa em comparação com os trabalhos relacionados selecionados em ambas RSLs, porque abrange todos os conceitos presentes em tais trabalhos. A ontologia foi verificada e validada através da instanciação de um sistema de notícias multimídia. A usabilidade, utilidade e facilidade de uso tanto da ontologia quanto do processo foram avaliadas por estudo de caso e um survey, onde os engenheiros de requisitos usaram o processo para instanciar um sistema de despacho de ambulância. Esta avaliação constatou que a ontologia é usável e útil, embora o processo não seja tão fácil de usar. Outra pesquisa foi respondida por especialistas em SAS para avaliar a corretude da ontologia, os quais concordam que a ontologia é correta.",TESE,An Ontology to aid the Goal-oriented Requirements Elicitation and Specification for  Self-Adaptive Systems,5670607,1
"Graph Databases (GDB) are an alternative to traditional Relational Databases and allow a better
scalability for the system, in addition to representing highly connected data in a more natural
way. GDBs also support different kind of network analysis, such as centrality measures and
community detection algorithms. Despite this, there are still no tools available in the market for
multidimensional analysis in graphs, such as existing OLAP systems that operate on Relational
DBs. In the academic field, there are some framework proposals that aim at the construction
of a multidimensional cube composed by aggregate graphs, which are obtained from the combination
of vertices and edges of the original graph, according to the dimensions and measures
being analysed. However, most part of the researches in this area are focused on the OLAP
analysis for homogeneous graphs, while the works dedicated to heterogeneous graphs require
an intermediate data model in order to execute the multidimensional analysis. This project
proposes a system to execute OLAP queries in a Graph Database without the need to generate
an intermediate data model to do multidimensional analysis on heterogeneous graphs. The
proposed system is able to answer OLAP queries using aggregate graphs obtained from the
original graph, as well as execute analysis about the topology of the graph. In this work, we
present experiments showing the effectiveness of the system to answer the analytical queries
and some qualitative comparisons between the proposed system and existing solutions.",,BANCO DE DADOS,NICOLLE CHAVES CYSNEIROS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,01/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'OLAP, Graph Databases, Graphs, Data Analysis'",BANCO DE DADOS,ANA CAROLINA BRANDAO SALGADO,81,"OLAP, Grafo, Banco de Dados em Grafo",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),"Enriquecimento Semântico de Processos de Integração de Dados com Ontologias, Contexto e Qualidade da Informação","Bancos de Dados (BDs) em Grafo são uma alternativa aos tradicionais BDs Relacionais e permitem uma melhor escalabilidade do sistema, além de uma maneira mais natural de representar dados altamente conectados. Os BDs em Grafo também permitem diferentes tipos de análises em grafos, como medidas de centralidade e algoritmos de detecção de comunidades. Apesar disso, ainda não existem ferramentas disponíveis no mercado para fazer análise multidimensional em grafos, como os sistemas OLAP existentes que operam sobre BDs Relacionais. No meio acadêmico, existem algumas propostas de frameworks que visam a construção de um cubo multidimensional composto por grafos agregados, obtidos a partir da combinação de nós e arestas do grafo original de acordo com as dimensões e medidas analisadas. Contudo, a maior parte das pesquisas são voltadas para a análise de grafos homogêneos, enquanto os trabalhos que se dedicam a grafos heterogêneos realizam a análise multidimensional a partir de um modelo intermediário do dado original. Esse projeto propõe um sistema para a realização de consultas OLAP em um Banco de Dados em Grafo sem a necessidade da geração de um modelo intermediário de dados para realizar análise em grafos heterogêneos. O sistema proposto é capaz de responder consultas OLAP a partir de grafos agregados extraídos do grafo original, além de também realizar análises acerca da topologia do grafo. Neste trabalho são apresentados experimentos mostrando a eficácia do sistema para responder às consultas analíticas e comparações específicas entre o sistema descrito e as soluções existentes",DISSERTAÇÃO,Using OLAP Queries for Data Analysis on Graph Databases,5670680,1
"Projects are undertaken at all organizational levels, both in the public and private
environments, and managing them is not a trivial activity, involving the application
of a large amount of knowledge, skills, tools and techniques, in order to better meet
requirements agreed between the parties involved. However, in addition to the need of
managing projects, one of the major challenges within organizations is how they define
which projects are priorities or what is the best combination of projects to archieve
strategic business objectives. In this scenario, Project Portfolio Management (PPM) gains
importance, becoming the area or process responsible, not only for a specific project, but for
activities that involve the management of an organization’s entire portfolio of projects. This
type of activity is vital because it has a strong impact on the direction of investments and
on the use of certain business opportunities. Nevertheless, researches such as those carried
out by the Tribunal de Contas da União (TCU), presented a number of concerns regarding
the adoption of good practices, present in the PPM, in the area of Information Technology
(IT). The Modelo de Referência do MPS para Software (MR-MPS-SW), which includes the
PPM process, is based on the concepts of maturity and process capability for evaluating
and improving software quality and productivity. The purpose of this research was to
define, execute and evaluate a strategy for the implementation of the IT Projects Portfolio
Management in the public environment, in light of the MR-MPS-SW, in order to improve
the practices of a public software development organization and generate knowledge. The
study was conducted through an Action Research, methodological approach of applied
and interventionist nature, which structured the generation of knowledge presented in this
work, contributing both to theory and practice. The analysis of the strategy was carried
out from the data collection, made through the application of a research questionnaire
with the people who executed the PPM strategy adopted in the organization.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,PETRONIO ARAUJO DE MEDEIROS,UNIVERSIDADE FEDERAL DE PERNAMBUCO,15/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Project Portfolio Management, Public Management, Action Research.'",ENGENHARIA DE SOFTWARE,HERMANO PERRELLI DE MOURA,112,"Gerência de Portfólio de Projetos, Gestão Pública, Pesquisa-ação.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Projetos são empreendidos em todos os níveis organizacionais, tanto no ambiente público quanto no privado, e gerenciá-los não é uma atividade trivial, envolve a aplicação de uma grande quantidade de conhecimento, habilidades, ferramentas e técnicas, de modo a atender, da melhor forma possível, os requisitos acordados entre as partes envolvidas. Entretanto, além da necessidade de gerenciar projetos, um dos grandes desafios dentro das organizações consiste na forma como elas definem quais projetos gerenciar prioritariamente, ou ainda, qual a sua melhor combinação de projetos, de forma a atender aos objetivos estratégicos de negócio. Nesse cenário, a Gerência de Portfólio de Projetos (GPP) ganha importância, tornando-se a área ou processo responsável, não apenas por um projeto específico, mas por atividades que envolvem o gerenciamento de toda a carteira de projetos de uma organização. Esse tipo de atividade é vital, pois impacta fortemente no direcionamento dos investimentos e no aproveitamento, ou não, de determinadas oportunidades de negócios. No entanto, pesquisas como as realizadas pelo Tribunal de Contas da União (TCU), apresentaram números preocupantes com relação a adoção das boas práticas, presentes na GPP, na área de Tecnologia da Informação (TI). O Modelo de Referência do MPS para Software (MR-MPS-SW), que inclui o processo de GPP, baseia-se nos conceitos de maturidade e capacidade de processo para a avaliação e melhoria da qualidade e produtividade de software. Assim, o objetivo desta pesquisa foi definir, executar e avaliar uma estratégia para implantação da Gerência de Portfólio de Projetos de TI no setor público, à luz do MR-MPS-SW, no intuito de melhorar as práticas de uma organização pública de desenvolvimento de software e gerar conhecimento. O estudo foi conduzido por meio de uma Pesquisa-Ação, abordagem metodológica de natureza aplicada e intervencionista, que estruturou a geração do conhecimento apresentado neste trabalho, contribuindo tanto para a teoria quanto para a prática. A análise da estratégia foi realizada a partir da coleta de dados, feita por meio da execução de um questionário de pesquisa junto às pessoas que executaram a estratégia de GPP adotada na organização.",DISSERTAÇÃO,Uma Estratégia para Implantação da Gerência de Portfólio de Projetos de TI no Setor Público à Luz do Modelo de Referência do MPS para Software,5670716,1
"An efficient network management is highly related to the knowledge about the data transported in network packets. Traditionally, a single point in the network is used to performing the network traffic classification. Further, the classification method and the supporting hardware are the focus of the improvements in this field of research. However, there are open research challenges in this area. This thesis proposes and evaluates a distributed architecture for network traffic classification, which is capable of providing online classification, fault resilience, and adaptive mechanisms to deal with traffic profile changes. The proposed architecture is based on several features provided by the Software-Defined Networking (SDN) and Network Function Virtualization (NFV) paradigms, enabling the deployment of a complete distributed approach in an effective way. The traffic classifiers are virtual network functions, which cooperate with each other. In this context, a proposed algorithm (called PICO) provides the placement of the virtual classifiers at a network topology. Results show that the classification module of the proposed architecture enhances the accuracy of the classification considering cooperative approaches, overcoming the traditional classification with an isolated classifier. Furthermore, the adaptive mechanism provides an increase of the accuracy compared to the traditional training method in about 14.5%. Additionally, the placement of the virtual network traffic classifiers provides a high rate of path coverage, overcoming the random approach. The PICO algorithm enhances the path coverage up to 21%. Finally, we study the deployment time to achieve a quick start of the classification.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,PETRONIO GOMES LOPES JUNIOR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,08/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Traffic classification. Flow-based traffic classification. Software-Defined Network. Traffic analysis. Computer network. Network Function Virtualization.',REDES DE COMPUTADORES,STENIO FLAVIO DE LACERDA FERNANDES,169,"Classificação de tráfego, Classificação baseada em fluxos, Software-Defined Network, Análise de tráfego, Redes de computadores e Network Function Virtualization.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Prover um gerenciamento eficiente de uma rede é uma tarefa intimamente ligada ao conhecimento sobre as informações que compõem o tráfego da rede. Tradicionalmente, um ponto único na rede é utilizado para realizar a classificação de tráfego. Além disso, o método de classificação escolhido, bem como o hardware de suporte são os focos das melhorias nessa área de pesquisa. Entretanto, existem outras questões de pesquisa em aberto nessa área. Esta tese propõe e avalia uma arquitetura distribuída para classificação de tráfego da rede, sendo capaz de prover classificação online, resiliência a falhas e mecanismos adaptativos para lidar com mudanças no perfil de tráfego. A arquitetura proposta é baseada nas características de redes SDN (Software-defined Networking) e NFV (Network Function Virtualization), possibilitando uma abordagem de classificação completamente distribuída. Os classificadores são tratados como funções virtuais da rede que colaboram entre si. Dessa forma, através da proposição de um algoritmo de posicionamento de classificadores (denominado Posicionamento Ideal de Classificadores Otimizado - PICO), o posicionamento de classificadores virtuais em uma determinada topologia é um problema abordado neste trabalho. Os resultados demonstram que o módulo de classificação da arquitetura proposta alcança melhorias em sua acurácia quando comparado à classificação tradicional. Além disso, o mecanismo de adaptação a mudanças no perfil de tráfego proposto provê um ganho de aproximadamente 14,5% de acurácia em comparação à abordagem tradicional. Adicionalmente, quando avaliando o PICO, é possível obter um posicionamento até 21% superior ao método aleatório do ponto de vista da cobertura de caminhos da topologia. Por fim, o tempo necessário para instanciar um classificador virtual também é avaliado, demonstrando a capacidade de prover uma rápida instanciação.",TESE,Classificação de Tráfego Distribuída: Construindo uma Arquitetura baseada em Redes Virtualizadas,5670737,1
"Competition among digital gaming companies has been increasing along with the advancement
of technology, creating a chasm between novice and veteran companies. The advancement
of computers and video games allows developers to create more complex and realistic games,
but there is a cost associated with the production of this content making it more expensive and
complex. In addition, consumer customers in this market require significant improvement with
each generation making development more expensive. The genre of racing games is particularly
known for having achieved graphical realism and simulation status relatively early in the industry.
This premature status imposes an even greater weight on its production. Some games use a
technique called procedural content generation (GPC), which uses algorithms to dynamically
create content or to overcome storage limits. This technique can also be used to create content
automatically, but its use may limit the interaction of the game designer. This research aims to
use procedurally generated content, associated with an artificial intelligence, to assist the designer
in the production of racing games. Our aim is to achieve an efficiency gain for the development
and cost reduction team, generating a higher added value product. The proposed tool aims to
guide the designer in the creation of racing game content, using a concept of curves to simplify
the design of the track, as well as to correct game design decisions, supporting creativity without
removing the power of decision. The tool was then evaluated by game developers from different
areas - among programmers, artists and designers - with the aim of evaluating the performance
and compliance of the tool with its initial proposal. The evaluation helped build strengths and
weaknesses, listing the needs of the developers, and illustrated new themes for future research.",,COMPUTAÇÃO INTELIGENTE,ANTONIO VASCONCELOS CAVALCANTE JUNIOR,UNIVERSIDADE FEDERAL DE PERNAMBUCO,05/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Digital Games. Artificial Intelligence. Procedural Content Generation. Digital Games Industry',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,GEBER LISBOA RAMALHO,56,"Jogos digitais, inteligência artificial, geração procedimental de conteúdo, indústria de jogos digitais",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A concorrência entre as empresas de jogos digitais vem aumentando juntamente com o avanço da tecnologia, criando um abismo entre empresas novatas e veteranas. O avanço dos computadores e dos videogames permite aos desenvolvedores criar jogos mais complexos e realistas, porém existe um custo associado à produção deste conteúdo tornando-o mais caro e complexo. Adicionalmente, os clientes consumidores deste mercado exigem uma melhora significativa a cada geração encarecendo o desenvolvimento. O gênero dos jogos de corrida é particularmente conhecido por ter alcançado o realismo gráfico e o status de simulação relativamente cedo na indústria. Esse status prematuro impõe um peso ainda maior na sua produção.

Alguns jogos utilizam uma técnica chamada geração procedimental de conteúdo (GPC), que utiliza algoritmos para criar conteúdo dinamicamente ou para superar limites de armazenamento. Essa técnica também pode ser usada para criar conteúdo automaticamente, porém seu uso pode limitar a interação do designer de jogos.

Esta pesquisa visa utilizar conteúdo gerado procedimentalmente, associado a uma inteligência artificial, para auxiliar o designer na produção de jogos de corrida. Nosso intuito é obter um ganho de eficiência para a equipe de desenvolvimento e redução dos custos, gerando um produto de maior valor agregado.

A ferramenta proposta pretende guiar o designer na criação do conteúdo de jogos de corrida, utilizando um conceito de curvas para simplificar a elaboração do traçado, além de corrigir decisões de game design, dando suporte a criatividade sem retirar o poder de decisão. A ferramenta foi então submetida a avaliação de desenvolvedores de jogos de diversas áreas - entre programadores, artistas e designers - com o intuído de avaliar o desempenho e conformidade da ferramenta com a sua proposta inicial. A avaliação ajudou a elaborar pontos fortes e fracos, elencando as necessidades dos desenvolvedores e ilustrou novos temas para futuras pesquisas.",DISSERTAÇÃO,UMA FERRAMENTA DE AUXÍLIO AO DESIGNER NA GERAÇÃO PROCEDIMENTAL DE CONTEÚDO PARA JOGOS DE CORRIDA RECIFE,5713166,1
"The Entity Resolution (ER) is the problem of identifying groups of tuples (records or instances) from single or multiple data sources which represent the same real-world entities. ER is an essential step in data integration tasks, and it often demands to obtain results at query-time (online). Especially in settings containing dynamic data sources with large volumes of data, the ER process can be still more challenging. However, most traditional ER techniques process all tuples at once, instead of considering tuples based on a query. This lead to a need for solutions to get around this problem.
This work proposes a query-driven incremental process for ER. In this case, incremental means that in each iteration phase, the currently processed tuples will increase the set of previous tuples. The term query-driven means that the process in each iteration considers only tuples regarding the query result. The contributions of this work are the specification, development, and evaluation of the proposed process. Regarding the evaluation, we have used it in existing algorithms on different data sources. We conclude that the use of previous results in ER tasks turns the process more efficient than comparing all pairs of tuples at query-time, without reducing the quality of results.",,BANCO DE DADOS,PRISCILLA KELLY MACHADO VIEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,27/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Entity Resolution. Data Integration. Duplicate Data. Deduplication',BANCO DE DADOS,ANA CAROLINA BRANDAO SALGADO,132,"Resolução de Entidades, Integração de Dados, Dados Duplicados, Deduplicação",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Resolução de Entidades (RE) é o problema de identificar grupos de tuplas (registros ou instâncias), em uma única ou múltiplas fontes de dados, que representam a mesma entidade do mundo real. Esta é uma etapa crucial do processo de integração de dados, que muitas vezes necessita integrar dados em tempo de consulta (online). Esta tarefa torna-se ainda mais onerosa quando são consideradas fontes dinâmicas e com grandes volumes de dados. Além disso, tais características, tornam o processo de RE mais desafiador, uma vez que a maioria das técnicas de RE (tradicional), processa todas as tuplas de uma única vez, ao invés de processar apenas as tuplas importantes para o usuário. Portanto, novas soluções são necessárias para contornar este problema.
Neste trabalho é proposto um processo incremental e orientado a consulta para RE. O processo é considerado incremental porque a cada iteração um conjunto de tuplas é processado e adicionado às demais. O termo orientado a consulta é proveniente do fato do processo proposto processar apenas resultados de consultas. As contribuições deste trabalho são: especificação, implementação e avaliação do processo proposto. O processo foi avaliado com diferentes algoritmos e sobre diferentes fontes de dados. Foram utilizadas medidas de qualidade e desempenho do processo. Observou-se que o processo proposto tem qualidade muito similar aos processos tradicionais de RE, contudo tem um desempenho melhor.",TESE,Um Processo Incremental e Orientado a Consulta para Resolução de Entidades em Sistemas de Integração de Dados,5713307,1
"Traditional Business Process Management Systems (BPMS) often comprise contextual
sharing information constraints, creating a problem known as Context Tunneling. It may
happen when the context information needed to control the process execution is only
visible for its respective actors and, although a user knows his immediate activities, he
may not be aware of the overall process, which can severely impact on future results. In
this way, the user cannot decide to do more impacting activities before, or that would
accelerate the conclusion of the process. This narrow view often results in errors and
inefficiencies. Activities that require multiple participants per goal are more vulnerable
and yet the most popular in real-world collaborative processes, for instance in medical
care, system development and event organization. In this type of process, communication
and collaboration become relevant features, since the decision made by a user can alter the
process flow, as well as the actions available (or not) to other users. The Case Management
approach expresses the process management based on the context information in which
it is inserted, so as to mitigate the Context Tunneling problem. This work proposes the
REFlex framework extension, introducing specific characteristics of Case Management
to the existing declarative scenario. To do this, we present a web-based solution (Web-
REFlex) capable of running multiple instances and managing context information for
collaborative business processes. Web-REFlex records data based on decisions previously
made by users and, through statistics and the AHP technique, provides comprehensive
suggestions and information about the process. Furthermore it is possible to allow users
to take a broad view, not only in local context, to take advantage of this information in
order to make better decisions about who should perform and/or which steps should be
performed to meet current goals of the process.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,RAFAEL ISAIAS RODRIGUES COELHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,10/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Collaborative Business Processes. Declarative Process. Case Management. Context Tunneling. MCDM. AHP.',REDES DE COMPUTADORES,NELSON SOUTO ROSA,97,Processos de negócio colaborativo. Processos declarativos. Regras de negócio. BPMS. Case Management. Context Tunneling. Case Handling. Multiple-criteria decision-making. MCDM. AHP. Contexto.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),MODELAGEM E AVALIAÇÃO DE SISTEMAS DINÂMICOS DE EVENTOS DISCRETOS,"Os sistemas de gerenciamento de processos de negócio (BPMS) tradicionais frequentemente incluem restrições no compartilhamento de informações contextuais, problema conhecido como Context Tunneling. Este ocorre quando o contexto necessário para controlar a execução de um processo apenas é visível para seus respectivos atores, portanto, um usuário conhece as atividades que foram destinadas a ele, mas não consegue ter uma visão geral de como a execução de suas atividades pode impactar no restante do processo. Dessa forma, o usuário não pode decidir por fazer atividades mais impactantes antes, ou que acelerariam o encerramento do processo. Esta visão restrita geralmente resulta em erros e ineficiências.
Essa limitação contextual possui maior impacto para processos nos quais múltiplos participantes estão envolvidos na execução das atividades para alcançar um objetivo comum, tais como cuidados médicos, desenvolvimento de sistemas e organização de eventos. Esses processos representam a maioria dos processos do mundo real, e são conhecidos como processos colaborativos. Nesse tipo de processo, a comunicação e a colaboração tornam-se características importantes, visto que a decisão tomada por um usuário pode influenciar o fluxo restante do processo, assim como as ações disponíveis (ou não) para os outros usuários desse processo de colaboração.
A abordagem Case Management expressa o gerenciamento do processo baseando-se nas informações do contexto no qual está inserido, com o objetivo de mitigar o problema de Context Tunneling. Dessa forma, este trabalho propõe a extensão do framework REFlex, introduzindo ao cenário declarativo já existente, características próprias do Case Management. Para isso, apresentamos uma solução web (Web-REFlex) capaz de executar várias instâncias e gerenciar informações de contexto para processos de negócio colaborativos. O Web-REFlex registra dados baseado em decisões tomadas previamente por usuários e, através de estatísticas e da técnica Analytic Hierarchy Process (AHP), disponibiliza sugestões e informações globais sobre o processo. Desse modo é possível permitir que os usuários tenham uma visão ampla, e não somente restrita ao seu contexto, para que tirem proveito dessas informações para tomar melhores decisões sobre quem deve realizar e/ou qual etapa deve ser executada para satisfazer os objetivos atuais do processo.",DISSERTAÇÃO,Web-REFlex: uma solução para evitar Context Tunneling na execução de processos de negócios declarativos,5713450,1
"Nowadays, many devices can connect to vehicles such as smartphones, smartwatches, and
other cars. Therefore, communication technologies with different purposes will be used at
the same time in vehicles, such as Bluetooth Smart, IEEE 802.15.4, IEEE 802.11p, and
4G / 5G / LTE. In addition, in Ethernet-based next-generation intravehicular networks,
Scalable service-Oriented MiddlewarE over IP (SOME/IP) is the service-oriented middleware
solution for non-critical data transmission in an automotive Ethernet network.
Using SOME/IP, data generated by automotive Ethernet network nodes can be consumed
through service-oriented requests. According to European Telecommunications Standards
Institute (ETSI), an Intelligent Transportation System Station (ITS-S) can be any device
that implements the ETSI ITS communication architecture. An ITS application has
several purposes and operates on the last layer of the communication architecture of an
ITS-S. Because of the different communication technologies that could be used, it would
be extremely complex and costly to create ITS applications to communicate with Internet
of Things (IoT) devices and consume data from the automotive Ethernet network. This
work aims to develop and evaluate an extension of the architecture of an ITS-S of the
ETSI standard. The extension provides communication between ITS applications, IoT
devices, and Ethernet network nodes. The evaluation was performed through simulation
in OMNeT ++ of ITS applications that communicate with IoT devices and nodes of the
intravehicular Ethernet network using SOME/IP. The measured metric was the end-toend
delay between the requests by ITS applications and the arrival of their responses.
Simulation results show that the ITS-S architecture extension meets the maximum delay
constraints allowed for non-critical data communication.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,RHUDNEY ALYSON MENNA BARRETO KONIG SIMOES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,15/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Connected Cars, Internet of Things, Architecture Evaluation, ETSI Standards, SOME/IP'",REDES DE COMPUTADORES,DIVANILSON RODRIGO DE SOUSA CAMPELO,71,"Carros Conectados, Internet das Coisas, Avaliação de Arquitetura, ETSI Standards, SOME/IP",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Atualmente, diversos dispositivos podem se conectar a veículos, como smartphones, smartwatches e outros carros. Sendo assim, tecnologias de comunicação com diferentes finalidades serão usadas ao mesmo tempo em veículos, como, por exemplo, Bluetooth Smart, IEEE 802.15.4, IEEE 802.11p e 4G /5G / LTE. Ademais, nas redes intraveiculares de próxima geração baseadas em Ethernet, o SOME/IP (Scalable service-Oriented MiddlewarE over IP) apresenta-se como a solução de middleware orientado a serviços para a transmissão de dados não críticos em uma rede Ethernet automotiva. Através do SOME/IP, dados gerados por nós da rede Ethernet automotiva podem ser consumidos através de requisições orientadas a serviços.
Segundo o European Telecommunications Standards Institute (ETSI), uma Estação ITS (Intelligent Transportation System Station, ITS-S) pode ser qualquer dispositivo que implemente a arquitetura de comunicação ETSI ITS. Uma aplicação ITS tem finalidades diversas e opera na última camada da arquitetura de comunicação de uma ITS-S. Devido às diferentes tecnologias de comunicação que poderiam ser usadas, seria extremamente complexo e dispendioso criar Aplicações ITS para se comunicarem com dispositivos IoT (Internet of Things) e consumirem dados oriundos da rede Ethernet automotiva.
Este trabalho tem como objetivo desenvolver e avaliar uma extensão da arquitetura de uma ITS-S do padrão ETSI. A extensão provê comunicação entre aplicações ITS, dispositivos IoT e nós da rede Ethernet. A avaliação foi realizada através da simulação em OMNeT++ de aplicações ITS que se comunicam com dispositivos IoT e nós da rede Ethernet intraveicular usando SOME/IP. A métrica avaliada foi o atraso fim-a-fim entre a realização de requisições por aplicações ITS e a chegada de suas respostas. Resultados de simulação mostram que a extensão da arquitetura de ITS-S atende as restrições de atraso máximo permitido para a comunicação de dados não-críticos.",DISSERTAÇÃO,"Uma Extensão da Arquitetura de ETSI ITS-S para comunicação entre Aplicações ITS, IoT e SOME/IP",5714027,1
"Currently, there are several works on smart cities and the advances offered to the routine of its inhabitants and optimization of resources, however, there is still no consensus on the definition of the term ""Smart Cities"", nor their domains and indicators. The lack of a clear and widely usable definition, as well as the delimitation of domains and indicators makes it impossible to compare or measure cities in this context. This work presents a proposal for a metamodel called SmartCluster, which was developed to allow uniformity in intelligent city models so that they can be used in any context and can be expanded at any time. The use of this metamodel will allow indicators drawn from public databases to serve to assist municipal managers in measuring, comparing and managing resources of smart cities.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,RICARDO ALEXANDRE AFONSO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,13/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Smart Cities, e-Government, Metamodel, Ontology'",ENGENHARIA DE SOFTWARE,VINICIUS CARDOSO GARCIA,166,"Cidades Inteligentes, Governo Eletrônico, Metamodelo, Ontologia",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),UM AMBIENTE COMO SERVIÇO PARA GERENCIAMENTO DE IMPLANTAÇÃO ÁGIL DE APLICAÇÕES NA NUVEM,"Atualmente, existem vários trabalhos sobre cidades inteligentes e os avanços oferecidos à rotina de seus habitantes e otimização de recursos, no entanto, ainda não há consenso sobre a definição do termo ""Cidades Inteligentes"", nem seus domínios e indicadores. A falta de uma definição clara e amplamente utilizável, bem como a delimitação de domínios e indicadores torna impossível comparar ou medir as cidades neste contexto. Este trabalho apresenta uma proposta para um metamodelo chamado SmartCluster, que foi desenvolvido para permitir a uniformidade em modelos de cidades inteligentes para que eles possam ser usados em qualquer contexto e podem ser expandidos a qualquer momento. A utilização deste metamodelo permitirá que indicadores elaborados a partir de bases de dados públicas sirvam para auxiliar os gestores municipais na medição, comparação e gestão de recursos das cidades inteligentes.",TESE,SMARTCLUSTER: A METAMODEL INDICATORS FOR SMART AND HUMAN CITIES,5714124,1
"This work gives a linear regression method of the clusterwise type aiming to provide
linear regression models that are based on homogeneous clusters of observations w.r.t. the
explanatory variables and that are well fitted w.r.t. the response variable. To achieve this
goal, this method combines the standard clusterwise linear regression method and the
K-means clustering method with the automatic weighting of the explanatory variables. The
relevance weights of the explanatory variables change in each iteration of the algorithm
and are different from one variable to another. Thus, this method is able to select the
relevant variables in the search for homogeneous clusters w.r.t. the explanatory variables.
Finally, since it simultaneously learns a prototype and a linear regression model for each
cluster, this method is able to assign an appropriate regression model to an unknown
observation based on its description through its explanatory variables. Experiments with
synthetic and real datasets corroborate the utility of the proposed method.",,COMPUTAÇÃO INTELIGENTE,RICARDO AZEVEDO MOREIRA DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,21/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Linear regression. Clusterwise regression. K-means Clustering. K-plane regression. Adaptive distance.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,FRANCISCO DE ASSIS TENORIO DE CARVALHO,94,"regressão linear, regressão clusterwise, k-means, regressão k-plane, distância adaptativa",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Métodos Não Supervisionados de Classificação para Dados Quantitativos e Simbólicos - Edital Universal 2013,"Este trabalho propõe um método de regressão linear do tipo clusterwise cujo objetivo é fornecer modelos de regressão linear baseados em grupos homogêneos de observações em relação às variáveis explicativas e que são bem ajustados em relação à variável de resposta. Para atingir esse objetivo, este método combina o método regressão linear do tipo clusterwise padrão e o método de agrupamento K-means com a ponderação automática das variáveis explicativas. Os pesos das variáveis explicativas mudam em cada iteração do algoritmo e são diferentes de uma variável para outra. Assim, este método é capaz de selecionar as variáveis relevantes na busca por clusters homogêneos em relação às variáveis explicativas. Por fim, uma vez que ele aprende simultaneamente um protótipo de grupo e um modelo de regressão linear para cada cluster, ele é capaz de atribuir um modelo de regressão apropriado para uma observação desconhecida com base na sua descrição através de suas variáveis explicativas. Experimentos com conjuntos de dados sintéticos e reais corroboram a utilidade do método proposto.",DISSERTAÇÃO,Combinando Regressão Linear Clusterwise e K-Means com Ponderação Automática das Variáveis Explicativas,5714262,1
"Spatial databases and location-based applications are hosted on the cloud looking for
high availability and easy configuration managing. However, the remotely stored data
are under foreign governments’ laws and shares resources with other users. Thus, to keep
the confidentiality of spatial data, this work proposed a cryptography technique, named
CR-ASPE, to enable searches over encrypted spatial data kept in the cloud. Based on
CR-ASPE, two cryptography schemes were proposed and submitted to a threat model
to evaluate their security level. The first scheme, named CR-ASPE, is faster, on the
other hand, the second scheme, named as CR-ASPEE, is more resistant to attacks. A
formal definition is shown for each scheme, together with a security analysis. Lastly, time
complexity analysis and performance analyses were made to evaluate the functions of each
scheme and the queries on a database based on the searches functions. At the end of this
work, a technique to encrypt spatial data and run circular, polygonal and kNN searches
over them faster than other studied cryptography techniques may be available.",,BANCO DE DADOS,RODRIGO BARBOSA FOLHA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'encrypted spatial data. cloud computing. searches over spatial data. spatial relationship. cryptography',BANCO DE DADOS,VALERIA CESARIO TIMES,107,"dados espaciais criptografados, computação em nuvem, busca sobre dados espaciais, relacionamento espacial, criptografia.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Diversas aplicações de banco de dados espaciais e sistemas baseados em localização hospedam-se na nuvem buscando alta disponibilidade e fácil gerenciamento dos recursos. Entretanto, os dados armazenados remotamente estão sujeitos a observação dos funcionários da empresa de hospedagem ou das entidades governamentais do país onde os centros de armazenamento estão localizados. Assim, para possibilitar a criptografia dos dados espaciais, e permitir operações de busca sobre eles, este trabalho elaborou um modelo de criptografia para dados espaciais armazenados na nuvem, o CR-ASPE. Com esquemas de segurança resistentes a diferentes níveis de ataque, o CR-ASPE, a formalização do CR-ASPE foi apresentada, juntamente, com uma análise de segurança, assim como uma análise de desempenho de suas funções, assim como seu desempenho em uma arquitetura na nuvem. No fim deste trabalho, espera-se um modelo que permita criptografar dados espaciais na nuvem e executar operações e busca sobre eles com um desempenho superior ao de outras abordagens de criptografia.",DISSERTAÇÃO,CR-ASPE: Um Modelo de Criptografia para Dados Espaciais Armazenados na Nuvem,5714456,1
"There is a great interest in society to find ways to predict the future of the stock market
to optimize the decision-making process in order to maximize profit on its investments. This
Doctoral Thesis presents a new morphological-linear artificial neuron model, called the General
Increasing Morphological Perceptron (GIMP), for low-frequency (daily, weekly and biweekly)
financial time series forecasting. The neuron GIMP is composed of a balanced combination
between a linear module and an increasing nonlinear module. In addition to that, for the design
of the proposed model, we present a descending gradient-based learning process with automatic
time phase adjustment. Furthermore, an experimental analysis is conducted using relevant
financial time series from the Brazilian stock market and the results is analyzed, according to a
relevant set of performance measures, and compared to those obtained using classical models in
the literature of financial time series prediction. The experimental results showed gains of more
than 200% when compared with other classical models.",,COMPUTAÇÃO INTELIGENTE,ROMULO CALADO PANTALEAO CAMARA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,12/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Time Series Prediction, Low-Frequency Stock Market, Artificial Neurons, Morphological Mathematics, Descending Gradient, Time Phase Adjustment'",INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,CRISTIANO COELHO DE ARAUJO,157,"Previsão de Séries Temporais, Mercado de Ações em Baixa Frequência, Neurônios Artiﬁciais, Gradiente Descendente, Ajuste de Fase Temporal.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Este trabalho apresenta um novo modelo de neurônio artiﬁcial morfológico-linear, de nominado de General Increasing Morphological Perceptron (GIMP), para previsão de séries temporais ﬁnanceiras, em baixa-frequência (diária, semanal e quinzenal). Este é composto por uma combinação balanceada entre um módulo linear e um módulo não-linear crescente. Além
disso, para o projeto do modelo proposto, é apresentado um processo de aprendizagem baseado em gradiente descendente com ajuste automático de fase temporal. Também, é realizada uma análise experimental utilizando um conjunto de séries temporais ﬁnanceiras provenientes do mercado de ações brasileiro, em baixa-frequência e os resultados obtidos foram analisados,
utilizando um conjunto relevante de medidas de desempenho, e comparados aqueles obtidos utilizando modelos clássicos da literatura de previsão de séries temporais ﬁnanceiras.",TESE,UM NEURÔNIO ARTIFICIAL MORFOLÓGICO-LINEAR COM APRENDIZAGEM BASEADA EM GRADIENTE DESCENDENTE PARA PREVISÃO DE SÉRIES TEMPORAIS FINANCEIRAS EM BAIXA FREQUÊNCIA,5714602,1
"For several years, cloud computing systems have been generating debate and interest
within IT corporations. These cloud computing environments provide storage and processing systems
that are adaptable, efficient and simple, thus allowing for rapid infrastructure modifications
to be made, according to constantly changing workloads. Organizations of any size and type are
migrating to the cloud supporting solutions based on Web. Due to the benefits of the pay-per-use
model and scalability factors, services such as Video Streaming and MBaaS OpenMobester rely
heavily on these cloud infrastructures to deliver a wide variety of multimedia content and Data
storage of mobile devices. Recent failure events in video streaming services have demonstrated
the critical importance of maintaining high availability in cloud computing infrastructures. One
of the methods used to identify the trends of occurrences of failures in computational systems,
occurs through the application of strategies of sensitivity analysis. Each strategy of sensitivity
analysis can obtain a differentiated ranking, thus it is suggested that we use to evaluate the
computational systems, more than one strategy, with the objective of obtaining high reliability
of these systems. This thesis proposes a methodology applied in the field of computational
systems, in particular in cloud computing, combining the proposition and adaptation of strategies
of sensitivity analysis with existing methods, making a comparison between them, with the
purpose of establishing a Index of sensitivity from the attribution of weights, the positions
that the parameters occupy in each strategies. The aim is to achieve a coherent ranking and to
minimize the discrepancies between the strategies, aiming to identify the main points that require
improvement in the availability of these environments. The methodology is based on the use
of sensitivity analysis strategies, together with the hierarchical modeling, and with the models
to represent redundancy mechanisms aiming to perform in the performance of the system. The
methodology has been tested over different case studies in the video streaming service and the
MBaaS OpenMobester service, from the basic infrastructure to the redundant infrastructures.
The case studies show that the proposed approach is useful for guiding cloud service providers in
the decision-making process, especially for eventual adjustments and architectural improvements
in the service.",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,ROSANGELA MARIA DE MELO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,08/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Cloud computing;availability;redundancy;analytical models;sensitivity analysis;strategies.',REDES DE COMPUTADORES,PAULO ROMERO MARTINS MACIEL,147,Computação em nuvem;disponibilidade;redundância;modelos analíticos;análise de sensibilidade;estratégias.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Durante vários anos, os sistemas de computação em nuvem vem gerando um debate e
interesse dentro das corporações de TI. Estes ambientes de computação em nuvem fornecem
sistemas de armazenamento e processamento que são adaptáveis, eficientes e simples, permitindo assim modificações na infraestrutura rápida de ser feita, de acordo com as cargas de trabalho de constante variação. Organizações de qualquer tamanho e tipo estão migrando para nuvem suportando soluções baseadas na Web. Devido às vantagens do modelo de pay-per-use e fatores de escalabilidade, serviços como o de Streaming de Vídeo e o MBaaS OpenMobester, dependem fortemente dessas infraestruturas de nuvem para oferecer uma grande variedade de conteúdos de multimídia e armazenamento de dados dos dispositivos móveis. Recentes eventos de falha em serviços de Streaming de Vídeo, demonstraram a importância fundamental da manutenção de alta disponibilidade em infraestruturas de computação em nuvem. Um dos métodos utilizados para identificar as tendências de ocorrências de falhas em sistemas computacionais, ocorre por meio da aplicação de estratégias de análise de sensibilidade. Cada estratégia de análise de sensibilidade pode obter um ranking diferenciado, desse modo sugerimos que utilizemos para avaliação dos sistemas computacionais, mais de uma estratégia, com o objetivo de obtermos alta confiabilidade desses sistemas. Esta tese propõe uma metodologia aplicada no domínio dos sistemas computacionais, em particular na computação em nuvem, combinando a proposição e adaptação de estratégias de análise de sensibilidade com métodos já existentes, realizando uma comparação entre elas, com o propósito de estabelecer um índice de sensibilidade a partir da atribuição de pesos, as posições que os parâmetros ocupam em cada estratégia. Pretende-se obter um ranking coerente e com minimização das discrepâncias entre as estratégias, visando identificar os principais pontos que requerem melhoria na disponibilidade desses ambientes. A metodologia baseia-se na utilização de estratégias de análise de sensibilidade, conjuntamente com a modelagem hierárquica, e com os modelos para representação de mecanismos de redundância visando atuar na performance do sistema. A metodologia foi testada ao longo de estudos de casos distintos, no serviço de Streaming de Vídeo e no serviço MBaaS OpenMobester, desde o nível de infraestrutura básica até a infraestrustrura com redundância. Os estudos de casos mostram que a abordagem proposta é útil para guiar os provedores de serviço de nuvem no processo de tomada de decisões, especialmente para ajustes eventuais e melhorias arquiteturais no serviço.",TESE,ANÁLISE DE SENSIBILIDADE APLICADAS À IDENTIFICAÇÃO DE PONTOS QUE REQUEREM MELHORIA NA DISPONIBILIDADE EM INFRAESTRURA DE CLOUD,5714758,1
"The Internet is evolving fast, but its current model no longer meets the requirements of new applications, such as multimedia applications. Maintaining minimal levels of resources for these applications to operate is one of the main challenges of today's Internet. As these problems are growing, the solutions become more complex and more expensive. This opens up a new perspective for new concepts and new approaches to an Internet that is more feasible and projected for the future. One of these new approaches is RINA, an acronym for Recursive Internetworking Architecture, which assumes that computer networks handle interprocess communication (IPC). Each layer performs (IPC) on different scopes in a recursive structure with the quality requested by its user. One of the abstractions that the RINA architecture does on the aspect of traffic control in the network is to bring a new look at the congestion control issue by proposing new mechanisms such as the congestion treatment that happens in all layers . Thus, one of the main improvements of this architecture is to bring the problem of congestion closer to where it really is, and to use mechanisms that result in a better efficiency in the fight against congestion, rather than the end-to-end approach. adopted by TCP. Therefore, this work has the objective of conducting new studies on the congestion control in the RINA architecture, using techniques of the TCP variant, called FAST TCP, which for having obtained good performance results, especially in the aspect of ""flow"" in the networks TCP / IP, motivated to evaluate its behavior in the RINA architecture. The comparative studies were performed with the FAST TCP variant and TCP Tahoe variant in different scenarios, in order to evaluate which characteristics differentiate them, especially in the context of the metrics analyzed: Goodput and RTT. In these experiments, the Goodput variation explained by the congestion control was 28.08%, while the variation explained by the bandwidth was 21.91%, and the variation explained by the interaction between them of 18.38%. the FAST TCP engine that presented the best values for Goodput. On the other hand, in relation to the RTT metric, the variation explained by the congestion control was even greater, of 35.38%, the variation explained by the bandwidth of 20.97% and the variation explained by the interaction between them, of 22, 21%, but this time favorable to the TAHOE variant that presented the least delays. It is concluded that, even the Fast TCP, presented higher delays, it is still the most suitable variant to compose the congestion control mechanisms of the RINA",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,SABINO ROGERIO DA SILVA ANTUNES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,13/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'RINA. Fast TCP. TCP Tahoe. Congestion Control.',REDES DE COMPUTADORES,JOSE AUGUSTO SURUAGY MONTEIRO,86,"Palavras-chave: RINA, Fast TCP, TCP Tahoe, Controle de Congestionamento.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Ambiente de Monitoração para Redes para Experimentação e Novas Arquiteturas de Rede,"A Internet está evoluindo rapidamente, contudo, o seu atual modelo, não mais atende com a mesma eficiência aos requisitos das novas aplicações, sobretudo no aspecto multimídia.
Manter níveis mínimos de recursos para que essas aplicações possam operar, é um dos principais desafios da Internet atual. À medida que esses problemas estão crescendo, as soluções se tornam mais complexas e mais caras. Com isso, abre-se uma nova perspectiva, para novos conceitos e novas abordagens para uma Internet, que seja mais viável e projetada para o futuro. Uma destas novas abordagens é a RINA, acrônimo de Recursive Internetworking Architecture, que tem por objetivo promover novos conceitos e modelos que visam simplificar e tornar mais eficiente a Internet. Uma das abstrações que a arquitetura RINA faz, no aspecto do controle do tráfego na rede, é trazer um novo olhar para a questão, controle de congestionamento, ao propor novos mecanismos que visam obter algumas melhorias, mas sem herdar os problemas existentes nas redes TCP/IP atuais. É fato, que o TCP é o protocolo da camada de transporte mais utilizado na Internet, e que está sempre passando por melhoramentos nas suas técnicas de transmissão. Entretanto, limitações em alguns dos seus mecanismos, fazem com que isso seja um grande obstáculo no desempenho do tráfego da rede, sobretudo no que diz respeito ao problema do congestionamento. Na arquitetura RINA, uma das melhorias, é trazer a resolução do problema do congestionamento para mais perto de onde realmente ele se encontra, e com isso, utilizar mecanismos que resultem em uma melhor eficiência no combate ao congestionamento, ao invés da abordagem fim a fim adotada pelo TCP, que estabelece o controle sem a participação dos dispositivos do núcleo da rede, executando, o controle longe de onde o congestionamento realmente acontece. Sendo assim, esse trabalho tem por objetivo, realizar novos estudos, sobre o controle de congestionamento na arquitetura RINA, utilizando técnicas da variante TCP, denominada FAST TCP, que por ter obtido bons resultados de desempenho, sobretudo no aspecto da “vazão” nas redes TCP/IP, motivou avaliar o seu comportamento na arquitetura RINA. Os experimentos, foram realizados no simulador RINASim e permitiu fazer um estudo comparativo, entre a variante FAST TCP com a variante TCP TAHOE, avaliando a métrica goodput. O objetivo foi analisar qual o impacto obtido no desempenho dessa nova abordagem FAST TCP em comparação com a TCP TAHOE na RINA.",DISSERTAÇÃO,Uma Avaliação Comparativa dos Mecanismos de Controle de Congestionamento das Variantes TCP FAST e TCP TAHOE na Arquitetura RINA,5714895,1
"The field of descriptor-based tracking has become increasingly important with the diverse
computer vision applications made available in the market. Even so, it is not trivial to track
certain types of images, for example, images in low lightness enviroment, motion blur or repeated
patterns. The tracking process optimization is addressed in the literature exclusively from the
software optimization point of view or by proposing novel descriptor extraction techniques.
However, nothing was found in the literature about color-to-grey convertion step. This was
observed due to the fact that all libraries and tracking softwares found in the literature and used
for business purposes employ uniquely the Luminance conversion method; besides, no works
could be found assessing its efficacy. Hence, the aim of this thesis is to identify whether a
conversion method outperforms other conversion methods to the point where its choice as a
standard conversion method for all existing computer vision libraries is justified. In order to
assess the different performance, 13 color-to-grayscale conversion methods found in the literature
were compared, therefore making it possible to define whether it makes sense to establish a
single fixed standard. The comparison is made during an image pre-processing stage—after it is
concluded, tracking proceeds as usual in real time. This pre-processing stage is necessary simply
to point out which conversion methods will perform well in that given image scenario. The
pre-processing begins by introducing uniform noise in the image. Afterwards, the conversion
of color-to-grayscale takes place, following one of the 13 conversion methods found; then,
descriptor extraction techniques are applied (in this dissertation, those were SIFT, SURF, e ORB).
Having identified the descriptors, the homography estimation is calculated and the projection
errors evaluated. The result of this process is a list of conversion methods that have achieved
good results in that given scenario, and all of them can be considered recommended methods.
The test results have shown that every single conversion method is selected at some point as
being the best option for a group of images. Although some methods outperform others in
specific situations, no conversion method has presented sufficient relevance to be considered a
generic standard, such as the Luminance is today. Moreover, the Luminance conversion method
presented insufficient performance according to the evaluation criteria in several cases, being
crossed off the list in over 80% of occurrences. The hypotheses tests carried out corroborated
that the performance difference between the methods that were crossed off the list and the ones
recommended is statistically significant. Observing these results, one concludes that the choice
of the color-to-grayscale conversion method is relevant to the tracking efficiency. However,
there is no silver bullet solution for which conversion method ought to be used— none of them
presents a performance which is consistently superior to the others, regardless of the scenario.
The solution suggested is to include a pre-processing stage in the tracking pipeline that assesses
the specific performance of each method and indicate which ones are adequate to the scenario.",,COMPUTAÇÃO INTELIGENTE,SAMUEL VICTOR MEDEIROS MACEDO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,12/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Color to gray. Descriptors. SIFT. SURF. Tracking.',PROCESSAMENTO DE IMAGENS,JUDITH KELNER,105,"color-gray;descriptors;SIFT, SURF;Tracking",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Este doutorado se concentra em uma parte negligenciada do pipeline de rastreamento baseado em descritores – a conversão da imagem colorida para tons de cinza. Esta informação pode ser comprovada pois todos software/bibliotecas de visão computacional encontrados, sejam versões desktop ou mobile, utilizam o mesmo método de conversão - Luminance. Contudo, durante a pesquisa para esta tese foram encontrados outros métodos de conversão e nenhum deles estavam sendo utilizados para fins de rastreamento, após essa constatação surgiu-se o questionamento se de fato os métodos de conversão poderiam influenciar o resultado final de um rastreamento.

Realizando a pesquisa do estado da arte, só foi encontrado um trabalho abordando o impacto dos métodos de conversão, que embora se valesse do uso das mesmas técnicas de descritores, o foco deste trabalho incidia na área de classificação de imagens. Apesar da comparação entre os métodos, não foi encontrado nenhuma metodologia capaz de indicar qual método deveria ser escolhido para cada imagem.
Visto esta brecha na literatura, decidiu-se investigar em mais detalhes como as conversões em escala de cinza impactavam o rastreamento baseado em descritores e verificar se o Luminance deveria ser o método de conversão padrão. Para resolver este problema, foi desenvolvido para esta tese uma metodologia capaz de indicar qual método conversão em escala de cinza era mais apropriado para cada imagem. Os experimentos com a metodologia proposta compararam 42 métodos de conversão em escala de cinza utilizando SIFT e SURF como técnicas descritor e sua aplicação foi executada em um benchmark com mais de 9000 imagens – caltech101.
Os resultados revelaram que todos os métodos de conversão foram selecionados como a melhor opção para algum grupo de imagem e apesar de alguns métodos apresentarem, em situações específicas, melhores resultados que outros, de maneira geral nenhum método de conversão apresentou relevância suficiente para ser considerado um padrão para qualquer tipo de imagem - como vem sendo feito com o Luminance até então.",TESE,Estudo da Influência dos Métodos de Conversão em Escala de Cinza para Rastreamento Baseado em Descritores,5721414,1
"A fundamental task in computer vision is extracting low-level features from the image.
Since this is one of the first tasks executed by most vision-based systems, imprecisions and
errors committed during its execution are propagated to the next stages thus affecting the
system overall performance. Therefore, robust and precise feature extractors are mandatory
in computer vision. In the literature, two kinds of low-level features are commonly used:
natural features, and artificial patterns of features. Natural features are extractable only
from scenarios rich in textured elements. On the other hand, artificial patterns of features
can be easily crafted by using commodity printers, which permits its application in a
diversity of scenarios. Moreover, since the real dimensions of the pattern are known
beforehand, the usage of artificial patterns allows the construction of metric systems. This
thesis presents a new detection technique for patterns formed by roundish features. The
new technique is composed of two stages: the extraction of candidates for features of
the pattern; and the searching for the elements (among the candidates) that actually
constitute the pattern. Differently from the techniques found in the related literature, the
proposed one does not restrict the patterns to be rectangular grids of regularly-spaced
features, but it allows the creation of a variety of patterns through the use of graphs (the
pattern template). Experimental results collected from two case studies evidence that the
new technique is robust to uneven and low-lighting conditions.",,COMPUTAÇÃO INTELIGENTE,SAULO ANDRADE PESSOA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,10/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Detection. Pattern. Roundish Features. Blobs.',PROCESSAMENTO DE IMAGENS,JUDITH KELNER,134,detecção. padrão. pontos característicos,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Em Visão Computacional, uma tarefa fundamental é a extração de pontos característicos da imagem. Por essa ser uma das primeiras etapas a serem realizadas na maioria dos sistemas computacionais baseados em visão, imprecisões e erros cometidos durante sua realização são propagados para as demais etapas afetando o resultado final obtido pelo sistema. Dessa forma, extratores de pontos característicos que sejam robustos e precisos são uma necessidade em Visão Computacional.
Na literatura, dois tipos de pontos característicos são amplamente utilizados: pontos característicos naturais; e padrões artificiais de pontos característicos. Pontos característicos naturais são extraíveis apenas de cenários ricos em elementos texturizados. Já padrões artificiais de pontos característicos podem ser facilmente confeccionados com impressoras domésticas, permitindo sua aplicação em diversos cenários. Além disso, o uso de padrões artificiais possibilita que as medidas reais entre os pontos característicos sejam previamente conhecidas (informação essencial à construção de sistemas métricos).
Esta tese propõe uma nova técnica para detecção de padrões artificiais de pontos característicos. Essa técnica é composta de dois estágios: a extração de candidatos a pontos característicos do padrão; e a busca para encontrar quais pontos dentre os candidatos de fato constituem o padrão de interesse.
Diferentemente das técnicas encontradas na literatura, a técnica proposta não restringe a customização dos padrões a grids retangulares com pontos uniformemente espaçados; de fato, o usuário é livre para criar o padrão da sua escolha. Resultados em imagens reais indicam que a técnica proposta é robusta a iluminação não uniforme e a baixo contraste.",TESE,A Robust Technique for Detecting Custom Patterns of Roundish Features,5742574,1
"We present a robot-building experiment, to work on some exchange of bitcoins. These
exchanges provide an online environment for trading bitcoins, which operates 24 hours every
day, the market is global and allows you to trade fiduciary currencies for digital currencies,
some of them are: real, dollar, euro, bitcoin, ethereum and litecoin. This exchange market
has high volatility, a large volume of daily transactions and differences in prices between the
bitcoins exchanges. This set of characteristics of the bitcoins market creates an environment
that can be exploited to generate benefits for the investor. To achieve our goal of creating
and using robots, we study and describe the behavior of the bitcoins market by highlighting
some differences between bitcoin exchanges, and using the application programming interface,
available in the exchanges to create robots, with methods to buying, sale and collect data of open
orders and transaction history, among others. The robots must act collecting and processing data
to extract information, and also to transact autonomously based on the data obtained from the
exchanges. As a result we present some differences detected between bitcoin exchanges, present
robots created for two exchanges, Mercado Bitcoin and Bitfinex, to present and to describe the
implementation of the robots, to discuss how the use of robots brings benefit to investors and to
present the results obtained with the use of robots.",,TEORIA DA COMPUTAÇÃO,SEVERINO MIZAEL DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,28/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Bitcoin. Robot. Exchange houses.',MATEMÁTICA COMPUTACIONAL,RUY JOSE GUERRA BARRETTO DE QUEIROZ,111,"Bitcoin. Câmbio. API's. Robôs, Exchange.",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Apresentaremos um estudo do uso de robô no mercado de câmbio de bitcoins, destacando algumas diferenças entre exchanges de bitcoin, que funcionam como casas de câmbio totalmente online, essas bolsas de bitcoin funcionam 24 horas todos os dias do ano. O mercado de bitcoins é global e permite negociar moedas fiduciárias por moedas digitais, algumas delas são: real, dólar, euro, bitcoin, ethereum e litecoin. Esse mercado de câmbio tem alta volatilidade e grande volume de transações diárias. Algumas exchanges disponibilizam API’s ao usuário, com métodos para compra, venda e analise do histórico de transações entre outros. Vamos descrever um experimento de criação de robôs, para atuar em algumas bolsas de bitcoins, discutir detalhes de sua implementação e mostrar como o uso destes robôs criados a partir das API’s podem beneficiar investidores que atuam no mercado de câmbio bitcoins.",DISSERTAÇÃO,Uso de Robôs em Bolsas de Bitcoin,5762162,1
"In-air gestures are part of humans everyday communications. Giving a 'thumbs up', pointing to an object of interest or raising a hand to call for attention are just a few examples. Current vision-based technologies such as the Microsoft Kinect, the Leap Motion have shown real-time tracking capabilities which enable a large range of developers and companies to explore these gestures on human-computer interactions. We approach the process of building these interactions. Our efforts are to understand and aid designers, developers and researchers of the field of Human-Computer Interaction to build in-air gestural interfaces. With that goal in mind, we provide a method, with detailed techniques and tools to explore and prototype concepts of possible gestural interactions for a given target task. The method is divided in two main phases: the conception of the gestures to be used; and the delivering of solutions using these gestures.
For the conception phase we propose the use of a set of creative techniques. In addition, we also introduce a pair of web catalogs (as tools) to be used for analysis and inspiration while suggesting and creating new gestural interactions. By reviewing the literature regarding how researchers define the used gestures, we cataloged several examples according to a developed taxonomy. We also performed a similar review and built a catalog of in-air gestures present on Science-Fiction (Sci-Fi) content. Sci-Fi contents, although not representing real interfaces, present potential while exploring innovative concepts which can influence the creation of new interfaces. For the deliver phase we focused on the steps of producing and testing low-fidelity and high-fidelity prototypes for the recognition of the conceptualized gestures. For low-fidelity prototyping we propose and validate the use of the Wizard of Oz technique, which enables fast testing of different concepts. For high-fidelity prototyping we introduce a recognition tool called Prepose, which aims the easiness of use for creating and editing the gesture recognizers. Prepose allows a gesture to be written in natural language making it easy to developers and non-developers to read, write and edit the target gestures. At the same time, Prepose allows a gesture to be automatically written with one sample its execution, speeding up the time to build recognizers for complex gestures. At last, we also conducted a case study to demonstrate the use of the method and each of its tools and techniques.",,COMPUTAÇÃO INTELIGENTE,LUCAS SILVA FIGUEIREDO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,24/02/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Gestural input;Systems and tools for interaction design;Design fiction;Interface design prototyping;Domain specific languages.',PROCESSAMENTO DE IMAGENS,VERONICA TEICHRIEB,140,Entrada gestual;Sistemas e ferramentas para design de interação;Design de ficção;Prototipação de interfaces;Linguagens específicas de domínio.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O uso de gestos ao ar (in-air gestures) é parte do cotidiano da comunicação entre seres humanos. Levantar o polegar para expressar concordância, apontar para um objeto de interesse ou levantar a mão para atrair a atenção são somente alguns exemplos. Tecnologias atuais baseadas em visão computacional como o Microsoft Kinect e o Leap Motion demonstram a capacidade de restrear o usuário em tempo real, habilitando desenvolvedores e empresas a explorar gestos como forma de interação entre seres humanos e máquinas. Neste trabalho nós abordamos o processo de construir tais interações. Nossos esforços são direcionados a entender e facilitar o processo de construção de interfaces gestuais para designers, desenvolvedores e pesquisadores do campo de Interação Humano-Computador. Com este objetivo em mente, nós introduzimos um método, incluindo técnicas e ferramentas, para explorar e prototipar conceitos de possíveis interações gestuais para determinada tarefa. O método é dividido em duas fases principais: a concepção dos gestos a serem utilizados; e a construção de soluções usando estes gestos.

Para a fase de concepção nós propomos o uso de um conjunto de técnicas criativas. Em adição, nós introduzimos dois catálogos web para serem usados para análise e inspiração durante a criação de novas interações gestuais. Ao revisar a literatura relativa ao uso de gestos por pesquisadores, nós catalogamos diversos exemplos do uso de gestos. Realizamos uma revisão similar (também apresentada em um catálogo web) do uso de gestos em conteúdos de Ficção Científica. Estes conteúdos, apesar de não representarem interfaces reais, apresental potencial ao explorar conceitos inovadores que podem influenciar a criação de novas interfaces. Para a fase de construção da solução nós focamos nas etapas relativas à produção e testes de protótipos de baixa e alta fidelidade. Para os protótipos de baixa fidelidade nós propomos o uso da técnica do Mágico de Oz, a qual permite testar rapidamente diversos conceitos. Para a prototipação de alta fidelidade nós apresentamos uma ferramenta para reconhecimento de gestos chamada Prepose. Prepose permite que cada gesto seja escrito em linguagem natural (usando o Inglês como idioma) tornando assim a atividade de construir e editar reconhecedores de gesto acessível para desenvolvedores e não-desenvolvedores. Ao mesmo tempo, Prepose permite que um gesto seja transcrito automaticamente através de um treinamento que requer uma única execução, acelerando o processo de construir reconhecedores de gestos complexos. Por fim, nós conduzimos um estudo de caso para demonstrar o uso do método e cada uma das técnicas e ferramentas propostas.",TESE,A Design Method for Building in-air Gestural Interactions,5781465,1
"Program transformation is current practice in software development, especially refactoring.
However, in general, there is no precise specification of these transformations
and, when it exists, it is neither proved sound nor validated systematically, which can
cause static semantic or behavioural errors in the resulting programs, after each transformation
application. This work proposes a strategy to validate program transformation
specifications grounded by a formal infrastructure built predominantly in Alloy.
In this work we focus on transformations in languages that support OO features such
as Java, ROOL, the calculus of refinement of component and object-oriented systems
known as rCOS and an OO language with reference semantic. The scope of this work,
concerning the strategy implementation, is a subset of Java.
Complementarily to testing, formal languages provide a mathematically solid reasoning
mechanism to establish the soundness of transformations. Unfortunately, a complete
formal treatment for transformations in OO languages is ruled out because even for Java
there is no complete formal semantics. We investigate the trustworthiness of program
transformations by using an approach that combines (bounded) model finding and testing.
Our Alloy formal infrastructure comprises four main Alloy models: (1) a metamodel
for a subset of OO language features and a set of predicates that capture the static
semantics, where the main predicate is able to determine if a program is well–formed; (2)
a Transformation-Specific model for each program transformation being investigated; (3)
a Static Semantics Validator model; and (4) a Dynamic Validator Model, which generates
all possible instances (according to the scope specified), each one having a representation
of a program before and after the transformation application.
If any instances are generated in (3), this means that there is a failure in the transformation
specification. So, in this case it is necessary to correct the specification and
re–submit it to the Alloy Analyzer. This process is repeated until no more instances are
found by the Static Semantics Validator Model. Hence, the instances generated by the
Dynamic Validator model only represent well–formed programs since it is only applied
after the Static Semantics Validator model. Afterwards, the instances generated by (4)
are further submitted to a tool, called Alloy-To-Java Translator, which generates Java
programs corresponding to these instances along with tests to be applied in each side of
the transformation. These programs are finally validated with regard to dynamic semantic
problems, based on these automatic tests generated in (4).
In this way, a developer can implement the transformations with some confidence on
their behavioural preservation, after validating the transformation specifications using the
vii
proposed framework. The strategy we propose seems promising since it is an alternative
to validate transformations even when a complete semantics of the languages is not
available. The results of the validation of a representative set of transformations found in
the literature show that some transformation issues, concerning both static and dynamic
behaviour, can be detected.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,TARCIANA DIAS DA SILVA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,23/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Java, Program Transformation Specification, Alloy, Alloy Analyzer, Validation'",ENGENHARIA DE SOFTWARE,AUGUSTO CESAR ALVES SAMPAIO,154,"Java, Transformações de Programas, Alloy, Validação",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),"Modelagem, Verificação e Teste Composicional de Sistemas com Aplicações na Indústria Aeronáutica","Transformações de programas é uma prática atual em desenvolvimento de software,
especialmente refactoring. No entanto, em geral, não há uma especificação precisa
dessas transformações e, quando existe, não é provada correta nem sistematicamente
validada, o que pode causar erros de semântica estática ou comportamentais nos programas
resultantes da transformação. Este trabalho propõe uma estratégia, baseada numa
infraestrutura formal construída predominantemente em Alloy, para validar especificações
de transformação de programas.
Neste trabalho, nós focamos em transformaçõoes em linguagens que suportam
características OO tais como Java [28, 7], ROOL [6], leis de refinamento no cálculo de
refinamento de componentes e sistemas orientados a objetos, conhecido como rCOS
[26] e em uma linguagem OO com semântica de referência descrita em [25]. Nosso
foco atual, com relação à implementação da estratégia, é um subconjunto de Java. Complementarmente
a testes, linguagens formais fornecem um mecanismo de raciocínio
matematicamente sólido para estabelecer a consistência (soundness) das transformaçõoes.
Infelizmente, um tratamento formal completo para transformações em linguagens OO
é descartado porque, mesmo para Java, não há uma semântica formal completa. Nós
investigamos a veracidade de transformações de programas usando uma abordagem que
combina (bounded) model finding e testes.
A entrada para nossa estratégia são especificações de transformações de programas
descritos numa variedade de notações incluindo um estilo algébrico de apresentação (representado
por uma lei algébrica ou regra de refatoramento), ou em notações de modelos
de componentes como rCOS. No caso da uma notação algébrica, cada especificação de
transformação pretende expressar uma equivalência semântica entre os programas inicial
(SS) e resultante (RS), após a transformação.
Nossa infra–estrutura formal Alloy é composta de quatro modelos Alloy principais:
(1) um metamodelo para um subconjunto de características de linguagens OO e um
conjunto de predicados que capturam a semântica estática correspondente, onde o predicado
principal é capaz de determinar se um programa é bem–formado; (2) um modelo
específico para cada transformação (que está sendo investigada); (3) um Validador de
Semântica Estática, que procura (se existir) por instâncias geradas pela transformação,
dos programas resultantes, com problemas de semântica estática; e (4) um Validador
Dinâmico, que gera instâncias possíveis da transformação (de acordo com o escopo
especificado), cada uma tendo uma representação de um programa antes e depois da
transformação.
Se quaisquer instâncias são geradas em (3), isto significa que há uma falha (de
semântica estática) na especificação da transformação. Neste caso, é necessário corrigir
a especificação e re–submetê–la para o Alloy Analyzer. Este procedimento é repetido
até nenhuma instância ser encontrada pelo modelo do Validador de Semântica Estática.
Logo, as instâncias geradas pelo modelo do Validador Dinâmico (4) tipicamente somente
representam programas bem formados já que este é aplicado na nossa estratégia apenas
depois que o modelo em (3) não retornar instância alguma. Em seguida, as instâncias
geradas em (4) são submetidas a uma ferramenta, denominada Tradutor de Alloy para Java (Alloy–To–Java Translator), que transforma as instâncias geradas dos pares de
programas em (4) em programas Java. Java é a linguagem escolhida para ilustrar a
estratégia proposta já que sua semântica (pelo menos o subconjunto necessário para
avaliar as transformações) é semelhante às das linguagens nas quais as transformações
foram especificadas. Estes programas são finalmente validados com relação a problemas
de semântica dinâmica. Para isso, testes gerados de forma simplificada são utilizados, em
que as chamadas aos métodos em comum de cada classe são geradas de maneira aleatório
e os resultados comparados estruturalmente.
Dessa forma, um desenvolvedor pode implementar as transformações com alguma
segurança em relação à preservação de comportamento depois de validar a especificação
das transformações usando o framework proposto. A estratégia que propomos parece
promissora já que é uma alternativa para validar transformações em geral (e talvez provar
formalmente a correturde de tais transformações) mesmo quando uma semântica completa
da linguagem não está disponível. Resultados da validação de um conjunto representativo
de refactorings, encontrados na literatura, mostram que alguns problemas de transformações
podem ser detectados, evitando o esforço para implementá–las numa linguagem
fonte e subsequentemente submetê–las a uma campanha de testes mais elaborada.",TESE,Validating Transformations of Programs using the Alloy Analyzer,5781777,1
"Machine learning is a subfield of artificial intelligence, whose goal is to develop algorithms
that are able to learn from data in order to perform different tasks, such as supervised
and semi-supervised classification and probability estimation. These tasks can be performed
intuitively and with interpretable predictions by prototype-based methods. Regarding these
methods, one needs to consider two important points: (i) they are susceptible to local minima
due to poor prototype initialization and (ii) they are sensible to the distance that is chosen to
compare prototypes and samples, because it has to be able to model the internal variability of
prototypes and classes to perform well. Therefore, this work aims at exploring the versatility of
prototype-based methods to provide solutions to the tasks of supervised and semi-supervised
classification, while also presenting solutions to both points mentioned above, especially regarding
new adaptive distances. For the first task, this work introduces a new method that provides
a solution to the local minima problem and uses a generalized distance applied to interval data,
which is capable of modeling imbalanced classes and class subregions with different shapes and
sizes. This algorithm is also capable of eliminating inactive prototypes and automatically selecting
features. For the semi-supervised classification task, this work proposes a graph-based
label propagation algorithm, which, in contrast to existing methods from literature, does not focus
only on unlabeled instance classification, but on the prediction of proper class probabilities.
This work also provides a performance analysis of the two proposed methods, comparing them
to existing algorithms, in terms of classification error rate (first method) and proper scoring
rules (second method), using real and synthetic datasets. Experiments show that both methods
perform significantly better than the state of the art.",,INTELIGÊNCIA COMPUTACIONAL,TELMO DE MENEZES E SILVA FILHO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,22/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Supervised classification. Semi-supervised classification. Class-probability estimation. Prototypes. Adaptive distances.',ANÁLISE DE DADOS SIMBÓLICOS E/OU NUMÉRICOS E MÉTODOS AFINS,RENATA MARIA MENDES CARDOSO,128,"Classificação supervisionada, Classificação semi-supervisionada, Estimação de probabilidades de classe, Protótipos, Distâncias adaptativas",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"A aprendizagem de máquina é um ramo da inteligência artificial, cujo objetivo é desenvolver algoritmos capazes de aprender a partir de dados. Os métodos da aprendizagem de máquina podem ser aplicados a várias tarefas diferentes, dentre as quais é possível citar classificação e estimação de probabilidades de classe supervisionadas e semi-supervisionadas, tarefas que podem ser realizadas de forma intuitiva e com predições interpretáveis pelos métodos baseados em protótipos.
Quanto a esses métodos, é preciso considerar dois pontos importantes: (i) são suscetíveis a mínimos locais causados pela má inicialização dos protótipos e (ii) a distância escolhida para comparar os protótipos e as amostras é fundamental para um bom desempenho, pois ela precisa levar em consideração as diferentes distribuições dos atributos. Assim, este trabalho visa a explorar a versatilidade dos métodos baseados em protótipos para apresentar soluções para as tarefas de classificação supervisionada e semi-supervisionada, ao mesmo tempo em que apresenta soluções para os dois pontos mencionados acima, principalmente na forma de novas distâncias adaptativas. Para a primeira tarefa, este trabalho introduz um novo método que apresenta uma solução para o problema dos mínimos locais e usa uma distância generalizada aplicada a dados intervalares, capaz de modelar classes desbalanceadas e sub-regiões de classe de diferentes formas e tamanhos. Esse algoritmo também é capaz de eliminar protótipos inativos e selecionar atributos automaticamente.
Para a tarefa de classificação semi-supervisionada, este trabalho propõe um algoritmo de propagação de rótulos através grafos que, ao contrário dos métodos presentes na literatura, não foca apenas na classificação de instâncias não-rotuladas, mas sim na predição de probabilidades de classe apropriadas. Este trabalho também provê uma análise de desempenho dos dois métodos propostos, comparando-os a métodos existentes, em termos de taxa de erro de classificação (primeiro método) e funções de escore apropriadas (segundo método), usando conjuntos de dados reais e sintéticos. Os experimentos mostram que ambos os métodos apresentam desempenhos significativamente superiores ao estado da arte.",TESE,Classificação Baseada em Protótipos de Decisão Mais Próximos e Distâncias Adaptativas,5781995,1
"The growth of data in a large proportion is a reality in a portion of current software,
especially those that are executed within the scope of distributed computing. This growth
occurs in diversified aspects of data, such as volume, velocity, and variety. In this context,
the Big Data concept emerges, which proposes a set of techniques and solutions to support
this scenario, in which current software tools can not play a useful role in the collection,
storage, processing and analysis of large data. Speed, one of these important aspects,
is a crucial requirement because there is a need for rapid responses to complex queries
in real-time data stream. Among the various architectural proposals for Big Data, the
Lambda reference architecture is one of the highlights, however a simplified version of
it has also been of interest to us, the Kappa architecture. In this context, given the
problem of efficient processing of data stream in real time and based on the Lambda
and Kappa architectures, this dissertation stopped in a first experiment to analyze a
system of distributed data stream processing system, the Apache Storm. From this, in
comparison, a second experiment directed to the processing of local data stream in the
scenario in Fog Computing was performed using Apache Edgent. In both, an evaluation
was made, in which performance metrics related to the speed, accuracy, and availability of
services were observed. The results obtained through experiments point to the possibility
of transposing part of the processing of data stream to devices at the edge of the network.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,THOMAS CRISTANIS CABRAL NOGUEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,05/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Data Stream Processing, Internet of Things, Big Data, Performance Evaluation.'",ENGENHARIA DE SOFTWARE,KIEV SANTOS DA GAMA,64,"Processamento de Fluxo de Dados, Big Data, Computação de Borda, Avaliação de Desempenho",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O crescimento dos dados em uma proporção grandiosa é uma realidade em uma parcela dos softwares atuais, principalmente os que são executados no âmbito da computação distribuído Esse crescimento, verifica-se em diversificados aspectos dos dados, no volume, na velocidade e na variedade. Nesse quadro, desponta o conceito de Big Data, que propõe um conjunto de técnicas e soluções para apoiar esse cenário, onde as atuais ferramentas de software não conseguem ter um papel efetivo na coleta, armazenamento, processamento e analise de grandes dados. A velocidade, um desses importantes aspectos, é um requisito crucial, pois existe a necessidade de respostas rápidas para consultas complexas em fluxos de dados em tempo real. Dentre as várias propostas de arquitetura de sistemas para Big Data, a arquitetura de referência Lambda é uma das que se destacam. Nesse contexto, dado o problema do processamento eficiente de fluxo de dados em tempo real e fundamentado nessa arquitetura, esta dissertação se deteve à camada de velocidade da arquitetura Lambda em um primeiro experimento, para analisar um sistema de processamento de fluxo de dados distribuído, o Apache Storm. A partir disso, em comparação, um segundo experimento direcionado ao processamento de fluxo de dados local no cenário na Computação de Borda foi realizado, usando o Apache Edgent. Em ambos, uma avaliação foi feita, onde foram observadas métricas de desempenho, relacionadas à velocidade, precisão e disponibilidade de serviços. Os resultados obtidos através de experimentos, apontam a possibilidade de transpor parte do processamento de fluxos de dados para a dispositivos na borda da rede.",DISSERTAÇÃO,Uma Avaliação de Desempenho de Sistemas de Processamento de Fluxo de Dados,5787642,1
"Several studies in Machine Learning demonstrate theoretically and empirically
that classification problems usually present better hit rates through the combination of
multiple classifiers. This strategy resembles human nature in seeking opinions from
different experts in order to obtain a more precise decision. However, the best
classifiers are not always selected to predict the pattern in question, especially when
dealing with the static combination of classifiers. Dynamic classifier selection (or
combining) is a technique that uses the idea of regions of competence, in which
different classifiers are assumed to be more appropriate for labeling patterns with
different degrees of difficulty. Although the current methods present differences in
several aspects, the rule that selects the classifier(s), in general, is generic and fixed. In
addition, the hypothesis used to decide whether a classifier will be selected is most often
defined based on only one criterion. Hybrid Intelligent Systems are models that result from
the combination of two, or more distinct techniques, aiming to unite advantages to supply
individual deficiencies. In this sense, the present research aims to propose and evaluate
the hybridization of techniques that guide both the generation of the set of classifiers
and the subsequent combination of these. The experimental results suggest that the
proposed system presents superior performance when compared to some of the main
techniques in the literature.",,COMPUTAÇÃO INTELIGENTE,TIAGO PESSOA FERREIRA DE LIMA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,31/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Machine Learning. Classifier Generation. Dynamic Classifier Selection. Hybrid Intelligent System.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,TERESA BERNARDA LUDERMIR,73,"Algoritmo Evolucionário, Geração de Classificadores, Raciocínio Baseado em Casos, Seleção Dinâmica de Classificadores, Sistema Híbrido Inteligente",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Diversos estudos em Aprendizagem de Máquina demonstram teoricamente e empiricamente que problemas de classificação geralmente apresentam melhores taxas de acerto quando classificadores são combinados ao invés de se utilizar apenas um único classificador, mesmo que este seja considerado o melhor de todos. A estratégia de combinar classificadores é bastante racional, uma vez que ela imita a natureza humana em buscar opiniões de diferentes especialistas a fim de se obter uma decisão mais precisa. Contudo nem sempre são selecionados os classificadores mais adequados para prever o padrão em questão, especialmente se tratando de comitês estáticos de classificadores.
A seleção dinâmica de classificadores é uma técnica que usa a ideia de regiões de com- petência, na qual acredita-se que diferentes subconjuntos de classificadores são mais apropriados para rotular padrões com diferentes graus de dificuldade. Vários métodos têm sido propostos na literatura para a realização de tal tarefa, porém, segundo o teorema no free lunch, não há um único método que possa ser considerado superior aos demais em todos os problemas existentes. Embora os atuais métodos apresentem diferenças em vários aspectos, a regra que seleciona o(s) classificador(es) é genérica e fixa. Além disso, a hipótese utilizada pelos algoritmos, para decidir se um dado classificador será selecionado ou não, é geralmente definida com base em apenas um único critério.
Sistemas Híbridos Inteligentes são estratégias utilizadas para um problema específico a fim de aumentar o seu desempenho. Neste sentido, um Algoritmo Evolucionário será utilizado para construir um conjunto de classificadores e Raciocínio Baseado em Casos para a construção de seletores que preveem, através de casos com características inspiradas nos principais métodos de seleção dinâmica de classificadores, quando um classificador é competente para rotular o padrão em questão. Resultados experimentais mostram que com uso dessa hibridização foi possível aprimorar a acuracidade de classificação dos problemas que foram analisados, além de apresentar desempenho superior quando comparado com algumas das principais técnicas existentes na literatura.",TESE,"Sistema Híbrido Inteligente para Geração, Seleção e Combinação de Classificadores",5787776,1
"Until the 1970s, control and automation of critical systems were based on wired electromechanical
devices, and were usually static and offered no flexibility. Next, advances
of electronic devices and computer networks introduced new technologies, coupled with
proprietary software and hardware philosophy. The deregulation of the electricity sector,
together with the market competitive pressure, forced companies producing and transmitting
electric energy to seek increasingly competitive automation projects. This context
enforced manufacturers to abandon proprietary systems and adopt open standards that
are economically more attractive. Consequently, the automation and control networks
started operating with known operating systems, based on open technologies and Ethernet
communication networks. Nonetheless, this made these networks more vulnerable
to cyberattacks. Furthermore, the growth of the terrorist groups in the world raised the
governments concern with the vulnerability of the critical infrastructure. As power outages
affect all sectors of critical infrastructure, the automation and control systems in this
industry have become a potential target for a potential cyberattack. This thesis presents a
conceptual model, referred to as, the secure information island. It applies to automation
networks of the systems of production and transmission of electric energy. This model
is based on the concept of policy-based access control (ORBAC), public key infrastructure
(PKI), and secure hardware. The access control defined in the model is enforced
for the computers of the automation network as well as users. The idea behind secure
information island was inspired from medieval castles, in order to create a controlled area
or domain that can be accessed through a single door. Another aspect that contributed
to this logical grouping is the heterogeneity of existing technologies in the automation
networks. This perimeter can be defined as a part or the entire automation network of a
substation, power plants and system control center. To validate this model, use cases were
idealized. The validation scenarios implement real tests and simulate the execution of the
model´s formal specification. The real system was the automation network of a power
system control center (SEP), belonging to Companhia Hidro Elétrica do São Francisco
(CHESF).",,REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS,UBIRATAN ALVES DO CARMO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Cyber security. automation networks. production and transmission of energy.',REDES DE COMPUTADORES,JUDITH KELNER,158,"Redes de automação, segurança, ilha de automação, controle de acesso,",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),Visualização 3D para Monitoramento de Ambientes Inteligentes Visando a Eficiência Energética,"No início da década passada, os sistemas de automação possuíam sistemas operacionais e protocolos de comunicações proprietários, nos quais pouquíssimos especialistas tinham conhecimento. Em geral, o domínio dessas soluções proprietárias estava restrito aos fabricantes de equipamentos. Não existia uma preocupação com o conceito de segurança cibernética como nos dias atuais. A segurança da informação era realizada basicamente por isolamento e desconhecimento dos sistemas existentes. Decorrente disso, a única preocupação das empresas na área de segurança para os sistemas de automação e controle era garantir a sua funcionalidade e controlar o acesso físico das suas instalações.
Com a desregulamentação do setor de energia elétrica e com o aumento da competitividade entre as empresas, houve a necessidade de procurar soluções cada vez mais econômicas para construção dos sistemas de automação e controle do Sistema elétrico de potência. A evolução das tecnologias Ethernet, o surgimento de redes sem fio, a criação de sistemas operacionais abertos e o crescimento da Internet proporcionaram a adoção destas tecnologias nos projetos dos novos sistemas de automação de forma a atingir os objetivos econômicos desejados pelas empresas.
Se por um lado essas tecnologias permitiram o barateamento dos custos de desenvolvimento dos sistemas de automação, por outro lado, decorrente do uso de protocolos aberto e de sistemas operacionais conhecidos não pertencentes ao setor elétrico, houve o aumento da vulnerabilidade das redes de automação.
Os dispositivos de rede e protocolos de comunicação dos sistemas de automação e controle existentes, em sua grande maioria, não foram projetados levando em consideração os requisitos de segurança cibernética atuais. Geralmente, os dispositivos que possuem tal característica são construídos utilizando mecanismos simples de segurança como login e senha.
Outro fator impactante é a convergência digital que direciona a interligação das redes de automação com as redes de dados tradicionais (corporativas). Desta forma, uma falha de segurança na rede tradicional pode afetar a rede industrial. Por fim, a falta de comunicação e/ou conhecimento entre a equipe de segurança e a equipe de controle da produção, também contribui para tornar as redes industriais vulneráveis a ataques e anomalias.
Nessa tese é explorada a hipótese de que é possível estabelecer regiões seguras e confiáveis compostas pelos diversos ativos que compõem o sistema de proteção e automação de um sistema elétrico de potência, sem a necessidade de substituição dos dispositivos eletrônicos inteligentes ou comprometimento da eficácia do sistema de automação por perda de desempenho durante a operação do mesmo. Fundamentado na hipótese, o objetivo principal deste trabalho compreende a definição de um modelo conceitual de arquitetura de segurança baseada no controle de acesso focado em papéis, hardware seguro e utilizando o conceito de redes definidas por softwares para sistemas de automação associado ao SEP
Para validar esta arquitetura foram idealizados cenários de implantação para duas instâncias do modelo. O primeiro cenário, consiste na implantação do modelo conceitual proposto em um sistema real, mais especificamente, o sistema de automação do centro de controle de sistema elétrico de potência da Companhia Hidro Elétrica do São Francisco – CHESF. O segundo cenário consiste em um modelo simulado utilizando redes definidas por softwares integrada a redes virtuais de locatários que oferece um melhor gerenciamento entre domínios e uma melhor granularidade no processo de decisão de políticas. Os resultados apresentados confirmam a robustez do modelo para as vulnerabilidades das redes de automação.",TESE,Ilha de Segurança para Redes de Automação de Sistemas Elétricos,5788018,1
"Background: Experiments play an essential role in evaluating solutions in software engineering.
A field of software engineering where experiments are frequently used is software development.
In this field, many solutions are proposed to foster coding activities, such as different
programming languages, developing techniques, tools, and other solutions.
Goals: In this context, this research has two primary goals. The first goal is to investigate
experiments performing coding activities (Coding Experiments). This investigation shall raise
the most common characteristics of such experiments, and how current solutions supporting
experiments address such coding context. The second goal is to propose a solution to support
coding experiments according to their particular context characterization.
Method: This research was divided in many sub-steps. Each sub-step adopted distinct method.
In the first step, we conducted a systematic analysis of coding experiments published in seven
renowned venues in software engineering from 2003 to 2016. In the next step, we systematically
evaluated the current solutions to support experiments in SE according to previous findings.
Based on the results from previous steps, we proposed our solution. Finally, in the final step, we
carried out a case study on replicating coding experiments with the proposed solution.
Results: The first study revealed many issues in coding experiments that can be addressed
to aid its execution. The second study brought to light which aspects of coding experiments
are covered by current solutions. In fact, the majority of general characteristics of coding
experiments are adequately addressed by current solutions. However, some context-specific
characteristics are not satisfactorily undertaken. Based on found lacks, a metamodel was
proposed to specify context-specific coding experiments characteristics. This metamodel can
be seen as a specialization of current solution focusing only on coding context characterization.
Also, a set of tools were developed to (i) specify models according to the proposed metamodel
and (ii) support the experiment execution according to its specification. The last study assessed
the proposed solution usage to help researchers carrying out coding activities in experiments.
From experiment planner’s perspective, the effort to conduct and collect data was reduced, even
considering the extra effort to specify the coding experiment. From participants’ perspective,
the proposed solution seemed reasonable to support experiments. However, some issues were
identified. Finally, although positive results, performing more assessments including different
settings is required to generalize these results.
Limitations: This research focuses only on supporting coding experiments, more precisely
planning and execution phases. This work does not deal with other aspects such as data analysis,
and we believe current solutions can deal with them.
Conclusions: By specifying coding context characteristics, many activities when carrying out a
coding experiment can be (semi-)automated, thus contributing to reduce effort to experiment.
Moreover, the proposed solution proved adequate for supporting coding experiments, and it is
available to support researches around the world through our repository.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,WALDEMAR PIRES FERREIRA NETO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,01/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,"b'Software Engineering, Coding Experimentation, Model-driven Environment'",ENGENHARIA DE SOFTWARE,SERGIO CASTELO BRANCO SOARES,180,"Engenharia de Software, Experimentação, Experimento de Codificação, Engenharia Baseada em Modelos",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Contexto: experimentos desempenham um papel essencial na avaliação de soluções em engenharia de software. Um campo de engenharia de software (ES) onde experimentos são freqüentemente utilizados é no desenvolvimento de software. Neste campo, muitas soluções são propostas para facilitar ou melhorar as atividades de codificação, como diferentes linguagens de programação, desenvolvimento de técnicas, ferramentas e outras soluções.
Objetivos: Neste contexto, esta pesquisa tem dois objetivos principais. O primeiro objetivo é investigar os experimentos que realizam atividades de codificação (Experimentos de Codificação). Esta investigação deve levantar as características mais comuns de tais experimentos, bem como as soluções atuais que auxiliam a sua realização. O segundo objetivo é propor uma solução para auxiliar experimentos de codificação de acordo com sua caracterização particular.
Método: Foram adotados vários métodos: (i) uma análise sistemática de experimentos de codificação publicados em sete fóruns científicos de renome na engenharia de software entre 2003 a 2016; (ii) uma avaliação sistemática das soluções atuais para apoiar experimentos em SE; e (iii) um estudo de caso com a solução proposta auxiliando na realização de uma replicação de um experimentos de codificação externo.
Resultados: O primeiro estudo revelou algumas limitações na realização de experimentos de codificação, tais limitações podem ser exploradas com o intuito de auxiliar a sua execução de tais experimentos. O segundo estudo revelou quais aspectos dos experimentos de codificação são cobertos pelas soluções atuais. Mais precisamente, a maioria das características gerais aos experimentos em ES são adequadamente abordadas pelas soluções atuais. No entanto, algumas características específicas dos experimentos de codificação não são cobertas de forma satisfatória. Com base nas limitações encontradas, foi proposto um metamodelo para especificar características de experimentos de codificação específicas do contexto de tais experimentos. Este metamodelo pode ser visto como uma especialização das soluções atuais, focado apenas na caracterização do contexto de codificação. Além disso, um conjunto de ferramentas foi desenvolvido para (i) especificar modelos de acordo com o metamodelo proposto e (ii) apoiar a execução da experimentos de acordo com sua especificação. O último estudo avaliou o uso da solução proposta para auxiliar pesquisadores a realizar experimentos de codificação. Do ponto de vista do planejador de experiências, o esforço para conduzir e coletar dados foi reduzido, mesmo considerando o esforço extra para aprender o metamodelo e especificar o experimento de codificação. Do ponto de vista dos participantes, a solução proposta aparenta útil para apoiar a realização do experimento. No entanto, alguns problemas foram identificados. Finalmente, apesar de resultados positivos, ainda se faz necessário a realização de mais avaliações, incluindo configurações diferentes, para podermos generalizar os resultados obtidos.
Limitações: Esta pesquisa concentra-se apenas no apoio a realização de experimentos de codificação, e mais precisamente as fases de planejamento e execução. Este trabalho não trata de outros aspectos, como análise de dados, e acreditamos que soluções atuais são satisfatórias para lidar com esses outros aspectos.
Conclusões: Ao especificar características de contexto de experimento de codificação, muitas atividades do experimento podem ser (semi-) automatizadas, contribuindo assim para reduzir o esforço de experimentação. Além disso, a solução proposta mostrou-se adequada para auxiliar experimentos de codificação, e está disponível para apoiar pesquisas em todo o mundo através do nosso repositório.",TESE,Choose the Middle Way: Supporting Coding Experiments According to Their Particular Context Characteristics,5788754,1
"Image processing is characterized by five steps: acquisition, pre-processing, segmentation,
representation/description and image recognition. In this thesis, we work with the
problem the 2D medical image segmentation. The main purpose of image segmentation
is to divide an image into regions. Image segmentation has been a subject of several
studies and research for the development of more comprehensive and computationally
more efficient methods, especially in medical imaging. A challenge that must be taken into
account in the development of medical image segmentation algorithms is how to evaluate
the performance of the method. This type of performance evaluation is usually done by
comparing the results of automatic algorithms versus a reference segmentation, called
the ground-truth. In particular, the estimation of ground-truth in medical images can be
obtained through manual segmentation or interactive segmentation methods. However,
these approaches can be extremely subjective and subject to both intra and inter user
variability. An alternative to reduce this variability is image segmentation using the consensus
of multiple segmentations of a single image. A segmentation consensus method has
the purpose of combining multiple segmentations of the same image. This work proposes
an approach to estimate the ground-truth in medical images. The approach consists of
two parts. At first, we development a collaborative system called COSE (COllaborative
SEgmentation) to obtain various segmentations of the same image in order to estimate
the ground-truth. The second proposals is a new method for the consensus of medical
image segmentations, called ISECO (Intelligent SEgmentation CCnsensus). The main
consensus segmentation methods in the state-of-the-art uses only binary information.
However, the ISECO proposes to use gray-scale image information to classify pixels in
foreground or background. However, ISECO has as main differential the use of information
extracted from the image in gray-scale to assist in the classification of pixels in foreground
or background using a learning model. The features extraction is based on three texture
extractors: Gabor filters, Local Binary Pattern (LBP) and Local Directional Pattern
(LDP). Another approach to segmentation consensus, called Distance Transform Merge
(DTM), is also proposed in this work, aiming to be a simple and fast method. The proposed
consensus methods are evaluated and compared with other approaches available in the
literature using 4 image database. Evaluation of the approaches shows that the ISECO
presents better results than the main methods of the literature.",,COMPUTAÇÃO INTELIGENTE,WENDESON DA SILVA OLIVEIRA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,22/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Image segmentation. Consensus segmentation. Collaborative segmentation. Ground-truth. Performace evaluation.',INTELIGÊNCIA ARTIFICIAL SIMBÓLICA,TSANG ING REN,132,Segmentação de imagens médicas. Consenso de segmentações. Segmentação colaborativa. Ground-truth. Avaliação de desempenho.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"O processamento e a análise de imagens caracterizam-se, de maneira geral, pelas fases de aquisição, pré-processamento, segmentação, representação/descrição, e reconhecimento de imagens. Este trabalho situa-se na área de segmentação de imagens 2D. O principal objetivo da segmentação consiste em dividir uma imagem em regiões ou objetos que a compõem.  A segmentação de imagens é alvo de vários estudos e pesquisas para o desenvolvimento de métodos mais abrangentes e computacionalmente mais eficientes, principalmente em imagens médicas. Um papel crucial para a extração de informações automatizadas em imagens médicas geralmente envolve a segmentação das regiões da imagem a fim de quantificar volumes e áreas de interesse de tecidos biológicos para posterior diagnóstico e localização de patologias. Um desafio importante que deve ser levado em consideração no desenvolvimento de algoritmos de segmentação de imagens médicas é como avaliar o desempenho do método.  Este tipo de avaliação de desempenho geralmente é feita através da comparação dos resultados de algoritmos automáticos versus uma segmentação de referência, denominada de ground-truth. Em particular, a estimativa de ground-truth em imagens médicas pode ser obtida de forma robusta através do contorno manual ou ainda através de métodos interativos de segmentação.  No entanto, essas abordagens podem ser extremamente subjetivas e sujeitas a variabilidades inter e intra usuários. Uma alternativa para diminuir essa variabilidade é a segmentação utilizando o consenso da resposta de vários usuários para uma mesma imagem. Um método de consenso de segmentações tem a finalidade de combinar várias segmentações de uma mesma imagem.  Este trabalho tem como principal objetivo propor uma abordagem para estimar a segmentação verdadeira em imagens médicas. A abordagem consiste em duas partes. Na primeira, utiliza-se o sistema colaborativo COSE (Collaborative Segmentation) para obter várias segmentações de uma mesma imagem com o intuito de estimar o ground-truth. Já a segunda, propõe um novo método para o consenso de segmentações de imagens médicas, denominado ICS (Intelligent Consensus Segmentation).  O método é avaliado e comparado com outras abordagens de consenso de segmentações disponíveis na literatura.",TESE,Consenso de Segmentações de Imagens usando Classificação de Padrões,5790529,1
"This work results from an Action Research process experienced in a target school, with
students from the Middle and High School levels, including, in addition to the researcher and
the students, eight teachers from several fields. The problem situation which gave birth to this
project was the student’s apathy and disinterest regarding their studies and the conduction of
their school activities. This problem came to light and was repeatedly reported in class council
meetings, demanding the search for alternatives capable of leveraging student engagement and
protagonism. We started from the understanding of playfulness as a means to provide experiences
which require persistence, skill, and articulation of strategies. By its turn, the Gamification,
presented as the use of game mechanics in other contexts, can provide playful activities. Thus,
the Gamification was chosen as the path to revert the problem situation. This way, we adopted as
the goal the development and implementation of a gamified model of education. The experience
began in the second semester of 2014 and lasted until the end of 2016, being reviewed and
adjusted based on results from periodic evaluations. This work was developed considering as
pillars theories from psychology, education and game design. The obtained results pointed out
satisfaction from both student and teachers regarding the proposed and implemented model of
gamification. These results include the improvement in the fulfillment of extra-class activities,
the cleaning and organization of the classrooms, as well as the development of skills such as
focused attention, active participation and punctuality. As future works, we want to create a
mobile application capable of registering the elements desired by teachers, as well as a primer
that provides subsidies for teachers to create their gamified models of education.",,COMPUTAÇÃO INTELIGENTE,YVONNE COSTA CARVALHO DE ARAUJO LIMA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,14/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Education. Gamification. Teaching-Learning Process.',PROCESSAMENTO DE IMAGENS,VERONICA TEICHRIEB,128,Gamificação;Educação;ensino-aprendizagem,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Este trabalho é resultado de um processo de pesquisa-ação vivenciado em uma escola-campo, com turmas dos anos finais do fundamental e do ensino médio, envolvendo, além da pesquisadora e dos alunos, oito professores de disciplinas diversas. A situação problema que deu origem ao projeto foi a apatia e desinteresse dos alunos em relação aos estudos e realização de atividades escolares, problema esse que vinha, sistematicamente, à tona nas reuniões de conselho de classe, demandando a busca de alternativas capazes de alavancar o engajamento e o protagonismo estudantil. A ludicidade, vista como meio de propiciar experiências prazerosas, ao mesmo tempo em que favorece o desenvolvimento de habilidades no uso de estratégias, apontou a gamificação, entendida como a aplicação de princípios usados nos jogos, como um possível caminho para a reversão daquele quadro. A experiência teve início no segundo semestre de 2014 e se estendeu até o final de 2016, passando por reformulações e ajustes a partir dos resultados de avaliações periódicas. O trabalho foi desenvolvido à luz de teóricos da psicologia, educação e \textit{desing} de jogos, e os resultados obtidos indicam satisfação com o modelo de gamificação construído e implementado por parte de professores e alunos. Entre esses destaca-se a melhoria no cumprimento de atividades extra-classe, na limpeza e organização das salas, bem como o desenvolvimento de habilidades como atenção focada, participação ativa e pontualidade. Como trabalhos futuros deseja-se criar um aplicativo para dispositivos móveis capaz de registrar os elementos desejados pelos professores, bem como uma cartilha que forneça subsídios para que os docentes criem seus próprios modelos de gamificação de ensino",DISSERTAÇÃO,Gamificação na Educação Básica: a construção de um modelo,5792084,1
"In this dissertation, we are concerned with the problem of counting mathematical objects
with regards to symmetry. Two major theorems in Combinatorics are Burnside’s
Lemma and Pólya’s Enumeration Theorem. Both theorems yield a formula that allows
one to compute the number of distinct mathematical objects with regards to symmetry.
Although Burnside’s Lemma is conceptually simpler, it presents a disadvantage in
that it has a high computational cost. Pólya’s Enumeration Theorem uses the concept
of cycle index and not only reduces the required amount of calculations but it also
allows for more complex problems to be solved. Moreover, the concept of cycle index
brings us information on each distinct pattern, which allows for a more complete description
of the problem. Building up from basic definitions taken from Group Theory,
a presentation of the theory leading up to the demonstration of Pólya’s Enumeration
Theorem is given. We conclude with several applications of this theory in different
types of problems to illustrate these concepts.",,TEORIA DA COMPUTAÇÃO,DEBORA VIRGINIA RAMOS BARBOSA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,08/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Burnsides Lemma. Cycle index. Orbits. Plyas Enumeration Theorem. Symmetries.',MATEMÁTICA COMPUTACIONAL,SOSTENES LUIZ SOARES LINS,88,índice de ciclos;Lema de Burnside;órbitas;simetrias;Teorema da Enumeração de Pólya.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Nesta dissertação, estamos preocupados com o problema de contar objetos matemáticos levando-se em conta as suas simetrias. Dois teoremas importantes na Área de Análise Combinatória são o Lema de Burnside e o Teorema da Enumeração de Pólya. Ambos fornecem uma fórmula matemática que permite calcular o número de objetos matemáticos distintos levando-se em conta as simetrias. O primeiro destes utiliza o conceito de órbitas para contar o número de objetos matemáticos. Embora o Lema de Burnside seja conceitualmente mais simples, ele apresenta a desvantagem de ter um alto custo computacional. O Teorema de Pólya utiliza o conceito de índice de ciclos e não só reduz a quantidade de cálculos necessária como também permite a resolução de problemas mais complexos. Além disso, o conceito de índice de ciclos nos trás informação sobre cada padrão distinto, o que permite uma descrição mais completa do problema.
A partir de definições básicas tomadas da Teoria dos Grupos, nós fornecemos uma apresentação da teoria que leva a demonstração do Teorema de Pólya. Concluímos com diversas aplicações desta teoria à diferentes tipos de problemas para ilustrar este conceito.",DISSERTAÇÃO,Teoria Enumerativa de Pólya,5869426,1
"Although two people share more than 99% of DNA, variations are extremely relevant for
determining phenotypic variations. Among these variations, single nucleotide polymorphisms
(SNPs) are punctual changes known to influence the increased risk of disease. SNPs can act
individually or through interactions with other SNPs (epistatic interactions). The inference of
epistatic interactions is a problem that has been extensively studied, using genomic data from
genome wide association studies (GWAS) with cases and controls patients. Several computational
approaches were proposed, using different strategies to deal with the challenges of inferring
the most relevant interactions. The first challenge found in this study is related to the large
amount of data (about 500 to 900 thousand SNPs). The second challenge is the number of
possible interactions between SNPs, which leads to a combinatorial problem. And the third
challenge is related to the low statistical power of the interactions, being more difficult to identify
them. The combination of these challenges makes this a hard problem to address. In this thesis,
different methodologies were used, they were selected to verify their abilities in dealing with
the problem of inference of the epistatic interactions. Among these, we evaluate techniques of
feature selection and computational approaches in the detection of interactions between SNPs, as
well as machine learning algorithms based on Relevance Learning Vector Quantization (RLVQ).
In the experiments performed, the RLVQ-based algorithms presented satisfactory results by
identifying the relevant interactions between SNPs in data with up to 5 interactions, using
relatively low computational requirements when compared to other approaches described in the
literature. A more extensive study was carried out with the objective of identifying an optimal
adjustment of the parameters and verifying the capacities and limitations of each algorithm. With
the results obtained through this adjustment of parameters, it was possible to raise hypotheses
regarding the influence of the amount of interactions between SNPs and the dimensionality of
the data as a function of the parameters used in the algorithms. Considering these analyzes, it
was possible to propose a new methodology called iGRLVQ-SNPi, based on RLVQ algorithms,
to deal more efficiently with the problem of inference of the interactions between the SNPs. With
iGRLVQ-SNPi, it was possible to evaluate n-order interactions, without it being necessary to
inform the number of interactions to be evaluated. In the experiments performed, iGRLVQ-SNPi
obtained an excellent accuracy in the different datasets tested, and was comparatively better or
as efficient as other evaluated epistatic inference approaches, using a lower computational cost.",,TEORIA DA COMPUTAÇÃO,FLAVIA ROBERTA BARBOSA DE ARAUJO,UNIVERSIDADE FEDERAL DE PERNAMBUCO,21/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'GWAS. SNPs. Epistatic Interaction. Feature Selection. Learning Vector Quantization.',ALGORITMOS E COMPLEXIDADE,KATIA SILVA GUIMARAES,127,"GWAS, SNPs, Interação, Feature Selection, Learning Vector Quantization",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),INTERAÇÕES ENTRE POLIMORFISMOS E SUA RELAÇÃO COM DOENÇAS,"Embora duas pessoas compartilhem semelhanças em mais do que 99% do seu DNA, existem ainda milhões de diferenças extremamente relevantes entre o genoma de dois indivíduos que podem estar associados a risco a doenças, diferentes padrões de resposta ao ambiente e medicamentos.
Dentre as diferenças encontradas no genoma destacam-se os polimorfismos de nucleotídeo único (SNP, do inglês Single Nucleotide Polimorphism). Os SNPs são conhecidos por influenciar no aumento no risco de doenças. A atuação dessas variações pode ser de forma
individual ou combinada com outros SNPs. Individualmente a influência sobre o risco de ocorrência de uma doença pode ser mais rara. Porém, quando combinados com outros fatores de risco ou outros SNPs, representam um maior risco de incidência de uma determinada doença.
Para a biologia, esse tipo de interação entre os SNPs influenciando um fenótipo é conhecida como epistasia.
A inferência das interações epistáticas é um problema que vem sendo amplamente estudado, e para isso são utilizados dados genotípicos oriundos de estudos de análise do genoma (GWAS) de pacientes casos e controles. Diversas abordagens computacionais buscam diferentes estratégias para inferir as interações mais relevantes nesses
conjuntos de dados.
No entanto, os principais desafios encontrados nesse estudo, esta primeiramente relacionado a grande quantidade de dados (cerca de 500 a 900 mil SNPs) em dados de amplo genoma. Um segundo desafio é o número de interações a serem investigadas sendo considerados interações de alta ordem, o que leva a um problema combinatorial e um terceiro e não menos importante se dá ao fato de que estas interações podem ter um baixo poder estatístico, tornando-as difíceis de serem identificadas. A combinação entre estes três entraves tornam este um problema muito difícil de ser tratado.
Com o objetivo de transpor estes desafios, diferentes estratégias de busca como métodos exaustivos a estocásticos utilizam diferentes métricas para avaliar a importância das interações. Além de fazer uso de técnicas de filtragem para reduzir informações irrelevantes dos dados.
Nesta Tese são estudados diferentes metodologias capazes de lidar com o problema da inferência da interações entre os SNPs. São estudadas técnicas de seleção de características e abordagens computacionais mais relevantes para a detecção das interações entre os SNPs e algoritmos baseados em learning vector quantization (LVQ), verificando suas deficiências frente aos diferentes desafios apresentados.
Os experimentos realizados mostraram que os algoritmo baseados em LVQ resultados promissores com requisitos computacionais relativamente baixos, no entanto, por não terem sido desenvolvidos com o propósito de realizar inferência das interações entre SNPs, apresentam certas deficiências para este tipo de aplicação, as quais são o principal objeto de estudo no momento. Diante disso, propomos uma nova metodologia denominada iGRLVQ-SNPi baseada em algoritmo de LVQ para lidar de forma eficiente com o problema da inferência das interações entre os SNPs.",TESE,Inferência de Polimorfismos de Nucleotídeo Único Utilizando Algoritmos Baseados em Relevance Learning Vector Quantization,5869860,1
"Knowing whether a room is occupied or not is crucial for improving electrical energy
efficiency. For instance, if a given room is empty there is usually no need for the lights to be
turned on. Usually in small spaces such as elevator halls, a Passive Infrared (PIR) sensor is
used together with the lighting, but as it lacks accuracy, people often are left in the dark after a
few minutes. Another factor that deteriorates energy efficiency is that these sensors are seldom
connected to a network, limiting the application scenarios to simple tasks, such as controlling
lamps. The same data could be used to improve other services such as adjusting the temperature
of an air conditioner, which usually has a high impact on energy costs in countries with warm
weather. In the present dissertation a wireless device capable of counting people in a room
is implemented using Infrared (IR) Light Emitting Diode (LED)s. The implemented device
is analyzed regarding energy consumption, cost, error count and installation time. It is also
compared to other existing solutions. An architecture for interfacing this device with the Internet
of Things (IoT) is provided as well as some of its applications in real scenarios. The results
show that the architecture provided as well as the device implemented are useful in the presented
scenarios, presenting a distance range of up to 30cm, a false negatives percentual error around
4% and an energy consumption of 1.519W.",,COMPUTAÇÃO INTELIGENTE,LEO HAPP BOTLER,UNIVERSIDADE FEDERAL DE PERNAMBUCO,02/03/2017,INGLES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Presence Sensors. Smart Lighting. Counting People. Occupancy Sensors. IoT.',PROCESSAMENTO DE IMAGENS,JUDITH KELNER,56,Sensores de presença. Iluminação inteligente. Contagem de pessoas. Sensores de ocupação. Internet das coisas.,CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),-,"Saber se um cômodo está ocupado ou não é crucial para melhorar a eficiência de energia
elétrica. Por exemplo, se um quarto está desocupado, geralmente, não há necessidade de as
lâmpadas estarem ligadas. Geralmente, em ambientes pequenos como em halls de elevador,
um sensor Infravermelho Passivo (PIR) é usado em conjunto com as lâmpadas, mas como
estes sensores não são precisos, as pessoas são frequentemente deixadas no escuro após alguns
minutos. Outro fator que prejudica a eficiência energética é que raramente estes sensores estão
conectados a uma rede, limitando os cenários de aplicação a tarefas simples, como controlar
lâmpadas, enquanto os dados do sensor poderiam ser utilizados para melhorar outros serviços,
como ajustar a temperatura de um aparelho de ar condicionado, que geralmente tem um alto
impacto nas contas de energia, em países quentes. Nesta dissertação, um dispositivo sem fio
capaz de contar pessoas em um quarto é implementado utilizando Diodos Emissores de Luz
(LED)s Infravermelhos (IR). O dispositivo implementado é analisado nos seguintes aspectos:
consumo de energia, custo, contagem de erros e tempo de instalação. Este também é comparado
a outras soluções existentes. Uma arquitetura para fazer a interface entre este dispositivo e a
Internet das Coisas (IoT) é fornecida, assim como alguns cenários em que esta pode ser aplicada.
Os resultados mostram que a arquitetura, assim como o dispositivo implementado são úteis nos
cenários apresentados, apresentando um alcance de 30cm, um percentual de erros do tipo falso
negativo da ordem de 4% e um consumo de energia de 1.519W.",DISSERTAÇÃO,AN IOT ARCHITECTURE FOR COUNTING PEOPLE,5870561,1
"Since the introduction of the concept of evidence based software engineering (EBSE) in 2004, systematic literature reviews and mapping studies have been widely applied as a research method in software engineering (SE). Three previous studies evaluated EBSE use by the scientific community through an analysis of 120 studies published between 1st January 2004 and 31st December 2009. It was verified that the reviews did not perform a quality assessment and they are failing in not providing guidelines to SE professionals. There was a huge increase on EBSE adoption as a research mechanism in SE in the last few years. This study has the main objective of evaluating the EBSE scenario between 2010 and 2015, focusing on the study quality, major topics covered and practice orientation. A mapping study was performed using automatic search strategies. The data extraction and synthesis was carried out, and the results were compared with findings from the previous tertiary studies. It was se-lected 545 secondary studies covering a variety of software engineering areas. Also, a huge increase in the number of researchers and institutions involved in EBSE publi-cations were noted. However, the overall quality of the studies is not improving and few papers provided practical advices to SE professionals. Although systematic re-views and mapping studies have been more accepted by the SE scientific community, a majority of the reviewed studies still do not perform the quality assessment. One of the main purposes of EBSE is to provide solutions and recommendations to SE pro-fessionals. However, there is a limitation in the knowledge transfer from academy to industry.",,ENGENHARIA DE SOFTWARE E LINGUAGENS DE PROGRAMAÇÃO,KENELLY SILVA RODRIGUES DE ALMEIDA,UNIVERSIDADE FEDERAL DE PERNAMBUCO,30/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DE PERNAMBUCO,b'Systematic literature review. Mapping Study. Software engineering. Ter-tiary study.',ENGENHARIA DE SOFTWARE,SERGIO CASTELO BRANCO SOARES,143,"Engenharia de Software, Engenharia de Software baseada em Evidências, Revisão Sistemática da Literatura, Mapeamento Sistemático, Estudo Terciário",CIÊNCIAS DA COMPUTAÇÃO (25001019004P6),INES - INSTITUTO NACIONAL DE CIÊNCIA E TECNOLOGIA PARA ENGENHARIA DE SOFTWARE,"O conceito de engenharia de software baseada em evidência (ESBE) foi introduzido em 2004 na conferência ICSE04. Desde então, revisão sistemática da literatura (RSL) tem sido amplamente utilizada como método de pesquisa em engenharia de software (ES). Um dos principais objetivos da ESBE é apresentar com base em evidências soluções e orientações para profissionais de engenharia de software em um determinado contexto. Três estudos foram conduzidos no período de 1º de janeiro de 2004 a 31 de dezembro de 2010 para avaliar a aplicação de ESBE na comunidade científica. No entanto, os resultados dos estudos terciários anteriores, evidenciam entre outros pontos, que as RSLs publicadas neste período não têm realizado avaliação de qualidade e têm falhado no fornecimento de guias para profissionais em ES. Nos últimos anos houve uma explosão no uso de RSL como mecanismo de pesquisa na ES. Este trabalho avalia a situação das RSL no período posterior a 2010, analisando a qualidade dos estudos, principais tópicos estudados e orientação à prática. Nossos resultados têm evidenciado um aumento considerável na utilização de RSL como método de pesquisa em engenharia de software, sendo comparados e integrados com os estudos terciários anteriores.",DISSERTAÇÃO,Uma Avaliação do Crescente uso de Revisões Sistemática da Literatura na Engenharia de Software,6055505,1
"Modularity Density Maximization is a graph clustering problem which avoids the resolution limit degeneracy of the Modularity Maximization problem. This thesis aims at solving larger instances than current Modularity Density heuristics do, and show how close the obtained solutions are to the expected clustering. Three main contributions arise from this objective. The first one is about the theoretical contributions about properties of Modularity Density based prioritizers. The second one is the development of eight Modularity Density Maximization heuristics. Our heuristics are compared with optimal results from the literature, and with GAOD, iMeme-Net, HAIN, BMD-λ heuristics. Our results are also compared with CNM and Louvain which are heuristics for Modularity Maximization that solve instances with thousands of nodes. The tests were carried out by using graphs from the “Stanford Large Network Dataset Collection”. The experiments have shown that our eight heuristics found solutions for graphs with hundreds of thousands of nodes. Our results have also shown that five of our heuristics surpassed the current state-of-the-art Modularity Density Maximization heuristic solvers for large graphs. A third contribution is the proposal of six column generation methods. These methods use exact and heuristic auxiliary solvers and an initial variable generator. Comparisons among our proposed column generations and state-of-the-art algorithms were also carried out. The results showed that: (i) two of our methods surpassed the state-of-the-art algorithms in terms of time, and (ii) our methods proved the optimal value for larger instances than current approaches can tackle. Our results suggest clear improvements to the state-of-the-art results for the Modularity Density Maximization problem.",,Teoria da Computação,RAFAEL DE SANTIAGO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,12/06/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Clustering;modularity density maximization;heuristic search;multilevel heuristics;local search;column generation',LINHA LÓGICA E MODELOS DE COMPUTAÇÃO,LUIS DA CUNHA LAMB,158,Clustering;modularity density maximization;heuristic search;multilevel heuristics;local search;column generation,COMPUTAÇÃO (42001013004P4),-,"Modularity Density Maximization is a graph clustering problem which avoids the resolution limit degeneracy of the Modularity Maximization problem. This thesis aims at solving larger instances than current Modularity Density heuristics do, and show how close the obtained solutions are to the expected clustering. Three main contributions arise from this objective. The first one is about the theoretical contributions about properties of Modularity Density based prioritizers. The second one is the development of eight Modularity Density Maximization heuristics. Our heuristics are compared with optimal results from the literature, and with GAOD, iMeme-Net, HAIN, BMD-λ heuristics. Our results are also compared with CNM and Louvain which are heuristics for Modularity Maximization that solve instances with thousands of nodes. The tests were carried out by using graphs from the “Stanford Large Network Dataset Collection”. The experiments have shown that our eight heuristics found solutions for graphs with hundreds of thousands of nodes. Our results have also shown that five of our heuristics surpassed the current state-of-the-art Modularity Density Maximization heuristic solvers for large graphs. A third contribution is the proposal of six column generation methods. These methods use exact and heuristic auxiliary solvers and an initial variable generator. Comparisons among our proposed column generations and state-of-the-art algorithms were also carried out. The results showed that: (i) two of our methods surpassed the state-of-the-art algorithms in terms of time, and (ii) our methods proved the optimal value for larger instances than current approaches can tackle. Our results suggest clear improvements to the state-of-the-art results for the Modularity Density Maximization problem.",TESE,Efficient Modularity Density Heuristics in Graph Clustering and Their Applications,5028800,01
"Embedded processors rely on the efficient use of instruction-level parallelism to answer the performance and energy needs of modern applications. Though improving performance is the primary goal for processors in general, it might lead to a negative impact on energy consumption, a particularly critical constraint for current systems. In this dissertation, we present SoMMA, a software-managed memory architecture for embedded multi-issue processors that can reduce energy consumption and energy-delay product (EDP), while still providing an increase in memory bandwidth. We combine the use of software-managed memories (SMM) with the data cache, and leverage the lower energy access cost of SMMs to provide a processor with reduced energy consumption and EDP. SoMMA also provides a better overall performance, as memory accesses can be performed in parallel, with no cost in extra memory ports. Compiler-automated code transformations minimize the programmer’s effort to benefit from the proposed architecture. Our experimental results show that SoMMA is more energy- and performance-efficient not only for the processing cores, but also at full-system level. Comparisons were done using the rVEX processor, a VLIW reconfigurable processor. The approach shows average speedups of 1.118x and 1.121x, while consuming up to 11% and 12.8% less energy when comparing two modified processors and their baselines. SoMMA also shows reduction of up to 41.5% on full-system EDP, maintaining the same processor area as baseline processors. Lastly, even with SoMMA halving the data cache size, we still reduce the number of data cache misses in comparison to baselines.",,Sistemas de Computação,TIAGO TREVISAN JOST,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,25/10/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Code generation process;instruction-level parallelism;memory bandwidth limitation;multi-issue processors;software-managed memory',LINHA ARQUITETURAS NÃO CONVENCIONAIS,LUIGI CARRO,82,geração  de  código;paralelismo  a  nível  de  instrução;limite  na  banda  de  memória;processador de despacho múltiplo;memória gerenciado por software,COMPUTAÇÃO (42001013004P4),-,"Processadores embarcados utilizam  eficientemente o paralelismo a nível de instrução para 
atender as necessidades de desempenho e energia em aplicações atuais. Embora a melhoria de 
performance seja um dos principais objetivos em processadores em geral, ela pode levar a um 
impacto  negativo  no  consumo  de  energia,  uma  restrição  crítica  para  sistemas  atuais.  Nesta 
dissertação,  apresentamos o SoMMA, uma arquitetura de memória  gerenciada  por software 
para processadores embarcados capaz de reduz consumo de energia e  energy-delay product
(EDP), enquanto ainda aumenta a banda de memória. A solução combina o uso de memórias 
gerenciadas por software com a cache de dados, de modo a reduzir o consumo de energia e 
EDP  do  sistema.  SoMMA  também  melhora  a  performance  do  sistema,  pois  os  acessos  à 
memória podem ser realizados em paralelo, sem custo em portas de memória extra na cache de 
dados. Transformações de código do compilador auxiliam o programador a utilizar a arquitetura 
proposta.  Resultados  experimentais  mostram  que  SoMMA  é  mais  eficiente  em  termos  de 
energia  e  desempenho  tanto  a  nível  de  processador  quanto  a  nível  do  sistema  completo.  A 
técnica  apresenta  speedups  de  1.118x  e  1.121x,  consumindo  11%  e  12.8%  menos  energia 
quando comparando processadores que utilizam e não utilizam SoMMA. Há ainda redução de 
até  41.5%  em  EDP do sistema,  sempre  mantendo a área dos processadores equivalentes. Por 
fim,  SoMMA  também  reduz  o  número  de  cache  misses  quando  comparado  ao  processador
baseline.",DISSERTAÇÃO,SoMMA – A Software-managed Memory Architecture for Multi-issue Processors.,5127200,01
"This thesis  presents application-driven temperature-aware solutions for next generation video 
coding systems, such as  the High Efficiency Video Coding (HEVC). Different from state-ofthe-art works, the proposed solutions raise the abstraction of temperature management to the 
application-level, where video coding characteristics and video content properties are used to 
leverage  thermal-aware  solutions  for  video  coding  with  low  QoS  (Quality  of  Service) 
degradation. Several video coding and temperature analyses are performed to understand the 
behavior  of  temperature  when  encoding  different  video  sequences.  Based  on  the  analyses 
results, different approaches are proposed to mitigate the temperature effects on video coding 
systems.  Application-driven  temperature  management  for  HEVC  uses  run-time  encoder 
configuration selection to keep temperature under safe operational state while providing good 
visual  quality results.  Temperature optimization using approximate computing uses contentdriven  approximations  to  reduce  the  on-chip  temperature  of  HEVC  encoding.  Applicationdriven  temperature-aware  scheduler  leverages  application-specific  knowledge  to  guide  a 
scheduling  technique  targeting  reducing  the  spatial  temperature  gradients  that  are  resulted 
from  the  unbalance  workload  nature  of  multi-threaded  video  coding  application.  The 
proposed  solutions  are  able  to  provide  up  to  10  °C  of  chip  temperature  reduction  with 
negligible  compression  efficiency  loss.  Besides,  when  compared  with  previous  works  the 
resulted  objective  video  quality  (PSNR)  is  from  12  dB  up  to  20  dB  higher.  Moreover,  the 
proposed  scheduler  eliminates  spatial  temperature  gradients  greater  than  5  ºC  of  multi-core 
architectures.  As  conclusion,  this  thesis  demonstrates  that  leveraging  application-specific 
knowledge  and  video  content  properties  has  a  significant  potential  to  improve  temperature 
profiles of video coding systems while still keeping good quality results.",,Projeto de Sistemas Eletrônicos e Computacionais,DANIEL MUNARI VILCHEZ PALOMINO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,10/04/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Temperature management;video coding;HEVC;application-driven;temperatureaware;application  knowledge;temperature  gradients;hardware  platforms;architectures;integrated circuits',LINHA CONCEPÇÃO DE CIRCUITOS E SISTEMAS INTEGRADOS,ALTAMIRO AMADEU SUSIN,87,Gerenciamento de temperatura;codificação de vídeo;HEVC;conhecimento da aplicação;gradientes de temperatura,COMPUTAÇÃO (42001013004P4),-,"Esta tese apresenta soluções para o gerenciamento e otimização de temperatura para sistemas de codificação de vídeo baseados nas características da aplicação e no conteúdo dos vídeos digitais. Diferente dos trabalhos estado-da-arte, as soluções propostas nesta tese focam em técnicas de gerenciamento de temperatura no nível da aplicação e características da aplicação codificação de vídeo e as propriedades dos vídeos digitais são explorados para desenvolver soluções termais para a codificação de vídeo com baixas perdas na qualidade de serviço  as aplicações. Diversas análises são realizadas considerando a aplicação de codificação de vídeo para entender o comportamento da temperatura durante o processo de codificação para diferentes sequências de vídeo. Com base nos resultados das análises, soluções com diferentes abordagens são propostas para atenuar os efeitos da temperatura nos sistemas de codificação de vídeo. Gerenciamento de temperatura baseado nas características da aplicação para o padrão de codificação HEVC usa uma técnica de seleção de configuração em tempo de execução para manter a temperatura abaixo dos limites seguros de operação com bons resultados de qualidade de vídeo. Otimização de temperatura baseado em computação imprecisa usa aproximações baseadas em conteúdo para reduzir a temperatura de chips executando o HEVC. Um escalonador de tarefas que usa características da aplicação para guiar o escalonamento de threads focando na redução dos gradientes espaciais de temperatura que são resultantes do desbalanceamento natural de cargas entre as threads codificando vídeos digitais. As soluções propostas são capazes de reduzir em até 10 oC a temperatura do chip com perdas insignificantes na eficiência de compressão. Os resultados de qualidade objetiva (medida usando PSNR) são de 12 dBs até 20 dBs maiores quando comparados com trabalhos da literatura. Além disso, o escalonador de tarefas proposto é capaz de eliminar os gradientes espaciais de temperatura maiores que 5 oC para arquitetura multi-cores. Como principal conclusão, esta tese demonstra que as técnicas de gerenciamento de temperatura que usam o conhecimento da aplicação de maneira conjunta com as propriedades dos vídeos digitais tem um alto potencial para melhorar os resultados de temperatura de sistemas de codificação de vídeo mantendo bons resultados de qualidade visual dos vídeos codificados.",TESE,Application-driven temperature-aware solutions for video coding,5006922,01
"This dissertation presents two different defocus blur estimation methods for still images. Both methods assume a Gaussian Point Spread Function (PSF) and explore the ratio of gradient magnitudes of reblurred images computed at edge location with different scales, which provides a closed form mathematical formulation for the local blur assuming continuous-time signals. The first approach computes 1D profiles along edge points orthogonal to the local contour, and evaluate the location of the edge (maximum of the derivative) to adaptively select the number of reblurring scales. Considering the time consumption of exploring 1D oriented edge profiles, a second method was proposed based on 2D multiscale image gradients, and local reblurring parameters were selected based on the agreement of an edge detector computed at several scales. Given an initial estimate of the blur scale at edge locations provided by either of these two methods, a correction step that accounts for the discretization of the continuous formulation is also proposed. A novel local filtering method that smooths the refined estimates along the image contours is also proposed, and a fast joint domain filter is explored to propagate blur information to the whole image to generate the full blur map. Experimental results on synthetic and real images show that the proposed methods have promising results for defocus blur estimation, with a good trade off between running time and accuracy when compared to state-of-the art defocus blur estimation methods. To deal with blurry video sequences, temporal consistency was also included in the proposed model. More precisely, Kalman Filters were applied to generate smooth temporal estimates for each pixel when the local appearance of the video sequence does not vary much, and allowing sharp transitions during drastic local appearance changes, which might relate to occlusions/disocclusions. Finally, this dissertation also shows applications of the proposed methods for image and video blur estimation. A new image retargeting method is proposed for photos taken by a shallow Depth of Field (DoF) camera. The method includes defocus blur information with the seam carving framework aiming to preserve in-focus objects with better visual quality. Assuming the in-focus pixels related to regions of interest of a blurry image, the proposed retargeting method starts with a cropping method, which removes the unimportant parts (blurry) parts of the image, thenthe seam carving method is applied with a novel energy function that prioritizes in-focus regions. Experimental results show that the proposed blur aware retargeting method works better at preserving in-focus objects than other well known competitive retargeting methods. The dissertation also explores the proposed blur estimation method in the context of image and video deblurring, and results were compared with several other blur estimation methods. The obtained results show that metrics typically used to evaluate blur estimation methods (e.g. Mean Absolute Error) might not be correlated with the quality of deblurred image metrics, such as Peak Signal to",,Computação Visual,ALI KARAALI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,17/03/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Spatially Varying Defocus;Optical Blur;Deblurring;Image retargeting',LINHA PROCESSAMENTO DE IMAGENS E VISÃO COMPUTACIONAL E RECONHECIMENTO DE PADRÕES,CLÁUDIO ROSITO JUNG,121,Desborramento.;Estimação de borramento;Remapeamento de imagens,COMPUTAÇÃO (42001013004P4),-,"Esta tese apresenta dois métodos diferentes de estimativa de desfocagem usando uma única imagem. Ambos os métodos assumem uma função de espalhamento de ponto (Point Spread Function - PSF) Gaussiana e exploram a razão de magnitudes de gradientes de versões re-borradas da imagem original com escalas diferentes nas bordas da imagem, o que fornece uma expressão matemática fechada para borramento local. A primeira abordagem calcula perfis 1D ao longo de pontos de borda ortogonais ao contorno local, e avalia a localização da borda (máximo da derivada primeira) para selecionar adaptativamente o número de escalas no re-borramento. Considerando o consumo de tempo de explorar perfis de aresta orientados 1D, um
segundo método foi proposto com base em gradientes de imagem diretamente no domínio 2D, e os parâmetros de re-borramento locais foram selecionados com base na concordância de um detector de bordas calculado em várias escalas. Dada uma estimativa inicial da escala de desfocagem nas posições de borda proporcionada por qualquer um destes dois métodos, é também proposto um passo de correção que atenua os erros introduzidos pela discretização da formulação contínua.
Um novo método de filtragem local que suaviza as estimativas refinadas ao longo dos contornos de imagem também é proposto, e um filtro de domínio conjunto (jointdomain filter ) rápido é explorado para propagar informações de desfocagem para toda a imagem, gerando o mapa de desfocagem completo. Os resultados experimentais em imagens sintéticas e reais mostram que os métodos propostos apresentam
resultados promissores para a estimativa de borramento por desfoco, com um bom compromisso entre qualidade e tempo de execução quando comparados a técnicas estado-da-arte.
Para lidar com sequências de vídeo desfocadas, a consistência temporal também foi incluída no modelo proposto. Mais precisamente, Filtros de Kalman foram aplicados para gerar estimativas temporais suaves para cada pixel quando a aparência local da sequência de vídeo não varia muito, permitindo transições durante mudanças drásticas da aparência local, que podem se relacionar com oclusões/desoclusões.
Finalmente, esta tese também mostra aplicações dos métodos propostos para a estimativa de desfocagem de imagem e vídeo. Um novo método de redimensionamento (retargeting ) de imagens é proposto para fotos tiradas por câmera com baixa profundidade de campo. O método inclui informação de desfocamento local no contexto do método seam carving, visando preservar objetos em foco com melhor qualidade
visual. Assumindo que os pixels em foco estejam relacionados às regiões de interesse de uma imagem com desfocamento, o método de redimensionamento proposto começa com um método de corte (cropping ), o qual remove as partes sem importância (borradas) da imagem, e então o método seam carving é aplicado com uma
nova função de energia que prioriza as regiões em foco. Os resultados experimentais mostram que o método proposto funciona melhor na preservação de objetos em foco do que outras técnicas de redimensionamento de imagens. A tese também explora o método de estimação de desfocagem proposto no contexto
de des-borramento de imagens e sequências de vídeo, e os resultados foram comparados com vários outros métodos de estimação de desfocagem. Os resultados obtidos mostram que as métricas tipicamente usadas para avaliar métodos de estimação de desfocagem (por exemplo, erro absoluto médio) podem não estar correlacionadas com a qualidade das métricas de imagem desfocada, como a Relação Sinal-Ruído de
Pico.",TESE,Spatially Varying Defocus Blur Estimation and Applications,5006988,01
"Performance interference has been a well-known problem in datacenter networks (DCNs) and one that remains a constant topic of discussion in the literature. Several measurement studies concluded that throughput achieved by virtual machines (VMs) in current datacenters can vary by a factor of five or more, leading to poor and unpredictable overall application performance. Recent efforts have proposed techniques that present some shortcomings, such as underutilization of resources, significant management overhead or negligence of non-network resources. In this thesis, we introduce three proposals that address performance interference in DCNs: IoNCloud, Predictor and Packer. IoNCloud leverages the key observation that temporal bandwidth demands of cloud applications do not peak at exactly the same time. Therefore, it seeks to provide predictable and guaranteed performance while minimizing network underutilization by (a) grouping applications in virtual networks (VNs) according to their temporal network usage and need of isolation; and (b) allocating these VNs on the cloud substrate. Despite achieving its objective, IoNCloud does not provide work-conserving sharing among VNs, which limits utilization of idle resources. Predictor, an evolution over IoNCloud, dynamically programs the network in Software-Defined Networking (SDN)-based DCNs and uses two novel algorithms to provide network guarantees with work-conserving sharing. Furthermore, Predictor is designed with scalability in mind, taking into consideration the number of entries required in flow tables and flow setup time in DCNs with high turnover and millions of active flows. IoNCloud and Predictor neglect resources other than the network at allocation time. This leads to fragmentation of non-network resources and, consequently, results in less applications being allocated in the infrastructure. Packer, in contrast, aims at providing predictable and guaranteed network performance while minimizing overall multi-resource fragmentation. Extending the observation presented for IoNCloud, the key insight for Packer is that applications have complementary demands across time for multiple resources. To enable multi-resource allocation, we devise (i) a new abstraction for specifying temporal application requirements (called Time-Interleaved Multi-Resource Abstraction - TI-MRA); and (ii) a new allocation strategy. We evaluated IoNCloud, Predictor and Packer, showing their benefits and overheads. In particular, all of them provide predictable and guaranteed network performance; Predictor reduces flow table size in switches and flow setup time; and Packer minimizes multi-resource fragmentation.",,Redes de Computadores,DANIEL STEFANI MARCON,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,21/03/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Datacenter networks;Performance interference;Bandwidth guarantees;Work-conserving sharing.',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",ANTONIO MARINHO PILLA BARCELLOS,205,Redes de datacenter;Interferência de desempenho;Garantias de largura de banda;Compartilhamento com conservação de trabalho.,COMPUTAÇÃO (42001013004P4),-,"A interferência de desempenho é um desafio bem conhecido em redes de datacenter (DCNs),
permanecendo um tema constante de discussão na literatura. Diversos estudos concluíram que a
largura de banda disponível para o envio e recebimento de dados entre máquinas virtuais (VMs)
pode variar por um fator superior a cinco, resultando em desempenho baixo e imprevisível para
as aplicações. Trabalhos na literatura têm proposto técnicas que resultam em subutilização de
recursos, introduzem sobrecarga de gerenciamento ou consideram somente recursos de rede.
Nesta tese, são apresentadas três propostas para lidar com a interferência de desempenho em
DCNs: IoNCloud, Predictor e Packer. O IoNCloud está baseado na observação que diferentes
aplicações não possuem pico de damanda de banda ao mesmo tempo. Portanto, ele busca prover desempenho previsível e garantido enquanto minimiza a subutilização dos recursos de rede.
Isso é alcançado por meio (a) do agrupamento de aplicações (de acordo com os seus requisitos
temporais de banda) em redes virtuais (VNs); e (b) da alocação dessas VNs no substrato físico.
Apesar de alcançar os seus objetivos, ele não provê conservação de trabalho entre VNs, o que
limita a utilização de recursos ociosos. Nesse contexto, o Predictor, uma evolução do IoNCloud,
programa dinamicamente a rede em DCNs baseadas em redes definidas por software (SDN) e
utiliza dois novos algoritmos para prover garantias de desempenho de rede com conservação
de trabalho. Além disso, ele foi projetado para ser escalável, considerando o número de regras em tabelas de fluxo e o tempo de instalação das regras para um novo fluxo em DCNs com
milhões de fluxos ativos. Apesar dos benefícios, o IoNCloud e o Predictor consideram apenas
os recursos de rede no processo de alocação de aplicações na infraestrutura física. Isso leva à
fragmentação de outros tipos de recursos e, consequentemente, resulta em um menor número
de aplicações sendo alocadas. O Packer, em contraste, busca prover desempenho de rede previsível e garantido e minimizar a fragmentação de diferentes tipos de recursos. Estendendo a
observação feita ao IoNCloud, a observação-chave é que as aplicações têm demandas complementares ao longo do tempo para múltiplos recursos. Desse modo, o Packer utiliza (i) uma nova
abstração para especificar os requisitos temporais das aplicações, denominada TI-MRA (TimeInterleaved Multi-Resource Abstraction); e (ii) uma nova estratégia de alocação de recursos.
As avaliações realizadas mostram os benefícios e as sobrecargas do IoNCloud, do Predictor e
do Packer. Em particular, os três esquemas proveem desempenho de rede previsível e garantido;
o Predictor reduz o número de regras OpenFlow em switches e o tempo de instalação dessas
regras para novos fluxos; e o Packer minimiza a fragmentação de múltiplos tipos de recursos.",TESE,"Achieving Predictable, Guaranteed and Work-Conserving Performance in Datacenter Networks",5006994,01
"Collective behavior detection and pedestrian tracking present many applications, specially in surveillance systems. In this dissertation, we proposed a complete pipeline for achieving robust tracking and collective behavior recognition based on calibrated static cameras. To remove the necessity of manual calibration, we first present a fully automatic self-calibration system that explores pedestrian detection results and background removal at non-consecutive frames in order to calibrate a static camera using a non-linear cost function. We also propose the use of camera calibration to generate geometrically coherent candidates for pedestrian detection. Our approach aims to reduce the scale range typically used in sliding-window techniques, which leads to less feature extractions and decreased number of false positives. Then, we propose a multi-target pedestrian tracking algorithm using a calibrated static camera. The tracking approach explores color histograms to track patches of each target. Obtained displacement vectors are combined with the expected motion of pedestrians in the world coordinate system. The proposed tracker also incorporates pedestrian detector results to improve the system’s accuracy and its ability to recover from failure. Finally, we propose a two-layered approach for collective behavior recognition based on Random Forests classifiers. In the first level, we use inter-personal distances and relative speeds computed in the world coordinate system to classify asymmetrical pair interactions. Those interactions are combined with group shape dynamics and mean velocity to recognize the collective behavior. We devise a set of experiments to attest the quality of our approaches using publicly available datasets. Results show competitive results against state-of-the-art techniques, and particularly good generalization across different databases.",,Computação Visual,GUSTAVO FUHR,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,08/03/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Pedestrian tracking;people detection;collective behavior;group activity;self- calibration;surveillance systems',LINHA PROCESSAMENTO DE IMAGENS E VISÃO COMPUTACIONAL E RECONHECIMENTO DE PADRÕES,CLÁUDIO ROSITO JUNG,101,rastreamento de pedestres;detecção de pessoas;análise de comportamento;calibração de câmera vigilância de vídeo,COMPUTAÇÃO (42001013004P4),-,"A análise de comportamento coletivo e rastreamento de pedestres apresentam diversas aplicações,
especialmente em sistemas de vigilância inteligente. Neste trabalho é proposta uma solução
compreensiva com objetivo de atingir rastreamento de pedestre e reconhecimento de atividade
coletiva de maneira robusta baseada na utilização de câmeras calibradas.
Primeiramente, com o objetivo de remover a necessidade de calibração manual, nós apresentamos
um método de calibração automática que explora detectores de pedestres e remoção de fundo
para calibragem baseada em otimização não-linear. Adicionalmente, nós propomos a utilização
da matriz de calibração para gerar candidatos coerentes com a geometria de cena em detectores
de pedestres. Nossa abordagem tem como objetivo diminuir o intervalo de escalas comumente
utilizado em detectores baseados em janelas deslizantes, gerando um número menor de extrações
de atributos e reduzindo o número de falsos positivos na detecção.
Em seguida, nós propomos um método de rastreamento de múltiplos pedestres utilizando
câmeras calibradas. Nossa abordagem explora histogramas de cor para rastrear os pequenas
regiões (patches) de cada alvo. Os vetores de deslocamento obtidos através do pareamento de
atributos de aparência são combinados com um vetor obtido através de um preditor de movimento
em coordenadas de mundo. Adicionalmente, nós incluímos informações originárias de detectores
de pedestres para aumentar a acurácia do sistema e sua habilidade de recuperação a falhas.
Por fim, nós propomos uma abordagem hierárquica de duas camadas para o problema de
reconhecimento de atividade coletiva baseada no uso de classificadores Random Forests. No
primeiro nível da técnica proposta, nós utilizamos distâncias entre pares de pessoas e suas
respectivas velocidades relativas para classificar interações de pares. Estas interações são
combinadas com a dinâmica do formato do grupo observado (e sua respectiva velocidade) para o
reconhecimento de atividades coletivas. Os experimentos realizados neste trabalho demonstram
a qualidade de nossas abordagens em sequências de vídeos disponíveis publicamente. Nossos
resultados mostram serem competitivos quando comparados com técnicas do estado da arte
e, particularmente, apresentam uma boa generalização entre diferentes cenários de captura de
vídeo.",TESE,Pedestrian Tracking and Collective Behavior Recognition,5007010,01
"A huge volume of data is produced every day, from the information provided by social networks (such as Facebook, Instagram, Whatsapp, etc.in terms of) or that generated by sensors on mobile devices, including Big Data applications like Google Searches. This deluge of data requires more computational resources to process the information more quickly. Although Cloud has grown rapidly in recent years, it still suffers from a lack of standardization and management resources. This complex scenario raises enormous challenges for new systems and infrastructure kinds. On the other hand, it provides several opportunities for the researcher to find solutions for Big Data Analytics. This work establishes: i) a new platform called SMART which offers Big Data Analytics in a Lambda architecture within a hybrid infrastructure; ii) presents a simulator called BIGhybrid to be a toolkit for the study of Big Data Analytics in hybrid infrastructures. Its goal is to enable the user achieves the nearest configuration for Big Data applications into deployment in real-world environments. In addition, defines data distribution strategies in this complex scenario for reducing the risks of trouble caused by common configuration mistakes; iii) evaluates the use of the Dispatcher module in the SMART platform and iv) defines strategies for the use of Desktop Grid and Cloud Computing in a geo-distributed environment within a hybrid infrastructure.",,Sistemas de Computação,JULIO CESAR SANTOS DOS ANJOS,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,08/02/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Big Data;Hybrid Systems;MapReduce;Distributed Systems;Cloud Computing;Desktop Grid',LINHA COMPUTAÇÃO DE ALTO DESEMPENHO E SISTEMAS DISTRIBUÍDOS,CLAUDIO FERNANDO RESIN GEYER,150,Big Data;MapReduce;Infra-estrutura Híbrida;Sistemas Distribuídos;Comptação em Nuvem;Grade de Desktop,COMPUTAÇÃO (42001013004P4),-,"Um grande volume de dados é produzido todos os dias, desde informações fornecidas
por redes sociais (tais como Facebook, Instagram, Whatsapp, etc) ou geradas por sensores
em dispositivos móveis, até aplicações Big Data como a busca do Google. Esta inundação de dados requer cada vez mais recursos computacionais para processar informações
mais rapidamente. Embora Cloud tenha crescido rapidamente nos últimos anos, ela ainda
sofre com falta de padronização e gerenciamento de recursos adequados. Os usuários que
necessitam executar aplicações podem não saber como mapear seus requisitos de sistemas para os recursos disponíveis. Esta falta de conhecimento sobre a infraestrutura dos
provedores de nuvem leva a superestimar ou subestimar a capacidade de processamento
necessária para as tarefas. Este cenário complexo apresenta enormes desafios para os pesquisadores em termos de sistemas e tipos de infraestruturas. Por outro lado, ele oferece
várias oportunidades para o pesquisador encontrar soluções para a análise de Big Data.
Este trabalho estabelece: i) uma nova plataforma chamada SMART que oferece a análise
de Big Data em uma arquitetura Lambda sobre uma infraestrutura híbrida; ii) apresenta
um simulador chamado BIGhybrid para ser um conjunto de ferramentas para o estudo da
análise de Big Data em infraestruturas híbridas. Este permite que o usuário encontre as
configurações mais próxima para as aplicações Big Data na implantação em ambientes
reais. Ainda, define estratégias para a distribuição de dados neste cenário complexo para
reduzir os riscos de problemas causados por erros comuns de configurações; iii) avalia
o uso do módulo Despachante na plataforma SMART e iv) define estratégias para o uso
de Desktop Grid e computação em nuvem em um ambiente geo-distribuído em uma infraestrutura híbrida. O objetivo é encontrar algumas das restrições a uma qualidade de
serviços (QoS) aceitável. Tais restrições estão relacionadas com a relação entre máquinas
voluntárias e nós estáveis, distribuição de dados, estratégias de balanceamento da carga
e assim por diante. Embora isto possa ser construído em um ambiente real, uma avaliação experimental em larga escala é somente possível através de simulação devido às
características de reprodutibilidade e previsibilidade de características ambientais. Os experimentos indicam um bom desempenho da plataforma SMART em baixa escala em um
ambiente real.",TESE,Big Data using Hybrid Infrastructures such as Cloud and Desktop Grid,5007032,01
"This thesis’ original contribution is a novel algorithm which integrates a data-efficient function approximator with reinforcement learning in continuous state spaces. The complete research includes the development of a scalable online and incremental algorithm capable of learning from a single pass through data. This algorithm, called Fast Incremental Gaussian Mixture Network (FIGMN), was employed as a sample-efficient function approximator for the state space of continuous reinforcement learning tasks, which, combined with linear Q-learning, results in competitive performance. Then, this same function approximator was employed to model the joint state and Q-values space, all in a single FIGMN, resulting in a concise and data-efficient algorithm, i.e., a reinforcement learning algorithm that learns from very few interactions with the environment. A single episode is enough to learn the investigated tasks in most trials. Results are analysed in order to explain the properties of the obtained algorithm, and it is observed that the use of the FIGMN function approximator brings some important advantages to reinforcement learning in relation to conventional neural networks.",,Inteligência Artificial,RAFAEL COIMBRA PINTO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,22/03/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Reinforcement Learning;Neural Networks;Gaussian Mixture Models',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",PAULO MARTINS ENGEL,116,Modelos de Mistura de Gaussianas;Aprendizagem por Reforço;Redes Neurais,COMPUTAÇÃO (42001013004P4),-,"A contribução original desta tese é um novo algoritmo que integra um aproximador de
funções com alta eficiência amostral com aprendizagem por reforço em espaços de estados contínuos. A pesquisa completa inclui o desenvolvimento de um algoritmo online
e incremental capaz de aprender por meio de uma única passada sobre os dados. Este
algoritmo, chamado de Fast Incremental Gaussian Mixture Network (FIGMN) foi empregado como um aproximador de funções eficiente para o espaço de estados de tarefas
contínuas de aprendizagem por reforço, que, combinado com Q-learning linear, resulta
em performance competitiva. Então, este mesmo aproximador de funções foi empregado
para modelar o espaço conjunto de estados e valores Q, todos em uma única FIGMN,
resultando em um algoritmo conciso e com alta eficiência amostral, i.e., um algoritmo de
aprendizagem por reforço capaz de aprender por meio de pouquíssimas interações com o
ambiente. Um único episódio é suficiente para aprender as tarefas investigadas na maioria
dos experimentos. Os resultados são analisados a fim de explicar as propriedades do algoritmo obtido, e é observado que o uso da FIGMN como aproximador de funções oferece
algumas importantes vantagens para aprendizagem por reforço em relação a redes neurais
convencionais.",TESE,Continuous Reinforcement Learning with Incremental Gaussian Mixture Models,5007039,01
"Many  computing  applications  imply  dealing  with  network  data,  for  example,  social 
networks,  communications  and  computing  networks,  epidemiological  networks,  among 
others.  These  applications  are  based  on  multivariate  graphs  representing  items  and 
relationships  characterized  by  multiple  attributes.  Most  of  the  visualization  techniques 
described  in  the  literature  for  dealing  with  multivariate  graphs  focus  either  on  problems 
associated with the visualization of topology  or on problems associated with the visualization 
of multiple attributes of items, separated from the graph topology. During the exploration of 
multivariate  graphs,  users  might  get  benefit  of  combining  these  diverse  visualization 
techniques.  In order to support users during that exploration, this thesis proposes an approach 
that  allows  users  to  combine  diverse  visualization  techniques  while  keeping  track  of  the 
history  of  chained  visualizations  in  an  integrated  way.  Users  are  able  to  compare  results 
provided by different visualization techniques, and thus the tools provide the synergism one 
needs to fully comprehend the data set. Three techniques were embedded in the approach. The 
first one emphasizes the visualization of relations between the attributes of nodes belonging to 
clusters, and thus is called ClusterVis. The second one is named  GlyphMatrix, and explores 
the  use  of  glyphs  and  adjacency  matrices  as  an  alternative  representation  of  the  relation 
between  the  attributes  of  edges.  Finally,  a  third  technique  (Iris)  provides  features  for  the visualization of attributes of edges of adjacent nodes.",,Computação Visual,RICARDO ANDRADE CAVA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,17/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Information visualization;Multivariate graphs;Cluster visualization',LINHA COMPUTAÇÃO GRÁFICA E VISUALIZAÇÃO DE DADOS,CARLA MARIA DAL SASSO FREITAS,145,Visualização de informações;Grafos multivariados;Visualização de agrupamentos,COMPUTAÇÃO (42001013004P4),-,"Muitas aplicações tratam dados estruturados na forma de grafos, como, por exemplo, redes sociais, redes de computação e comunicação, redes epidemiológicas, entre outras. Essas aplicações são baseadas em grafos multivariados representando itens e relacionamentos caracterizados por múltiplos atributos. A maioria das técnicas descritas na literatura para lidar com grafos multivariados concentram-se em problemas associados com visualização da topologia ou em problemas associados com a visualização de múltiplos atributos de itens separados da topologia do grafo. Durante a exploração de grafos multivariados, os usuários podem se beneficiar da combinação de diversas técnicas de visualização. A fim de apoiar os usuários durante essa exploração, esta tese propõe uma abordagem que permite ao usuário combinar diversas técnicas de visualização, mantendo o controle da história das visualizações encadeando-as de uma maneira integrada. Os usuários são capazes de comparar os resultados fornecidos por diferentes técnicas de visualização, o que proporciona o sinergismo necessário para a compreensão mais completa do conjunto de dados. São propostas três técnicas para a exploração de grafos multivariados. A primeira técnica (ClusterVis) fornece a visualização das relações entre atributos de nodos pertencentes a agrupamentos. A segunda, denominada Iris, permite a visualização de atributos associados às arestas de nodos adjacentes. E, finalmente, a terceira (GlyphMatrix) explora o uso de glifos e matriz de adjacência, para visualizar a relação entre atributos associados às arestas.",TESE,Abordagens heterogêneas para a exploração interativa de grafos multivariados,5007049,01
"Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users' demands, and improve the scalability and availability of origin servers. Application-level caching, in which developers manually control cached content, has been adopted when traditional forms of caching are insufficient to meet such requirements. Despite its popularity, this level of caching is typically addressed in an ad-hoc way, given that it depends on specific details of the application. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. This dissertation advances work on application-level caching by providing an understanding of its state-of-practice and automating the decision regarding cacheable content, thus providing developers with substantial support to design, implement and maintain application-level caching solutions. More specifically, we provide three key contributions: structured knowledge derived from a qualitative study, a survey of the state-of-the-art on static and adaptive caching approaches, and a technique and framework that automate the challenging task of identifying cache opportunities. The qualitative study, which involved the investigation of ten web applications (open-source and commercial) with different characteristics, allowed us to determine the state-of-practice of application-level caching, along with practical guidance to developers as patterns and guidelines to be followed. Based on such patterns and guidelines derived, we also propose an approach to automate the identification of cacheable methods, which is often manually done and is not supported by existing approaches to implement application-level caching. We implemented a caching framework that can be seamlessly integrated into web applications to automatically identify and cache opportunities at runtime, by monitoring system execution and adaptively managing caching decisions. We evaluated our approach empirically with three open-source web applications, and results indicate that we can identify adequate caching opportunities by improving application throughput up to 12.16%. Furthermore, our approach can prevent code tangling and raise the abstraction level of caching.",,Ciência de Dados e Engenharia de Software,JHONNY MARCOS ACORDI MERTZ,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,24/02/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Web application;application-level caching;self-adaptive systems;software maintenance;software evolution;guideline;pattern',LINHA ENGENHARIA DE SOFTWARE,INGRID OLIVEIRA DE NUNES,114,Aplicações web;Cache;sistemas adaptativos;manutenção de software;evolução de software. Diretrizes;padrões,COMPUTAÇÃO (42001013004P4),-,"O custo de serviços na Internet tem encorajado o uso de cache a nível de aplicação para suprir as demandas dos usuários e melhorar a escalabilidade e disponibilidade de aplicações.
Cache a nível de aplicação, onde desenvolvedores manualmente controlam o conteúdo
cacheado, tem sido adotada quando soluções tradicionais de cache não são capazes de
atender aos requisitos de desempenho desejados. Apesar de sua crescente popularidade,
este tipo de cache é tipicamente endereçado de maneira ad-hoc, uma vez que depende de
detalhes específicos da aplicação para ser desenvolvida. Dessa forma, tal cache consiste
em uma tarefa que requer tempo e esforço, além de ser altamente suscetível a erros. Esta
dissertação avança o trabalho relacionado a cache a nível de aplicação provendo uma compreensão de seu estado de prática e automatizando a identificação de conteúdo cacheável,
fornecendo assim suporte substancial aos desenvolvedores para o projeto, implementação e manutenção de soluções de caching. Mais especificamente, este trabalho apresenta
três contribuições: a estruturação de conhecimento sobre caching derivado de um estudo
qualitativo, um levantamento do estado da arte em abordagens de cache estáticas e adaptativas, e uma técnica que automatiza a difícil tarefa de identificar oportunidades de cache.
O estudo qualitativo, que envolveu a investigação de dez aplicações web (código aberto
e comercial) com características diferentes, permitiu-nos determinar o estado de prática
de cache a nível de aplicação, juntamente com orientações práticas aos desenvolvedores
na forma de padrões e diretrizes. Com base nesses padrões e diretrizes derivados, também propomos uma abordagem para automatizar a identificação de métodos cacheáveis,
que é geralmente realizado manualmente por desenvolvedores. Tal abordagem foi implementada como um framework, que pode ser integrado em aplicações web para identificar
automaticamente oportunidades de cache em tempo de execução, com base na monitoração da execução do sistema e gerenciamento adaptativo das decisões de cache. Nós
avaliamos a abordagem empiricamente com três aplicações web de código aberto, e os
resultados indicam que a abordagem é capaz de identificar oportunidades de cache adequadas, melhorando o desempenho das aplicações em até 12,16%.",DISSERTAÇÃO,Understanding and Automating Application-level Caching,5007116,01
"Nowadays, there are several resources for cataloging and making available learning objects in 
various metadata standards. However, we found a problem related to the existence of the storage 
and  retrieval  of  learning  objects.  Perhaps  because  the  teachers/authors  do  not  have  time  to 
storage their objects or even because  the process of storage metadata  in a repository is very
laborious. As  one way to this  problem, it is here presented a possibility of minimizing it and 
with this research we aim to facilitate the process to storage and making available the learning 
objects through the interoperability of the virtual learning platform Moodle with the repository 
of  Cognix.  This  work  aims  to  propose  a  tool  that  allows  the  automatic  storage  of  Moodle 
learning objects in the Cognix repository. For such, four methodological steps were described 
for the development of the project as well as for the implementation of the tool. This tool is 
expected  rant  reutilization  and  access  to  learning  objects  for  their  users,  thus  ensuring  the 
uniform and complete description of the metadata.",,Inteligência Artificial,CECILIA RAFAEL JOSE TIVIR,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,15/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Learning  Objects;Learning  Environments;Interoperability;Computers  in  Education',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",ROSA MARIA VICARI,65,Objetos de Aprendizagem;Ambientes Virtuais de Aprendizagem;Interoperabilidade;Informática na Educação,COMPUTAÇÃO (42001013004P4),-,"Atualmente, existem vários recursos para catalogar e disponibilizar objetos de aprendizagem em diferentes padrões de metadados. No entanto, a tarefa de inserir metadados em um repositório é muito trabalhosa. Como resposta a esta realidade, apresenta-se aqui a possibilidade de minimizar esta tarefa, pois com esta pesquisa pretendemos facilitar o processo de armazenamento e recuperação dos objetos de aprendizagem, através da interoperabilidade entre uma plataforma virtual de aprendizagem e um repositório de objetos de aprendizagem. A interoperabilidade entre estas plataformas fornece metadados automaticamente, o que facilita o armazenamento de objetos de aprendizagem em um repositório. Para tanto, foi descrito um conjunto de etapas metodológicas para o desenvolvimento do Projeto, bem como para a implementação da ferramenta.",DISSERTAÇÃO,Disseminação de conteúdo educacional através de sua catalogação automática em Repositório Educacional,5007128,01
"In this work, we present CivisAnalysis, an open-source web-based system for the visualization of roll calls in the Brazil's Chamber of Deputies. Using roll calls of six legislatures as well as six presidential elections, CivisAnalysis provides a unique view of the political history of our country. It combines classical roll calls visualization techniques with techniques for the visualization of temporal data. Votes of each representative are interpreted as a set of multidimensional data, which are represented in a n-dimensional space mapped to a bi-dimensional representation. The Votes of each representative determine their position in the political spectrum of deputies, and the votes of all deputies determine the political spectrum of roll calls. The work also shows how exploring subsets of deputies allows the discovery of voting patterns in the roll call spectrum, as well as exploring the roll calls spectrum allows the discovery of behavioral voting patterns in the political spectrum of deputies. For supporting a long-term political analysis, CivisAnalysis provides a time line visualization integrated with election data (election results and political alliances). The tool implements textual and visual filtering and includes auxiliary visualizations that provide an overview of the political scenario regarding deputies, parties, coalitions and their behavior along time.",,Computação Visual,FRANCISCO GERDAU DE BORJA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,23/02/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Visualization;Political Data;Roll Call Analysis',LINHA COMPUTAÇÃO GRÁFICA E VISUALIZAÇÃO DE DADOS,CARLA MARIA DAL SASSO FREITAS,69,Visualização de Dados Políticos;Espectro Político;Análise de Votações,COMPUTAÇÃO (42001013004P4),-,"Este trabalho apresenta CivisAnalysis, uma aplicação web de código aberto para
visualização das votações ocorridas na Câmara dos Deputados do Brasil. Com dados
abrangendo seis legislaturas e seis eleições presidenciais, CivisAnalysis provê uma
visão inovadora da história política do Congresso Brasileiro combinando técnicas
clássicas de visualização de votações e técnicas de apresentação de dados temporais.
As votações de cada deputado podem ser interpretadas como dados multidimensionais e são representadas em um espectro n-dimensional mapeado para uma
representação bidimensional. Os votos de cada deputado determinam suas posições
numa representação conhecida como espectro político. O trabalho mostra como
a exploração de conjuntos de Deputados permite a descoberta de padrões de votos no espectro de votações, assim como explorar conjuntos de votações permite
a descoberta de padrões de comportamento no espectro de Deputados. Para uma
análise política de longo termo, foi criada uma linha do tempo apresentando as distâncias relativas dos partidos no espectro político durante seis legislaturas e eleições
presidenciais. Na linha de tempo é possível perceber os alinhamentos políticos e mudanças comportamentais de partidos conforme o progresso de legislaturas, alianças
eleitorais e coalizões de governo.
A interface de CivisAnalysis oferece filtros textuais e visuais e inclui visualizações auxiliares para destacar cenários políticos de acordo com Deputados, partidos,
unidades federativas, alianças eleitorais e suas posições no espectro político",DISSERTAÇÃO,CivisAnalysis: Exploring Representatives' Voting Behaviour,5007136,01
"In High Performance Computing (HPC) environments, scientific applications rely on Parallel File Systems (PFS) to obtain Input/Output (I/O) performance especially when handling large amounts of data. However, I/O is still a bottleneck for an increasing number
of applications, due to the historical gap between processing and data access speed. To
alleviate the concurrency caused by thousands of nodes accessing a significantly smaller
number of PFS servers, intermediate I/O nodes are typically employed between processing nodes and the file system. Each intermediate node forwards requests from multiple
clients to the parallel file system, a setup which gives this component the opportunity to
perform optimizations like I/O scheduling. The objective of this dissertation is to evaluate
different scheduling algorithms, at the I/O forwarding layer, that work to improve concurrent access patterns by aggregating and reordering requests to avoid patterns known to
harm performance. We demonstrate that the FIFO (First In, First Out), HBRR (HandleBased Round-Robin), TO (Time Order), SJF (Shortest Job First) and MLF (Multilevel
Feedback) schedulers are only partially effective because the access pattern is not the
main factor that affects performance in the I/O forwarding layer, especially for read requests. A new scheduling algorithm, TWINS, is proposed to coordinate the access of
intermediate I/O nodes to the parallel file system data servers. Our approach decreases
concurrency at the data servers, a factor previously proven to negatively affect performance. The proposed algorithm is able to improve read performance from shared files by
up to 28% over other scheduling algorithms and by up to 50% over not forwarding I/O
requests.",,Sistemas de Computação,JEAN LUCA BEZ,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,13/01/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'High Performance I/O;Parallel File Systems. Parallel I/O;I/O Forwarding;I/O Scheduling;Access Coordination',LINHA COMPUTAÇÃO DE ALTO DESEMPENHO E SISTEMAS DISTRIBUÍDOS,PHILIPPE OLIVIER ALEXANDRE NAVAUX,73,E/S de Alto Desempenho;Sistemas de Arquivos Paralelos;E/S Paralela;Encaminhamento de E/S;Escalonamento de E/S;Coordenação de Acessos,COMPUTAÇÃO (42001013004P4),-,"Em ambientes de Computação de Alto Desempenho, as aplicações científicas dependem dos Sistemas de Arquivos Paralelos (SAP) para obter desempenho de Entrada/Saída (E/S), especialmente ao lidar com grandes quantidades de dados. No entanto, E/S ainda é um gargalo para um número crescente de aplicações, devido à diferença histórica entre a velocidade de processamento e de acesso aos dados. Para aliviar a concorrência causada por milhares de nós que acessam um número significativamente menor de servidores SAP, normalmente nós intermediários de E/S são adicionados entre os nós de processamento e o sistema de arquivos. Cada nó intermediário encaminha solicitações de vários clientes para o sistema, uma configuração que dá a este componente a oportunidade de executar otimizações como o escalonamento de requisições de E/S. O objetivo desta dissertação é avaliar diferentes algoritmos de escalonamento, na camada de encaminhamento de E/S, cuja finalidade é melhorar o padrão de acesso das aplicações, agregando e reordenando requisições para evitar padrões que são conhecidos por prejudicar o desempenho. Demonstramos que os escalonadores FIFO (First In, First Out), HBRR (Handle-Based Round-Robin), TO (Time Order), SJF (Shortest Job First) e MLF (Multilevel Feedback) são apenas parcialmente eficazes porque o padrão de acesso não é o principal fator que afeta o desempenho na camada de encaminhamento de E/S, especialmente para requisições de leitura. Um novo algoritmo de escalonamento chamado TWINS é proposto para coordenar o acesso de nós intermediários de E/S aos servidores de dados do sistema de arquivos paralelo. Nossa abordagem reduz a concorrência nos servidores de dados, um fator previamente demonstrado como reponsável por afetar negativamente o desempenho. O algoritmo proposto é capaz de melhorar o tempo de leitura de arquivos compartilhados em até 28% se comparado a outros algoritmos de escalonamento e em até 50% se comparado a não fazer o encaminhamento de requisições de E/S.",DISSERTAÇÃO,Evaluating I/O Scheduling Techniques at the Forwarding Layer and Coordinating Data Server Accesses,5007138,01
"Passing strategies analysis has always been of interest for soccer research. Since the beginning of soccer, managers have used scouting, video footage, training drills and data feeds to collect information about tactics and player performance. However, the dynamic nature of passing strategies is complex enough to reflect what is happening in the game and makes it hard to understand its dynamics. Furthermore, there exists a growing demand for pattern detection and passing sequence analysis popularized by FC Barcelona’s tiki-taka. We propose an approach to abstract passing strategies and group them based on the geometry of the ball trajectory. To analyse passing sequences, we introduce a interactive visualization scheme to explore the frequency of usage, spatial location and time occurrence of the sequences. The frequency stripes visualization provide an overview of passing groups frequency on three pitch regions: defense, middle, attack. A trajectory heatmap coordinated with a passing timeline allow for the exploration of most recurrent passing shapes in temporal and spatial domains. Results show eight common ball trajectories for three-long passing sequences which depend on players positioning and on the angle of the pass. We demonstrate the potential of our approach with data from the Brazilian league under several case studies, and report feedback from a soccer expert.",,Ciência de Dados e Engenharia de Software,JOSÉ LUIS SOTOMAYOR MALQUI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,21/02/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Visual analytics;visual knowledge discovery;sport analytics;pattern recognition',"LINHA MINERAÇÃO, INTEGRAÇÃO E ANÁLISE DE DADOS",JOAO LUIZ DIHL COMBA,64,Visual analytics;visual knowledge discovery;sport analytics;pattern recognition,COMPUTAÇÃO (42001013004P4),-,"As estrategias de passes têm sido sempre de interesse para a pesquisa de futebol. Desde
os inícios do futebol, os técnicos tem usado olheiros, gravações de vídeo, exercícios de
treinamento e feeds de dados para coletar informações sobre as táticas e desempenho
dos jogadores. No entanto, a natureza dinâmica das estratégias de passes são bastante
complexas para refletir o que está acontecendo dentro do campo e torna difícil o entendimento do jogo. Além disso, existe uma demanda crecente pela deteção de padrões e
analise de estrategias de passes popularizado pelo tiki-taka utilizado pelo FC. Barcelona.
Neste trabalho, propomos uma abordagem para abstrair as sequências de pases e agrupálas baseadas na geometria da trajetória da bola. Para analizar as estratégias de passes,
apresentamos um esquema de visualização interátiva para explorar a frequência de uso,
a localização espacial e ocorrência temporal das sequências. A visualização Frequency
Stripes fornece uma visão geral da frequencia dos grupos achados em tres regiões do
campo: defesa, meio e ataque. O heatmap de trajetórias coordenado com a timeline de
passes permite a exploração das formas mais recorrentes no espaço e tempo. Os resultados demostram oito trajetórias comunes da bola para sequências de três pases as quais
dependem da posição dos jogadores e os ângulos de passe. Demonstramos o potencial da
nossa abordagem com utilizando dados de várias partidas do Campeonato Brasileiro sob
diferentes casos de estudo, e reportamos os comentários de especialistas em futebol.",DISSERTAÇÃO,A Visual Analytics Approach for Passing Strategies Analysis in Soccer using Geometric Features,5007140,01
"Monitoring natural environments and their changes over time requires the analysis of a large amount of image data, often collected by orbital remote sensing platforms. However, variations in the observed signals due to changing atmospheric conditions often result in a data distribution shift for different dates and locations making it difficult to discriminate between various classes in a dataset built from several images. This work introduces a novel supervised classification framework, called Classify-Normalize-Classify (CNC), to alleviate this data shift issue. The proposed scheme uses a two classifier approach. The first classifier is trained on non-normalized top-of-the-atmosphere reflectance samples to discriminate between pixels belonging to a class of interest (COI) and pixels from other categories (e.g. forest vs. non-forest). At test time, the estimated COI’s multivariate median signal, derived from the first classifier segmentation, is subtracted from the image and thus anchoring the data distribution from different images to the same reference. Then, a second classifier, pre-trained to minimize the classification error on COI median centered samples, is applied to the median-normalized test image to produce the final binary segmentation. 
The proposed methodology was tested to detect deforestation using bitemporal Landsat 8 OLI images over the Amazon rainforest. Experiments using top-of-the-atmosphere multispectral reflectance images showed that the deforestation was mapped by the CNC framework more accurately as compared to running a single classifier on surface reflectance images provided by the United States Geological Survey (USGS). Accuracies from the proposed framework also compared favorably with the benchmark masks of the PRODES program.",,Computação Visual,CESAR SALGADO VIEIRA DE SOUZA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,30/03/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Image normalization;Radiometric correction;Pixel classification;Forest Segmentation;Deforestation detection;Supervised learning',LINHA PROCESSAMENTO DE IMAGENS E VISÃO COMPUTACIONAL E RECONHECIMENTO DE PADRÕES,JACOB SCHARCANSKI,77,Normalização de imagens;Correção radiométrica;Classificação de pixels;Segmentação de floresta;Detecção de desflorestamento;Aprendizado supervisionado,COMPUTAÇÃO (42001013004P4),-,"O monitoramento do meio ambiente e suas mudanças requer a análise de uma grade quantidade de imagens muitas vezes coletadas por satélites. No entanto, variações nos sinais
devido a mudanças nas condições atmosféricas frequentemente resultam num deslocamento da distribuição dos dados para diferentes locais e datas. Isso torna difícil a distinção dentre as várias classes de uma base de dados construída a partir de várias imagens.
Neste trabalho introduzimos uma nova abordagem de classificação supervisionada, chamada Classifica-Normaliza-Classifica (CNC), para amenizar o problema de deslocamento
dos dados. A proposta é implementada usando dois classificadores. O primeiro é treinado
em imagens não normalizadas de refletância de topo de atmosfera para distinguir dentre
pixels de uma classe de interesse (CDI) e pixels de outras categorias (e.g. floresta versus não-floresta). Dada uma nova imagem de teste, o primeiro classificador gera uma
segmentação das regiões da CDI e então um vetor mediano é calculado para os valores
espectrais dessas áreas. Então, esse vetor é subtraído de cada pixel da imagem e portanto
fixa a distribuição de dados de diferentes imagens num mesmo referencial. Finalmente, o
segundo classificador, que é treinado para minimizar o erro de classificação em imagens
já centralizadas pela mediana, é aplicado na imagem de teste normalizada no segundo
passo para produzir a segmentação binária final.
A metodologia proposta foi testada para detectar desflorestamento em pares de imagens
co-registradas da Landsat 8 OLI sobre a floresta Amazônica. Experimentos usando imagens multiespectrais de refletância de topo de atmosfera mostraram que a CNC obteve
maior acurácia na detecção de desflorestamento do que classificadores aplicados em imagens de refletância de superfície fornecidas pelo United States Geological Survey. As
acurácias do método proposto também se mostraram superiores às obtidas pelas máscaras
de desflorestamento do programa PRODES.",DISSERTAÇÃO,Classify-Normalize-Classify: A Novel Data-driven Framework for Classifying Forest Pixels in Remote Sensing Imagery,5007147,01
"Music rankings are mainly aimed at marketing purposes but also help users in discovering new music as well as comparing songs, artists, albums, etc. This work presents an interactive way to visualize, find and compare music rankings using different techniques, including the display of music attributes. The technique was conceived after a remote survey we conducted to collect data about how people choose music. Our visualization makes easier to obtain information about artists and tracks, and also to compare the data gathered from the two major music rankings, namely Billboard and Spotify. The tool also provides interaction with personal data. The results obtained from experiments with potential users showed that the tool was considered interesting, with an attractive layout. Compared to traditional music ranking tools users preferred ours, but with not such a large difference from using Billboard or Spotify. However, when evaluating the usability of our tool, results are positive, mainly concerning to data filtering and comparison features. MusicVis was also  considered easy to learn.",,Computação Visual,LEANDRO SOARES GUEDES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,31/01/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Music data visualization;Music rankings;Music charts;Interactive visualization',LINHA COMPUTAÇÃO GRÁFICA E VISUALIZAÇÃO DE DADOS,CARLA MARIA DAL SASSO FREITAS,68,Visualização de Dados Musicais;Rankings de Músicas;Visualização Interativa.,COMPUTAÇÃO (42001013004P4),-,"Os rankings musicais destinam-se principalmente a fins de marketing, mas também ajudam os
usuários a descobrir novas músicas, bem como a comparar artistas, álbuns, etc. Este trabalho
apresenta uma ferramenta interativa para visualizar, encontrar e comparar rankings musicais
usando diferentes técnicas além de exibir atributos das músicas. A técnica foi concebida após
uma pesquisa remota que coletou dados sobre como as pessoas escolhem música. As técnicas de
visualização tornam mais fácil obter informações sobre artistas e faixas, e também comparar os
dados obtidos a partir dos dois principais rankings de música, Billboard e Spotify. A ferrament
também permite a interação com dados pessoais. Resultados de experimentos conduzidos com
usuários potenciais mostraram que a ferramenta foi considerada interessante, com um layout
atrativo. Comparando com as formas tradicionais de visualizar rankings de músicas, usuários
preferiram a ferramenta aqui desenvolvida, mas a diferença para Billboard e Spotify não foi
grande. Entretanto, quando avaliada a usabilidade da ferramenta, os resultados foram melhores,
principalmente no que se refere à filtragem e às técnicas de comparação. MusicVis foi também
considerado fácil de aprender.",DISSERTAÇÃO,MusicVis:  Interactive Visualization Tool For Exploring Music Rankings,5007155,01
"Memetic algorithms are evolutionary metaheuristics intrinsically concerned with the exploiting and incorporation of all available knowledge about the problem under study. In
this dissertation, we present a knowledge-based memetic algorithm to tackle the threedimensional protein structure prediction problem without the explicit use of template experimentally determined structures. The algorithm was divided into two main steps of
processing: (i) sampling and initialization of the algorithm solutions; and (ii) optimization of the structural models from the previous stage. The first step aims to generate and
classify several structural models for a determined target protein, by the use of the strategy Angle Probability List, aiming the definition of different structural groups and the
creation of better structures to initialize the initial individuals of the memetic algorithm.
The Angle Probability List takes advantage of structural knowledge stored in the Protein
Data Bank in order to reduce the complexity of the conformational search space. The
second step of the method consists in the optimization process of the structures generated
in the first stage, through the applying of the proposed memetic algorithm, which uses a
tree-structured population, where each node can be seen as an independent subpopulation
that interacts with others, over global search operations, aiming at information sharing,
population diversity, and better exploration of the multimodal search space of the problem. The method also encompasses ad-hoc global search operators, whose objective is
to increase the exploration capacity of the method turning to the characteristics of the
protein structure prediction problem, combined with the Artificial Bee Colony algorithm
to be used as a local search technique applied to each node of the tree. The proposed
algorithm was tested on a set of 24 amino acid sequences, as well as compared with two
reference methods in the protein structure prediction area, Rosetta and QUARK. The results show the ability of the method to predict three-dimensional protein structures with
similar foldings to the experimentally determined protein structures, regarding the structural metrics Root-Mean-Square Deviation and Global Distance Total Score Test. We also
show that our method was able to reach comparable results to Rosetta and QUARK, and
in some cases, it outperformed them, corroborating the effectiveness of our proposal",,Inteligência Artificial,LEONARDO DE LIMA CORREA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,16/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Optimization;metaheuristics;evolutionary algorithms;knowledge based algorithm;structural bioinformatics;memetic algorithms',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",MARCIO DORN,154,Otimização;meta-heurísticas;algoritmos evolutivos;algoritmo baseado em conhecimento;bioiformática estrutural;algoritmos meméticos,COMPUTAÇÃO (42001013004P4),-,"Algoritmos meméticos são meta-heurísticas evolutivas voltadas intrinsecamente à exploração de conhecimentos relacionados ao problema em estudo. A incorporação de conhecimento prévio acerca de propriedades e características do problema não é um mecanismo opcional, mas uma característica fundamental dos algoritmos meméticos. Estes permitem grandes flexibilizações no emprego de heurísticas de otimização global e local, facilitando a exploração e o refinamento do espaço de soluções em problemas reais. Nesta dissertação, foi proposto um algoritmo memético multi populacional baseado em conhecimento para lidar com o problema de predição de estruturas tridimensionais de proteínas. O algoritmo em questão, foi estruturado em duas etapas principais de processamento: (i) etapa de amostragem e inicialização de soluções; e (ii) etapa de otimização dos modelos estruturais provenientes da etapa anterior. A etapa I de amostragem e inicialização de indivíduos objetiva a geração e classificação de diversos modelos estruturais para determinada proteína-alvo, a partir da estratégia Lista de Probabilidades Angulares, buscando a definição de diferentes grupos estruturais e a criação de melhores estruturas a serem incorporadas à meta-heurística como soluções iniciais das multi populações de otimização. A Lista de Probabilidades Angulares foi utilizada como forma de inicialização de indivíduos, visando a redução da complexidade do espaço de busca conformacional, por meio da exploração do conhecimento prévio acerca de estruturas 3-D de proteínas determinadas experimentalmente e armazenadas no Protein Data Bank. A segunda etapa do método consiste no processo de otimização das estruturas oriundas da etapa I, realizado por meio da aplicação do algoritmo memético de otimização, o qual é fundamentado na organização da população de indivíduos em uma estrutura em árvore ternária, onde cada nodo pode ser interpretado como uma subpopulação independente, que ao longo do processo interage com outros nodos por meio de operações de busca global, visando o compartilhamento de informações, a diversificação da população de indivíduos, e a exploração mais eficaz do espaço de busca multimodal do problema. O algoritmo engloba ainda operadores de busca global modificados, como operações de cruzamento considerando a estrutura secundária da proteína e mutação baseada em regiões mais flexíveis das estruturas, cujo objetivo consiste no aumento da capacidade de exploração do método voltando-se às características do problema de predição de estruturas de proteínas, combinados a uma implementação do algoritmo colônia artificial de abelhas (Artificial Bee Colony), com o propósito de ser utilizado como uma técnica de busca centralizada a ser aplicada em cada nodo da árvore ternária. O algoritmo proposto foi testado em um conjunto de 24 sequências de aminoácidos, assim como comparado a dois métodos de referência na área de predição de estruturas tridimensionais de proteínas, Rosetta e QUARK. Os resultados obtidos mostraram a capacidade do método em predizer estruturas tridimensionais de proteínas com conformações similares às estruturas determinadas experimentalmente, em termos das métricas de avaliação estrutural Root-Mean-Square Deviation e Global Distance Total Score Test. Verificou-se que o algoritmo desenvolvido também foi capaz de atingir resultados comparáveis ao Rosetta e ao QUARK, sendo que em alguns casos, os superou. Corroborando assim, a eficácia do método.",DISSERTAÇÃO,Uma Proposta de Algoritmo Memético Baseado em Conhecimento para o Problema de Predição de Estruturas 3-D de Proteínas,5007368,01
"Network Functions Virtualization (NFV) is driving a paradigm shift in telecommunications networks and computer networks, by fostering new business models and creating innovation opportunities. In NFV-enabled networks, service providers have the opportunity to build a business model where tenants can purchase Virtual Network Functions (VNFs) that provide distinct network services and functions (\eg Firewall, NAT, and transcoders). However, the amount of managed data grows in a fast pace. The network operator must understand and manipulate many data to effectively manage the network. To tackle this problem, we introduce VISION, a platform based on visualizations techniques to help network operators to determine the cause of not obvious problems. For this, we provide: \1 an approach to collect and organize data from the NFV environments; \2 five distinct visualizations that can aid in NFV management tasks, such as in the process of identifying VNFs problems and planning of NFV-enabled businesses; and \3 a template model that supports new visualization applications. To evaluate our dissertation, we implemented a prototype of VISION platform and each of the proposed visualizations. We then conducted distinct case studies to provide evidence of the feasibility of our visualizations. These case studies cover different scenarios, such as the identification of misplacement of VNFs that are generating bottlenecks in a forwarding graph and the investigation of investment priorities to supply tenants demands. Finally, we present a usability evaluation with network operators to indicate the benefits of the VISION platform. The results obtained show that our visualizations allow the operator to access relevant information and have insights to identify not obvious problems in the context of NFV-enabled networks. In addition, we received positive feedback about general usability aspects related to our prototype.",,Redes de Computadores,MURIEL FIGUEREDO FRANCO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,10/04/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Network Functions Virtualization;Network Management;Information Visualization',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",LISANDRO ZAMBENEDETTI GRANVILLE,90,Funções de Rede Virtualizadas;Gerenciamento de Rede;Visualização de Informações,COMPUTAÇÃO (42001013004P4),-,"A Virtualização de Funções de Rede (Network Functions Virtualization - NFV) está mudando o
paradigma das redes de telecomunicações. Esta nova tecnologia permite diversas oportunidades
de inovações e possibilita o desenvolvimento de novos modelos de negócio. Em relação às redes
NFV, os provedores de serviços têm a oportunidade de criar modelos de negócio que permitam
aos clientes contratarem Funções de Rede Virtualizadas (Virtual Network Functions - VNFs)
que proveem diferentes serviços de rede (e.g., Firewall, NAT e transcoders). Porém, nestes
modelos, a quantidade de informações a serem gerenciadas cresce rapidamente. Baseado nisso,
os operadores de rede devem ser capazes de entender e manipular uma grande quantidade de
informação para gerenciar, de forma efetiva, as redes NFV. Para enfrentar esse problema, introduzimos uma plataforma de visualização denominada VISION, a qual tem como principal
objetivo ajudar os operadores de rede na identificação da causa raiz de problemas em NFV. Para
isso, propusemos: (i) uma abordagem para coleta e organização de dados do ambiente NFV
gerenciado; (ii) cinco diferentes visualizações que auxiliam nas tarefas de gerenciamento de
NFV como, por exemplo, no processo de identificação de problemas em VNFs e no planejamento de negócios e (iii) um modelo baseado em templates que suporta o desenvolvimento e o
reuso de visualizações. Para fins de avaliação desta dissertação, foi desenvolvido um protótipo
da plataforma VISION e de todas as visualizações propostas. Após, conduzimos um conjunto
de casos de estudo para prover evidências sobre a viabilidade e utilidade de nossas visualizações. Os diferentes casos analisados, abordam por exemplo, a identificação de problemas na
alocação de VNFs que estão impactando no desempenho do serviço oferecido e também na
investigação de prioridades de investimento para suprir as demandas dos clientes da rede. Por
fim, apresentamos uma avaliação de usabilidade realizada juntamente a especialistas em redes
de computadores para avaliar os recursos e benefícios da plataforma VISION. Os resultados
obtidos demonstram que nossas visualizações possibilitam ao operador de rede um rápido e
fácil acesso às informações importantes para o gerenciamento de redes NFV, assim facilitando
a obtenção de insights para a identificação de problemas complexos no contexto de redes NFV.
Além disso, os resultados demonstram uma avaliação positiva por especialistas sobre os aspectos gerais de usabilidade do protótipo desenvolvido.",DISSERTAÇÃO,Interactive Visualizations for Management of NFV-Enabled Networks,5008235,01
"New models of Unnamed Aerial Vehicles (UAVs) are developed on a daily basis and
applied to several tasks, such as mapping terrains and surveillance. GPS sensors
are usually the main source of information to estimate the robot position, however, a downside of these sensors is that a minimum number of satellites must be
available and emitting high quality signals. A vision-based system can be used to
overcome this problem by using a robot embedded camera and satellite images as
maps. Computational vision systems estimate the UAV localization through the
comparison between the robot extracted image and several parts of the satellite image. These comparisons are performed in order to localize the part of the map which
is most similar to the robot perspective. Taking into account all this information,
we propose BRIEF descriptor variation, called abBRIEF, to be used on a novel
observation model, also proposed in this master thesis. An observation model is
responsible for evaluate how similar the robot measurements and the different map
parts are. The used descriptor must run fast, consume low computational resources
and be robust regarding image changes, all to compensate the large number of necessary comparisons. The proposed model is applied with Monte Carlo Localization
(MCL) method to the auto localization of UAVs and presented solid results that
corroborate the model and descriptor efficiency.",,Inteligência Artificial,MATHIAS FASSINI MANTELLI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,24/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'UAV;MCL;BRIEF;localization;images',"LINHA PLANEJAMENTO, SISTEMAS MULTIAGENTES E ROBÓTICA",MARIANA LUDERITZ KOLBERG,85,VANTs;MCL;BRIEF;localização;imagens,COMPUTAÇÃO (42001013004P4),-,"A cada dia novos modelos de Veículos Aéreos Não Tripulados (VANTs) estão sendo lançados no mercado para serem utilizados em diversas aplicações, tais como mapeamento de ambientes e vigilância. Geralmente, estes robôs utilizam um sensor GPS como fonte de estimativa de localização. Contudo, para um bom funcionamento, este sensor depende de um número mínimo de satélites sincronizados com ele e que o sinal emitido pelos satélites seja recebido com boa qualidade, o que pode ser considerado um fator negativo. Uma forma de contornar este problema é empregar um sistema de localização baseado em visão computacional utilizando a câmera que já está embarcada no robô e imagens de satélite como mapa. Este sistema estima a localização do VANT através de comparações entre a imagem capturada por ele e diversas partes da imagem de satélite, buscando encontrar a que mais se assemelha com esta imagem capturada. Neste contexto, apresentamos uma variação do descritor BRIEF, o abBRIEF, para ser utilizado em um novo modelo de observação que também está sendo proposto. O modelo de observação é responsável por medir quão parecidas são as leituras do robô com diversas partes do mapa, para estimar a sua localização correta. Devido ao grande número de comparações necessárias, éimportante que o descritor utilizado no processo seja rápido, não consuma muitos recursos computacionais e seja robusto para lidar com as várias diferenças entre as imagens. O modelo proposto foi utilizado no algoritmo de Monte Carlo (Monte Carlo Localization, MCL) para realizar a localização de VANTs e apresentou resultados satisfatórios que corroboram a eficácia do modelo e do descritor.""",DISSERTAÇÃO,Um Novo Modelo de Observação para o MCL Aplicado ao Problema de Localização Global de VANTs sobre Imagens de Satélite,5008257,01
"To enable effective business process management, the first step is the design of appropriate
process models to the organization’s objectives. These models are used to describe roles
and responsibilities of the employees in an organizations. In addition, business process
modeling is very important to report, understand and automate processes. However, the
documentation existent in organizations about such processes is mostly unstructured and
difficult to be understood by analysts. In this context, process modeling becomes highly
time consuming and expensive, generating process models that do not comply with the
reality of the organizations. The extracting of process models from textual descriptions
may contribute to minimize the effort required in process modeling. In this context, this
dissertation proposes a semi-automatic approach to identify process elements in natural
language text. Based on the study of natural language processing, it was defined a set of
mapping rules to identify process elements in text. In addition, in order to evaluate the
mapping rules and to demonstrate the feasibility of the proposed approach, a prototype
was developed able to identify process elements in text in a semiautomatic way. To measure the performance of the proposed prototype metrics were used to retrieve information
such as precision, recall, and F-measure. In addition, two surveys were developed with
the purpose of verifying the acceptance of the users. The evaluations present promising
results. The analyses of 70 texts presented, on average, 73.61% precision, 70.15% recall
and 71.82% F-measure. In addition, the results of the first and second surveys presented
on average 91.66% acceptance of the participants. The main contribution of this work is
to provide mapping rules for identify process elements in natural language text to support
and minimize the time required for process modeling performed by process analysts.",,Ciência de Dados e Engenharia de Software,RENATO CESAR BORGES FERREIRA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,22/02/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Mapping Rules;Business Process Model and Notation;Business Process Management;Process Element;Process Model;Natural Language Processing;Process Modeling',LINHA MODELAGEM DE DADOS E DE PROCESSOS DE NEGÓCIOS,LUCINEIA HELOISA THOM,103,Regras de Mapeamento;Notação e Modelo de Processo de Negócio;Gerenciamento de Processos de Negócio;Elementos de Processo;Modelo de Processo;Processamento de Linguagem Natural;Modelagem de Processos,COMPUTAÇÃO (42001013004P4),-,"Para permitir um efetivo gerenciamento de processos de negócio, o primeiro passo é o desenvolvimento de modelos de processo adequados aos objetivos das organizações. Tais modelos são utilizados para descreverem papéis e responsabilidades dos colaboradores nas organizações. Além disso, a modelagem de processos é de grande importância para documentar, entender e automatizar processos. As organizações, geralmente provêm documentos não estruturados e de difícil entendimento por parte dos analistas. Neste panorama, a modelagem de processos se torna demorada e de alto custo, podendo gerar modelos de processo que estão em desacordo com a realidade prevista pelas organizações. A extração de modelos ou fragmentos de processo a partir de descrições textuais pode contribuir para minimizar o esforço necessário à modelagem de processos. Neste contexto, esta dissertação propõe uma abordagem para identificar elementos de processo de negócio em texto em linguagem natural de forma semiautomática. Baseado no estudo de processamento de linguagem natural, foi definido um conjunto de regras de mapeamento para identificar elementos de processo em descrição textual. Além disso, para avaliar as regras de mapeamento e viabilizar a abordagem proposta, foi desenvolvido um protótipo capaz de identificar elementos de processo em texto de forma semiautomática. Para medir o desempenho do protótipo proposto, foram utilizadas métricas de recuperação de informação, tais como precisão, revocação e medida-F. Além disso, foram aplicados dois questionários com o objetivo de verificar a aceitação perante os usuários. As avaliações apresentam resultados promissores. A análise de 70 textos, apresentou, em média, 73,61% de precisão, 70,15% de revocação e 71,82% de medida-F. Além disso, os resultados do primeiro e segundo questionários apresentaram, em média, 91,66% de aceitação dos participantes. A principal contribuição deste trabalho é propor regras de mapeamento para identificar elementos de processo em texto em linguagem natural para auxiliar e minimizar o tempo necessário à modelagem de processos realizada pelos analistas de processo.",DISSERTAÇÃO,Uma Abordagem Semiautomática para Identificação de Elementos de Processo de Negócio em Texto em Linguagem Natural,5008272,01
"Subtractive  and  additive  synthesis  are  two  powerful  sound  synthesis  techniques  that 
caused a revolution when the first electronic and electro mechanic music instruments started to 
appear  some decades ago.  Subtractive synthesis became very popular during the 60s and 70s 
after the creation of analog hardware modules that could be interconnected, creating the concept 
of the modular synthesizers. After the initial impact, for some years these instruments faced a 
slow-down in its usage, a tendency that was reverted on the past decade. Nevertheless, the prices 
of these instruments are often high.  Digital synthesizers also offer the subtractive synthesis 
technique,  by  using  customized  electronic  components  designed  and  developed  by  the 
synthesizers vendors in order to use the most up-to-date technologies and signal processing 
techniques, which also leads to high prices.
In this project, we investigate the hypothesis that it is possible to design and develop a 
good quality music instrument with low budget electronic components and limited processing 
capabilities, by implementing this on a low budget and easy to use platform. The development 
is based on object oriented design, creating software modules that replicates the functionalities 
of analog synthesizer hardware modules. With this approach, we have a modular software that 
can be easily changed based on programmers’ preferences.
The implementation was tested  on the Arduino Due  board, which is a cheap, easy to use 
and widely available platform and powered by a 32-bits ARM 84Mhz processor. We were able 
to add oscillators with anti-aliasing  algorithms, filters, envelope  generators, delay  effects, a 
MIDI interface and a keybed, making a complete synthesizer. In addition to this, we included 
an  additive  synthesis  organ  design  with  full  polyphony  based  on  classic  organs  design, 
demonstrating the possibility of having two powerful synthesis methods on a cheap and widely 
available platform. 
With this design, suitable for low cost platforms, we intend to contribute to the maker 
movement and encourage new implementations in this area, especially in the computing and 
engineering  fields,  increasing  the  usage  and  access  to  (electronic)  musical  instruments  and 
musical creativity.",,Computação Visual,RODOLFO PEDO PIROTTI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,09/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Music  computing;Synthesizers;Digital  audio  processing;Arduino  Due;DIY',"LINHA INTERAÇÃO HUMANO-COMPUTADOR, REALIDADE VIRTUAL E AUMENTADA",MARCELO SOARES PIMENTA,99,Computação e música;Sintetizadores;Processamento digital de áudio;Arduino;DIY.,COMPUTAÇÃO (42001013004P4),-,"Existem inúmeras técnicas de síntese de áudio utilizadas atualmente em instrumentos musicais profissionais, dentre as quais as mais fundamentais são a síntese aditiva e a síntese subtrativa. A síntese subtrativa se tornou popular e foi muito explorada entre as décadas de 60 e 70 com a criação de módulos analógicos de hardware que podiam ser interconectados, criando o conceito de sintetizador analógico modular. Apesar do uso deste tipo de sintetizador ter diminuído durante as décadas subsequentes, nos últimos anos sua utilização voltou a crescer e diversos modelos deste tipo de instrumento são vendidos atualmente, porém em geral a preços elevados. Sintetizadores digitais também disponibilizam a técnica de síntese subtrativa utilizando componentes eletrônicos dedicados, o que ainda mantém seus preços elevados.
Neste trabalho investigamos a hipótese de que é possível desenvolver um instrumento musical funcional e de qualidade com recursos limitados de processamento, e exploramos essa hipótese implementando síntese subtrativa em uma plataforma acessível e de baixo custo. O desenvolvimento é baseado em linguagem orientada a objetos para criação de módulos de software replicando as características dos módulos encontrados em sintetizadores analógicos modulares. Com esta abordagem, obtemos um software modular que pode ser facilmente modificado baseado nas preferências do programador.
A implementação foi testada na plataforma Arduino Due, que é uma plataforma de baixo custo e contém um processador 32-bits ARM 84 MHz. Foi possível adicionar osciladores com algoritmo anti-aliasing, filtros, geradores de envelope, módulo de efeito, uma interface MIDI e um teclado externo, obtendo assim um sintetizador subtrativo completo. Além disto, incluímos no desenvolvimento a implementação de um órgão baseado em síntese aditiva, com polifonia completa e inspirado na arquitetura de órgãos clássicos, mostrando a possibilidade de possuir dois importantes e poderosos métodos de síntese em uma plataforma acessível e de baixo custo.
Com esta implementação aberta e pública, buscamos contribuir com o movimento maker e faça-você-mesmo, incentivando novos e inovadores desenvolvimentos nesta área, em especial na computação e engenharia, aumentando o uso e acesso a instrumentos musicais eletrônicos e a criatividade musical.",DISSERTAÇÃO,Arquitetura e Implementação Aberta de um Sintetizador Subtrativo e Aditivo para Plataforma de Baixo Custo,5008273,01
"Social Media platforms have become key as a means of spreading information, opinions or awareness about real-world events. Twitter stands out due to the huge volume of messages about all sorts of topics posted every day. Such messages are an important source of useful information about events, presenting many useful applications (e.g. the detection of breaking news, real-time awareness, updates about events). However, text classification on Twitter is by no means a trivial task that can be handled by conventional Natural Language Processing techniques. In addition, there is no consensus about the definition of which kind of tasks are executed in the Event Identification and Classification in tweets, since existing approaches often focus on specific types of events, based on specific assumptions, which makes it difficult to reproduce and compare these approaches in events of distinct natures. In this work, we aim at building a unifying framework that is suitable for the classification of events of distinct natures. The framework has as key elements: a) external enrichment using related web pages for extending the conceptual features contained within the tweets; b) semantic enrichment using the Linked Open Data cloud to add related semantic features, and c) a pruning technique that selects the semantic features with discriminative potential. We evaluated our proposed framework using a broad experimental setting, that includes: a) seven target events of different natures; b) different combinations of the conceptual features proposed (i.e. entities, vocabulary and their combination); c) distinct feature extraction strategies (i.e. from tweet text and web related documents); d) different methods for selecting the discriminative semantic features (i.e. pruning, feature selection, and their combination); e) and two classification algorithms. We also compared the proposed framework against another kind of contextual enrichment based on word embeddings. The results showed the advantages of using the proposed framework, and that our solution is a feasible and generalizable method to support the classification of distinct event types.",,Ciência de Dados e Engenharia de Software,SIMONE APARECIDA PINTO ROMERO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,16/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Semantic Web;DBPedia;LOD;Twitter;Event Classification',"LINHA MINERAÇÃO, INTEGRAÇÃO E ANÁLISE DE DADOS",KARIN BECKER,128,Web Semântica;DBpedia;LOD;Twitter;Classificação de Eventos,COMPUTAÇÃO (42001013004P4),-,"As plataformas de Mídias Sociais se tornaram um meio essencial para a disponibilização de
informações. Dentre elas, o Twitter tem se destacado, devido ao grande volume de mensagens que são compartilhadas todos os dias, principalmente mencionando eventos ao redor do
mundo. Tais mensagens são uma importante fonte de informação e podem ser utilizadas em
diversas aplicações. Contudo, a classificação de texto em tweets é uma tarefa não trivial. Além
disso, não há um consenso quanto à quais tarefas devem ser executadas para Identificação e
Classificação de Eventos em tweets, uma vez que as abordagens existentes trabalham com tipos
específicos de eventos e determinadas suposições, que dificultam a reprodução e a comparação
dessas abordagens em eventos de natureza distinta.
Neste trabalho, nós elaboramos um framework para a classificação de eventos de natureza distinta. O framework possui os seguintes elementos chave: a) enriquecimento externo a partir
da exploração de páginas web relacionadas, como uma forma de complementar a extração de
features conceituais do conteúdo dos tweets; b) enriquecimento semântico utilizando recursos
da Linked Open Data cloud para acrescentar features semânticas relacionadas; e c) técnica de
poda para selecionar as features semânticas mais discriminativas.
Nós avaliamos o framework proposto através de um vasto conjunto de experimentos, que incluem: a) sete eventos alvos de natureza distinta; b) diferentes combinações das features conceituais propostas (i.e. entidades, vocabulário, e a combinação de ambos); c) estratégias distintas
para a extração de features (i.e. a partir do conteúdo dos tweets e das páginas web); d) diferentes
métodos para a seleção das features semânticas mais relevantes de acordo com o domínio (i.e.
poda, seleção de features, e a combinação de ambos); e) dois algoritmos de classificação. Nós
também comparamos o desempenho do framework em relação a outro método utilização para
o enriquecimento contextual, o qual tem como base word embeddings.
Os resultados mostraram as vantagens da utilização do framework proposto e que a nossa solução é factível e generalizável, dando suporte a classificação de diferentes tipos de eventos.",DISSERTAÇÃO,A Framework for Event Classification in Tweets Based on Hybrid Semantic Enrichment,5008369,01
"Simultaneous Localization and Mapping (SLAM), fundamental for building robots with true autonomy, is one of the most difficult problems in Robotics and consists of estimating the position of a robot that is moving in an unknown environment while incrementally building the map of such environment. Arguably the most crucial requirement to obtain proper localization and mapping is precise place recognition, that is, determining if the robot is at the same place in different occasions just by looking at the observations taken by the robot. Most approaches in literature are good when using highly expressive sensors such as cameras or when the robot is situated in low ambiguous environments. However, this is not the case, for instance, using robots equipped only with range-finder sensors in highly ambiguous indoor structured environments. A good SLAM strategy must be able to handle these scenarios, deal with noise and observation errors, and, especially, model the environment and estimate the robot state in an efficient way. 
Our proposal in this work is to translate sequences of raw laser measurements into an efficient and compact text representation and deal with the place recognition problem using linguistic processing techniques. First, we translate raw sensor measurements into simple observation values computed through a novel observation model based on kernel-density estimation called Free-Space Density (FSD). These values are quantized into significant classes allowing the division of the environment into contiguous regions of homogeneous spatial density, such as corridors and corners. Regions are represented in a compact form by simple words composed of three syllables – the value of spatial density, the size and the variation of orientation of that region. At the end, the chains of words associated to all observations made by the robot compose a text, in which we search for matches of n-grams (i.e. sequences of words), which is a popular technique from shallow linguistic processing. The technique is also successfully applied in some scenarios of long-term operation, where we must deal with semi-static objects (i.e. that can move occasionally, such as doors and furniture). All approaches were evaluated in simulated and real scenarios obtaining good results.",,Inteligência Artificial,RENAN DE QUEIROZ MAFFEI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,06/04/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Mobile Robots;Localization;Mapping;Place Recognition;Free-Space Density;n-grams',"LINHA PLANEJAMENTO, SISTEMAS MULTIAGENTES E ROBÓTICA",EDSON PRESTES E SILVA JUNIOR,162,Robôs móveis;Localização;Mapeamento;Reconhecimento de local;Densidade de Espaço Livre;n-gramas,COMPUTAÇÃO (42001013004P4),-,"Localização e Mapeamento Simultâneos (SLAM), fundamental para robôs dotados de
verdadeira autonomia, é um dos problemas mais difíceis na Robótica e consiste em estimar a posição de um robô que está se movendo em um ambiente desconhecido, enquanto
incrementalmente constrói-se o mapa de tal ambiente. Provavelmente o requisito mais importante para localização e mapeamento adequados seja um preciso reconhecimento de
local, isto é, determinar se um robô estava no mesmo lugar em diferentes ocasiões apenas
analizando as observações feitas pelo robô em cada ocasião. A maioria das abordagens da
literatura são boas quando se utilizam sensores altamente expressivos, como câmeras, ou
quando o robô está situado em ambientes com pouco ambiguidade. No entanto, este não
é o caso, por exemplo, quando o robô equipado apenas com sensores de alcance está em
ambientes internos estruturados altamente ambíguos. Uma boa estratégia deve ser capaz
de lidar com tais ambientes, lidar com ruídos e erros nas observações e, especialmente,
ser capaz de modelar o ambiente e estimar o estado do robô de forma eficiente. Nossa
proposta consiste em traduzir sequências de medições de laser em uma representação de
texto eficiente e compacta, para então lidar com o problema de reconhecimento de local
usando técnicas de processamento lingüísticos. Nós traduzimos as medições dos sensores em valores simples computados através de um novo modelo de observação baseado
em estimativas de densidade de kernel chamado de Densidade de Espaço Livre (FSD).
Estes valores são quantificados permitindo a divisão do ambiente em regiões contíguas
de densidade homogênea, como corredores e cantos. Regiões são representadas de forma
compacta por simples palavras descrevendo o valor de densidade espacial, o tamanho e
a variação da orientação daquela região. No final, as cadeias de palavras compõem um
texto, no qual se buscam casamentos de n-gramas (isto é, sequências de palavras). Nossa
técnica também é aplicada com sucesso em alguns cenários de operação de longo-prazo,
onde devemos lidar com objetos semi-estáticos (i.e. que se movem ocasionalmente, como
portas e mobílias). Todas as abordagens foram avaliadas em cenários simulados e reais
obtendo-se bons resultados.",TESE,Translatting sensor measurements into texts for localization and mapping with mobile robots,5008387,01
"The popularization of high-resolution digital video applications brings several challenges 
on developing new and efficient techniques to maintain the video compression efficiency. To 
respond to this demand, the HEVC standard was proposed aiming to duplicate the compression 
rate when compared to its predecessors. However, to achieve such goal, HEVC imposes a high 
computational cost and, consequently, energy consumption increase. This scenario becomes 
even  more  concerned  under  battery-powered  mobile devices  which  present  computational 
constraints  to  process  multimedia  applications.  Most  of  the  related  works  about  encoder 
realization,  typically  concentrate  their  contributions  on  computational  effort  reduction  and 
management. Therefore, there is a lack of information regarding energy consumption on video 
encoders, specially about the energy impact of the cache hierarchy in this context.
This thesis presents a methodology for energy characterization of the HEVC video encoder
in general purpose processors. The main goal of this methodology is to provide quantitative 
data regarding the HEVC energy consumption. This methodology is composed of two modules, 
one focuses on the HEVC processing and the other focuses on the HEVC behavior regarding
cache memory-related consumption. One of the main advantages of this second module is to 
remain independent of application or processor architecture.
Several analyzes are performed aiming at the energetic characterization of HEVC coding 
considering different video sequences, resolutions, and parameters. In addition, an extensive 
and detailed analysis of different cache configurations is performed in order to evaluate the 
energy impact of such configurations during the video coding execution. The results obtained 
with  the  proposed  characterization  demonstrate  that  the  management  of  the  video  coding 
parameters in conjunction with the cache specifications has a high potential for reducing the 
energy consumption of video coding whereas maintaining good coding efficiency results.",,Projeto de Sistemas Eletrônicos e Computacionais,EDUARDA RODRIGUES MONTEIRO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,24/01/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Video  Coding;HEVC;Energy  Consumption;Cache Memory;General  Purpose  Processor',LINHA CONCEPÇÃO DE CIRCUITOS E SISTEMAS INTEGRADOS,SERGIO BAMPI,144,Codificação de Vídeo;HEVC;Consumo de Energia;Memória Cache;Processadores de Propósito Geral,COMPUTAÇÃO (42001013004P4),-,"A popularização das aplicações que manipulam vídeos digitais de altas resoluções incorpora diversos desafios no desenvolvimento de novas e eficientes técnicas para manter a eficiência na compressão de vídeo.  Para lidar com esta demanda, o padrão HEVC foi proposto com o objetivo de duplicar as taxas de compressão quando comparado com padrões predecessores. No entanto, para atingir esta meta, o HEVC impõe introduz um elevado custo computacional e, consequentemente, o aumento no consumo de energia. Este cenário torna-se ainda mais preocupante quando considerados dispositivos móveis alimentados por bateria os quais apresentam restrições computacionais no processamento de aplicações multimídia. A maioria dos trabalhos relacionados com este desafio, tipicamente, concentram suas contribuições no redução e controle do esforço computacional refletido no processo de codificação. Entretanto, a literatura indica uma carência de informações com relação ao consumo de energia despendido pelo processamento da codificação de vídeo e, principalmente, o impacto energético da hierarquia de memória cache neste contexto.
Esta tese apresenta uma metodologia para caracterização energética da codificação de vídeo HEVC em processador de propósito geral. O principal objetivo da metodologia proposta nesta tese é fornecer dados quantitativos referentes ao consumo de energia do HEVC. Esta metodologia é composta por dois módulos, um deles voltado para o processamento da codificação HEVC e, o outro direcionado ao comportamento do padrão HEVC no que diz respeito à memória cache. Uma das principais vantagens deste segundo módulo é manter-se independente de aplicação ou de arquitetura de processador.
Neste trabalho diversas análises foram realizadas visando a caracterização energética da codificação HEVC em processador de propósito geral, considerando diferentes sequências de vídeo, resoluções e parâmetros do codificador. Além disso, uma análise extensa e detalhada de diferentes configurações possíveis de memória cache foi realizada com o propósito de avaliar o impacto energético destas configurações na codificação. Os resultados obtidos com a caracterização proposta demonstram que o gerenciamento dos parâmetros da codificação de vídeo de maneira conjunta com as especificações da memória cache tem um alto potencial para redução do consumo energético de codificação de vídeo mantendo bons resultados de qualidade visual das sequências codificadas.",TESE,Caracterização Energética da Codificação de Vídeo de Alta Eficiência (HEVC) em Processador de Propósito Geral,5009911,01
"Software defined networks are a novel approach to design and operation of computer networks. Although this paradigm is employed successfully in many data-centers and campus, software defined access networks (SDAN) are still in their infancy. Carriers and ISPs
have not converged on a standard architecture to build their infrastructure upon, a problem
many initiatives are trying to solve. Regardless of the chosen model, the option for such a
disruptive change as SDN needs to be justified. Techno-economic models are commonly
used to estimate the impact of a change in the operation. The Total Cost of Ownership (TCO) is a metric commonly used for this purpose. The present work developed
a simplified evaluation model for network architectures, which focuses on the services
offered to the subscriber and on the cost of managing them. The analysis is performed
in two dimensions. In the first, the effort to operate the network is estimated through a
qualitative analysis, similar to the one performed by other existing models. The second
axis is the amount of resources required to provision and monitor each service. This estimation is produced through the use of discrete time simulation of selected elements and
protocols. The combination of the analyzes allows us to identify the behavioral trends
provoked by the adoption of a new network architecture, and to evaluate the convenience
of such migration. The simulation in this work demanded models for the equipment and
protocols involved. The package NS-3 was the chosen simulation tool. Defining and implementing models for all services analyzed could become as costly as the techniques we
wish to overcome, so an indirect approach has been used. Only the essential components
are simulated, and the costs of each service are inferred from them. The SDAN model
evaluated in this work, called SDCN, is a simplification of the SplitArchitecture proposed
by the SPARC project.",,Redes de Computadores,ALEXSANDER SILVA DE SOUZA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,28/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Software defined networks;Internet Architecture;Modeling',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",JUERGEN ROCHOL,102,Redes definidas por software;arquitetura da Internet;modelagem,COMPUTAÇÃO (42001013004P4),-,"Redes definidas em software (SDN) são uma abordagem recente para o projeto e operação de redes de computadores. Este paradigma é empregado com sucesso em cenários de datacenter, no entanto rede de acesso definidas em software (SDAN) ainda são raras em operadoras de telefonia e provedores de Internet. Isso deve-se em grande parte a inexistência de uma arquitetura de referência, algo que diversos projetos tentaram suprir nos últimos anos. Independente da vertente escolhida, a opção por uma mudança tão disruptiva como o SDN precisa ser justificada. Modelos tecno-econômicos são comumente utilizados para estimar o impacto da mudança na rentabilidade da operação. O custo total de propriedade (TCO) é uma métrica utilizada para esse fim. O presente trabalho desenvolveu um modelo simplificado de comparação de arquiteturas de rede, que diferentemente dos demais, tem foco nos serviços oferecidos ao assinante e no custo de gerenciamento dos mesmos. A análise é realizada em duas dimensões. Na primeira o esforço de operar a rede é estimado através de uma análise qualitativa, de forma similar ao realizado por outros modelos. O segundo eixo é a quantidade de recursos exigida para provisionar e monitorar cada serviço. Essa estimativa é gerada através do uso de simulação de tempo discreto dos elementos e protocolos relevantes. A combinação das duas análises permite identificar as tendências de comportamento dos serviços geradas pela adoção de uma nova arquitetura de rede, e avaliar a conveniência dessa migração. O uso de simulação neste trabalho demandou a construção de modelos dos equipamentos e protocolos envolvidos. Elegeu-se o pacote NS-3 como ferramenta de simulação. Definir e implementar modelos para todos os serviços analisados poderia tornar-se tão oneroso quanto as técnicas que desejamos suplantar, por isso utilizou-se uma abordagem indireta. São simulados apenas os componentes essenciais, e os custos de cada serviço são inferidos a partir deles. O modelo de rede SDAN utilizado, denominado SDCN, é inspirado no SplitArchitecture proposto pelo projeto SPARC.",DISSERTAÇÃO,Modelo de avaliação de redes de acesso banda larga baseadas no paradigma SDN,5012461,01
"Pedestrian detection reliability is a fundamental problem for autonomous or aided driving. Methods that use object detection algorithms such as Histogram of Oriented Gradients or Convolutional Neural Networks are today very popular in automotive applications. Embedded Graphics Processing Units (GPUs) are exploited to make object detection in a very efficient manner. Unfortunately, GPUs architecture has been shown to be particularly vulnerable to radiation-induced failures. This work presents an experimental evaluation and analytical study of the reliability of two types of object detection algorithms: HOG and CNNs. This research aim is not just to quantify but also to qualify the radiation-induced errors on object detection applications executed in embedded GPUs.  HOG experimental results were obtained using two different architectures of embedded GPUs (Tegra and AMD APU), each exposed for about 100 hours to a controlled neutron beam at Los Alamos National Lab (LANL). Precision and Recall metrics are considered to evaluate the error criticality. The reported analysis shows that, while being intrinsically resilient (65% to 85% of output errors only slightly impact detection), HOG experienced some particularly critical errors that could result in undetected pedestrians or unnecessary vehicle stops. This works also evaluates the reliability of two Convolutional Neural Networks for object detection: You Only Look Once (YOLO) and Faster RCNN. Three different GPU architectures were exposed to controlled neutron beams (Kepler, Maxwell, and Pascal) detecting objects in both Caltech and Visual Object Classes data sets. By analyzing the neural network corrupted output, it is possible to distinguish between tolerable errors and critical errors, i.e., errors that could impact detection. Additionally, extensive GDB-level and architectural-level fault-injection campaigns were performed to identify HOG and Darknet critical procedures. Results show that not all stages of object detection algorithms are critical to the final classification reliability. Thanks to the fault injection analysis it is possible to identify HOG and Darknet portions that, if hardened, are more likely to increase reliability without introducing unnecessary overhead. The proposed HOG hardening strategy is able to detect up to 70% of errors with a 12% execution time overhead.",,Projeto de Sistemas Eletrônicos e Computacionais,FERNANDO FERNANDES DOS SANTOS,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,22/02/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Fault tolerance;soft error;pedestrian detection;hardening',LINHA CONFIABILIDADE E TOLERÂNCIA A FALHAS,PAOLO RECH,73,tolerância a falhas;soft error;detecção de pedestres;proteção por software,COMPUTAÇÃO (42001013004P4),-,"A confiabilidade de algoritmos para detecção de pedestres é um problema fundamental
para carros auto dirigíveis ou com auxílio de direção. Métodos que utilizam algoritmos
de detecção de objetos como Histograma de Gradientes Orientados (HOG - Histogram
of Oriented Gradients) ou Redes Neurais de Convolução (CNN – Convolutional Neural
Network) são muito populares em aplicações automotivas. Unidades de Processamento
Gráfico (GPU – Graphics Processing Unit) são exploradas para executar detecção de objetos de uma maneira eficiente. Infelizmente, as arquiteturas das atuais GPUs tem se mostrado particularmente vulneráveis a erros induzidos por radiação. Este trabalho apresenta
uma validação e um estudo analítico sobre a confiabilidade de duas classes de algoritmos
de detecção de objetos, HOG e CNN. Esta pesquisa almeja não somente quantificar, mas
também qualificar os erros produzidos por radiação em aplicações de detecção de objetos em GPUs embarcadas. Os resultados experimentais com HOG foram obtidos usando
duas arquiteturas de GPU embarcadas diferentes (Tegra e AMD APU), cada uma foi exposta por aproximadamente 100 horas em um feixe de nêutrons em Los Alamos National
Lab (LANL). As métricas Precision e Recall foram usadas para validar a criticalidade
do erro. Uma análise final mostrou que por um lado HOG é intrinsecamente resiliente
a falhas (65% a 85% dos erros na saída tiveram um pequeno impacto na detecção), do
outro lado alguns erros críticos aconteceram, tais que poderiam resultar em pedestres não
detectados ou paradas desnecessárias do veículo. Este trabalho também avaliou a confiabilidade de duas Redes Neurais de Convolução para detecção de Objetos:Darknet e Faster
RCNN. Três arquiteturas diferentes de GPUs foram expostas em um feixe de nêutrons
controlado (Kepler, Maxwell, e Pascal), com as redes detectando objetos em dois data
sets, Caltech e Visual Object Classes. Através da análise das saídas corrompidas das
redes neurais, foi possível distinguir entre erros toleráveis e erros críticos, ou seja, erros
que poderiam impactar na detecção de objetos. Adicionalmente, extensivas injeções de
falhas no nível da aplicação (GDB) e em nível arquitetural (SASSIFI) foram feitas, para
identificar partes críticas do código para o HOG e as CNNs. Os resultados mostraram que
não são todos os estágios da detecção de objetos que são críticos para a confiabilidade
da detecção final. Graças a injeção de falhas foi possível identificar partes do HOG e
da Darknet, que se protegidas, irão com uma maior probabilidade aumentar a sua confiabilidade, sem adicionar um overhead desnecessário. A estratégia de tolerância a falhas
proposta para o HOG foi capaz de detectar até 70% dos erros com 12% de overhead de
tempo.",DISSERTAÇÃO,Reliability evaluation and errors mitigation in pedestrian detection algorithms for embedded GPUs,5012714,01
"Truly autonomous robots must know the environment in order to execute complex tasks. In unknown environments, the robot must construct a map and localize itself using noisy proprioceptive and exteroceptive sensors. This is problematic,since the partial and possibly inaccurate map of the environment will be used to correct localization errors. This important problem of mobile robotics is known as Simultaneous Localization and Mapping (SLAM). When a robot autonomously execute a SLAM algorithm concurrently with an exploration strategy, this problem is called Active SLAM or Integrated Exploration. One of the main challenges behind both these problems is the treatment of loop closures. While the robot traverses unknown regions or sparse environments, the robot pose and the map may not be properly corrected due to lack of information. When this happens, the uncertainties about the map and the robot pose increase, which may lead to unrecoverable SLAM errors. On the other hand, when a loop is closed successfully, these uncertainties drastically decrease. Therefore, path chosen to explore the environment can considerably improve or degrade the quality of both localization and mapping. One well known way to explore the environment is the adaptation of the Boundary Value Problem (BVP) forthe Laplace Equation considering Dirichlet boundary conditions. Even though it is easy to implement, resulting in smooth exploration trajectories, it does not carefully address SLAM errors, since it follows a gradient decent which not always allows revisits, a crucial limitation for Active SLAM. Despite being a greedy frontier driven exploration strategy, we consider the ﬂexibility of the BVP and Dirichlet boundary conditions still under-explored for Active SLAM. Our proposal is to modify the BVP Exploration algorithm to execute complex exploration behaviors, such as revisits and, in particular, loop-closures. We present two new approaches: the ﬁrst makes use of a time driven boundary value condition together with potential distortions to generate loop closing behaviors and a potential ﬁeld that never ceases to exist, even after the exploration ends; the second enables loop closure behaviors with BVP by taking advantage of potential propagation in unknown space generated by a pair of dynamic boundary conditions functioning as virtual walls and goals. Both approaches take advantage of a local optimization that uses the Voronoi Skeleton to reduce the computational cost of the algorithm. Results show that the proposed approaches present better results than competing techniques.",,Inteligência Artificial,VITOR AUGUSTO MACHADO JORGE,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,23/03/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Active SLAM;BVP;Integrated Exploration;Potential Rails;Ouroboros;SLAM',"LINHA PLANEJAMENTO, SISTEMAS MULTIAGENTES E ROBÓTICA",EDSON PRESTES E SILVA JUNIOR,160,SLAM Ativo;BVP;Exploração Integrada;Potential Rails;Ouroboros;SLAM.,COMPUTAÇÃO (42001013004P4),-,"Robôs verdadeiramente autônomos devem conhecer o ambiente para executar tarefas complexas. Em ambientes desconhecidos o robô deve concorrentemente construir o mapa do ambiente e se localizar usando
sensores proprioceptivos e exteroceptivos imprecisos. Isto é problemático, uma vez que o mapa parcial e
possivelmente incorreto do ambiente será usado para corrigir erros de localização. Este problema importante da robótica móvel é conhecido como Localização e Mapeamento Simultâneos (SLAM). Quando
um robô autonomamente executa o algoritmo de SLAM concorrentemente com uma estratégia de exploração, o problema passa a se chamar SLAM Ativo ou Exploração Integrada. Um dos principais desafios
por trás destes problemas é o tratamento de fechamento de ciclos. Ao atravessar regiões desconhecidas
ou ambientes esparsos, a pose do robô e o mapa podem não ser propriamente corrigidos por falta de
informação. Quando isto acontece, as incertezas da posição do robô e do mapa aumentam, podendo
levar a erros irrecuperáveis. Por outro lado, quando o ciclo é fechado corretamente, estas incertezas
diminuem consideravelmente. Portanto, a escolha do caminho para explorar o ambiente pode drasticamente melhorar ou degradar a qualidade do mapeamento e da localização. Uma técnica bem conhecida
de exploração de ambientes é a adaptação do problema de valor de contorno (BVP) para a equação de
Laplace e condições de contorno de Dirichlet. Apesar de ser fácil de implementar, resultando em trajetórias de exploração suaves, esta técnica não endereça cuidadosamente erros de SLAM, uma vez que ela
segue a descida do gradiente, o que pode não possibilitar revisitas, uma limitação crucial para o SLAM
Ativo. Mesmo sendo uma técnica de exploração gulosa e direcionada a fronteiras, consideramos que a
flexibilidade do BVP e condições de contorno de Dirichlet ainda são pouco exploradas. Nossa proposta
é modificar o algoritmo de Exploração por BVP para executar comportamentos complexos, tais como
revisitas e, em particular, fechamentos de ciclo. Apresentamos duas novas abordagens: a primeira faz
uso de uma condição de contorno direcionada pelo tempo combinada a distorções de potencial para gerar comportamentos de fechamento de ciclo, além de um potencial que nunca cessa de existir, mesmo
após o ambiente ter sido completamente explorado; a segunda, propicia o fechamento de ciclos aproveitando a propagação do potencial em regiões desconhecidas, através de um par dinâmico de condições
de contorno que funcionam como obstáculos e objetivos virtuais. Ambas abordagens aproveitam o Esqueleto de Voronoi do ambiente para reduzir o custo computacional do algoritmo. Testes em ambientes
reais e simulados usando o robô Pioneer 3DX mostram que as técnicas apresentadas apresetam melhores
resultados quando comparadas a técnicas concorrentes.",TESE,Enabling Loop-Closures and Revisits in Active SLAM Techniques by using Dynamic Boundary Conditions and Local Potential Distortions,5013702,01
"Recently a new application domain characterized by the continuous and low-latency processing of large volumes of data has been gaining attention. The growing number of applications of such genre has led to the creation of Stream Processing Systems (SPSs), systems that abstract details that have no direct relation to the problem at hand. More recently, the ever increasing volumes of data to be processed gave rise to distributed SPSs. Currently there are in the market several distributed SPSs, however the existing benchmarks designed for evaluating this kind of system cover only a few applications and workloads, while these systems have a much wider set of applications. This project proposes a benchmark suite composed of applications from diverse areas with the purpose of testing SPSs with different workloads and scenarios. This suite was evaluated against Storm and Spark on the Azure Platform and the results have demonstrated the usefulness of the benchmark suite in comparing these systems.",,Sistemas de Computação,MAYCON VIANA BORDIN,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,11/04/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'distributed systems;benchmark suite;stream processing;real-time processing;big data',LINHA COMPUTAÇÃO DE ALTO DESEMPENHO E SISTEMAS DISTRIBUÍDOS,CLAUDIO FERNANDO RESIN GEYER,114,sistemas distribuídos;benchmark suíte;stream processing;processamento em tempo-real;big data,COMPUTAÇÃO (42001013004P4),-,"Um dado por si só não possui valor algum, a menos que ele seja interpretado, contextualizado e agregado com outros dados, para então possuir valor, tornando-o uma informação.
Em algumas classes de aplicações o valor não está apenas na informação, mas também na
velocidade com que essa informação é obtida. As negociações de alta frequência (NAF)
são um bom exemplo onde a lucratividade é diretamente proporcional a latência (LOVELESS; STOIKOV; WAEBER, 2013). Com a evolução do hardware e de ferramentas de
processamento de dados diversas aplicações que antes levavam horas para produzir resultados, hoje precisam produzir resultados em questão de minutos ou segundos (BARLOW,
2013).
Este tipo de aplicação tem como característica, além da necessidade de processamento
em tempo-real ou quase real, a ingestão contínua de grandes e ilimitadas quantidades
de dados na forma de tuplas ou eventos. A crescente demanda por aplicações com esses
requisitos levou a criação de sistemas que disponibilizam um modelo de programação que
abstrai detalhes como escalonamento, tolerância a falhas, processamento e otimização de
consultas. Estes sistemas são conhecidos como Stream Processing Systems (SPS), Data
Stream Management Systems (DSMS) (CHAKRAVARTHY, 2009) ou Stream Processing
Engines (SPE) (ABADI et al., 2005).
Ultimamente estes sistemas adotaram uma arquitetura distribuída como forma de lidar
com as quantidades cada vez maiores de dados (ZAHARIA et al., 2012). Entre estes
sistemas estão S4, Storm, Spark Streaming, Flink Streaming e mais recentemente Samza
e Apache Beam.
Estes sistemas modelam o processamento de dados através de um grafo de fluxo com
vértices representando os operadores e as arestas representando os data streams. Mas
as similaridades não vão muito além disso, pois cada sistema possui suas particularidades com relação aos mecanismos de tolerância e recuperação a falhas, escalonamento e
paralelismo de operadores, e padrões de comunicação.
Neste senário seria útil possuir uma ferramenta para a comparação destes sistemas em
diferentes workloads, para auxiliar na seleção da plataforma mais adequada para um trabalho específico. Este trabalho propõe um benchmark composto por aplicações de diferentes áreas, bem como um framework para o desenvolvimento e avaliação de SPSs
distribuídos.",DISSERTAÇÃO,A Benchmark Suite for Distributed Stream Processing Systems,5015875,01
"Mobile applications, originally developed for entertainment, nowadays are present in a wide 
range  of  domains,  being  common  even  in  areas  of  high  value  such  as  retailer,  logistics, 
banking,  and  medical,  among  others.  However,  the  quality  and  correctness  of  mobile 
applications  become  mandatory  and  testing  activities  are  essential.  However,  the  quality  of 
mobile applications is not always  good enough. This is because these applications suffer from 
market pressure and pass  through a very rapid development process  where the testing phase
usually  is  neglected  or  superficially  performed  by  the  development  team  itself,  thus
compromising  the  quality  of  the  application.  This  work  proposes  an  approach  based  on 
Behavior Driven Development to help to  define system tests for native Android applications. 
The  proposed  approach  uses  the  application's  layout  files  to  extract  information  about  the 
interface  components  and  the  events  expected  by  the  system.  From  this  information,   it  is 
possible to  check  out  the coverage of existing test scenarios against events available in the 
user  interface.  In  addition,  it  is  possible  to  identify  unexercised  usage  scenarios  from  the
existing  test  scenarios.  The  proposed  approach  is  implemented  by  a  tool  called  Android 
Behavior  Testing  Tool  which,  through  the  interpretation  of  the  BDD  usage  scenarios, 
provides  to  the  tester  an  overview  of  the  behavioral  flow  of  the  application  (otherwise 
unavailable), thus providing a notion of easy understanding of test coverage in relation to  the
application interface elements. In this way, the tester can judge the integrity of the available 
test  cases  in  relation  to  the  functionalities  implemented  and,  if  necessary,  implement  new 
tests. The tool also makes use of the application's layout files to identify  untested  interface 
components  and  in  this  case  generates  test  scenario  models  in  the  BDD  format,  thus 
automating  the  writing  task  of  the  scenarios.  The  proposed  approach  was  used  in  four 
Android applications and proved to be  useful, since in three case studies bugs were detected. 
Detected  bugs  originated  from  logical  inconsistencies  in  the  test  scenarios  or  elements  that 
were not exercised by the scenarios.",,Ciência de Dados e Engenharia de Software,FERNANDO WEBER ALBIERO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,25/04/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'System  test;Scenarios  automatization;Android;Mobile  computing;Behavior  Driven Development',LINHA ENGENHARIA DE SOFTWARE,ERIKA FERNANDES COTA,75,Teste de sistema;Automatização de cenários;Android;Computação móvel;BDD,COMPUTAÇÃO (42001013004P4),-,"Os aplicativos móveis, desenvolvidos originalmente para a área do entretenimento, hoje estão presentes nos mais diversos domínios, sendo comuns inclusive em áreas de alto valor agregado, como: varejista, logística, bancária, médica, entre outras. Portanto, a qualidade e correção dos aplicativos móveis tornam-se obrigatórios e as atividades de teste essenciais. Porém a qualidade das aplicações móveis nem sempre é satisfatória. Isso ocorre devido ao fato dessas aplicações sofrerem com a pressão do mercado e passarem por um processo muito rápido de desenvolvimento, onde geralmente a fase de testes é negligenciada ou realizada de forma superficial, pela própria equipe de desenvolvimento, comprometendo assim a qualidade da aplicação. Este trabalho propõe uma abordagem baseada no Behavior Driven Development para ajudar na definição de testes de sistema para aplicativos nativos do Android. A abordagem proposta utiliza os arquivos de leiaute da aplicação para extrair informações sobre os componentes da interface e sobre os eventos esperados pelo sistema. A partir dessas informações, é possível verificar a cobertura dos cenários existentes em relação aos eventos disponíveis na interface com o usuário. Além disso, é possível identificar elementos do leiaute que não são exercitados pelos cenários existentes. A abordagem proposta é implementada por uma ferramenta chamada Android Behavior Testing Tool que, por meio da interpretação dos cenários do BDD, fornece uma visão geral do fluxo comportamental da aplicação ao testador (visão hoje não disponível), proporcionando assim uma noção de fácil compreensão sobre a cobertura dos testes em relação aos elementos da interface do aplicativo. Desta forma, o testador pode julgar a integridade dos casos de teste disponíveis em relação às funcionalidades implementadas e, se necessário, implementar novos testes. A ferramenta também faz uso dos arquivos de leiaute do aplicativo para identificar os componentes da interface que não foram testados e gera, neste caso, modelos de cenários no formato do BDD, automatizando assim a tarefa de escrita dos mesmos. A abordagem proposta foi utilizada em quatro aplicativos Android e se mostrou útil, uma vez que, em três estudos de caso foram detectados bugs oriundos de inconsistências lógicas nos cenários ou elementos não exercitados pelos cenários.",DISSERTAÇÃO,Uma abordagem de teste para aplicativos Android utilizando os cenários do Behavior Driven Development,5017095,01
"The world’s elderly population is growing and, with it, the number of diagnoses of diseases related to old age, such as cognitive declines as well. These diseases usually affect
the autonomy of the elderly in their home, especially when it comes to performing daily
activities. With this in mind, it is necessary to employ caregivers and health services that
end up implying high costs. In this sense, the need arises to design robust, automated,
usable and low-cost systems for personal assistance. The design of these systems makes
reference to the area of Ambient Assisted Living. Therefore, this dissertation proposes
an approach that benefits the Ambient Assisted Living systems with the ability to predict
human actions for the facilitation of daily activities, particularly when cognitive declines
related to them occur. In this sense, a semantic meta-model was conceived for the generation of conceptual models of context and behavior, composed by human actions. From
this, the prediction of actions (information of support) is realized by a mechanism of
prediction and inference composed by a probabilistic semantic model. The approach is
demonstrated through a case study whose scenario represents a situation of cognitive decline, faced by a user, that prevents the conduct of a daily activity. Then, the prediction
and inference mechanism, using the probabilistic semantic model, predicts the most appropriate action that facilitates the conclusion of the activity. This forecast is evaluated to
gauge how well a user would be assisted, that is, if the intended operation was performed
by him. For this, a dataset related to the case study scenario and performance measures
such as precision, recall, and F-measure were used. The results of this evaluation are
promising, averaging 69.5% for precision, 100% for recall and 81% for F-measure. The
main contributions of this work are related to the semantic meta-model from which research in the area of this work can be used to generate behavioral models, and to the
probabilistic semantic model that performs prediction through uncertain reasoning over
behavior models, providing better decisions to help users with cognitive decline.",,Ciência de Dados e Engenharia de Software,GABRIEL MACHADO LUNARDI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,23/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Context-awareness;Probabilistic Ontologies;Ambient Assisted Living;Activities of Daily Living',LINHA MODELAGEM DE DADOS E DE PROCESSOS DE NEGÓCIOS,JOSE PALAZZO MOREIRA DE OLIVEIRA,76,Sensibilidade ao Contexto;Ontologias Probabilísticas;Ambientes de Vivência Assistida;Atividades Diárias,COMPUTAÇÃO (42001013004P4),-,"A população idosa mundial está crescendo e, com ela, o número de diagnósticos de doenças relacionadas à velhice como, por exemplo, declínios cognitivos também. Essas doenças costumam afetar a autonomia do idoso no seu lar, especialmente no que se refere à realização de atividades diárias. Com isso em vista, é preciso empregar cuidadores e serviços de saúde que acabam por implicar em altos custos. Nesse sentido, surge a necessidade de conceber sistemas robustos, automatizados, utilizáveis e de baixo custo para a assistência pessoal. A concepção desses sistemas faz menção à área de Ambientes de Vivência Assistida. Portanto, esta dissertação propõe uma abordagem que beneficia os sistemas para Ambientes de Vivência Assistida com a capacidade de prever ações humanas para a facilitação de atividades diárias, particularmente quando declínios cognitivos relacionados à elas ocorrerem. Nesse sentido, foi concebido um meta-modelo semântico para a geração de modelos conceituais de contexto e de comportamento, compostos pelas ações humanas. A partir disso, a previsão de ações (informação de suporte) é realizada por um mecanismo de predição e inferência composto por um modelo semântico probabilístico. A abordagem é demonstrada através de um estudo de caso cujo cenário representa uma situação de declínio cognitivo, enfrentada por um usuário, que impede a condução de uma atividade diária. Então, o mecanismo de predição e inferência, utilizando o modelo semântico probabilístico, prevê qual a ação mais adequada que facilite a conclusão da atividade. Essa previsão é avaliada para aferir o quão bem um usuário seria auxiliado, isto é, se a operação prevista foi por ele realizada. Para isso, foi utilizado um dataset relacionado ao cenário do estudo de caso e medidas de desempenho como a precisão, a revocação e a medida-F. Os resultados dessa avaliação se mostraram promissores sendo,
em média, 69,5% para a precisão, 100% para a revocação e 81% para a medida-F. As principais contribuições deste trabalho dizem respeito ao meta-modelo semântico a partir do qual pesquisas na área deste trabalho podem utilizar para gerar modelos de comportamento, e ao modelo semântico probabilístico que realiza predição através de raciocínio incerto sobre os modelos de comportamento, propiciando decisões mais precisas para auxiliar usuários com declínio cognitivo.",DISSERTAÇÃO,Previsão de Ações em Atividades Diárias para Assistir Pessoas com Declínio Cognitivo através de um Modelo Ontológico Probabilístico.,5017099,01
"A key challenge in Information Centric Networking (ICN) is to develop cache units
(also called Content Store - CS) that meet three requirements: large storage space, fast operation, and affordable cost. The so-called HCS (Hierarchical Content Store) is a promising approach to satisfy these requirements jointly. It explores the correlation between
content requests to predict future demands. Theoretically, this idea would enable proactively content transfers from a relatively large but slow cache area (Layer 2 - L2) to a
faster but smaller cache area (Layer 1 - L1). Thereby, it would be possible to increase the
throughput and size of CS in one order of magnitude, while keeping the cost.
However, the development of HCS introduces several practical challenges. HCS requires a careful coupling of L2 and L1 memory levels considering their transfer rates and
sizes. This requirement depends on both hardware specifications (e.g., read rate L2, use
of multiple physical SSD in parallel, bus speed, etc.), and software aspects (e.g., the SSD
controller, memory management, etc.).
In this context, this thesis presents two main contributions. First, we propose an architecture for overcoming the HCS bottlenecks by parallelizing multiple HCS. In summary,
the proposed scheme overcomes racing condition related challenges through deterministic
partitioning of content requests among multiple threads. Second, we propose a methodology to investigate the development of HCS exploiting emulation techniques and analytical modeling jointly. The proposed methodology offers advantages over prototyping and
simulation-based methods. We emulate the L2 to enable the investigation of a variety of
boundary scenarios that are richer (regarding both hardware and software aspects) than
would be possible through prototyping (considering current technologies). Moreover, the
emulation employs real code from a prototype for the other components of the HCS (e.g.,
L1, layers management and API) to provide more realistic results than would be obtained
through simulatio",,Redes de Computadores,RODRIGO BRANDAO MANSILHA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,21/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'information centric networking;caching;hierarquical memory;evaluation;emulation',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",ANTONIO MARINHO PILLA BARCELLOS,90,Redes orientadas a conteúdo;caching;memória hierárquica;avaliação;emulação,COMPUTAÇÃO (42001013004P4),-,"Um desafio fundamental em ICN (do inglês Information-Centric Networking) é desenvolver Content Stores (ou seja, unidades de cache) que satisfaçam três requisitos: espaço de armazenamento grande, velocidade de operação rápida e custo acessível. A chamada Hierarchical Content Store (HCS) é uma abordagem promissora para atender a esses requisitos. Ela explora a correlação temporal entre requisições para prever futuras solicitações. Por exemplo, assume-se que um usuário que solicita o primeiro minuto de um filme também solicitará o segundo minuto. Teoricamente, essa premissa permitiria transferir proativamente conteúdos de uma área de cache relativamente grande, mas lenta (Layer 2 - L2), para uma área de cache mais rápida, porém menor (Layer 1 - L1). A estrutura hierárquica tem potencial para incrementar o desempenho da CS em uma ordem de grandeza tanto em termos de vazão como de tamanho, mantendo o custo. Contudo, o desenvolvimento de HCS apresenta diversos desafios práticos. É necessário acoplar as hierarquias de memória L2 e L1 considerando as suas taxas de transferência e tamanhos, que dependem tanto de aspectos de hardware (por exemplo, taxa de leitura da L2, uso de múltiplos SSD físicos em paralelo, velocidade de barramento, etc.), como de software (por exemplo, controlador do SSD, gerenciamento de memória, etc.). Nesse contexto, esta tese apresenta duas contribuições principais. Primeiramente, é proposta uma arquitetura para superar os gargalos inerentes ao sistema através da paralelização de múltiplas HCS. Em resumo, o esquema proposto supera desafios inerentes à concorrência (especificamente, sincronismo) através do particionamento determinístico das requisições de conteúdos entre múltiplas threads. Em segundo lugar, é proposta uma metodologia para investigar o desenvolvimento de HCS explorando técnicas de emulação e modelagem analítica conjuntamente. A metodologia proposta apresenta vantagens em relação a metodologias baseadas em prototipação e simulação. A L2 é emulada para viabilizar a investigação de uma variedade de cenários de contorno (tanto em termos de hardware como de software) maior do que seria possível através de prototipação (considerando as tecnologias atuais). Além disso, a emulação emprega código real de um protótipo para os outros componentes do HCS (por exemplo L1, gerência das camadas e API) para fornecer resultados mais realistas do que seriam obtidos através de simulação.",TESE,Paralelizando Unidades de Cache Hierárquicas para Roteadores ICN,5028751,01
"This thesis presents algorithmic contributions for the encoding process of multiview videos. Analysis of current multi-view video coding standards is demonstrated, aiming to understand the key challenges of these encoders at system and algorithmic-modular level. In the system level it is presented a thread management technique associated to workload balancing and power control in multi-view video processing on multi-core platforms. In addition, at the algorithmic-modular level, techniques of rate control and data flow adjustment are proposed in the frame and basic unit levels, targeting best accuracy in the output dataflow of the encoder while delivering the desired video quality, considering the restrictions imposed by the transmission system. At the system level, the results obtained by the proposed techniques show that the thread management associated with workload balancing allows a significant reduction in coding complexity. This technique applied to a 32-core system reached 51% saving in energy consumption with up to 2% degradation in the visual quality of the video. At the modular level, the predictive techniques applied in rate control level associated with the regions of interest algorithm in the second level generate a reinforcement of learning the overall control model. With these techniques there was a significant reduction in the flow variation, not exceeding 1% of the variation in the output data, in addition, the visual quality suffered a maximum loss of 1.5%.",,Projeto de Sistemas Eletrônicos e Computacionais,BRUNO BOESSIO VIZZOTTO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,07/06/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Digital Video Coding;Multiview Videos;Rate Control;Workload Balance;Threads Management',LINHA CONCEPÇÃO DE CIRCUITOS E SISTEMAS INTEGRADOS,SERGIO BAMPI,95,Codificação de Vídeo Digital;Vídeos de Múltiplas Vistas;Controle de  Taxa;Balanceamento de Workload;Gerenciamento de Threads,COMPUTAÇÃO (42001013004P4),-,"Esta tese propõe contribuições para o processo de codificação de vídeos de múltiplas 
vistas. Uma análise dos  padrões  atuais de codificação de vídeos de múltiplas vistas  é 
apresentado  destacando  os  principais  desafios  destes  codificadores  considerando 
comunicação e processamento. Esta tese apresenta duas contribuições. Primeiramente, 
técnicas de controle de taxa e ajuste de fluxo de dados são propostos nos níveis de quadros 
e  unidades  básicas,  objetivando  melhor  precisão  na  saíta  do  bitstream  do  codificado 
enquanto  entregando  uma  determinada  qualidade  visual,  ao  considerar  as  restrições 
impostas pelo sistema de transmissão. Técnicas preditivas no nível de quadros associadas 
com  um  algoritmo  de  região  de  interesses  no  nível  de  unidades  básicas  gerando 
aprendizagem por reforço no modelo de controle geral apresentam significativa redução 
na variação da taxa de bits. O modelo proposto não excede 1% de variação nos dados de 
saída.  Ademais,  a  qualidade  visual  sofre  uma  perda  máxima  de  1,5%.  Segundo,  um 
gerenciador de threads associado a um  balanceador de carga de trabalho e controle de 
potência para processamento de vídeos de múltiplas vistas em plataformas de múltiplos 
núcleos. Esta técnica aplicada a um sistema de 32 núcleos atinge até 51% de economia 
no consumo de energia com uma degradação visual na qualidade do vídeo de até 2% se 
comparada ao software de referência.",TESE,Efficient Algorithms for Process and Communication of Multiview Videos: Contributions in Rate Control and Thread Management for 3D-Videos,5028776,01
"Power grids have great influence on the development of the world economy. Given the importance of the electrical energy to our society, power grids are often target of network intrusion
motivated by several causes. To minimize or even to mitigate the aftereffects of network intrusions, more secure protocols and standardization norms to enhance the security of power grids
have been proposed. In addition, power grids are undergoing an intense process of modernization, and becoming highly dependent on networked systems used to monitor and manage
power components. These so-called Smart Grids comprise energy generation, transmission,
and distribution subsystems, which are monitored and managed by Supervisory Control and
Data Acquisition (SCADA) systems. In this Masters dissertation, we investigate and discuss
the applicability and benefits of using Software-Defined Networking (SDN) to assist in the deployment of next generation SCADA systems. We also propose an Intrusion Detection System
(IDS) that relies on specific techniques of traffic classification and takes advantage of the characteristics of SCADA networks and of the adoption of SDN/OpenFlow. Our proposal relies on
SDN to periodically gather statistics from network devices, which are then processed by OneClass Classification (OCC) algorithms. Given that attack traces in SCADA networks are scarce
and not publicly disclosed by utility companies, the main advantage of using OCC algorithms
is that they do not depend on known attack signatures to detect possible malicious traffic. As
a proof-of-concept, we developed a prototype of our proposal. Finally, in our experimental
evaluation, we observed the performance and accuracy of our prototype using two OCC-based
Machine Learning (ML) algorithms, and considering anomalous events in the SCADA network,
such as a Denial-of-Service (DoS), and the failure of several SCADA field devices.",,Redes de Computadores,EDUARDO GERMANO DA SILVA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,30/01/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'supervisory control and data acquisition;software-defined networking;smart grids;network-based intrusion detection system;one-class classification',LINHA SEGURANÇA CIBERNÉTICA,ALBERTO EGON SCHAEFFER FILHO,100,occ.;nids;smart grids;scada;sdn,COMPUTAÇÃO (42001013004P4),-,"Sistemas elétricos possuem grande influência no desenvolvimento econômico mundial. Dada a importância da energia elétrica para nossa sociedade, os sistemas elétricos frequentemente são alvos de intrusões pela rede causados pelas mais diversas motivações. Para minimizar ou até mesmo mitigar os efeitos de intrusões pela rede, estão sendo propostos mecanismos que aumentam o nível de segurança dos sistemas elétricos, como novos protocolos de comunicação e normas de padronização. Além disso, os sistemas elétricos estão passando por um intenso processo de modernização, tornando-os altamente dependentes de sistemas de rede responsáveis por monitorar e gerenciar componentes elétricos. Estes, então denominados Smart Grids, compreendem subsistemas de geração, transmissão, e distribuição elétrica, que são monitorados e gerenciados por sistemas de controle e aquisição de dados (SCADA). Nesta dissertação de mestrado, investigamos e discutimos a aplicabilidade e os benefícios da adoção de Redes Definidas por Software (SDN) para auxiliar o desenvolvimento da próxima geração de sistemas SCADA. Propomos também um sistema de detecção de intrusões (IDS) que utiliza técnicas específicas de classificação de tráfego e se beneficia de características das redes SCADA e do paradigma SDN/OpenFlow. Nossa proposta utiliza SDN para coletar periodicamente estatísticas de rede dos equipamentos SCADA, que são posteriormente processados por algoritmos de classificação baseados em exemplares de uma única classe (OCC). Dado que informações sobre ataques direcionados à sistemas SCADA são escassos e pouco divulgados publicamente por seus mantenedores, a principal vantagem ao utilizar algoritmos OCC é de que estes não dependem de assinaturas de ataques para detectar possíveis tráfegos maliciosos. Como prova de conceito, desenvolvemos um protótipo de nossa proposta. Por fim, em nossa avaliação experimental, observamos a performance e a acurácia de nosso protótipo utilizando dois tipos de algoritmos OCC, e considerando eventos anômalos na rede SCADA, como um ataque de negação de serviço (DoS), e a falha de diversos dispositivos de campo.",DISSERTAÇÃO,A One-Class NIDS for SDN-based SCADA Systems,5028813,01
"Indirect illumination on a rendered scene can add a great deal to its visual quality, but it is also a costly operation. Therefore, a lot of research targets how to render indirect illumination in real-time. While powerful techniques for real-time indirect illumination currently exist, they provide only coarse-grained artistic control over the trade-off between quality and speed. We propose a Virtual Light Mesh to compute the scene's diffuse indirect illumination, inspired by the use of other current auxiliary meshes such as Navigation Meshes and Collision Meshes. A Virtual Light Mesh (VLM) is a simplified mesh of polygonal lights used to approximate the light bounced by the real geometry. Together with the VLM, we design an acceleration data structure for efficient indirect illumination performance with a complex VLM. The use of VLM presents some positive properties: greater artistic control of the indirect illumination characteristics; the possibility of integration with existing techniques such as skeletal animation and procedural generation; and simple integration into existing asset production tools and pipelines. Our experimental results show that artist controlled indirect illumination is a viable alternative to existing methods.",,Computação Visual,FABIO DE ALMEIDA AQUOTTE,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,05/05/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'3D;Graphics;Rendering;Real-time;Global Illumination',LINHA COMPUTAÇÃO GRÁFICA E VISUALIZAÇÃO DE DADOS,MARCELO WALTER,60,3D;Gráficos;Renderização;Tempo Real. Iluminação Global,COMPUTAÇÃO (42001013004P4),-,"A iluminação indireta é capaz de elevar consideravelmente a qualidade visual de cenas
renderizadas, mas é também uma operação custosa. Por este motivo, há muito esforço
de pesquisa voltado para a renderização de iluminação indireta em tempo real. Apesar
de atualmente existirem técnicas poderosas para a iluminação indireta em tempo real,
elas fornecem ao artista apenas um controle grosseiro do equilíbrio entre qualidade e
desempenho. Nós propomos uma Malha de Luzes Virtuais para calcular a iluminação
indireta difusa numa cena, inspirados pelo uso de outras malhas auxiliares, como Malhas
de Navegação e Malhas de Colisão. Uma Malha de Luzes Virtuais (MLV) é uma malha
simplificada de luzes poligonais usadas para aproximar a luz refletida pela geometria real.
Juntamente com a MLV, nós projetamos uma estrutura de dados de aceleração para atingir
um desempenho eficiente com iluminação indireta usando uma MLV complexa. O uso
da MLV apresenta algumas características vantajosas: maior controle artístico dos atributos da iluminação indireta; a possibilidade de integração com técnicas existentes como
animação esquelética e geração procedural; e integração simples com ferramentas e processos de produção de arte existentes. Nossos resultados experimentais mostram que a
iluminação indireta controlada por artistas é uma alternativa viável a métodos existentes",DISSERTAÇÃO,Real-time Diffuse Indirect Illumination with Virtual Light Meshes,5028852,01
"Radiology is considered the most digital medical specialty because of the diffusion of protocols and the digitization of processes. With fast technological advances, the imaging-based diagnosis remains in constant evolution and such changes have never had as much repercussion on the health workers as today. Our challenge as computer scientists is to propose new approaches that facilitate medical work, instigate collaboration and, finally, as the most audacious goal, improve people's healthcare.
In such context, this thesis investigates the use of alternative interfaces for medical images analysis. Our research is organized in two main areas. The first one concerns mobile diagnosis, from the development of tools that allow access to medical images in computation environments with limited resources, to the evaluation of the diagnosis performed in these environments when compared to traditional devices (radiological workstations). The second stage is related to advanced approaches to visualize volumetric exams. In this case, we investigate the diagnostic capability of visualizing in virtual reality, as well as the quality of the reconstructions provided in such environments, usability and user discomforts. All the experimentation and development were carried out on professional systems and validated by specialists in diagnostic imaging.
Techniques for efficient medical images access in environments with limited resources, such as tablets and smartphones, are the main contributions regarding mobile applications. This also includes a study of the typical behavior of radiologist physicians when using a mobile viewer and the respective usability evaluations of that application. For the mobile diagnosis, the high accuracy rate for the evaluation of computed tomography (CT), magnetic resonance imaging (MRI) and radiography when compared to desktop computers, stands out.
In addition, results obtained in the virtual reality visualization showed a highly accurate rate for the identification of superficial fractures (in 3D CT studies). The radiologists perception after using the immersive application has brought indications of what areas in medicine virtual reality can bring real benefits. Examples include surgeries planning, visualization of complex fractures and distraction of patients in painful procedures, among others.
In light of the presented results, the potential of the mobile devices for the evaluation of diagnostic images, mainly in cases of emergency, being fundamental for agility in the patients health care is noticeable. Virtual reality in radiology has the potential to revolutionize the interfaces for manipulation of clinical data, creating a new paradigm of interpretation in diagnostic medical images.",,Computação Visual,JOSE EDUARDO VENSON,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,10/07/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Radiology;Mobile Devices;Healthcare;Virtual Reality',"LINHA INTERAÇÃO HUMANO-COMPUTADOR, REALIDADE VIRTUAL E AUMENTADA",ANDERSON MACIEL,75,Radiologia;Dispositivos Móveis;Cuidados a Saúde;Realidade Virtual,COMPUTAÇÃO (42001013004P4),-,"A radiologia é especialidade médica mais informatizada, graças a difusão de protocolos e a digitalização dos processos. Com os avanços da tecnologia, o diagnóstico por
imagens se mantém em constante evolução e tais mudanças nunca tiveram tanta repercussão sobre o trabalho dor profissionais de saúde como agora. Nosso desafio enquanto
pesquisadores de computação é propor abordagens que facilitem o trabalho médico, instigue a colaboração e, por fim, como objetivo mais audacioso, melhore os cuidados à saúde
das pessoas.
Nesse contexto, esta dissertação investiga a inserção de interfaces alternativas para
análise de imagens médicas diagnósticas. Ela está organizada em duas áreas principais, a
primeira diz respeito ao diagnóstico móvel, desde o desenvolvimento de ferramentas que
permitam o acesso a imagens digitais em ambientes com recursos limitados, até a avaliação do diagnóstico realizado nesses ambientes comparado a dispositivos tradicionais
(workstations radiológicas). A segunda etapa está relacionada a visualização avançada de
exames volumétricos, nesse caso investigamos a capacidade diagnóstica de visualizar em
realidade virtual, bem como a qualidade das reconstruções nesse ambiente, usabilidade e
desconfortos dos usuários. Toda a experimentação e desenvolvimento foi realizada sobre
sistemas profissionais e validados por médicos especialistas em imagens diagnósticas.
As principais contribuições referentes a aplicações móveis são técnicas para acesso
eficiente a imagens médicas em ambientes com recursos limitados, como tablets e smartphones, um estudo do comportamento típico de médicos radiologistas ao utilizarem um visualizador móvel e as respectivas avaliações de usabilidade dessa aplicação. Para o diagnóstico móvel, destaca-se a alta taxa de acerto para avaliação de exames de tomografia
computadorizada (TC), ressonância magnética (RM) e radiografias, quando comparado a
computadores desktop.
Além disso, resultados obtidos na visualização de imagens médicas em realidade virtual mostraram uma alta taxa de acerto na identificação de fraturas expostas (para imagens
3D de TC). A percepção dos radiologistas após utilizar a aplicação imersiva trouxe indicativos de quais áreas da medicina a realidade virtual pode trazer reais benefícios, como
no planejamento de cirurgias, visualização de fraturas complexas, distração de pacientes
em procedimentos dolorosos, entre outros.
À luz dos resultados apresentados, verifica-se o potencial dos dispositivos móveis para
avaliação de imagens diagnósticas, principalmente em casos de emergência, sendo fundamental para agilidade no cuidado à saúde de pacientes. A realidade virtual na radiologia
tem o potencial de revolucionar as interfaces para manipulação de dados clínicos, criando
uma novo paradigma de interpretação em imagens médicas diagnósticas.",DISSERTAÇÃO,Medical Image Analysis Based on Mobile and Virtual Reality Interfaces,5028877,01
"Embedded Multiprocessor systems are a reality, in both industry and academia sectors. Such devices offer parallel processing capabilities, aiming at covering the increasing requirements of complex applications. Underlying application workloads are susceptible to variation at runtime, which if not properly handled, may lead to the performance and power efficiency degradation. The continuous increase in the complexity of application workload and the size of emerging multiprocessor systems, calls for dynamic and distributed mapping solutions. The majority of the promoted mapping techniques are bespoke implementations, which consider an in-house operating system developed to a particular processor architecture. This practice restricts its adoption in other platforms, leading to extra design time, re-validation and, consequentially, a hidden cost that may well be quite high. In this scenario, this dissertation proposes a FreeRTOS extension that integrates the support to dynamic and distributed tasks mapping in multiprocessor systems. FreeRTOS is portable to more than 30 embedded processors architectures, increasing software portability and reducing development time. The proposed extension employs mapping techniques allowing FreeRTOS for handle high demands of application mapping in runtime. Another contribution of this work is the development of a framework, which enables the exploration of large systems while providing debugging facilities. The proposed framework provides the automatic generation of multiprocessor platforms, considering parameters of size, processor architecture, and an application set. The proposed FreeRTOS extension have been validated in more than one processor architecture from ARM Cortex-M family. Test cases were executed on large-scale platforms and at different levels of abstraction with cases of more than 120 applications incorporating more than 600 tasks processed. The results show that the proposed extension presents better or equal results to the literature.",,Projeto de Sistemas Eletrônicos e Computacionais,GEANCARLO ABICH,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,26/05/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Dynamic Mapping;Distributed Mapping;Embedded Kernel;Multiprocessor Systems;Modelling;Simulation',LINHA CONCEPÇÃO DE CIRCUITOS E SISTEMAS INTEGRADOS,RICARDO AUGUSTO DA LUZ REIS,86,Mapeamento  Dinâmico;Mapeamento  Distribuído;Kernel  Embarcado;Sistemas Multiprocessados;Multicore;Manycore;Modelagem e Simulação,COMPUTAÇÃO (42001013004P4),-,"Sistemas  de  Multiprocessados  Embarcados  são  uma  realidade,  tanto  no  setor  da 
indústria  e  quanto  no  setor  acadêmico.  Esses  dispositivos  oferecem  capacidades  de 
processamento  paralelo  objetivando  cobrir  requisitos  cada  vez  maiores  de  aplicações 
complexas.  A  carga  de  trabalho  subjacente  das  aplicações  é  suscetível  a  variação  em 
tempo de execução o que, se não for tratada adequadamente, pode levar a degradação de 
eficiência em  desempenho e energia. O aumento contínuo da complexidade da carga de 
trabalho  das  aplicações,  bem  como  do  tamanho  dos  sistemas  multiprocessados 
emergentes,  requer  soluções  de  mapeamento  dinâmicas  e  distribuídas.  A  maioria  das 
técnicas de mapeamento propostas são implementações personalizadas,  considerando  um 
sistema operacional interno desenvolvido para uma arquitetura  de processador  específica.
Essa prática restringe sua aplicação em outras plataformas, levando a um design extra, 
revalidação e, consequentemente, um custo oculto que pode ser um tanto quanto alto.
Neste  cenário,  esta  dissertação  propõe  a  extensão  do  FreeRTOS  para  suportar 
mapeamento  dinâmico  e  distribuído  de  tarefas  em  sistemas  multiprocessados.  O 
FreeRTOS tem portabilidade para mais de 30 arquiteturas de processadores embarcados,
aumentando  a  portabilidade  de  software  e  reduzindo  o  tempo  de  desenvolvimento.  A 
extensão proposta utiliza técnicas de mapeamento que permitem ao FreeRTOS atender a 
altas demandas de mapeamento de aplicações em tempo de execução. Outra contribuição 
deste  trabalho  é  o  desenvolvimento  de  um  framework  que  permite  a  exploração  de 
grandes sistemas fornecendo, simultaneamente, resultados para depuração. O framework 
proposto possibilita a geração automática de plataformas multiprocessadas considerando 
seu tamanho, a arquitetura do processador e um conjunto de aplicações.  A  descrição da
plataforma resultante é altamente escalável permitindo extração de dados em tempo de 
execução  e  alta  depuração.  Estas  características  permitiram  validar  a  extensão  do 
FreeRTOS proposta em mais de uma arquitetura de processador da família ARM CortexM. Os casos de teste foram executados em plataformas de grande escala e em diferentes 
níveis de abstração com casos de mais de 120 aplicações incorporando mais de 600 tarefas 
processadas.  Os  resultados  mostram  que  a  extensão  proposta  apresenta  resultados 
melhores ou iguais à literatura.",DISSERTAÇÃO,Extending FreeRTOS to Support Dynamic and Distributed Task Mapping in Multiprocessor Systems,5028893,01
"The  scaling  of  MOS  transistor  has  been  the  main  manufacturing  strategy  for  improving 
integrated  circuit  (IC)  performance.  However,  as  the  device  dimensions  shrink,  the  scaling 
becomes harder to be achieved. In this context, much effort has been done in order to develop 
alternative  devices  that  may  allow  further  progress  in  computation  capability.  Among  the 
promising  emerging  technologies  is  the  multiple  independent-gate  field  effect  transistors 
(MIGFETs).  MIGFETs  are  switch-based  devices,  which  allow  more  logic  capability  in  a 
single  device.  In  general,  switch  networks  built  through  MIGFET  devices  tend  to  be  more 
compact  than  the  traditional  switch  networks.  However,  there  is  a  tradeoff  between  the 
number of logic switches merged and the area  and  performance  of a given MIGFET. Thus, 
we aim to explore such  a  tradeoff in order to evaluate the MIGFET impacts in the building 
digital  integrated  circuits.  To  achieve  this  goal,  in  this  work,  we  present  an  area  and 
performance  evaluation  based  on  digital  circuit  built  using  MIGFET  devices,  where  each 
MIGFET is represented through RC modelling. In particular, such  an evaluation is applied on 
full-custom design of binary adder circuits and  on  standard-cell  design flow targeting  in a set 
of  benchmark  circuits.  Through  the  experiments,  it  is  possible  have  an  insight,  even 
superficial  and  pessimist,  about  how  big  can  be  the  layout  of  a  given  MIGFET  than  the 
single-gate  FinFET  and  still  show  a  reduction  in  the  final  circuit  area  due  to  the  logic 
compaction.",,Projeto de Sistemas Eletrônicos e Computacionais,JEFERSON JOSE BAQUETA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,12/04/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Digital Circuits;Emerging Technlogies;MIGFET;Adders;Cell libraries;Nanotechnology',LINHA FERRAMENTAS PARA AUTOMAÇÃO DE PROJETO ELETRÔNICO,RENATO PEREZ RIBAS,113,Circuitos digitais;tecnologias emergentes;MIGFET;somadores;bibliotecas  de células;nanotecnologia,COMPUTAÇÃO (42001013004P4),-,"A diminuição das dimensões do transistor MOS tem sido a principal estratégia adotada para alcançar otimizações de desempenho na fabricação de circuitos integrados. Contudo, a medida que os transistores diminuem, reduções de escala do transistor tornam-se cada vez mais difíceis de serem alcançadas. Nesse contexto, vários esforços estão sendo feitos para encontrar dispositivos alternativos que permitam futuros avanços em relação à capacidade computacional. Entre as mais promissoras tecnologias emergentes estão os transistores de efeito de campo com múltiplos e independentes gates (MIGFETs). MIGFETs são dispositivos que se comportam como uma chave lógica e possuem maior capacidade lógica por dispositivo. No geral, redes de chaves construídas com dispositivos MIGFET tendem a ser mais compactas do que as redes de chaves tradicionais. No entanto existe um compromisso em relação a redução no número de chaves, devido a maior capacidade lógica, e um maior tamanho e pior desempenho do dispositivo. Portanto, neste trabalho, pretendemos explorar tal balanceamento no sentido de avaliar os impactos do uso de MIGFETs na construção de circuitos integrados digitais. Dessa forma, alguns critérios de avaliação são apresentados no sentido de analisar área e atraso de circuitos baseada em dispositivos MIGFET, onde cada transistor é representado a partir de um modelo RC. Em particular, tal avaliação de área e desempenho é aplicada no projeto de circuitos somadores binários específicos (metodologia full-custom) e na síntese automática de circuitos através de metodologia standard-cell focado em um conjunto de circuitos alvos.",DISSERTAÇÃO,Evaluation of using MIGFET Devices in Digital Integrated Circuit Design,5028895,01
"The Network Function Virtualization (NFV) paradigm promises to make computer networks more scalable and flexible by decoupling the network functions (NFs) from dedicated and vendor-specific hardware. However, network and compute intensive NFs may be difficult to virtualize without performance degradation. In this context, Field-Programmable Gate Arrays (FPGAs) have been shown to be a good option for hardware acceleration of virtual NFs that require high throughput, without deviating from the concept of an NFV infrastructure which aims at high flexibility. Regular expression matching is an important and compute intensive mechanism used to perform Deep Packet Inspection, which can be FPGA-accelerated to meet performance constraints. This solution, however, introduces new challenges regarding dependability requirements. Particularly for SRAM-based FPGAs, soft errors on the configuration memory are a significant dependability threat. In this work we present a comprehensive fault tolerance mechanism to deal with configuration faults on the functionality of FPGA-based regular expression matching engines. Moreover, a placement-aware scrubbing mechanism is introduced to reduce the system repair time, improving the system reliability and availability. Experimental results show that the overall failure rate and the system mean time to repair can be reduced in 95.12% and 92.2%, respectively, with manageable area and performance costs.",,Sistemas de Computação,MARCOS TOMAZZOLI LEIPNITZ,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,14/07/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Field-Programmable Gate Array;Network Function Virtualization;Regular Expression Matching;Fault-Tolerance;Repair Time',LINHA SISTEMAS EMBARCADOS,GABRIEL LUCA NAZAR,67,Field-Programmable  Gate  Array;Network  Function  Virtualization;Avaliação de Expressões Regulares;Tolerância a Falhas;Tempo de Reparo,COMPUTAÇÃO (42001013004P4),-,"O paradigma Network Function Virtualization  (NFV) promete tornar as redes de computadores 
mais  escaláveis  e  flexíveis,  através  do  desacoplamento  das  funções  de  rede  de  hardware 
dedicado e fornecedor específico. No entanto, funções de rede computacionalmente intensivas
podem  ser  difíceis  de  virtualizar  sem  degradação  de  desempenho.  Neste  contexto,  FieldProgrammable  Gate  Arrays  (FPGAs)  têm  se  mostrado  uma  boa  opção  para  aceleração  por
hardware de  funções  de rede  virtuais que requerem alta  vazão, sem se desviar do conceito de 
uma infraestrutura  NFV que visa alta flexibilidade. A avaliação de expressões regulares  é um
mecanismo  importante  e  computacionalmente  intensivo,  usado  para  realizar  Deep  Packet 
Inpection, que pode ser acelerado  por FPGA para atender aos requisitos  de desempenho. Esta 
solução,  no  entanto,  apresenta  novos  desafios  em  relação  aos  requisitos  de  confiabilidade. 
Particularmente para FPGAs baseados em SRAM, soft errors na memória de configuração são
uma ameaça de confiabilidade significativa. Neste trabalho, apresentamos um mecanismo de 
tolerância  a  falhas  abrangente  para  lidar  com  falhas  de  configuração  na  funcionalidade  de 
módulos  de avaliação de expressões  regulares  baseados em FPGA. Além disso, é  introduzido 
um mecanismo de correção de erros que considera o posicionamento desses módulos no FPGA 
para reduzir o tempo de reparo do sistema, melhorando a confiabilidade e a disponibilidade. Os 
resultados experimentais mostram que a taxa de falha geral e  o tempo de reparo do sistema 
podem ser reduzidos em 95.12% e 92.2%, respectivamente, com custos de área e performance
admissíveis.",DISSERTAÇÃO,Resilient Regular Expression Matching on FPGAs with Fast Error Repair,5033974,01
"For beyond 2020 it is expected that cellular networks must raise the coverage area in 10-fold, support 100-fold more user equipments, and increase the data rate capacity by 1000-fold in comparison with current cellular networks. The dense deployment of small cells is considered a promising solution to reach such aggressive improvements, once moves the antennas closer to users, achieving higher data rates due to the signal quality at short distances. However, operating a massive number of antennas can significantly increase the energy consumption of the network infrastructure. Furthermore, the large insertion of new radios brings greater spectral interference between the cells. In this scenery, the optimal management of radio resources turn an exaction due to the impact on the quality of service provided to the users. For example, low transmission powers can leave users without connection, while high transmission powers can contribute to inter radios interference. Furthermore, the interference can be raised on the unplanned reuse of the radio resources, resulting in low data transmission per radio resource, as well as the under-reuse of radio resources limits the overall data transmission capacity. A solution to control the transmission power, assign the spectral radio resources, and ensure the service to the users is essential. In this thesis, we propose an Adaptive Monte Carlo algorithm to perform global energy efficient resource allocation for Heterogeneous Cloud Radio Access Network (H-CRAN) architectures, which are forecast as future fifth-generation (5G) networks. We argue that our global proposal offers an efficient solution to the resource allocation for both high and low density scenarios. Our contributions are threefold: (i) the proposal of a global approach to the radio resource assignment problem in H-CRAN architecture, whose stochastic character ensures an overall solution space sampling; (ii) a critical comparison between our global solution and a local model; (iii) the  demonstration that for high density scenarios energy efficiency is not a well suited metric for efficient allocation, considering data rate capacity, fairness, and served users. Moreover, we compare our proposal against three state-of-the-art resource allocation algorithms for 5G networks.",,Redes de Computadores,MATIAS ARTUR KLAFKE SCHIMUNECK,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,14/08/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Radio resource allocation;Cellular network;Adaptative monte carlo;5G;Energy efficiency',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",JUERGEN ROCHOL,94,Radio resource allocation;Cellular network;Adaptative monte carlo;5G;Energy efficiency.,COMPUTAÇÃO (42001013004P4),-,"Até 2020 espera-se que as redes celulares aumentam em dez vezes a área de cobertura, suporte
cem vezes mais equipamentos de usuários e eleve a capacidade da taxa de dados em mil vezes,
comparada as redes celulares atuais. A densa implantação de pequenas células é considerada
uma solução promissora para alcançar essas melhorias, uma vez que aproximar as antenas dos
usuários proporciona maiores taxas de dados, devido à qualidade do sinal em curtas distâncias.
No entanto, operar um grande número de antenas pode aumentar significativamente o consumo
de energia da infraestrutura de rede. Além disso, a grande inserção de novos rádios pode ocasionar maior interferência espectral entre as células. Nesse cenário, a gestão dos recursos de rádio
é essencial devido ao impacto na qualidade do serviço prestado aos usuários. Por exemplo,
baixas potências de transmissão podem deixar usuários sem conexão, enquanto altas potências
elevam a possibilidade de ocorrência de interferência. Além disso, a reutilização não planejada
dos recursos de rádio causa a ocorrência de interferência, resultando em baixa capacidade de
transmissão, enquanto a subutilização de recursos limita a capacidade total de transmissão de
dados. Uma solução para controlar a potência de transmissão, atribuir os recursos de rádio e
garantir o serviço aos usuários é essencial. Nesta dissertação, é proposto um algoritmo adaptativo de Monte Carlo para realizar alocação global de recursos de forma eficiente em termos de
energia, para arquiteturas Heterogeneous Cloud Radio Access Network (H-CRAN), projetadas
como futuras redes de quinta geração (5G). Uma solução eficiente para a alocação de recursos
em cenários de alta e baixa densidade é proposta. Nossas contribuições são triplas: (i) proposta
de uma abordagem global para o problema de atribuição de recursos de rádio na arquitetura HCRAN, cujo caráter estocástico garante uma amostragem geral de espaço de solução; (ii) uma
comparação crítica entre nossa solução global e um modelo local; (iii) a demonstração de que,
para cenários de alta densidade, a Eficiência Energética não é uma medida adequada para alocação eficiente, considerando a capacidade de transmissão, justiça e total de usuários atendidos.
Além disso, a proposta é comparada em relação a três algoritmos de alocação de recursos de
última geração para redes 5G.",DISSERTAÇÃO,Adaptive Monte Carlo Algorithm to Global Radio Resources Optimization in H-CRAN,5036574,01
"The static model currently applied by governmental authorities for allocating the spectrum of frequencies and the increasing demand for network resources imposed by modern applications and services may lead to a resources scarcity problem in the near future. Dealing with this problem demands improvements on resources allocation. One of the ways of providing such improvements is by allowing resources sharing among network operators in both homogeneous and heterogeneous network scenarios. These network operators may implement different tech- nologies, such as collective use of spectrum and licensed shared access to the spectrum of frequencies. 
Many related works have been proposed in the same context of the presented research, however these related works generally identify the need for additional resources and search for available resources without taking into account the QoS requirements of the resources renter and the costs involved in the resources sharing initiative. Therefore, in this thesis, a novel architecture is proposed to facilitate the implementation of resources sharing and consequently encourage network operators to lease their underutilized resources taking into account both the cost and the QoS requirements. This approach allows the network operator which is serving the resources to improve its profits at the same time that allows quality of service improvements to the resources renter. 
The main contributions of the proposed architecture include but are not limited to the design of a multilevel resources broker to control the resources sharing process. This broker is concerned on dynamically establishing a service level agreement that takes into account the quality of service requirements of resources renter. This process focuses on exchanging a small amount of control information to prevent the overhead from interfering with the legitimate traffic of the network operators. Another important contribution of the proposed approach is to improve the resources allocation in comparison with related work. Furthermore, the proposed solution is capable of taking fast decisions regarding resources allocation, what leads to the implementation of fast handover, allowing the traffic steering without interfering with incumbent users. 
The proposed architecture is modeled analytically and simulated using Matlab to evaluate its be- havior in three different scenarios, considering both homogeneous and heterogeneous networks. The overhead in practical operation scenarios is kept under 1% of the total network traffic, what is considered not to interfere with the transmissions of the network operators. The fast decisions taken by the resources sharing architecture are based on accurate traffic load forecasting, what leads to fast handover, attaining times up to 46% lower than the maximum allowed handover duration. Results also show that both delay and jitter metrics are controlled to be maintained below their specific thresholds of the analyzed applications and therefore, the QoS is guaranteed for the resources renter, considering the coexistence of up to 500 devices.",,Redes de Computadores,RAFAEL KUNST,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,09/08/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Resources sharing;wireless networks;resources broker',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",JUERGEN ROCHOL,139,broker;compartilhamento de recursos;redes sem fio heterogêneas,COMPUTAÇÃO (42001013004P4),-,"O atual modelo de alocação espectral implementado pelas autoridades governamentais somado
à crescente demanda por recursos imposta pela implementação de modernas aplicações e serviços de rede irá resultar em um problema relacionado à escassez de recursos em um futuro
próximo. Lidar com este problema demanda esforços no sentido de melhorar a alocação de
recursos. Uma das maneiras de atingir este tipo de melhoria é permitir o compartilhamento
de recursos entre operadores em redes homogêneas e heterogêneas que podem implementar
diferentes tecnologias, como a utilização coletiva do espectro e de recursos licenciados.
Diversos trabalhos relacionados à esta pesquisa foram propostos. Entretanto, estes trabalhos
geralmente identificam a necessidade de obter recursos adicionais, porém buscam por esses
recursos sem levar em conta os requisitos de qualidade de serviço e o custo envolvido no compartilhamento desses recursos. Considerando esse contexto, nesta tese, uma nova arquitetura é
proposta para permitir a implementação do compartilhamento de recursos e para encorajar operadores a alugarem recursos sobressalentes levando em conta o custo e a qualidade de serviço
oferecida. Esta abordagem permite que operadores tenha ganhos com o aluguel dos recursos,
ao mesmo tempo em que o cliente recebe serviços com maior qualidade.
As principais contribuições da arquitetura proposta incluem o projeto de um controlador de
recursos para coordenar o processo de compartilhamento. Esse controlador busca estabelecer
contratos de serviço dinâmicos levando em conta a qualidade de serviço requerida. Para tanto,
é necessária a troca de informações que, no caso da arquitetura proposta, é mantida baixa para
evitar que a rede seja sobrecarregada e acabe interferindo com o tráfego de dados. Além disso, a
solução proposta é capaz de tomar decisões rápidas sobre a alocação de recursos, o que permite
o redirecionamento do tráfego sem que ocorram interferências com os demais usuários.
A arquitetura proposta é modela analiticamente e simulada com o auxílio da ferramenta Matlab.
O desempenho da proposta é medido em três diferentes cenários, considerando tanto redes
homogêneas, quanto heterogêneas. A sobrecarga gerada pela troca de informações de controle
corresponde a menos de 1% do tráfego total da rede, o que é desprezível do ponto de vista da
interferência com o tráfego de dados. As decisões rápidas tomadas pela arquitetura são baseadas
na previsão acurada do tráfego futuro da rede e permitem o redirecionamento do tráfego para
outras redes em um tempo até 46% abaixo do limite máximo especificado na literatura para este
tipo de redirecionamento. Os resultados mostram ainda que as métricas de atraso e variação do
atraso também são mantidas abaixo dos limites especificados, o que indica que a qualidade de
serviço é garantida nos cenários avaliados.",TESE,A QoS-Aware Resources Sharing Architecture for Homogeneous and Heterogeneous Wireless Networks,5043248,01
"There has been a steady growth in the traffic generated by Mobile Network Operators (MNOs), and by 2020 it is expected to overload the existing licensed spectrum capacity and lead to the problem of scarce resources. One method to deal with this traffic overload is to access unlicensed and shared spectrum bands using an opportunistic approach. The use of Licensed Shared Access (LSA) is a novel approach for spectrum sharing between the incumbent user (i.e., the current owner of the shared spectrum) and the LSA licensee (i.e., the temporary user of frequencies, such as an MNO). The LSA system allows the incumbent users to temporarily provide the LSA licensee with access to its spectrum resources. However, licensees must adopt vertical handover and traffic steering procedures to vacate their customers from the LSA band without causing interference, whenever this is required by the incumbent. These procedures should be carried out, de facto, before the base station is turned off as a part of a rapid release of unscheduled LSA band facing evacuation scenarios. Thus, in this dissertation, a cognitive mechanism is proposed to make decisions in advance to find the best target network(s) for evacuated customers in connected mode and with active traffic per class of service. On the basis of these decisions, the vertical handover and traffic steering procedures are carried out for the best target network(s), which are selected in advance and undertaken immediately to avoid interference between the licensee and incumbent services. Furthermore, this guarantees the seamless connectivity and QoS of evacuated customers and their traffic respectively, during and after the unscheduled evacuation scenarios. A performance evaluation conducted in a simulating scenario consisting of one LTE-LSA and three Wi-Fi networks, demonstrated that the proposed solution could be completed within the time required for the unscheduled evacuation, as well as, being able to ensure the QoS and seamless connectivity of the evacuees. The total execution time obtained during the performance evaluation of the proposed solution was around 46% faster than of two related works and could thus avoid interference between the licensee and incumbent services.",,Redes de Computadores,JEAN ELI CERRILLO FERNANDEZ,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,27/06/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Cognitive mechanism;In-advance decisions;Vertical Handover;Traffic Steering;Spectrum Sharing;Licensed Shared Access;Mobile Network Operators;Unscheduled Evacuation;Long-Term Evolution Network;Wi-Fi Networks',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",JUERGEN ROCHOL,108,Mecanismo cognitivo;Decisões Antecipadas;Transferência vertical;Orientação do tráfego;Acesso Compartilhado Licenciado;Operadoras de rede móvel;Redes de Evolução de longo termo;Redes Wi-Fi;Evacuação não programada,COMPUTAÇÃO (42001013004P4),-,"O tráfego gerado pelas Operadoras de Rede Móvel (ORMs) está crescendo constantemente
e se estima que no ano 2020 a capacidade do espectro licenciado estará sobrecarregado, levando ao problema de escassez deste recurso. Um método para lidar com essa sobrecarga de
tráfego é acessar às bandas do espectro não licenciadas e compartilhadas usando uma abordagem oportunista. O Acesso Compartilhado Licenciado (ACL) é uma nova abordagem para o
compartilhamento do espectro entre o usuário incumbente (i.e., o atual proprietário do espectro compartilhado) e o licenciado ACL (i.e., o usuário temporal da frequências compartilhado,
como uma ORM). O sistema ACL permite que o(s) usuário(s) incumbente(s) forneçam temporariamente acesso ao seu espectro para o licenciado ACL. Além disso, devido ao fato de o
usuário incumbente manter seus direitos sobre o espectro compartilhado, ele pode requisitar a
desocupação da banda no momento e no lugar que precisar. Ademais, no caso de um usuário
temporal (e.g., ORM) estar utilizando o espectro no mesmo momento em que um usuário incumbente requisitar a banda AC, a ORM deve executar a transferência e a orientação do tráfego
dos usuários que estejam na banda ACL para outras redes, em bandas diferentes. Além disso,
é necessário garantir a conectividade com um determinado nível de QoS e ainda evitar a interferência entre os serviços do usuário incumbente e a ORM. Assim, nesta dissertação propõe-se
um mecanismo cognitivo que toma antecipadamente decisões para encontrar a melhor rede de
destino para os usuários, que estejam em modo conectado e com tráfego ativo por classe de
serviço na evacuação. Com base nessas decisões, são executados a transferência vertical e a
orientação do tráfego dos usuários evacuados para as melhores redes de destino selecionadas
anteriormente. Estes processos são efetuados imediatamente para evitar a interferência entre os
serviços do usuário licenciado e incumbente. Isso também permite garantir a conectividade contínua dos usuários evacuados e o QoS do tráfego, durante e após os cenários de evacuação não
programados. Uma avaliação de desempenho realizada num cenário de simulação, composto
por uma rede de Evolucao de longo termo na banda ACL e três redes Wi-Fi, demonstrou que a
solução proposta cumpre o tempo exigido pela evacuação não programada, além de garantir o
QoS e a conectividade sem interrupções dos usuários evacuados. O tempo total de evacuação
obtido durante a avaliação de desempenho da solução proposta é cerca do 46% mais rápido do
que o de dois trabalhos relacionados, evitando assim a interferência entre os serviços do usuário
licenciado e incumbente",DISSERTAÇÃO,A Cognitive Mechanism for Vertical Handover and Traffic Steering to Handle Unscheduled Evacuations of the Licensed Shared Access Band,5043284,01
"This doctoral thesis describes a novel model for coupling continuous chemical diffusion and discrete cellular events inside a biologically inspired simulation environment. Our goal is to define and explore a minimalist set of features that are also expressive, enabling the creation of complex 2D patterns using just a few rules. By not being constrained into a static or regular grid, we show that many different phenomena can be simulated, such as traditional reaction-diffusion systems, cellular automata, and pigmentation patterns from living beings. In particular, we demonstrate that adding chemical saturation increases significantly the range of simulated patterns using reaction-diffusion, including patterns not possible before. Our results suggest a possible universal model that can integrate previous pattern formation approaches, providing new ground for experimentation and realistic-looking textures for general use in Computer Graphics.",,Computação Visual,MARCELO DE GOMENSORO MALHEIROS,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,27/07/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'morphogenesis;reaction-diffusion;pigmentation patterns;computer graphics',LINHA COMPUTAÇÃO GRÁFICA E VISUALIZAÇÃO DE DADOS,MARCELO WALTER,128,Morfogênese;reação-difusão;padrões de pigmentação;computação gráfica,COMPUTAÇÃO (42001013004P4),-,"Esta tese de doutorado descreve um novo modelo para o acoplamento de difusão química contínua e eventos celulares discretos dentro de um ambiente de simulação biologicamente inspirado.
Nosso objetivo é definir e explorar um conjunto minimalista de recursos que também são expressivos, permitindo a criação de padrões 2D complexos usando apenas poucas regras. Por não
nos restringirmos a uma grade estática ou regular, mostramos que muitos fenômenos diferentes
podem ser simulados, como sistemas tradicionais de reação-difusão, autômatos celulares e padrões de pigmentação de seres vivos. Em particular, demonstramos que a adição de saturação
química aumenta significativamente a gama de padrões simulados usando reação-difusão, incluindo padrões que não eram possíveis anteriormente. Nossos resultados sugerem um possível
modelo universal que pode integrar abordagens de formação de padrões anteriores, fornecendo
nova base para experimentação e texturas de aparência realista para uso geral em Computação
Gráfica.",TESE,The Mechanochemical Basis of Pattern Formation,5043867,01
"Stance Detection is the task of automatically identifying if the author of a text is in favor of the
given target, against the given target, or whether neither inference is likely. With the wide use
of Twitter as a platform to express opinions and stances, the automatic analysis of this content
becomes of high regard for companies, organizations and public figures. In general, works that
explore such task adopt supervised or semi-supervised approaches. The present work proposes
and evaluates a non-supervised process to detect stance in texts of tweets that has as entry only
the target and a set of tweets to classify and is based on a hybrid approach composed by 2 stages:
a) automatic labelling of tweets based on a set of heuristics and b) complementary classification
based on supervised machine learning. The proposal succeeds when applied to public figures,
overcoming the state-of-the-art. Beyond that, some alternatives are evaluated with the intention
of increasing the performance when applied to other domains, revealing the possibility of use
of strategies such as using seed targets and profiles depending on each domain characteristics",,Ciência de Dados e Engenharia de Software,MARCELO DOS SANTOS DIAS,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,04/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Sentiment Analysis;Stance Detection;Automatic labelling;Twitter',"LINHA MINERAÇÃO, INTEGRAÇÃO E ANÁLISE DE DADOS",KARIN BECKER,85,Análise de Sentimento;Detecção de Posicionamento;Rotulação Automática;Twitter,COMPUTAÇÃO (42001013004P4),-,"Detecção de posicionamento é a tarefa de automaticamente identificar se o autor de um texto é favorável, contrário, ou nem favorável e nem contrário a uma dada proposição ou alvo. Com o amplo uso do Twitter como plataforma para expressar opiniões e posicionamentos, a análise automatizada deste conteúdo torna-se de grande valia para empresas, organizações e figuras públicas. Em geral, os trabalhos que exploram tal tarefa adotam abordagens supervisionadas ou semi-supervisionadas. O presente trabalho propõe e avalia um processo não supervisionado de detecção de posicionamento em textos de tweets que tem como entrada apenas o alvo e um
conjunto de tweets a rotular e é baseado em uma abordagem híbrida composta por 2 etapas: a) rotulação automática de tweets baseada em um conjunto de heurísticas e b) classificação complementar baseada em aprendizado supervisionado de máquina. A proposta tem êxito quando aplicada a figuras públicas, superando o estado-da-arte. Além disso, são avaliadas alternativas no intuito de melhorar seu desempenho quando aplicada a outros domínios, revelando a possibilidade de se empregar estratégias tais como o uso de alvos e perfis semente dependendo das características de cada domínio.",DISSERTAÇÃO,Detecção Não Supervisionada de Posicionamento em Textos de Tweets,5048317,01
"Multi-Processors Systems-on-Chips (MPSoC) became the established hardware platform for a wide variety of applications and devices. High parallelism allied with energy efficiency allowed MPSoCs to accomplish the requirements of a new era on computation, the Internet-of-Things (IoT). In a near future, it is expected that the IoT technology will merge both virtual and physical systems, creating the concept of the Internet-of-Everything. Consequently, even more devices and systems will be interconnected by the Internet. The Internet link already brings several security concerns because all sensitive information stored on these devices can be reachable by external agents, and this prognostics will only increase the security issues. One of the most dangerous attacks is the Side Channel Attack (SCA). This type of attack explores features of the target system (indirect information) that reveals some secret or valuable data. This threat can be implemented physically through specialized instrumentation coupled directly to the device, or logical from architectural behavior accessed remotely through the network. The present thesis defines this particular logical SCA as a subcategory called Architectural Channel Attack (ACA). In the literature, several ACAs are targeting System-on-Chips (SoC) and ASICs. However, there is no study of ACAs running in MPSoCs. Consequently, this research project revised the bibliography to identify, analyze and explore the potential vulnerabilities of MPSoCs. The most vulnerable parts recognized were the shared cache and the Network-on-Chip (NoC). Within this knowledge, this thesis developed four new attacks aiming MPSoCs. They are the Hourglass, the Firecracker, the Arrow, and the Earthquake. Besides, the proposition that the hardware can provide security being transparent to applications resulted in a proposal of a hardware countermeasure, the Gossip NoC. The proposed attacks executed in a real MPSoC environment in an FPGA, breaking the Advanced Encryption Standard (AES). These evaluations were the first practical demonstration of an  ACA performed in a NoC-based MPSoC entirely. The efficiency of different countermeasures, the Gossip NoC and three other ones from the literature, was evaluated under these attacks. Results showed that i) the shared cache and the NoC are critical vulnerabilities of complex MPSoCs; ii) the proposed attacks optimize the traditional cache ACAs found in literature making possible to attack even in limited environments, such as found in IoT/IoE; iii) the Earthquake makes the differential collision strategy feasible; iv) the NoC is a suitable candidate to implement security mechanisms, since it can access all elements in the system; v) the Gossip NoC avoids only one type of attack, but a protection mechanism for such complex systems demands multiple countermeasure strategies integrated to be a complete solution.",,Projeto de Sistemas Eletrônicos e Computacionais,CEZAR RODOLFO WEDIG REINBRECHT,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,10/07/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Internet-of-Things;MPSoC;SoC;Security;Network-on-Chip;Side Channel Attack;Timing Attack;NoC Timing Attack;Secure NoC',LINHA CONCEPÇÃO DE CIRCUITOS E SISTEMAS INTEGRADOS,ALTAMIRO AMADEU SUSIN,146,"MPSoC;Segurança;Ataque de Canal Lateral;Rede-em-Chip;Sistemaem-Chip;Timing Attack, NoC Timing Attack;NoC Segura",COMPUTAÇÃO (42001013004P4),-,"Sistemas em Chip Multi-Processados (do inglês, MPSoCs) tornaram-se a plataforma de
hardware estabelecida para uma ampla variedade de aplicações e dispositivos. Cada vez
mais dispositivos e sistemas serão interligados pela Internet. A conexão com a Internet já
traz várias preocupações de segurança, porque todas as informações confidenciais armazenadas nesses dispositivos podem ser acessadas por agentes externos, e esse prognóstico
só aumentará as questões relacionadas à segurança. Um dos ataques mais perigosos é
o Ataque de Canal Lateral (do inglês, SCA). Este tipo de ataque explora características
do sistema de destino (informação indireta) que revela alguns dados secretos ou valiosos.
Esta ameaça pode ser implementada fisicamente através de instrumentação especializada
acoplada diretamente ao dispositivo, ou lógica pelo comportamento arquitetural que é
acessado remotamente através da rede. Esta tese define este SCA lógico em particular
como uma subcategoria chamada Ataque de Canal Arquitetural (do inglês, ACA). Este
projeto de pesquisa revisou a bibliografia para identificar, analisar e explorar as potenciais vulnerabilidades dos MPSoCs. As partes mais vulneráveis reconhecidas foram as
Caches compartilhadas e a Rede-em-Chip (do inglês, NoC). Uma vez adquirido este conhecimento, esta tese desenvolveu quatro novos ataques para MPSoCs - Hourglass, Firecracker, Arrow, e Earthquake. Além disso, a proposição de que o hardware pode fornecer
segurança sendo transparente para aplicações culminou em uma proposta de uma contramedida de hardware, o Gossip NoC. Os ataques propostos foram executados em um ambiente real de MPSoC em um FPGA, quebrando a criptografia AES. Estes experimentos
práticos foram a primeira demonstração de um ACA realizado em um MPSoC baseado
em NoC. A eficiência de diferentes contra-medidas foi avaliada sob estes ataques. Os
resultados mostraram que i) a Cache compartilhada e a NoC são vulnerabilidades críticas
em MPSoCs; ii) os ataques propostos otimizam os ACAs tradicionais de Cache encontrados na literatura; iii) o Earthquake torna viável a estratégia de colisão diferencial; iv) a
NoC é uma candidata adequada para implementar mecanismos de segurança; v) a Gossip
NoC evita apenas um tipo de ataque.",TESE,Architectural Channel Attacks in NoC-based MPSoCs and its Countermeasures,5048318,01
"Two  of  the  major  drivers  of  increased  performance  in  single-thread  applications  -increase  in  operation  frequency  and  exploitation  of  instruction-level  parallelism  -  have  had 
little  advances  in  the  last  years  due  to  power  constraints.  In  this  context,  considering  the 
intrinsic imprecision-tolerance (i.e., outputs may present an acceptable level of noise without 
compromising  the  result)  of  many  modern  applications,  such  as  image  processing  and 
machine learning, approximate computation becomes a promising approach. This technique is 
based on computing approximate instead of accurate results, which can increase  performance 
and reduce energy consumption at the cost of quality.
In  the  current  state  of  the  art,  the  most  common  way  of  exploiting  the  technique  is 
through  neural  networks  (more  specifically,  the  Multilayer  Perceptron  model),  due  to  the 
ability of these structures to learn arbitrary functions and to approximate them. Such networks 
are  usually  implemented  in  a  dedicated  neural  accelerator.  However,  this  implementation 
requires  a  large  amount  of  chip  area  and  usually  does  not  offer  enough  improvements  to 
justify this additional cost.
The  goal  of  this  work  is  to  propose  a  new  mechanism  to  address  approximate 
computation,  based  on  approximate  reuse  of  functions  and  code  fragments.  This  technique 
automatically  groups  input  and  output  data  by  similarity  and  stores  this  information  in  a 
sofware-controlled memory. Based on these data, the quantized values can be reused through 
a search to this table, in which the most appropriate output will be selected and, therefore, 
execution of the original code will be replaced. Applying this technique is effective, achieving 
an  average  97.1%  reduction  in  Energy-Delay-Product  (EDP)  when  compared  to  neural 
accelerators.",,Sistemas de Computação,GUILHERME MENEGUZZI MALFATTI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,28/07/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Approximate Computing;Data Clustering;Neural networks;High Performance',LINHA SISTEMAS EMBARCADOS,ANTONIO CARLOS SCHNEIDER BECK FILHO,90,Computação  Aproximativa;Clusterização  de  Dados;Redes  Neurais;Alta performance.,COMPUTAÇÃO (42001013004P4),-,"Dois dos principais fatores do aumento da performance em aplicações single-thread – frequência de operação e exploração do paralelismo no nível das instruções – tiveram pouco avanço nos últimos anos devido a restrições de potência. Neste contexto, considerando a natureza tolerante a imprecisões (i.e.: suas saídas podem conter um nível aceitável de ruído sem comprometer o resultado final) de muitas aplicações atuais, como processamento de imagens e aprendizado de máquina, a computação aproximativa torna-se uma abordagem atrativa. Esta técnica baseia-se em computar valores aproximados ao invés de precisos o que, por sua vez, pode aumentar o desempenho e reduzir o consumo energético ao custo de qualidade. 
 	No atual estado da arte, a forma mais comum de exploração da técnica é através de redes neurais (mais especificamente, o modelo Multilayer Perceptron), devido à capacidade destas estruturas de aprender funções arbitrárias e aproximá-las. Tais redes são geralmente implementadas em um hardware dedicado, chamado acelerador neural. Contudo, essa execução exige uma grande quantidade de área em chip e geralmente não oferece melhorias suficientes que justifiquem este espaço adicional.
Este trabalho tem por objetivo propor um novo mecanismo para fazer computação aproximativa, baseado em reuso aproximativo de funções e trechos de código. Esta técnica agrupa automaticamente entradas e saídas de dados por similaridade, armazena-os em uma tabela em memória controlada via software. A partir disto, os valores quantizados podem ser reutilizados através de uma busca a essa tabela, onde será selecionada a saída mais apropriada e desta forma a execução do trecho de código será substituído. A aplicação desta técnica é bastante eficaz, sendo capaz de alcançar uma redução, em média, de 97.1% em Energy-Delay-Product (EDP) quando comparado a aceleradores neurais.",DISSERTAÇÃO,Técnicas de Agrupamento de Dados para Computação Aproximativa,5048339,01
"Many aspects of the management of computer networks, such as quality of service and security, must be taken into consideration to ensure that the network meets the users and clients demands. Fortunately, management solutions were developed to address these aspects, such as Intent-Based Networking (IBN). IBN is a novel networking paradigm that abstracts network configurations by allowing administrators to specify how the network should behave and not what it should do. In this dissertation, we introduce an IBN solution called INSpIRE (Integrated NFV-based Intent Refinement Environment). INSpIRE implements a refinement technique to translate intents into a set of configurations to perform a desired service chain in both homogeneous environments (virtualized functions only) and heterogeneous environments (virtualized functions and physical middleboxes). This refinement technique relies on Non-Functional Requirements (NFRs) and clustering to determine the network functions that will compose the service chain. Our solution is capable of (i) determining the specific functions required to fulfill an intent, (ii) chaining these functions according to their dependencies, and (iii) presenting enough low-level information to network devices for posterior traffic steering. Furthermore, to assess the feasibility of our solution we detail case studies that reflects real-world management situations and evaluate the scalability of the refinement process. Finally, the results showed that INSpIRE is capable of delivering a service chain that meets the requirements specified in the intent in small and large scenarios.",,Redes de Computadores,EDER JOHN SCHEID,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,07/07/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'policy-based management;policy refinement;software-defined networking;network functions virtualization;intent-based networking',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",LISANDRO ZAMBENEDETTI GRANVILLE,83,Gerenciamento de Rede Baseado em Políticas;Refinamento de Políticas;Redes Definidas por Sofware;Virtualização de Funções de Rede;Redes Baseadas em Intenções,COMPUTAÇÃO (42001013004P4),-,"Muitos aspectos da gestão de redes de computadores, como a Qualidade de Serviço (QoS) e
segurança, devem ser levados em consideração para garantir que a rede atenda às exigências
de usuários e clientes. Felizmente, soluções de gestão de rede foram desenvolvidas para lidar
com estes aspectos, tais como Redes Baseadas em Intenção (Intent-based Networking - IBN).
IBN é um novo paradigma de rede que abstrai configurações de rede, permitindo que administradores especifiquem como a rede deve se comportar e não o que ele deve fazer. Nesta
dissertação, apresentamos uma solução de IBN chamada INSpIRE (Integrated NFV-based Intent Refinement Envirorment). INSpIRE implementa uma técnica de refinamento para traduzir intenções em um conjunto de configurações para executar uma desejada cadeia de serviço
em ambos, ambientes homogêneos (somente funções virtualizadas) e ambientes heterogêneos
(funções virtualizadas e middleboxes físicas). A técnica de refinamento baseia-se em Requisitos Não Funcionais (Non-Functional Requirements - NFRs) e clustering para determinar quais
funções de rede deverão compor a cadeia de serviços. Nossa solução é capaz de (i) determinar
as funções específicas necessárias para o cumprimento de uma intenção, (ii) encadear estas funções de acordo com suas dependências e (iii) apresentar informações de baixo nível suficientes
para que dispositivos de rede possam posteriormente orientar o tráfego de rede por essa cadeia
de serviço. Além disso, para avaliar a viabilidade da nossa solução, estudos de caso no qual
refletem situações de gestão do mundo real e uma avaliação da escalabilidade do processo de
refinamento são detalhados. Por fim, os resultados mostraram que INSpIRE é capaz de fornecer uma cadeia de serviços que atende aos requisitos especificados na intenção em cenários
pequenos e grandes.",DISSERTAÇÃO,INSpIRE: an Integrated NFV-baSed Intent Refinement Environment,5048342,01
"Given the huge quantity of data currently being generated, just a small portion of it can be
manually labeled by human experts. This is a challenge for machine learning applications.
Semi-supervised learning addresses this problem by handling unlabeled data alongside labeled
ones. However, if only a limited quantity of labeled examples is available, the performance of
the machine learning task (e.g., classification) can be very unsatisfactory. Many solutions address this issue by using a classifier ensemble because this increases diversity. Algorithms such
as co-training and tri-training use multiple views or multiple learning algorithms in order to improve the classification of unlabeled instances through simple majority agreement. Also, there
are approaches that extend this idea and adopt less trivial voting processes to define the labels,
like weighted majority voting. Nevertheless, these solutions require some confidence level on
the label in order to use it for training. Hence, not all information is used, i.e., information associated with low confidence level is disregarded completely. An approach called social-training
is proposed, which uses all information available in the semi-supervised learning task. For this,
multiple heterogeneous classifiers are trained with the labeled data and generate diverse classifications for the same unlabeled instances. Social-training then aggregates these results into a
single label by means of social choice functions that work with rank aggregation over the instances. The solution addresses binary classification cases. The results show that working with
the full ranking, i.e., labeling all unlabeled instances, is able to reduce the classification error
for some UCI data sets used.",,Inteligência Artificial,MATHEUS ALVES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,08/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'semi-supervised learning;classifier ensembles;social choice functions',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",ANA LUCIA CETERTICH BAZZAN,84,aprendizado semi-supervisionado;machine learning distribuído;sistemas de  múltiplos classificadores;funções de escolha social,COMPUTAÇÃO (42001013004P4),-,"Dada a grande quantidade de dados gerados atualmente, apenas uma pequena porção dos mesmos pode ser rotulada manualmente por especialistas humanos. Isto é um desafio comum para aplicações de aprendizagem de máquina. Aprendizado semi-supervisionado endereça este problema através da manipulação dos dados não rotulados juntamente aos dados rotulados. Entretanto, se apenas uma quantia limitada de exemplos rotulados está disponível, o desempenho da tarefa de aprendizagem de máquina (e.g., classificação) pode ser não satisfatória. Diversas soluções endereçam este problema através do uso de uma ensemble de classificadores, visto que isto aumenta a diversidade dos classificadores. Algoritmos como o co-training e o tri-training utilizam múltiplas partições de dados ou múltiplos algoritmos de aprendizado para melhorar a qualidade da classificação de instâncias não rotuladas através de concordância por maioria simples. Além disso, existem abordagens que estendem esta ideia e adotam processos de votação menos triviais para definir os rótulos, como eleição por maioria ponderada, por exemplo. Contudo, estas soluções requerem que os rótulos possuam um certo nível de confiança para serem utilizados no treinamento.
Consequentemente, nem toda a informação disponível é utilizada. Por exemplo: informações associadas a níveis de confiança baixos são totalmente ignoradas. Este trablho propõe uma abordagem chamada social-training, que utiliza toda a informação disponível na tarefa de aprendizado semi-supervisionado. Para isto, múltiplos classificadores heterogêneos são treinados com os dados rotulados e geram diversas classificações para as mesmas instâncias não rotuladas. O social-training então agrega estes resultados em um único rótulo por meio de funções de escolha social que trabalham com agregação de rankings. Os resultados mostram que trabalhar com o ranking completo, ou seja, rotular todas as instâncias não rotuladas, é capaz de reduzir o erro de classificação para alguns conjuntos de dados da base da UCI utilizados.",DISSERTAÇÃO,Social-Training: Aprendizado Semi-Supervisionado Utilizando Funções de Escolha Social,5064727,01
"PIM - a technique which computational elements are added close, or ideally, inside memory devices - was one of the attempts created during the 1990s to try to mitigate the memory wall problem. Nowadays, with the maturation of 3D integration technologies, a new landscape for novel PIM architectures can be investigated. To exploit this new scenario, researchers rely on software simulators to navigate throughout the design evaluation space. Today, most of the works targeting PIM implement in-house simulators to perform their experiments. However, this methodology might hurt overall productivity, while it might also preclude replicability. In this work, we showed the development of a precise, modular and parametrized PIM simulation environment. Our simulator, named CLAPPS, targets the HMC architecture, a popular 3D-stacked memory widely employed in state-of-the-art PIM accelerators. We have designed our mechanism using the SystemC programming language, which allows native parallel simulation. The primary contribution of our work lies in developing a user-friendly interface to allow easy PIM architectures exploitation. To evaluate our system, we have implemented a PIM module that can perform vector operations with different operand sizes using the proposed set of tools.",,Sistemas de Computação,GERALDO FRANCISCO DE OLIVEIRA JUNIOR,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,25/08/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'In-Memory Processing;Simulators;Hybrid Memory Cube;3D-Stacked.',LINHA ARQUITETURAS NÃO CONVENCIONAIS,LUIGI CARRO,70,Processamento em Memória;Simuladores;Hybrid Memory Cube,COMPUTAÇÃO (42001013004P4),-,"PIM - uma técnica onde elementos computacionais são adicionados perto, ou idealmente,
dentro de dispositivos de memória - foi uma das tentativas criadas durante os anos 1990
visando mitigar o notório memory wall problem. Hoje em dia, com o amadurecimento
do processo de integração 3D, um novo horizonte para novas arquiteturas PIM pode ser
explorado. Para investigar este novo cenário, pesquisadores dependem de simuladores em
software para navegar pelo espaço de exploração de projeto. Hoje, a maioria dos trabalhos
que focam em PIM, implementam simuladores locais para realizar seus experimentos. Porém,
esta metodologia pode reduzir a produtividade e reprodutibilidade. Neste trabalho,
nós mostramos o desenvolvimento de um simulador de PIM preciso, modular e parametrizável.
Nosso simulador, chamado CLAPPS, visa a arquitetura de memória HMC, uma
memória 3D popular, que é amplamente utilizada em aceleradores PIM do estado da arte.
Nós desenvolvemos nosso mecanismo utilizando a linguagem de programação SystemC,
o que permite uma simulação paralela nativamente. A principal contribuição do nosso
trabalho se baseia em desenvolver a interface amigável que permite a fácil exploração de
arquiteturas PIM. Para avaliar o nosso sistema, nós implementamos um modulo de PIM
que pode executar operações vetoriais com diferente tamanhos de operandos utilizando o
proposto conjunto de ferramentas.",DISSERTAÇÃO,A Generic Processing in Memory Cycle Accurate Simulator under Hybrid Memory Cube Architecture,5070703,1
"Due to the continuous and overwhelming growth of scientific data in the last few years, data-intensive analysis on this vast amount of scientific data is very important to extract valuable scientific information. The GRIB scientific data format is widely used within the meteorological community and is used to store historical meteorological data and weather forecast simulation results. However, current libraries for processing the GRIB files do not perform the computation in a distributed environment. This situation limits the analytical capabilities of scientists who need to perform analysis on large data sets in order to obtain information in the shortest time possible using of all available resources. 
In this context, this work presents an alternative to data processing in the GRIB format using the well-know Manager-Worker pattern, which was implemented with the Actor
model provided by the Akka toolkit. Likewise, we compare our proposal with other mechanisms, such as the round-robin, random and an adaptive load balancing, as well as with
one of the main frameworks currently existing for big data processing, Apache Spark. The methodology used considers several factors to evaluate the processing of the GRIB files. The experiments were conducted on a cluster in Microsoft Azure platform. The results show that our proposal scales well as the number of worker nodes increases. Our work reached a better performance in relation to the other mechanisms used for the comparison particularly when eight worker virtual machines were used. Thus, our proposal upon using metadata achieved a gain of 53.88%, 62.42%, 62.97%, 61.92%, 62.44% and 59.36% in relation to the mechanisms: round-robin, random, an adaptive load balancing that used CPU, JVM Heap and mix metrics, and the Apache Spark respectively, in a scenario where a search criteria is applied to select 2 of 27 total parameters found in the dataset used in the experiments",,Sistemas de Computação,JIMMY KRAIMER MARTIN VALVERDE SANCHEZ,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,31/07/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'GRIB;Akka;Actor model;Big data;Manager-Worker',LINHA COMPUTAÇÃO DE ALTO DESEMPENHO E SISTEMAS DISTRIBUÍDOS,NICOLAS BRUNO MAILLARD,104,Akka;GRIB;Modelo de atores;Big data;Manager-Worker,COMPUTAÇÃO (42001013004P4),-,"Devido ao contínuo crescimento dos dados científicos nos últimos anos, a análise intensiva
de dados nessas quantidades massivas de dados é muito importante para extrair informações
valiosas. Por outro lado, o formato de dados científicos GRIB (GRIdded Binary) é
amplamente utilizado na comunidade meteorológica para armazenar histórico de dados e
previsões meteorológicas. No entanto, as ferramentas atuais disponíveis e métodos para
processar arquivos neste formato não realizam o processamento em um ambiente distribuído.
Essa situação limita as capacidades de análise dos cientistas que precisam realizar
uma avaliação sobre grandes conjuntos de dados com o objetivo de obter informação no
menor tempo possível fazendo uso de todos os recursos disponíveis. Neste contexto, este
trabalho apresenta uma alternativa ao processamento de dados no formato GRIB usando
o padrão Manager-Worker implementado com o modelo de atores fornecido pelo Akka
toolkit. Realizamos também uma comparação da nossa proposta com outros mecanismos,
como o round-robin, random, balanceamento de carga adaptativo, bem como com
um dos principais frameworks para o processamento de grandes quantidades de dados
tal como o Apache Spark. A metodologia utilizada considera vários fatores para avaliar
o processamento dos arquivos GRIB. Os experimentos foram conduzidos em um cluster
na plataforma Microsoft Azure. Os resultados mostram que nossa proposta escala bem à
medida que o número de nós aumenta. Assim, nossa proposta atingiu um melhor desempenho
em relação aos outros mecanismos utilizados para a comparação, particularmente
quando foram utilizadas oito máquinas virtuais para executar as tarefas. Nosso trabalho
com o uso de metadados alcançou um ganho de 53.88%, 62.42%, 62.97%, 61.92%, 62.44%
e 59.36% em relação aos mecanismos round-robin, random, balanceamento de carga adaptativo
que usou métricas CPU, JVM Heap e um combinado de métricas, e o Apache Spark,
respectivamente, em um cenário onde um critério de busca é aplicado para selecionar 2
dos 27 parâmetros totais encontrados no conjunto de dados utilizado nos experimentos.",DISSERTAÇÃO,Distributed Data Analysis over Meteorological Datasets using the Actor Model,5070754,1
"The aggressive shrinking of transistors, which led to the reductions in the operating voltage,
has been providing enormous benefits in terms of computational power while keeping
the energy consumption at an acceptable level. However, as feature size and voltage
decrease, the susceptibility to soft errors tends to increase, and the importance of fault
evaluations grows. Superscalar processors, which nowadays dominate the market, are a
significant example of systems that take advantage of these technological improvements
and are more susceptible to errors. Along with that, there exist several methods for fault
injection, which is an efficient means to evaluate the resiliency of such processors. However,
traditional fault injection methods, such as the hardware-based technique, impose
that the processor must be physically implemented before the tests can be conducted,
while not providing reasonable levels of controllability. On the other hand, techniques
based on simulators implemented in Software offer high levels of controllability. However,
while high-level SW simulators (which are fast) may lead to an incomplete, or even
misguided, evaluation of the system’s resiliency since they don’t model the hardware internals
(such as the pipeline registers), low-level SW simulators are extremely slow and
are hardly available at RTL (Register-Transfer Level). Considering this scenario, we propose
a platform that bridges the gap between the HW and SW approaches to evaluate
faults in superscalar processors: it is fast, with high controllability, available in software,
flexible, and, most importantly, it models the processor at RTL. The tool was implemented
on top of the framework used to generate the Berkeley Out-of-Order Machine (BOOM)
superscalar processor, which is a highly scalable and parameterizable processor. This
property allowed us to experiment with three different architectures of the processor:
single-, dual-, and quad-issue out-of-order cores, and, by analyzing how the resiliency
to faults is influenced by the complexity of different processors, use them to validate our
tool.",,Sistemas de Computação,RAFAEL BILLIG TONETTO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,24/08/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Fault injection;superscalar processor;register-transfer level',LINHA ARQUITETURAS NÃO CONVENCIONAIS,ANTONIO CARLOS SCHNEIDER BECK FILHO,93,Injeção de falhas;processadores superscalares;nível de transferência de registradores.,COMPUTAÇÃO (42001013004P4),-,"A diminuição agressiva dos transistores, a qual levou a reduções na tensão de operação,
vem proporcionando enormes benefícios em termos de poder computacional, mantendo o
consumo de energia em um nível aceitável. No entanto, à medida que o tamanho dos recursos
e a tensão diminuem, a susceptibilidade a falhas tende a aumentar e a importância
das avaliações com falhas cresce. Os processadores superescalares, que hoje dominam o
mercado, são um exemplo significativo de sistemas que se beneficiam destas melhorias
tecnológicas e são mais suscetíveis a erros. Juntamente com isso, existem vários métodos
para injeção de falhas, que é um meio eficiente para avaliar a resiliência desses processadores.
No entanto, os métodos tradicionais de injeção de falhas, como a técnica baseada
em hardware, impõem que o processador seja implementado fisicamente antes que os testes
possam ser conduzidos, sem fornecer níveis razoáveis de controlabilidade. Por outro
lado, as técnicas baseadas em simuladores implementados em software oferecem altos
níveis de controlabilidade. No entanto, enquanto os simuladores em SW de alto nível
(que são rápidos) podem levar a uma avaliação incompleta, ou mesmo equivocada, da
resiliência do sistema, uma vez que não modelam os componentes internos do hardware
(como os registradores do pipeline), simuladores em SW de baixo nível são extremamente
lentos e dificilmente estão disponíveis em RTL (Register-Transfer Level). Considerando
este cenário, propomos uma plataforma que preenche a lacuna entre as abordagens em
HW e SW para avaliar falhas em processadores superescalares: é rápida, tem alta controlabilidade,
disponível em software, flexível e, o mais importante, modela o processador
em RTL. A ferramenta foi implementada sobre a plataforma usada para gerar o processador
superescalar The Berkeley Out-of-Order Machine (BOOM), que é um processador
altamente escalável e parametrizável. Esta propriedade nos permitiu experimentar três
arquiteturas diferentes do processador: single-, dual- e quad-issue, e, ao analisar como a
resiliência a falhas é influenciada pela complexidade de diferentes processadores, usamos
os processadores para validar nossa ferramenta.",DISSERTAÇÃO,A Platform to Evaluate the Fault Sensitivity of Superscalar Processors,5071225,1
"Information regarding bug fixes has been explored to build bug predictors, which provide
support for the verification of software systems, by identifying fault-prone elements, such
as files. A wide range of static and change metrics have been used as features to build
such predictors. Many bug predictors have been proposed, and their main target is objectoriented
systems. Although object-orientation is currently the choice for most of the
software applications, the procedural paradigm is still being used in many—sometimes
crucial—applications, such as operating systems and embedded systems. Consequently,
they also deserve attention. This dissertation extends work on bug prediction by evaluating
and tailoring bug predictors to procedural software systems. We provide three key
contributions: (i) comparison of bug prediction approaches in context of procedural software
systems, (ii) proposal of the use of software quality features as prediction features
in the studied context, and (iii) evaluation of the proposed features in association with the
best approach found in (i). Our work thus provides foundations for improving the bug
prediction performance in the context of procedural software systems.",,Ciência de Dados e Engenharia de Software,CRISTIANO WERNER ARAUJO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,01/08/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Bug prediction;procedural programming;static code metrics',LINHA ENGENHARIA DE SOFTWARE,INGRID OLIVEIRA DE NUNES,90,predição de defeitos;programação procedural;métricas estáticas de código.,COMPUTAÇÃO (42001013004P4),-,"Informação relacionada a concertos de bugs tem sido explorada na construção de preditores
de bugs cuja função é o suporte para a verificação de sistemas de software identificando
quais elementos, como arquivos, são mais propensos a bugs. Uma grande variedade
de métricas estáticas de código e métricas de mudança já foi utilizada para construir tais
preditores. Dos muitos preditores de bugs propostos, a grande maioria foca em sistemas
orientados à objeto. Apesar de orientação a objetos ser o paradigma de escolha para a
maioria das aplicações, o paradigma procedural ainda é usado em várias — muitas vezes
cruciais — aplicações, como sistemas operacionais e sistemas embarcados. Portanto,
eles também merecem atenção. Essa dissertação extende o trabalho na área de predição
de bugs ao avaliar e aprimorar preditores de bugs para sistemas procedurais de software.
Nós proporcionamos três principais contribuições: (i) comparação das abordagens existentes
de predição de bugs no contexto de sistemas procedurais, (ii) proposta de uso dos
atributos de qualidade de software como atributos de predição no contexto estudado e
(iii) avaliação dos atributos propostos em conjunto com a melhor abordagem encontrada
em (i). Nosso trabalho provê, portanto, fundamentos para melhorar a performance de
preditores de bugs no contexto de sistemas procedurais.",DISSERTAÇÃO,Bug Prediction in Procedural Software Systems,5071232,1
"Entity-page is a Web page which publishes data that describe an entity of a specific type. Acquiring the attribute values of the real-world entities that are published in these pages is a strategic task for various companies. This acquisition involves the tasks of discovering the entitypages in the websites and extracting the attribute values that are published in them. However,
the current approaches that carry out the tasks of discovering entity-pages and extracting data
in an integrated way have limited applications because they are restricted to a particular application domain or require an a priori annotation. This thesis presents Orion, which is an
approach to acquire the attribute values of real-world entities from template-based entity-pages.
Orion discovers the entity-pages in the websites and extracts the attribute values that are published in them. What is original about the Orion approach is that it carries out the tasks of
discovering entity-pages and extracting data in a way that is integrated, domain-independent,
and independent of any a priori annotation. The Orion approach includes an entity-page discovery stage that combines the HTML and URL features without requiring the user to define
the similarity threshold between the pages. The discovery stage employs a new URL-based
similarity function that assigns different weights to the URL terms in accordance with their capacity to distinguish entity-pages from other pages. Orion also includes a stage during which
the attribute values are extracted by means of Cypher queries in a graph database. This stage automatically induces the queries. It should be noted that the Orion approach is robust because it
includes an additional reinforcement stage for handling attributes with template variations. This
stage involves exploring a linear combination of different similarity functions. We carried out
exhaustive experiments through real-world websites with the aim of evaluating the effectiveness
of each stage of the approach both in isolation and in an integrated manner. It was found that
the Orion approach was numerically and statistically more effective than the baselines.",,Ciência de Dados e Engenharia de Software,EDIMAR MANICA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,26/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Entity-page discovery;Domain-independent data extraction;Graph databases;Cypher;Similarity functions',"LINHA MINERAÇÃO, INTEGRAÇÃO E ANÁLISE DE DADOS",RENATA DE MATOS GALANTE,127,Descoberta de páginas-entidade;Extração de dados independente de domínio;Banco de dados orientado a grafos;Cypher;Funções de similaridade,COMPUTAÇÃO (42001013004P4),-,"Uma página-entidade publica dados que descrevem uma entidade de um tipo particular. Adquirir os valores dos atributos de entidades do mundo real publicados nessas páginas é uma tarefa estratégia para diversas empresas. Essa aquisição  envolve as tarefas de encontrar as páginas-entidade nos sites e extrair os valores dos atributos publicados nessas páginas. Essas tarefas têm sido tratadas na literatura de forma isolada, salvo poucas exceções. Esta tese apresenta Orion, uma abordagem para aquisição de valores de atributos de entidades do mundo real a partir de páginas-entidade baseadas em template. Orion descobre as páginas-entidade nos sites e extrai os valores dos atributos publicados nessas páginas. A principal originalidade da abordagem Orion é realizar as tarefas de descoberta de páginas-entidade e extração de dados de forma integrada, independente de domínio de aplicação e de anotação a priori. A abordagem Orion inclui uma etapa de descoberta de páginas-entidade que combina características de HTML e URL sem a necessidade de intervenção do usuário para definição dos limiares de similaridade entre as páginas. Essa etapa utiliza uma nova função de similaridade entre páginas baseada na URL que atribui diferentes pesos para os termos de URL de acordo com a capacidade de distinção de páginas-entidade das demais páginas. A abordagem Orion, também, inclui uma etapa de extração de valores de atributos a partir de consultas Cypher em um banco de dados orientado a grafos que infere as consultas automaticamente. A abordagem Orion é robusta porque inclui uma etapa adicional de reforço que realiza o tratamento de atributos com variação no template. Esse tratamento é realizado através de uma combinação linear de diferentes funções de similaridade. Foram realizados experimentos exaustivos utilizando sites reais com o objetivo de avaliar a eficácia de cada etapa da abordagem isoladamente e da abordagem de forma integral. A abordagem Orion foi mais eficaz que os baselines.",TESE,ORION: uma abordagem eficaz e robusta para aquisição de valores de atributos de entidades do mundo real,5124525,01
"One of the main research areas in Systems Biology concerns the discovery of biological networks from microarray datasets. These networks consist of a great number of genes whose expression levels affect each other in various ways. We present a new way of analyzing microarray datasets, based on the different kind of cycles found among genes of the co-expression networks constructed using quantized data obtained from the microarrays. The input of the analysis method is formed by raw data, a set of interest genes  (for example, genes from a known pathway) and a function (activator or inhibitor) of these genes. The output of the method is a set of cycles. A cycle is a closed walk with all vertices (except the first and last) distinct. Thanks to the new way of finding relations among genes, a more robust interpretation of gene correlations is possible, because cycles are associated with feedback mechanisms, that are very common in biological networks. Our hypothesis is that negative feedbacks allow finding relations among genes that may help explaining the stability of the regulatory process within the cell. Positive feedback cycles, on the other hand, may show the amount of imbalance of a certain cell in a given time. The cycle-based analysis allows identifying the stoichiometric relationship between the genes of the network. This methodology provides a better understanding of the tumor biology. As a consequence, it may enable the development of more effective treatment therapies. Furthermore, cycles help differentiate, measure and explain the phenomena identified in healthy and diseased tissues. Cycles may also be used as a new method for classification of samples of a microarray (cancer diagnosis). Compared to other classification methods, cycle-based classification provides a richer explanation of the proposed classification, that can give hints on the possible therapies. Therefore, the main contributions of this thesis are: (i) a new cycle-based analysis method; (ii) a new classification method; (iii) and, finally, application and achievement of practical results. We use the proposed methodology to analyze the genes of four networks closely related with cancer - apoptosis, glucolysis, cell cycle and NFkB - in tissues of the most aggressive type of brain tumor (Gliobastoma multiforme – GBM) and in healthy tissues. Because most patients with GBMs die in less than a year, and essentially no patient has long-term survival, these tumors have drawn significant attention. Our main results show that the stoichiometric relationship between genes involved in apoptosis, glucolysis, cell cycle and NFkB pathways is unbalanced in GBM samples versus control samples. This dysregulation can be measured and explained by the identification of a higher percentage of positive cycles in these networks. This conclusion helps to understand more about the biology of this tumor type. The proposed cycle-based classification method achieved the same performance metrics as a neural network, a classical classification method. However, our method has a significant advantage with respect to neural networks. The proposed classification method not only classifies samples, providing diagnosis, but also explains why samples were classified in a certain way in terms of the feedback mechanisms that are present/absent. This way, the method provides hints to biochemists about possible laboratory experiments, as well as on potential drug target genes.",,Teoria da Computação,FABIANE CRISTINE DILLENBURG,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,22/09/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Bioinformatics;gene co-expression networks;cycle;negative feedback;positive feedback;systems biology;microarrays;gene expression;gliobastoma multiforme (GBM).',LINHA LÓGICA E MODELOS DE COMPUTAÇÃO,LEILA RIBEIRO,133,Bioinformática;Redes de Co-expressão Gênica;Ciclo;Feedback Negativo;Feedback Positivo;Biologia de Sistemas;Microarrays;Expressão Gênica;Apoptose;Glicólise;Ciclo Celular;NF B;Gliobastoma multiforme;GBM;Análise;Classificação,COMPUTAÇÃO (42001013004P4),-,"Uma das principais áreas de pesquisa em Biologia de Sistemas refere-se à descoberta de redes
biológicas a partir de conjuntos de dados de microarrays. Estas redes consistem de um grande
número de genes cujos níveis de expressão afetam os outros genes de vários modos. Nesta tese,
apresenta-se uma nova maneira de analisar os conjuntos de dados de microarrays, com base
nos diferentes tipos de ciclos encontrados entre os genes das redes de co-expressão construídas
com dados quantificados obtidos a partir dos microarrays. A entrada do método de análise é
formada pelos dados brutos, um conjunto de genes de interesse (por exemplo, genes de uma via
conhecida) e uma função (ativador ou inibidor) destes genes. A saída do método é um conjunto
de ciclos. Um ciclo é um caminho fechado com todos os vértices (exceto o primeiro e o último)
distintos. Graças à nova forma de encontrar relações entre os genes, é possível uma interpretação mais robusta das correlações dos genes, porque os ciclos estão associados a mecanismos
de feedback, que são muito comuns em redes biológicas. A hipótese é que feedbacks negativos permitem encontrar relações entre os genes que podem ajudar a explicar a estabilidade do
processo regulatório dentro da célula. Ciclos de feedback positivo, por outro lado, podem mostrar a quantidade de desequilíbrio de uma determinada célula em um determinado momento.
A análise baseada em ciclos permite identificar a relação estequiométrica entre os genes da
rede. Esta metodologia proporciona uma melhor compreensão da biologia do tumor. Como
consequência, pode permitir o desenvolvimento de terapias de tratamento mais eficazes. Além
disso, os ciclos ajudam a diferenciar, medir e explicar os fenômenos identificados em tecidos
saudáveis e doentes. Os ciclos também podem ser usados como um novo método para a classificação de amostras de um microarray (diagnóstico de câncer). Em comparação com outros
métodos de classificação, a classificação baseada em ciclos fornece uma explicação mais rica
da classificação proposta, que pode dar pistas sobre as possíveis terapias. Portanto, as principais
contribuições desta tese são: (i) um novo método de análise baseada em ciclos; (ii) um novo
método de classificação; (iii) e, finalmente, aplicação dos métodos e a obtenção de resultados
práticos. A metodologia proposta foi utilizada para analisar os genes de quatro redes fortemente
relacionadas com o câncer - apoptose, glicólise, ciclo celular e NFB - em tecidos do tipo mais
agressivo de tumor cerebral (Gliobastoma multiforme - GBM) e em tecidos cerebrais saudáveis.
A maioria dos pacientes com GBM morrem em menos de um ano, essencialmente nenhum pa-
ciente tem sobrevivência a longo prazo, por isso estes tumores têm atraído atenção significativa.
Os principais resultados nesta tese mostram que a relação estequiométrica entre genes envolvidos na apoptose, glicólise, ciclo celular e NFB está desequilibrada em amostras de GBM
em comparação as amostras de controle. Este desequilíbrio pode ser medido e explicado pela
identificação de um percentual maior de ciclos positivos nas redes das primeiras amostras. Esta
conclusão ajuda a entender mais sobre a biologia deste tipo de tumor. O método de classificação
baseado no ciclo proposto obteve as mesmas métricas de desempenho como uma rede neural,
um método clássico de classificação. No entanto, o método proposto tem uma vantagem significativa em relação às redes neurais. O método de classificação proposto não só classifica as
amostras, fornecendo diagnóstico, mas também explica porque as amostras foram classificadas
de uma certa maneira em termos dos mecanismos de feedback que estão presentes/ausentes.
Desta forma, o método fornece dicas para bioquímicos sobre possíveis experiências laboratoriais, bem como sobre potenciais genes alvo de terapias.",TESE,An Approach for Analyzing and Classifying Microarray Data Using Gene Co-expression Networks Cycles,5124569,01
"In Distance Learning (EaD), supporting software such as Virtual Learning Environments (VLE) are considered resources that favor communication between the actors
involved, allowing the exchange of information. Assigning Articial Intelligence to
these VLEs, using Multi-Agent Systems (MAS) is a way of ensuring they have a
good performance and that its resources facilitate the learning process. This work
contains a study on the major existing VLEs and on alternative methods to integrate
VLE with MAS. When analyzing the state of the art of the VLEs it is possible to see
that all of them work as aid tools for students, but none of them work on management aspects of distance learning that support the relevant aspects of the legislation
for this type of education. Therefore, this thesis aims to create a VLE-MAS integration model that can make the VLE MOODLE able to help distance learning
managers in their dierent tasks, based on incorporating a legislation representation
model to it. To accomplish this integration of the legislation-oriented VLE-MAS
model, a specic Multi-Agent System organizational model was developed. At last,
based on a case study, simulations will be conducted to verify the functionalities of
the VLE-MAS System Model oriented to legislation, proposed in this thesis.",,Inteligência Artificial,MARIA ISABEL GIUSTI MOREIRA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,01/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'multi-agent systems;organizational models;virtual learning environments;distance education;legal systems',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",ANTONIO CARLOS DA ROCHA COSTA,142,Sistemas Multiagentes;modelos organizacionais;ambientes virtuais de aprendizagem;educação a distância;sistemas jurídicos,COMPUTAÇÃO (42001013004P4),-,"Dentro da Educação a Distância (EaD), os softwares de apoio como os Ambientes Virtuais de Aprendizagem (AVA) são considerados recursos que favorecem a comunicação entre os atores envolvidos, permitindo a troca de informação. Atribuir Inteligência Artificial a esses AVAs, utilizando Sistemas Multiagentes (SMA)  é uma forma de procurar que os mesmos tenham um bom desempenho e que seus recursos facilitem o processo de aprendizagem. Esse trabalho contém um estudo sobre os principais AVAs existentes e sobre os métodos alternativos de integração de AVA com SMA. Ao analisar o estado da arte dos AVAs pode-se observar que todos trabalham como ferramentas de auxílio ao aluno, porém nenhum deles trabalha aspectos da gestão da EaD dando suporte aos aspectos relevantes da legislação dessa modalidade. Por esse motivo, essa Tese tem por objetivo a criação de um modelo de integração AVA-SMA que possa tornar o AVA MOODLE capaz de auxiliar os gestores da EaD em suas diferentes tarefas, com base na incorporação, ao mesmo, de um modelo de representação de legislação. Para realizar essa integração do modelo AVA-SMA orientado à Legislação foi desenvolvido um específico modelo organizacional de Sistema Multiagentes. Por fim, com base em um estudo de caso, será realizado simulações para verificar as funcionalidades do Modelo de Sistema AVA-SMA orientado à Legislação, proposto nesta Tese.",TESE,Um Modelo de Sistema AVA-SMA orientado a Legislac~ao,5124621,01
"The problem of handling user mobility has been around since mobile devices became capable of handling multimedia content and is still one of the most relevant challenges in networking. The conventional Internet architecture is inadequate in dealing with an ever-growing number of mobile devices that are both consuming and producing content. Named Data Networking (NDN) is a network architecture that can potentially overcome this mobility challenge. It supports consumer mobility by design but fails to offer the same level of support for content mobility. Content mobility requires guaranteeing that consumers manage to find and retrieve desired content even when the corresponding producer (or primary host) is not available. In this thesis, we propose a proactive replication mechanism that increases content availability through data redundancy in the context of the NDN architecture. Proactive Data Replication Mechanism (PDRM) is a proactive, locality-aware, best-effort, and hint-based replication mechanism that explores available resources from end-users in the vicinity to improve content availability even in the case of producer mobility. Throughout the thesis, we discuss the design of PDRM, investigate its ideal internal parameters, evaluate the impact of the number of available providers in the vicinity and in-network cache capacity on its operation, and compare its performance to Vanilla NDN and two state-of-the-art proposals. The evaluation indicates that PDRM improves content mobility support due to using object popularity information and spare resources in the vicinity to help the proactive replication. Results show that PDRM can reduce the download times up to 53.55%, producer load up to 71.6%, inter-domain traffic up to 46.5%, and generated overhead up to 25% compared to Vanilla NDN and other evaluated mechanisms.",,Redes de Computadores,MATHEUS BRENNER LEHMANN,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,01/11/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Named Data Networking;Content Mobility;Data Replication;Location Awareness',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",ANTONIO MARINHO PILLA BARCELLOS,118,Named Data Networking;Mobilidade de Conteúdo;Replicação de Dados;Consciência de Localização,COMPUTAÇÃO (42001013004P4),-,"O problema de lidar com a mobilidade dos usuários existe desde que os dispositivos móveis se tornaram capazes de lidar com conteúdo multimídia e ainda é um dos desafios
mais relevantes na área de redes de computadores. A arquitetura de Internet convencional é inadequada em lidar com um número cada vez maior de dispositivos móveis que
estão tanto consumindo quanto produzindo conteúdo. Named Data Networking (NDN)
é uma arquitetura de rede que pode potencialmente superar este desafio de mobilidade.
Ela suporta a mobilidade do consumidor nativamente, mas não oferece o mesmo nível
de suporte para a mobilidade de conteúdo. A mobilidade de conteúdo exige garantir que
os consumidores consigam encontrar e recuperar o conteúdo desejado mesmo quando o
produtor correspondente (ou o hospedeiro principal) não estiver disponível. Nesta tese,
propomos o PDRM (Proactive Data Replication Mechanism), um mecanismo de replicação de dados proativo e consciente de localização, que aumenta a disponibilidade de
conteúdo através da redundância de dados no contexto da arquitetura NDN. Ele explora
os recursos disponíveis dos usuários finais na vizinhança para melhorar a disponibilidade
de conteúdo, mesmo no caso da mobilidade do produtor.w Ao longo da tese, discutimos
o projeto do PDRM, avaliamos o impacto do número de provedores disponíveis na vizinhança e a capacidade de cache na rede em sua operação e comparamos seu desempenho
com NDN padrão e duas propostas do estado-da-arte. A avaliação indica que o PDRM
melhora o suporte à mobilidade de conteúdo devido ao uso de informações de popularidade dos objetos e recursos extras na vizinhança para ajudar a replicação pró-ativa. Os
resultados mostram que o PDRM pode reduzir os tempos de download até 53,55%, o
carregamento do produtor até 71,6%, o tráfego entre domínios até 46,5% e a sobrecarga
gerada até 25% em comparação com NDN padrão e os demais mecanismos avaliados.",TESE,PDRM: A Proactive Data Replication Mechanism to Improve Content Mobility Support in NDN using Location Awareness,5124699,01
"Choosing the most suitable conference to submit a paper is a task that depends on various
factors: (i) the topic of the paper needs to be among the topics of interest of the conference; (ii) submission deadlines need to be compatible with the necessary time for paper
writing; (iii) conference location and registration costs; and (iv ) the quality or impact of
the conference. These factors allied to the existence of thousands of conferences, make
the search of the right event very time consuming, especially when researching in a new
area. Intending to help researchers finding conferences, this work presents a method developed to retrieve and extract data from conference web sites. Our method combines the
identification of conference URL and deadline extraction. This is a challenging task as
each web site has its own layout. Here, we propose C ONFTRACKER, which combines
the identification of the URLs of conferences listed in the Qualis Table and the extraction
of their deadlines. Information extraction is carried out independent from the page’s layout and how the dates are presented. To evaluate our proposed method, we carried out
experiments with real web data from Computer Science conferences. The results show
that C ONFTRACKER outperformed a baseline method based on the position of labels and
dates. Finaly, the extracted data is stored in a database to be searched with an online tool.",,Ciência de Dados e Engenharia de Software,CASSIO ALAN GARCIA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,05/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Information Extraction;Conditional Random Fields',"LINHA MINERAÇÃO, INTEGRAÇÃO E ANÁLISE DE DADOS",VIVIANE PEREIRA MOREIRA,50,Extração de informação;Conferências;Deadlines;Conditional Random Fields,COMPUTAÇÃO (42001013004P4),-,"A escolha da conferência adequada para o envio de um artigo é uma tarefa que depende de diversos fatores: (i) o tema do trabalho deve estar entre os temas de interesse do evento; (ii) o prazo de submissão do evento deve ser compatível com tempo necessário para a escrita do artigo; (iii) localização da conferência e valores de inscrição são levados em consideração; e (iv) a qualidade da conferência (Qualis) avaliada pela CAPES. Esses fatores aliados à existência de milhares de conferências tornam a busca pelo evento adequado bastante demorada, em especial quando se está pesquisando em uma área nova. A fim de auxiliar os pesquisadores na busca de conferências, o trabalho aqui desenvolvido apresenta um método para a coleta e extração de dados de sites de conferências. Esta é uma tarefa desafiadora, principalmente porque cada conferência possui seu próprio site, com diferentes layouts. O presente trabalho apresenta um método chamado CONFTRACKER que combina a identificação de URLs de conferências da Tabela Qualis à identificação de deadlines a partir de seus sites. A extração das informações é realizada independente da conferência, do layout do site e da forma como são apresentadas as datas (formatação e rótulos). Para avaliar o método proposto, foram realizados experimentos com dados reais de conferências da Ciência da Computação. Os resultados mostraram que CONFTRACKER obteve resultados significativamente melhores em relação a um baseline baseado na posição entre rótulos e datas. Por fim, o processo de extração é executado para todas as conferências da Tabela Qualis e os dados coletados populam uma base de dados que pode ser consultada através de uma interface online.",DISSERTAÇÃO,Extração de Informações de Conferências em Páginas Web,5124776,01
"There are thousands of snake species in the world, many with intricate and distinct skin patterns. This diversity becomes a problem for users who need to create snake skin textures to apply on 3D models, as the difficulty for creating such complex patterns is considerable.
We first propose a categorization of snake skin patterns considering their visual characteristics. We then present a procedural model capable of synthesizing a wide range of texture skin patterns from snakes. The model uses simple image processing (such as synthesizing spots and stripes) as well as cellular automata and noise generators to create realistic textures for use in a physically-based renderer.
Our results show good visual similarity with real skin found in snakes. The resulting textures can be used not only for computer graphics texturing, but also in education about snakes and their visual characteristics. We have also performed a user study to assess the usability of our tool. The score from the System Usability Scale was 85.8, suggesting a highly effective texturing tool.",,Computação Visual,JEFFERSON MAGALHAES PINHEIRO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,30/10/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'computer graphics;texture synthesis;procedural texture generation;mathematical;Biology',LINHA COMPUTAÇÃO GRÁFICA E VISUALIZAÇÃO DE DADOS,MARCELO WALTER,100,Computação gráfica;síntese de texturas;geração procedural de texturas;biologia matemática,COMPUTAÇÃO (42001013004P4),-,"Existem milhares de espécies de serpentes no mundo, muitas com padrões distintos e intricados. Esta diversidade se torna um problema para usuários que precisam criar texturas de pele
de serpente para aplicar em modelos 3D, pois a dificuldade em criar estes padrões complexos é
considerável. Nós primeiramente propomos uma categorização de padrões de pele de serpentes
levando em conta suas características visuais. Então apresentamos um modelo procedural capaz
de sintetizar uma vasta gama de textura de padrões de pele de serpentes. O modelo usa processamento de imagem simples (tal como sintetizar bolinhas e listras) bem como autômatos celulares
e geradores de ruído para criar texturas realistas para usar em renderizadores modernos. Nossos
resultados mostram boa similaridade visual com pele de serpentes reais. As texturas resultantes
podem ser usadas não apenas em computação gráfica, mas também em educação sobre serpentes e suas características visuais. Nós também realizamos testes com usuários para avaliar
a usabilidade de nossa ferramenta. O escore da Escala de Usabilidade do Sistema foi de 85:8,
sugerindo uma ferramenta de texturização altamente efetiva.",DISSERTAÇÃO,A Procedural Model for Snake Skin Texture Generation,5127136,01
"This thesis reviews and investigates social problem-solving with a particular  focus on artificial and heterogeneous systems. More specifically, we not only compile and comprehensively examine recent research results, but also discuss future directions in the study of such heterogeneous complex systems. Given their complex nature, such systems often defy analyses. Even computationally simple models can behave unpredictably after a few iterations. Therefore, one central issue in Social Computing is to devise models of social interaction that are amenable to investigation. This way, one can understand the complex relationships among the components and the outcome of the social process. This thesis surveys scientific inquiries concerned with fundamental aspects in social problem-solving systems and their impact in ability and performance of such systems. These aspects include modeling, communication structure and individual problem-solver traits. This thesis also reports the student endeavour during the period of research and summarizes several already published contributions. Among them there is (i) the study of general frameworks for the study of social problem-solving, (ii) the investigation of the role of centrality in individual and collective outcomes, and (iii) the exploration of heterogeneous models of social problem-solving. These three points, in an integrated perspective underpin the understanding of network and communication structures, adjust the strategic systems’ composition, and exploit problems’ structures and patterns in social problem-solving systems.",,Inteligência Artificial,DIEGO VRAGUE NOBLE,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,14/12/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Computational social systems;computational social science;social computing',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",LUIS DA CUNHA LAMB,73,Sistemas Sociais Computacionais;Computação Social;Inteligência Artificial,COMPUTAÇÃO (42001013004P4),-,"Metódos analíticos de investigação são usualmente ineficazes para sistemas computacionais
sociais já que apenas algumas iterações do sistema já são suficientes para que o
sistema se torne imprevisível. Portanto, uma das principais questões na Computação Social
é o desenvolvimento de modelos sociais passíveis de investigação. Assim é possível
que se compreenda o relacionamento complexo entre os componentes de sistemas sociais
computacionais e o resultado. Este aspectos incluem a modelagem, a estrutura de comunicação
e características individuais do agentes envolvidos na resolução dos problemas.
do processo social. Esta tese explora sistemas computacionais de resolução de problemas
com foco em sistemas artificiais e heterogêneos. Nela é feita uma compilação extensiva
da literatura relacionada em sistemas complexos onde as contribuições do candidato são
expostas dentro de contextos específicos da área. Entre elas está o estudo de modelos
abstratos e gerais de resolução social de problemas, a investigação do impacto da centralidade
no resultado individual e coletivo, a análise experimental de modelos heterogêneos
de resolução social de problemas. Quando integradas, estas contribuições reforçam o entendimento
sobre a importância da rede e das estruturas de comunicação, a composição
estratégica do sistema, a estrutura do problema e possíveis padrões gerais na resolução
social de problemas.",TESE,The Role of Heterogeneity in Social Problem-Solving,5294177,01
"The difference among workstations is assumed to be negligible in traditional assembly lines.
Heterogeneous assembly lines consider the problem of industries in which the task times vary
according to some property to be selected for the task. In the Assembly Line Worker Assignment
and Balancing Problem (ALWABP), workers are assigned to workstations and according
to their abilities, they execute tasks in different amounts of time. In some cases they can even be
incapable of executing some tasks. In the Robotic Assembly Line Balancing Problem (RALBP)
there are different types of robots and each station must be executed by a robot. Multiple robots
of the same type may be used.
We propose exact and heuristic methods for minimizing the cycle time of these two problems,
for a fixed number of stations. The problems have similar characteristics that are explored
to produce lower bounds, heuristic methods, mixed-integer programming models, and reduction
and dominance rules. For the branching strategy of the branch-and-bound method, however, the
differences among the problem force the use of two different algorithms. A task-oriented strategy
has the best results for the ALWABP-2 while a station-oriented strategy has the best results
for the RALBP-2. The lower bounds, heuristics, MIP models and branch-and-bound algorithms
for these two problems are shown to be competitive with the state-of-the-art methods in the literature.",,Teoria da Computação,LEONARDO DE MIRANDA BORBA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,24/11/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Assembly Line Balancing;ALWABP-2;RALBP-2;MMALBP-2;Branch-and- Bound;Beam Search',LINHA ALGORITMOS E OTIMIZAÇÃO,MARCUS ROLF PETER RITT,98,Balanceamento de Linhas de Montagem;ALWABP-2;RALBP-2;Branch-and-Bound;Beam Search;MIP,COMPUTAÇÃO (42001013004P4),-,"A diferença entre estações de trabalho é considerada desprezível em linhas de montagem tradicionais. Por outro lado, linhas de montagem heterogêneas consideram o problema de indústrias nas quais os tempos das tarefas variam de acordo com alguma característica a ser selecionada para a tarefa. No Problema de Balanceamento e Atribuição de Trabalhadores em Linhas de Montagem (do inglês Assembly Line Worker Assignment and Balancing Problem, ALWABP), os trabalhadores são responsáveis por estações de trabalho e de acordo com as suas habilidades, eles executam as tarefas em diferentes quantidades de tempo. Em alguns casos, os trabalhadores podem até ser incapazes de executar algumas tarefas. No Problema de Balanceamento de Linhas de Montagem Robóticas (do inglês Robotic Assembly Line Balancing Problem, RALBP), há diferentes tipos de robôs e o conjunto de tarefas de cada estação deve ser executada por um robô. Robôs do mesmo tipo podem ser usados múltiplas vezes. 
Nós propomos métodos exatos e heurísticos para a minimização do tempo de ciclo destes dois problemas, para um número fixo de estações. Os problemas têm características similares que são exploradas para produzir limitantes inferiores, métodos inferiores, models de programação inteira mista, e regras de redução e dominância. Para a estratégia de ramificação do método de branch-and-bound, entretanto, as diferenças entre os problemas forçam o uso de dois algoritmos diferentes. Uma estratégia orientada a tarefas tem os melhores resultados para o ALWABP-2, enquanto uma estratégia orientada a estações tem os melhores resultados para o RALBP-2. Nós mostramos que os limitantes inferiores, heurísticas, modelos de programação inteira mista e algoritmos de branch-and-bound para estes dois problemas são competitivos com os métodos do estado da arte da literatura.",TESE,Métodos Exatos e Heurísticos para Problemas de Balanceamento de Linhas de Montagem Heterogêneas do Tipo 2,5294262,01
"Sentiment Analysis/Opinion Mining has been adopted in software engineering for problems
such as software usability and sentiment of developers in projects. This work proposes methods
to evaluate the sentiment contained in tickets for IT (Information Technology) support.
IT tickets are broad in coverage (e.g. infrastructure, software), and involve errors, incidents,
requests, etc. The main challenge is to automatically distinguish between factual information,
which is intrinsically negative (e.g. error description), from the sentiment embedded in the
description.
Our approach is to automatically create a domain dictionary that contains terms with sentiment
in IT context, used to filter terms in tickets for sentiment analysis. We created and evaluate three
classification methods for calculating the polarity of terms in tickets. Our study was developed
using 34,895 tickets from five organizations.
For polarity, we randomly selected 2.333 tickets to compose a gold standard. Our best results
display an average precision and recall of 82.83% and 88.42%, respectively, which outperforms
the compared sentiment analysis solutions.
Complementarily, emotions in tickets were studied considering the models of Ekman and VAD.
One of the three classification methods created has been adapted to also identify emotions in the
tickets. Possible correlations between polarity and emotions were verified through association
rules. Results correlate positive tickets with valence and dominance high and low excitation,
besides presence of joy and surprise and absence of fear. Negative tickets correlate with valence,
neutral excitement and dominance, besides absence of joy and presence of fear. However the
results for negative polarity are not accurate.",,Ciência de Dados e Engenharia de Software,CASSIO CASTALDI ARAUJO BLAZ,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,13/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Sentiment Analysis;Opinion Mining;IT Tickets;Domain Dictionary',"LINHA MINERAÇÃO, INTEGRAÇÃO E ANÁLISE DE DADOS",KARIN BECKER,96,Análise de Sentimentos;Mineração de Opinião;Tíquetes para TI;Dicionário de Domínio,COMPUTAÇÃO (42001013004P4),-,"Análise de Sentimentos/Mineração de Opinião é adotada na engenharia de software para questões como usabilidade e sentimentos de desenvolvedores em projetos. Este trabalho propõe métodos para avaliar os sentimentos presentes em tíquetes abertos à área de suporte de TI. Há diversos tipos de tíquetes abertos à TI (e.g. infraestrutura, software), que envolvem erros, incidentes, requisições, etc. O maior desafio é automaticamente distinguir entre a necessidade em si, a qual é intrinsecamente negativa (por exemplo, a descrição de um erro), de um sentimento embutido na descrição. Nossa abordagem automaticamente cria um dicionário de domínio que contém termos que expressam sentimentos no contexto de TI, utilizados para filtrar expressões em um tíquete para análise de sentimentos. Nós criamos e avaliamos três métodos de classificação para calcular a polaridade em tíquetes. Nosso estudo utilizou 34.895 tíquetes de cinco organizações. Para polaridade, 2.333 tíquetes foram selecionados aleatoriamente para compor nosso gold standard. Nossos melhores resultados apresentam uma precisão e revocação de 82,83% e 88,42%, respectivamente, o que supera outras soluções de análise de sentimentos comparadas. De forma complementar, emoções em tíquetes foram estudadas considerando os modelos de Ekman e VAD. Um dos três métodos de classificação criados foi adaptado para também identificar emoções nos tíquetes. Possíveis correlações entre polaridade e emoções foram verificadas via regras de associação. Resultados correlacionam tíquetes positivos com valência e dominância altas e excitação baixa, além de presença de alegria e surpresa e ausência de medo. Tíquetes negativos correlacionam com valência, excitação e dominância neutras, além de ausência de alegria e presença de medo. Contudo os resultados para a polaridade negativa não são precisos.",DISSERTAÇÃO,Análise de Sentimentos em Tíquetes para o Suporte de TI,5294307,01
"The quest for performance has been a constant through the history of computing systems. It has been more than a decade now since the sequential processing model had shown its first signs of exhaustion to keep performance improvements. Walls to the sequential computation pushed a paradigm shift and established the parallel processing as the standard in modern computing systems. With the widespread adoption of parallel computers, many algorithms and applications have been ported to fit these new architectures. However, in unconventional applications, with interactivity and real-time requirements, achieving efficient parallelizations is still a major challenge. Real-time performance requirement shows-up, for instance, in user-interactive simulations where the system must be able to react to the user’s input within a computation time-step of the simulation loop. The same kind of constraint appears in streaming data monitoring applications. For instance, when an external source of data, such as traffic sensors or social media posts, provides a continuous flow of information to be consumed by an on-line analysis system. The consumer system has to keep a controlled memory budget and delivery fast processed information about the stream. Common optimizations relying on pre-computed models or static index of data are not possible in these highly dynamic scenarios. The dynamic nature of the data brings up several performance issues originated from the problem decomposition for parallel processing and from the data locality maintenance for efficient cache utilization. In this thesis we address data-dependent problems on two different applications: one in physically based simulations and other on streaming data analysis. To the simulation problem, we present a parallel GPU algorithm for computing multiple shortest paths and Voronoi diagrams on a grid-like graph. To the streaming data analysis problem we present a parallelizable data structure, based on packed memory arrays, for indexing dynamic geo-located data while keeping good memory locality.",,Computação Visual,JULIO TOSS,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,26/10/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Parallel processing;data locality;stream processing;real-time processing;physically based simulation',LINHA COMPUTAÇÃO GRÁFICA E VISUALIZAÇÃO DE DADOS,JOAO LUIZ DIHL COMBA,89,algoritmos paralelos;localidade de dados;processamento de fluxo de dados;processamento em tempo real;simulação baseada em física,COMPUTAÇÃO (42001013004P4),-,"A busca por desempenho tem sido uma constante na história dos sistemas computacionais.
Ha mais de uma década, o modelo de processamento sequencial já mostrava seus
primeiro sinais de exaustão pare suprir a crescente exigência por performance. Houveram
""barreiras""para a computação sequencial que levaram a uma mudança de paradigma
e estabeleceram o processamento paralelo como padrão nos sistemas computacionais modernos.
Com a adoção generalizada de computadores paralelos, novos algoritmos foram
desenvolvidos e aplicações reprojetadas para se adequar às características dessas novas
arquiteturas. No entanto, em aplicações menos convencionais, com características de interatividade
e tempo real, alcançar paralelizações eficientes ainda representa um grande
desafio.
O requisito por desempenho de tempo real apresenta-se, por exemplo, em simulações interativas
onde o sistema deve ser capaz de reagir às entradas do usuário dentro do tempo
de uma iteração da simulação. O mesmo tipo de exigência aparece em aplicações de
monitoramento de fluxos contínuos de dados (streams). Por exemplo, quando dados provenientes
de sensores de tráfego ou postagens em redes sociais são produzidos em fluxo
contínuo, o sistema de análise on-line deve ser capaz de processar essas informações em
tempo real e ao mesmo tempo manter um consumo de memória controlada.
A natureza dinâmica desses dados traz diversos problemas de performance, tais como a
decomposição do problema para processamento em paralelo e a manutenção da localidade
de dados para uma utilização eficiente da memória cache. As estratégias de otimização
tradicionais, que dependem de modelos pré-computados ou de índices estáticos sobre os
dados, não atendem às exigências de performance necessárias nesses cenários.
Nesta tese, abordamos os problemas dependentes de dados em dois contextos diferentes:
um na área de simulações baseada em física e outro em análise de dados em fluxo
contínuo. Para o problema de simulação, apresentamos um algoritmo paralelo, em GPU,
para computar múltiplos caminhos mínimos e diagramas de Voronoi em um grafo com
topologia de grade. Para o problema de análise de fluxos de dados, apresentamos uma
estrutura de dados paralelizável, baseada em Packed Memory Arrays, para indexar dados
dinâmicos geo-localizados ao passo que mantém uma boa localidade de memória.",TESE,Parallel Algorithms and Data Structures for Interactive Applications,5294422,01
"Many years ago, the ensemble systems have been shown to be an efficient method to increase
the accuracy and stability of learning algorithms in recent decades, although its construction has
a question to be elucidated: diversity. The disagreement among the models that compose the
ensemble can be generated when they are built under different circumstances, such as training
dataset, parameter setting and selection of learning algorithms. The ensemble may be viewed
as a structure with three levels: input space, the base components and the combining block of
the components responses. In this work is proposed a multi-level approach using genetic algorithms
to build the ensemble of Least Squares Support Vector Machines (LS-SVM), performing
a feature selection in the input space, the parameterization and the choice of which models will
compose the ensemble at the component level and finding a weight vector which best represents
the importance of each classifier in the final response of the ensemble. In order to evaluate the
performance of the proposed approach, some benchmarks from UCI Repository have been used
to compare with other classification algorithms. Also, the results obtained by our approach were
compared with some deep learning methods on the datasets MNIST and CIFAR and proved very
satisfactory.",,Inteligência Artificial,CARLOS ALBERTO DE ARAUJO PADILHA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,19/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Ensemble Systems;Genetic Algorithms;Least Squares Support Vector Machines;Feature Selection;Diversity;Deep Learning.',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",DANTE AUGUSTO COUTO BARONE,119,Comitê de Máquinas;Algoritmos Genéticos;Máquinas de Vetor de Suporte  por Mínimos Quadrados;Seleção de Atributos;Diversidade;Aprendizagem Profunda,COMPUTAÇÃO (42001013004P4),-,"Há muitos anos, os sistemas de comitê já tem se mostrado um método eficiente para aumentar a acurácia e estabilidade de algoritmos de aprendizado nas décadas recentes, embora sua cons- trução tem uma questão para ser elucidada: diversidade. O desacordo entre os modelos que compõe o comitê pode ser gerado quando eles são contruídos sob diferentes circunstâncias, tais como conjunto de dados de treinamento, configuração dos parâmetros e a seleção dos algorit- mos de aprendizado. O ensemble pode ser visto como uma estrutura com três níveis: espaço de entrada, a base de componentes e o bloco de combinação das respostas dos componentes. Neste trabalho é proposto uma abordagem multi-nível usando Algoritmos Genéticos para construir um ensemble de Máquinas de Vetor de Suporte por Mínimos Quadrados ou LS-SVM, realizando uma seleção de atributos no espaço de entrada, parametrização e a escolha de quais modelos irão compor o comitê no nível de componentes e a busca por um vetor de pesos que melhor represente a importância de cada classificador na resposta final do comitê. A combinação de seleção de atributos e parametrização deve ajudar a promover ainda mais diversidade. De forma a avaliar a performance da abordagem proposta, nós usamos alguns benchmarks do repositório da UCI para comparar com outros algoritmos de classificação, incluindo algumas mudanças na função de fitness da nossa abordagem. Além disso, também comparamos os resultados da nossa abordagem com métodos de aprendizagem profunda nas bases de dados MNIST e CI- FAR, e mostramos resultados bastante competitivos.",TESE,Uma Abordagem Multinível usando Algoritmos Genéticos em um Comitê de LS-SVM,5652168,01
": Natural language processing systems often rely on the idea that language is compositional, that is, the meaning of a linguistic entity can be inferred from the meaning of its parts. This expectation fails in the case of multiword expressions (MWEs). For example, a person who is a sitting duck is neither a duck nor necessarily sitting. Modern computational techniques for inferring word meaning based on the distribution of words in the text have been quite successful at multiple tasks, especially since the rise of word embedding approaches. However, the representation of MWEs still remains an open problem in the field. In particular, it is unclear how one could predict from corpora whether a given MWE should be treated as an indivisible unit (e.g. nut case) or as some combination of the meaning of its parts (e.g. engine room). This thesis proposes a framework of MWE compositionality prediction based on representations of distributional semantics, which we instantiate under a variety of parameters. We present a thorough evaluation of the impact of these parameters on three new datasets of MWE compositionality, encompassing English, French and Portuguese MWEs.  Finally, we present an extrinsic evaluation of the predicted levels of MWE compositionality on the task of MWE identification. Our results suggest that the proper choice of distributional model and corpus parameters can produce compositionality predictions that are comparable to the state of the art.",,Inteligência Artificial,SILVIO RICARDO CORDEIRO,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,18/12/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'distributional semantics;multiword expressions;compositionality;idiomaticity',"LINHA APRENDIZADO DE MÁQUINA, REPRESENTAÇÃO DE CONHECIMENTO E RACIOCÍNIO",ALINE VILLAVICENCIO,167,Semântica distribucional;Expressões multipalavras;Composicionalidade;Idiomaticidade,COMPUTAÇÃO (42001013004P4),-,"Sistemas de processamento de linguagem natural baseiam-se com frequência na hipótese
de que a linguagem humana é composicional, ou seja, que o significado de uma entidade
linguística pode ser inferido a partir do significado de suas partes. Essa expectativa falha
no caso de expressões multipalavras (EMPs). Por exemplo, uma pessoa caracterizada
como pão-duro não é literalmente um pão, e também não tem uma consistência molecular
mais dura que a de outras pessoas. Técnicas computacionais modernas para inferir o
significado das palavras com base na sua distribuição no texto vêm obtendo um considerável
sucesso em múltiplas tarefas, especialmente após o surgimento de abordagens de
word embeddings. No entanto, a representação de EMPs continua a ser um problema em
aberto na área. Em particular, não existe um método consolidado que prediga, com base
em corpora, se uma determinada EMP deveria ser tratada como unidade indivisível (por
exemplo olho gordo) ou como alguma combinação do significado de suas partes (por exemplo
tartaruga marinha). Esta tese propõe um modelo de predição de composicionalidade
de EMPs com base em representações de semântica distribucional, que são instanciadas
no contexto de uma variedade de parâmetros. Também é apresentada uma avaliação minuciosa
do impacto desses parâmetros em três novos conjuntos de dados que modelam
a composicionalidade de EMP, abrangendo EMPs em inglês, francês e português. Por
fim, é apresentada uma avaliação extrínseca dos níveis previstos de composicionalidade
de EMPs, através da tarefa de identificação de EMPs. Os resultados obtidos sugerem que
a escolha adequada do modelo distribucional e de parâmetros de corpus pode produzir
predições de composicionalidade que são comparáveis às observadas no estado da arte.",TESE,Distributional models of multiword expression compositionality prediction,5672824,01
"One of the main research challenges in UbiComp is to provide mechanisms for context-aware
to promote the development of applications that react according to the dynamics of user interest
environment. To keep the knowledge of this environment, the area of UbiComp presupposes
the use of information produced and made available in different locations, all the time. In this
sense, the recent advances in the field of Internet of Things (IoT) have provided an increasing
availability of sensors and actuators networked. These sensors are potential producers of
contextual information. With this motivation, this thesis is presented the CoIoT, a middleware
for Internet of Things (IoT) designed in order to manage the collect and processing of contextual
information of the physical environment as well as remote actuation on it. The CoIoT was
designed considering the work previously developed by the research group GPPD (Parallel
Processing Group and distributed) of UFRGS, particularly middleware EXEHDA (Execution
Environment for Highly Distributed Applications). In designing the CoIoT it was adopted
a distributed approach of context processing that includes both the principles of IoT as the
demands of the applications of UbiComp. The proposed architecture also includes rules based
and triggers mechanisms to deal with events that characterize the changes of states of the
contexts of interest. In addition, the proposed architecture manages other important aspects
of IoT scenarios such as the treatment of interoperability, heterogeneity, support the control of
scalability and resource discovery. Until now, the central contributions of this thesis include:
(i) the design of an architecture for IoT able to perform distributed way both the collect and
processing of contextual information, such as remote actuation in the environment in order to
meet UbiComp applications and, (ii) the proposition of a distributed event processing model
appropriate to the IoT scenarios. In order to evaluate the CoIoT architecture, two case studies
were carried out in the area of agriculture. The first case study was developed in a production
environment based on the demands of agricultural researchers, particularly seed analysis. On
the other hand, the second case study was based on precision testing of viticulture environments.",,Sistemas de Computação,RODRIGO SANTOS DE SOUZA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,25/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'ubiquitous computing;internet of things;context-aware',LINHA COMPUTAÇÃO DE ALTO DESEMPENHO E SISTEMAS DISTRIBUÍDOS,CLAUDIO FERNANDO RESIN GEYER,120,Computação ubíqua;internet das coisas;ciência de contexto,COMPUTAÇÃO (42001013004P4),-,"Um dos principais desafios de pesquisa na UbiComp consiste em fornecer mecanismos para a ciência de contexto que promovam o desenvolvimento de aplicações que reajam de acordo com a dinâmica do ambiente de interesse do usuário. Para manter o conhecimento a respeito desse ambiente, a área da UbiComp pressupõe a utilização de informações produzidas e disponibilizadas em diferentes localizações, o tempo todo. Nesse sentido, os recentes avanços na área da Internet das Coisas (IoT) têm proporcionado uma crescente disponibilidade de sensores conectados em rede, os quais são potenciais produtores de informações contextuais do ambiente para aplicações ubíquas. Com essa motivação, nessa tese é apresentado o CoIoT, um middleware para Internet das Coisas concebido com o objetivo de gerenciar a coleta e o processamento das informações contextuais do ambiente físico, bem como a atuação remota sobre o mesmo. O CoIoT foi idealizado considerando os trabalhos previamente desenvolvidos pelo grupo de pesquisa GPPD (Grupo de Processamento Paralelo e Distribuído) da UFRGS, particularmente o middleware EXEHDA (Execution Environment for Highly Distributed Applications). Na concepção do CoIoT foi adotada uma abordagem distribuída de processamento de contexto que contempla tanto as premissas da IoT quanto as demandas das aplicações da UbiComp. A arquitetura proposta também contempla o gerenciamento de eventos distribuídos através de regras e triggers para tratar as mudanças de estados dos contextos de interesse. Além disso, a arquitetura proposta gerencia outros aspectos importantes nos cenários da IoT, como o tratamento da interoperabilidade, da heterogeneidade, apoio ao controle da escalabilidade e descoberta de recursos. As principais contribuições desta tese são: (i) a concepção de uma arquitetura para IoT capaz de realizar de forma distribuída tanto a coleta e processamento das informações contextuais, como a atuação remota no meio a fim de atender as aplicações da UbiComp e, (ii) a proposição de um modelo de processamento de eventos distribuídos adequado aos cenários da IoT. Para avaliar a arquitetura do CoIoT foram realizados dois estudos de caso na área da agricultura. O primeiro estudo de caso foi desenvolvido em ambiente de produção a partir de demandas de pesquisadores da área da agricultura, particularmente da análise de sementes. Já o segundo estudo de caso teve como cenário de testes ambientes da viticultura de precisão.",TESE,Um middleware para Internet das Coisas com suporte ao processamento distribuído do contexto,6036881,01
"Network Function Virtualization (NFV) is a novel concept that is reshaping the middlebox
arena, shifting network functions (e.g. firewall, gateways, proxies) from specialized hardware
appliances to software images running on commodity hardware. This concept has potential to
make network function provision and operation more flexible and cost-effective, paramount in
a world where deployed middleboxes may easily reach the order of hundreds. Despite recent
research activity in the field, little has been done towards scalable and cost-efficient placement
& chaining of virtual network functions (VNFs) – a key feature for the effective success of
NFV. More specifically, existing strategies have neglected the chaining aspect of NFV (focusing
on efficient placement only), failed to scale to hundreds of network functions and relied
on unrealistic operational costs. In this thesis, we approach VNF placement and chaining as
an optimization problem in the context of Inter- and Intra-datacenter. First, we formalize the
Virtual Network Function Placement and Chaining (VNFPC) problem and propose an Integer
Linear Programming (ILP) model to solve it. The goal is to minimize required resource allocation,
while meeting network flow requirements and constraints. Then, we address scalability
of VNFPC problem to solve large instances (i.e., thousands of NFV nodes) by proposing a fixand-
optimize-based heuristic algorithm for tackling it. Our algorithm incorporates a Variable
Neighborhood Search (VNS) meta-heuristic, for efficiently exploring the placement and chaining
solution space. Further, we assess the performance limitations of typical NFV-based deployments
and the incurred operational costs of commodity servers and propose an analytical model
that accurately predict the operational costs for arbitrary service chain requirements. Then, we
develop a general service chain intra-datacenter deployment mechanism (named OCM – Operational
Cost Minimization) that considers both the actual performance of the service chains
(e.g., CPU requirements) as well as the operational incurred cost. Our novel algorithm is based
on an extension of the well-known reduction from weighted matching to min-cost flow problem.
Finally, we tackle the problem of monitoring service chains in NFV-based environments.
For that, we introduce the DNM (Distributed Network Monitoring) problem and propose an
optimization model to solve it. DNM allows service chain segments to be independently monitored,
which allows specialized network monitoring requirements to be met in a efficient and
coordinated way. Results show that the proposed ILP model for the VNFPC problem leads to a
reduction of up to 25% in end-to-end delays (in comparison to chainings observed in traditional
infrastructures) and an acceptable resource over-provisioning limited to 4%. Also, we provide
strong evidences that our fix-and-optimize based heuristic is able to find feasible, high-quality
solutions efficiently, even in scenarios scaling to thousands of VNFs. Further, we provide indepth
insights on network performance metrics (such as throughput, CPU utilization and packet
processing) and its current limitations while considering typical deployment strategies. Our
OCM algorithm reduces significantly operational costs when compared to the de-facto standard
placement mechanisms used in Cloud systems. Last, our DNM model allows finer grained network
monitoring with limited overheads. By coordinating the placement of monitoring sinks
and the forwarding of network monitoring traffic, DNM can reduce the number of monitoring
sinks and the network resource consumption (54% lower than a traditional method).",,Redes de Computadores,MARCELO CAGGIANI LUIZELLI,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,11/08/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Network Function Virtualization;Service Chaining;NFV Orchestration;Combinatorial Optimization;Mathematical Programming;Math-heuristic;Variable Neighborhood Search;Operational Cost;Open vSwitch;Performance Evaluation',"LINHA ARQUITETURAS, PROTOCOLOS E GERÊNCIA DE REDES E SERVIÇO",LUCIANO PASCHOAL GASPARY,157,Funções Virtualizadas de Rede;Encadeamento de Serviços;Orquestração de NFV;Otimização Combinatória;Programação Matemática;Custo Operacional;Avaliação de Desempenho,COMPUTAÇÃO (42001013004P4),-,"A Virtualização de Funções de Rede (NFV – Network Function Virtualization) é um novo conceito
arquitetural que está remodelando a operação de funções de rede (e.g., firewall, gateways
e proxies). O conceito principal de NFV consiste em desacoplar a lógica de funções de rede
dos dispositivos de hardware especializados e, desta forma, permite a execução de imagens
de software sobre hardware de prateleira (COTS – Commercial Off-The-Shelf). NFV tem o
potencial para tornar a operação das funções de rede mais flexíveis e econômicas, primordiais
em ambientes onde o número de funções implantadas pode chegar facilmente à ordem de centenas.
Apesar da intensa atividade de pesquisa na área, o problema de posicionar e encadear
funções de rede virtuais (VNF – Virtual Network Functions) de maneira escalável e com baixo
custo ainda apresenta uma série de limitações. Mais especificamente, as estratégias existentes
na literatura negligenciam o aspecto de encadeamento de VNFs (i.e., objetivam sobretudo o posicionamento),
não escalam para o tamanho das infraestruturas NFV (i.e., milhares de nós com
capacidade de computação) e, por último, baseiam a qualidade das soluções obtidas em custos
operacionais não representativos. Nesta tese, aborda-se o posicionamento e o encadeamento de
funções de rede virtualizadas (VNFPC – Virtual Network Function Placement and Chaining)
como um problema de otimização no contexto intra- e inter-datacenter. Primeiro, formaliza-se
o problema VNFPC e propõe-se um modelo de Programação Linear Inteira (ILP) para resolvêlo.
O objetivo consiste em minimizar a alocação de recursos, ao mesmo tempo que atende
aos requisitos e restrições de fluxo de rede. Segundo, aborda-se a escalabilidade do problema
VNFPC para resolver grandes instâncias do problema (i.e., milhares de nós NFV). Propõe-se
um um algoritmo heurístico baseado em fix-and-optimize que incorpora a meta-heurística Variable
Neighborhood Search (VNS) para explorar eficientemente o espaço de solução do problema
VNFPC. Terceiro, avalia-se as limitações de desempenho e os custos operacionais de estratégias
típicas de aprovisionamento ambientes reais de NFV. Com base nos resultados empíricos coletados,
propõe-se um modelo analítico que estima com alta precisão os custos operacionais para
requisitos de VNFs arbitrários. Quarto, desenvolve-se um mecanismo para a implantação de
encadeamentos de VNFs no contexto intra-datacenter. O algoritmo proposto (OCM – Operational
Cost Minimization) baseia-se em uma extensão da redução bem conhecida do problema de
emparelhamento ponderado (i.e., weighted perfect matching problem) para o problema de fluxo
de custo mínimo (i.e., min-cost flow problem) e considera o desempenho das VNFs (e.g., requisitos
de CPU), bem como os custos operacionais estimados. Os resultados alcaçados mostram
que o modelo ILP proposto para o problema VNFPC reduz em até 25% nos atrasos fim-a-fim
(em comparação com os encadeamentos observados nas infra-estruturas tradicionais) com um
excesso de provisionamento de recursos aceitável – limitado a 4%. Além disso, os resultados
evidenciam que a heurística proposta (baseada em fix-and-optimize) é capaz de encontrar soluções
factíveis de alta qualidade de forma eficiente, mesmo em cenários com milhares de VNFs.
Além disso, provê-se um melhor entendimento sobre as métricas de desempenho de rede (e.g.,
vazão, consumo de CPU e capacidade de processamento de pacotes) para as estratégias típicas
de implantação de VNFs adotadas infraestruturas NFV. Por último, o algoritmo proposto no
contexto intra-datacenter (i.e. OCM) reduz significativamente os custos operacionais quando
comparado aos mecanismos de posicionamento típicos utilizados em ambientes NFV.",TESE,Scalable Cost-Efficient Placement and Chaining of Virtual Network Functions,6037037,01
"Business Process Model and Notation (BPMN) makes it possible to display information
about the data flow of a process through data and artifacts such as objects, associations
and data stores. However, these elements provide a limited capacity, especially when referring
to the mapping of the data sources (e.g. web service) in a process model. In this
context, this paper proposes an approach to associate data sources to BPMN elements
(e.g. service task). Such approach aims to serve as a starting point to business process
management professionals in order to map, in design time, the data source that is being
used by the process, through the use of BPMN elements. In order to demonstrate the
results, five correlations were evidenced, which are called “definitions proposals”, followed
by their textual description and an example of use. These definitions proposals,
together with the data sources used, were evaluated through a survey. As a result, the
selected data sources were validated by the participants of the survey that demonstrated
knowledge in at least one of the data sources, providing evidence that the selected data
sources are the most appropriated for the work concerned. Also, as results, the definitions
proposals were validated with the participants, obtaining positive results regarding to the
process fragments designed to represent the data sources internal data, file transfer and
shared database. The main contribution of this thesis is to assist the identification of data
sources of a business process, from the set of elements provided by BPMN 2.0.2, where,
previously, such identification was only possible through the process documentation.",,Ciência de Dados e Engenharia de Software,MARCELO BALBINOT,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,18/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'The Business Process Management;BPMN;Process Modeling;Data Sources',LINHA MODELAGEM DE DADOS E DE PROCESSOS DE NEGÓCIOS,LUCINEIA HELOISA THOM,78,Gerenciamento de processos de negócio;BPMN;Modelagem de processos;Fontes de dados.,COMPUTAÇÃO (42001013004P4),-,"A Notação e Modelo de Processo de Negócio (BPMN - Business Process Model and Notation)
possibilita apresentar informações sobre o fluxo de dados de um processo por meio
de dados e artefatos, tais como objetos, associações e repositórios de dados. No entanto,
esses elementos apresentam poder de expressão limitado, principalmente quando se referem
ao mapeamento das fontes de dados (ex.: serviço web) em um modelo de processo.
Neste contexto, o presente trabalho propõe uma abordagem para associar fontes de dados
a elementos de BPMN (ex.: tarefa de serviço). Tal abordagem deve servir como um ponto
de partida para profissionais de gerenciamento de processos de negócio quanto ao mapeamento,
na etapa de modelagem, da fonte de dados que está sendo utilizada pelo processo,
através da utilização de elementos de BPMN. Visando demonstrar os resultados, foram
evidenciadas cinco correlações, que são chamadas de “propostas de definições”, seguidas
por sua descrição textual e um exemplo de uso. Essas propostas de definições, juntamente
com as fontes de dados utilizadas, foram avaliadas por meio de uma pesquisa de
opinião. Como resultado, destaca-se que as fontes de dados selecionadas foram validadas
pelos participantes da pesquisa de opinião, que demonstraram conhecimento em ao
menos uma das fontes de dados, fornecendo indícios que as fontes de dados selecionadas
são as mais apropriadas para o trabalho em questão. Ainda, como resultados, as propostas
de definições foram validadas com os participantes, obtendo resultados positivos
quanto aos fragmentos de processo elaborados para representar as fontes de dados internos,
transferência de arquivos e base de dados compartilhada. A principal contribuição
deste trabalho é auxiliar na identificação de fontes de dados de um processo de negócio,
a partir do conjunto de elementos disponibilizado pela BPMN 2.0.2, onde, anteriormente,
tal identificação era possível apenas através da documentação do processo.",DISSERTAÇÃO,Identificando Fontes de Dados em Modelos de Processos de Negócio com base em Elementos de BPMN,6037196,1
"Campimetry is an important test to detect and monitor central and peripheral ocular dysfunctions, which might indicate the existence of serious conditions such as glaucoma, or the occurrence of strokes or brain tumors. Commercially available campimeters are expensive and lack portability. We present a portable, low-cost, easy-to-manufacture smartphone-based campimeter. We evaluated our prototype in a user-study, which has shown that the results obtained with our prototype are consistent with the ones obtained with the Humphrey Field Analyzer - HFA II-i campimeter, with a Pearson’s correlation coefficient above 0.98 for all sampling positions on the visual field. Moreover, its reproducibility is also comparable to the one of the Humprey campimeter using the SITA Fast algorithm. Given its portability and low cost, our mobile campimeter provides a promising alternative for patient screening in schools and community health centers; for visual evaluation of patients laying in beds or with mobility restrictions; for keeping track of the visual field at home; and for use in communities with limited access to medical services.",,SISTEMAS DE COMPUTACAO,MARCELO DA MATA OLIVEIRA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,22/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Visual field;eyecare;campimetry',"COMPUTAÇÃO GRÁFICA, PROCESSAMENTO DE IMAGENS E INTERAÇÃO",MANUEL MENEZES DE OLIVEIRA NETO,73,Campo visual;saúde visual;campimetria,COMPUTAÇÃO (42001013004P4),-,"A campimetria é um importante exame para detectar e monitorar disfunções na visão periférica,

podendo indicar a existência e monitorar a progressão de doenças sérias como o glaucoma ou

tumor no cérebro, por exemplo. Os dispositivos comerciais são caros e não são portáteis. Esta

dissertação apresenta o projeto e construção de um campímetro portátil, de baixo custo e de

simples confecção. O campímetro desenvolvido utiliza um

smartphone

acoplado aos óculos,

que são semelhantes aos utilizados para aplicações de realidade virtual. O

smartphone

é uti-

lizado para a exibição dos pontos a serem identificados pelo paciente, para o processamento

de testes psicofísicos, e para a geração de relatórios de exames. Para fins de validação, foi

realizado experimento com vinte voluntários utilizando tanto o protótipo desenvolvido como o

campímetro Humphrey, um dos mais utilizados na prática clínica internacionalmente. A aná-

lise dos dados deste experimento demonstra que os resultados obtidos com nosso protótipo são

consistentes com os do campímetro Humphrey, apresentando um coeficiente de correlação de

Pearson superior a 0.98 para todos os pontos do campo visual. Além disso, sua reprodutibi-

lidade também é comparável à do campímetro Humphrey utilizando o algoritmo SITA Fast.

Mesmo utilizando um hardware modesto, os nossos resultados mostram uma boa aproximação

para os exames de campo visual realizados por dispositivos comerciais. Sendo portável e de

baixo custo, nosso dispositivo oferece uma alternativa para triagem de pacientes em escolas e

postos de saúde; para acompanhamento de pacientes acamados ou com restrições motoras; para

acompanhamento domiciliar do campo visual, e para utilização em comunidades remotas ou

com pouco ou nenhum acesso a cuidados médicos.",DISSERTAÇÃO,Desenvolvimento de um Campímetro Portátil e de Baixo Custo Baseado em Smartphone,6076427,01
"Keyframe-based monocular SLAM (Simultaneous Localization and Mapping) is one of the main visual SLAM approaches, used to estimate the camera motion together with the map reconstruction over selected frames. These techniques based on keyframes represent the environment by map points located in the three-dimensional space that can be recognized and located in the frames. However, many of these techniques cannot combine map points corresponding to the same three-dimensional point or detect when a map point becomes outlier and an obsolete information. In this work, we present a robust method to maintain a refined map that uses the covisibility graph and an algorithm based on information fusion to build a probabilistic map, which explicitly models outlier measurements. In addition, we incorporate a pruning mechanism to reduce redundant information and remove outliers. In this way our approach manages the map size maintaining essential information of the environment. Finally, in order to evaluate the performance of our method, we incorporate it into an ORB-SLAM system and measure the accuracy achieved on publicly available benchmark datasets which contain indoor images sequences recorded with a hand-held monocular camera.",,INTELIGENCIA ARTIFICIAL,EDISON KLEIBER TTITO CONCHA,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,07/12/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL,b'Information Fusion;Robot Vision;Visual SLAM;Keyframe-Based;Robotic',INTELIGÊNCIA ARTIFICIAL,EDSON PRESTES E SILVA JUNIOR,67,Information Fusion;Robot Vision;Visual SLAM;Keyframe-Based;Robotic,COMPUTAÇÃO (42001013004P4),-,"SLAM (do inglês Simultaneous Localization and Mapping) Monocular baseado em Keyframes
é uma das principais abordagens de SLAM Visuais, usado para estimar o movimento
da câmera juntamente com a reconstrução do mapa sobre frames selecionados.
Estas técnicas representam o ambiente por pontos no mapa localizados em um espaço
tri-dimensional, que podem ser reconhecidos e localizados no frame. Contudo, estas técnicas
não podem decidir quando um ponto do mapa se torna um outlier ou uma informação
obsoleta e que pode ser descartada, ou combinar pontos do mapa que correspondem
ao mesmo ponto tri-dimensional. Neste trabalho, apresentamos um método robusto para
manter um mapa refinado. Esta abordagem usa o grafo de covisibilidade e um algoritmo
baseado na fusão de informações para construir um mapa probabilístico, que explicitamente
modela medidas de outlier. Além disso, incorporamos um mecanismo de poda
para reduzir informações redundantes e remover outliers. Desta forma, nossa abordagem
gerencia a redução do tamanho do mapa, mantendo informações essenciais do ambiente.
Finalmente, a fim de avaliar a performance do nosso método, ele foi incorporado ao sistema
do ORB-SLAM e foi medido a acurácia alcançada em datasets publicamente disponíveis
que contêm sequências de imagens de ambientes internos gravados com uma
câmera monocular de mão.",DISSERTAÇÃO,Map Point Optimization in Keyframe-Based SLAM using Covisibility Graph and Information Fusion,6094988,01
"Este trabalho apresenta uma investigação acerca do desenvolvimento e
exercício de habilidades do Pensamento Computacional em crianças do Ensino
Fundamental I, a partir do aprendizado de conceitos básicos de programação, por
meio de recursos baseados em Tecnologia Livre e materiais de baixo custo
associados a estratégias pedagógicas alicerçadas na Robótica Educacional e
desenvolvidas especificamente para esse público. Com esse propósito, buscou-se,
por meio de um estudo de caso realizado com sete crianças do terceiro e quarto anos
do Ensino Fundamental, levantar hipóteses acerca da possível relação existente entre
a maturidade cognitiva de crianças nessa faixa etária e o aprendizado de estruturas
básicas de programação, bem como verificar de que forma o aprendizado dessas
estruturas, por meio das estratégias pedagógicas propostas por este trabalho, pode
apoiar o exercício de determinadas habilidades do Pensamento Computacional. Foi
desenvolvido, para a realização do estudo, um kit didático denominado
DuinoBlocks4kids (DB4K), composto por um Ambiente de Programação Visual em
Blocos para plataforma de prototipagem eletrônica Arduino, uma proposta

metodológica, uma série de atividades e um conjunto de materiais de robótica. Pôde-
se observar, como resultado do uso deste kit, a possibilidade do exercício das

seguintes habilidades do Pensamento Computacional: capacidade de abstração,
compreensão de fluxos de controle, depuração e detecção sistemática de erros, uso
da lógica condicional e decomposição de problemas. Foram encontrados também
indícios da existência de uma relação direta entre algumas características cognitivas
de crianças no período operatório concreto (tais como a habilidade de sequenciar
eventos ou ideias, a habilidade de realizar operações mentais a partir de experiências
concretas, dentre outras) e as habilidades necessárias para a realização de
determinadas tarefas relacionadas à programação de computadores.",Rubens Lacerda Queiroz.pdf,CIÊNCIA DA COMPUTAÇÃO,RUBENS LACERDA QUEIROZ,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,30/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'Pensamento Computacional. Ensino de Programao;. Robtica Educacional. Linguagem de Programao Visual. Maturidade Cognitiva e Aprendizado de Programao.',INFORMÁTICA NA EDUCAÇÃO,FABIO FERRENTINI SAMPAIO,243,Pensamento Computacional. Ensino de Programação;Robótica Educacional. Linguagem de Programação Visual. Maturidade Cognitiva e Aprendizado de Programação.,INFORMÁTICA (31001017110P8),-,"Este trabalho apresenta uma investigação acerca do desenvolvimento e
exercício de habilidades do Pensamento Computacional em crianças do Ensino
Fundamental I, a partir do aprendizado de conceitos básicos de programação, por
meio de recursos baseados em Tecnologia Livre e materiais de baixo custo
associados a estratégias pedagógicas alicerçadas na Robótica Educacional e
desenvolvidas especificamente para esse público. Com esse propósito, buscou-se,
por meio de um estudo de caso realizado com sete crianças do terceiro e quarto anos
do Ensino Fundamental, levantar hipóteses acerca da possível relação existente entre
a maturidade cognitiva de crianças nessa faixa etária e o aprendizado de estruturas
básicas de programação, bem como verificar de que forma o aprendizado dessas
estruturas, por meio das estratégias pedagógicas propostas por este trabalho, pode
apoiar o exercício de determinadas habilidades do Pensamento Computacional. Foi
desenvolvido, para a realização do estudo, um kit didático denominado
DuinoBlocks4kids (DB4K), composto por um Ambiente de Programação Visual em
Blocos para plataforma de prototipagem eletrônica Arduino, uma proposta

metodológica, uma série de atividades e um conjunto de materiais de robótica. Pôde-
se observar, como resultado do uso deste kit, a possibilidade do exercício das

seguintes habilidades do Pensamento Computacional: capacidade de abstração,
compreensão de fluxos de controle, depuração e detecção sistemática de erros, uso
da lógica condicional e decomposição de problemas. Foram encontrados também
indícios da existência de uma relação direta entre algumas características cognitivas
de crianças no período operatório concreto (tais como a habilidade de sequenciar
eventos ou ideias, a habilidade de realizar operações mentais a partir de experiências
concretas, dentre outras) e as habilidades necessárias para a realização de
determinadas tarefas relacionadas à programação de computadores.",DISSERTAÇÃO,DUINOBLOCKS4KIDS: utilizando tecnologia livre e materiais de baixo custo para o exercício do Pensamento Computacional no Ensino Fundamental I por meio do aprendizado de programação aliado à Robótica Educacional.,5057196,01
"Esta dissertação descreve uma abordagem sistemática para geração automática de 
casos de teste independentes de plataforma utilizando a ferramenta USE. A proposta 
segue os princípios encontrados nas estratégias lightweight de validação de modelos. A 
principal contribuição desta dissertação está relacionada à criação de métodos baseados 
na hipótese do escopo reduzido utilizando o gerador de snapshots do USE através da 
linguagem ASSL juntamente com os critérios de seleção conhecidos em Teste de Software, 
como Particionamento em Classes de Equivalência e Analise do Valor-Limite. O objetivo é 
possibilitar a geração dos casos de teste e oráculos através da verificação e validação das 
instâncias do modelo conceitual expresso por meio do diagrama de classes da UML 
anotados com restrições OCL. Para isso são adotados critérios de cobertura baseados em 
multiplicidade, atributos e generalização. Um quase-experimento e um exemplo de 
domínio foram utilizados para demonstrar a viabilidade das técnicas e métodos. A partir 
dos resultados obtidos foi possível obter evidências de que a proposta do trabalho é viável 
e eficaz para cobertura  de teste.",,CIÊNCIA DA COMPUTAÇÃO,MARCOS VINICIUS FURRIEL AMORIM DIAS,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,26/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Teste de software;UML, OCL, ASSL, Teste Baseado em Modelos'",SISTEMAS DE INFORMAÇÃO,EBER ASSIS SCHMITZ,79,"Teste de software;UML, OCL, ASSL, Teste Baseado em Modelos",INFORMÁTICA (31001017110P8),GESTÃO ESTRATÉGICA DE TI,"Esta dissertação descreve uma abordagem sistemática para geração automática de 
casos de teste independentes de plataforma utilizando a ferramenta USE. A proposta 
segue os princípios encontrados nas estratégias lightweight de validação de modelos. A 
principal contribuição desta dissertação está relacionada à criação de métodos baseados 
na hipótese do escopo reduzido utilizando o gerador de snapshots do USE através da 
linguagem ASSL juntamente com os critérios de seleção conhecidos em Teste de Software, 
como Particionamento em Classes de Equivalência e Analise do Valor-Limite. O objetivo é 
possibilitar a geração dos casos de teste e oráculos através da verificação e validação das 
instâncias do modelo conceitual expresso por meio do diagrama de classes da UML 
anotados com restrições OCL. Para isso são adotados critérios de cobertura baseados em 
multiplicidade, atributos e generalização. Um quase-experimento e um exemplo de 
domínio foram utilizados para demonstrar a viabilidade das técnicas e métodos. A partir 
dos resultados obtidos foi possível obter evidências de que a proposta do trabalho é viável 
e eficaz para cobertura  de teste.",DISSERTAÇÃO,UMA ABORDAGEM SISTEMÁTICA PARA GERAÇÃO DE CASOS DE TESTE ABSTRATOS UTILIZANDO MODELOS DE CLASSES UML ANOTADOS COM RESTRIÇÕES OCL,5057199,01
"A teoria discreta de Morse, introduzida por Forman em 1995, é uma ferramenta que torna possível estudar funções escalares a partir de construções especíﬁcas em um espaço. Visando estudar a geometria dessas funções, essa teoria busca criar um objeto importante conhecido como campo gradiente discreto. Comumente a teoria faz uso de complexos simpliciais gerais de forma combinatória e estruturas baseadas em grafos, sumarizando o problema na obtenção de um casamento. Isso a torna extremamente atraente para computadores. Mesmo assim, o estudo geométrico procurado pode ser comprometido em alguns casos: seja em virtude da forma de processamento ou seja em virtude de dados inexatos. Este trabalho busca levantar ideias e determinar conjunturas no sentido de tornar o entendimento geométrico de funções escalares ainda mais conﬁável através do gradiente discreto. Isso é feito entendendo melhor os algoritmos de casamento envolvidos na sua construção e adicionando novos resultados que a aproxima do que chamamos de ﬁdelidade geométrica. Dentro desse contexto, outras diretrizes também são propostas. Deﬁnimos um novo casamento geométrico que introduz um tipo de noção de suavização, isto é, que reduz uma espécie de ruído encontrado no próprio gradiente discreto. Além disso, através da alteração de triangulações, procuramos motivar a importância do objeto discreto de entrada na construção de um gradiente discreto geométrico",JOAO LUIZ LAGOAS.pdf,CIÊNCIA DA COMPUTAÇÃO,JOAO LUIZ LAGOAS DE ALMEIDA BERTOLINO,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,09/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Gradiente Discreto;, Teoria Discreta de Morse, Casamento Geomtrico de Morse, Cancelamento, Triangulao, Casamento Guloso.'",ALGORITMOS E MÉTODOS NUMÉRICOS,JOAO ANTONIO RECIO DA PAIXAO,103,"Gradiente Discreto,;Teoria Discreta de Morse, Casamento Geométrico de Morse, Cancelamento, Triangulação, Casamento Guloso.",INFORMÁTICA (31001017110P8),-,"A teoria discreta de Morse, introduzida por Forman em 1995, é uma ferramenta que torna possível estudar funções escalares a partir de construções especíﬁcas em um espaço. Visando estudar a geometria dessas funções, essa teoria busca criar um objeto importante conhecido como campo gradiente discreto. Comumente a teoria faz uso de complexos simpliciais gerais de forma combinatória e estruturas baseadas em grafos, sumarizando o problema na obtenção de um casamento. Isso a torna extremamente atraente para computadores. Mesmo assim, o estudo geométrico procurado pode ser comprometido em alguns casos: seja em virtude da forma de processamento ou seja em virtude de dados inexatos. Este trabalho busca levantar ideias e determinar conjunturas no sentido de tornar o entendimento geométrico de funções escalares ainda mais conﬁável através do gradiente discreto. Isso é feito entendendo melhor os algoritmos de casamento envolvidos na sua construção e adicionando novos resultados que a aproxima do que chamamos de ﬁdelidade geométrica. Dentro desse contexto, outras diretrizes também são propostas. Deﬁnimos um novo casamento geométrico que introduz um tipo de noção de suavização, isto é, que reduz uma espécie de ruído encontrado no próprio gradiente discreto. Além disso, através da alteração de triangulações, procuramos motivar a importância do objeto discreto de entrada na construção de um gradiente discreto geométrico",DISSERTAÇÃO,ALGORITMOS E TRIANGULAÇÕES PARA O GRADIENTE DISCRETO,5080371,01
"O crescimento da produção bibliográfica em todas as áreas da ciência é evidente, porém, a organização de suas informações ainda é um desafio, ainda mais quando pensamos em uma organização padronizada e estruturada. Vemos em trabalhos relacionados que discutir e propor atualizações em organizações da informação é um processo demorado e custoso, onde é necessária uma reunião e discussão de especialistas.  Diante disso, o trabalho parte de uma proposta para melhorar a organização da informação, neste caso uma taxonomia, utilizando a análise de redes sociais para auxiliar na gestão e atualização do conhecimento. Iniciamos com uma hipótese de que “A utilização da análise de redes sociais pode auxiliar na melhoria de uma organização da informação? ”. Sendo assim, desenvolvemos uma metodologia que vai desde a escolha das publicações, definição de uma taxonomia de referência, aplicação de métricas de análise de redes sociais até a recomendação de uma nova taxonomia. Avaliamos o método quantitativamente e qualitativamente, onde obtivemos resultados consistentes e conseguimos recomendar novas formas de organização da informação. Os resultados foram avaliados por especialistas, que, de uma maneira geral, aprovaram os resultados e consideraram que a metodologia pode auxiliar sim em uma atualização de uma organização da informação, com isso, consideramos que a metodologia atinge o objetivo e confirma nossa hipótese. Por fim, ressaltamos que apesar da metodologia trazer bons resultados, a participação de especialistas ainda é imprescindível para finalmente atualizar uma organização da informação.",,CIÊNCIA DA COMPUTAÇÃO,RAFAEL LOUREIRO SERPA DE MORAES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,22/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Analise de Redes Sociais;Organizao da Informao, Taxonomia, reas de Conhecimento, Gesto do Conhecimento.'",SISTEMAS DE INFORMAÇÃO,JONICE DE OLIVEIRA SAMPAIO,125,"Analise de Redes Sociais;, Organização da Informação, Taxonomia, Áreas de Conhecimento, Gestão do Conhecimento.",INFORMÁTICA (31001017110P8),-,"O crescimento da produção bibliográfica em todas as áreas da ciência é evidente, porém, a organização de suas informações ainda é um desafio, ainda mais quando pensamos em uma organização padronizada e estruturada. Vemos em trabalhos relacionados que discutir e propor atualizações em organizações da informação é um processo demorado e custoso, onde é necessária uma reunião e discussão de especialistas.  Diante disso, o trabalho parte de uma proposta para melhorar a organização da informação, neste caso uma taxonomia, utilizando a análise de redes sociais para auxiliar na gestão e atualização do conhecimento. Iniciamos com uma hipótese de que “A utilização da análise de redes sociais pode auxiliar na melhoria de uma organização da informação? ”. Sendo assim, desenvolvemos uma metodologia que vai desde a escolha das publicações, definição de uma taxonomia de referência, aplicação de métricas de análise de redes sociais até a recomendação de uma nova taxonomia. Avaliamos o método quantitativamente e qualitativamente, onde obtivemos resultados consistentes e conseguimos recomendar novas formas de organização da informação. Os resultados foram avaliados por especialistas, que, de uma maneira geral, aprovaram os resultados e consideraram que a metodologia pode auxiliar sim em uma atualização de uma organização da informação, com isso, consideramos que a metodologia atinge o objetivo e confirma nossa hipótese. Por fim, ressaltamos que apesar da metodologia trazer bons resultados, a participação de especialistas ainda é imprescindível para finalmente atualizar uma organização da informação.",DISSERTAÇÃO,UM MÉTODO DE INDEXAÇÃO E ORGANIZAÇÃO DE INFORMAÇÕES BASEADO EM SOCIOGRAMA,5166485,01
"The present work is based on a mixture theory of poroelastic media which in contrast to Biot’s theory, no relative acceleration is introduced as an interactive force in the equations of motion to account for the effect of added mass. The propagation of plane harmonic waves in homogeneously deformed region are considered. For different poroelastic models, both P1 and P2 waves are obtained except the model with both incompressible solid and fluid constituents, for which only one P wave exists. Phase velocities and attenuation coefficients from dispersion relations are numerically determined in order to show graphically their qualitative behaviors on the variation of the frequency and the porosity.",,CIÊNCIA DA COMPUTAÇÃO,ROMULO BRITO DA SILVA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,17/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'continuum mechanics;mixture theory;porous media;wave propagation',ALGORITMOS E MÉTODOS NUMÉRICOS,I SHIH LIU,141,Mecânica do contínuo;teoria de misturas;meio poroso;propagação de onda,INFORMÁTICA (31001017110P8),ANÁLISE E SIMULAÇÃO NUMÉRICA DA TECTÔNICA DO SAL -- GRANDE DEFORMAÇÃO E VARIAÇÃO TÉRMICA,"O presente trabalho baseia-se em uma teoria de mistura de meios poro-elásticos que, em contraste com a teoria de Biot, nenhuma aceleração relativa é introduzida como uma força interativa nas equações de movimento para explicar o efeito da 
massa adicional. A propagação de ondas harmônicas planas em região homogeneamente deformada é considerada. Para diferentes modelos poro-elásticos, tanto as ondas P1 quanto P2 são obtidas, exceto para o modelo com componentes sólidos e fluidos incompressíveis, para os quais existe apenas uma onda P. As velocidades de fase e os coeficientes de atenuação das relações de dispersão são determinados numericamente para mostrar graficamente seus comportamentos qualitativos sob efeito da variação da frequência e da porosidade.",TESE,PROPAGAÇÃO DE ONDAS EM UMA TEORIA DE MEIOS PORO-ELÁSTICOS,5167582,1
"Este trabalho descreve um framework com um processo computacional para auxiliar intérpretes durante a atividade de interpretação sísmica, detectando bright spots como possíveis indicadores de hidrocarbonetos em dados sísmicos. Esse conjunto de dados compreende um conjunto de atributos sísmicos, uma vez que  tais dados podem ser vastos, a interpretação sísmica manual pode ser difícil ou mesmo inviável, assim nosso framework surge para tratar esse problema. Nossa proposta usa a teoria de Rough Set e uma infra-estrutura de processamento massivamente paralelo. Através de Rough Set, podemos tentar reduzir o conjunto de atributos sísmicos e criamos um conjunto de regras de classificação para detectar bright spots. Para gerar as regras de classificação, executamos um processo de treinamento supervisionado via Rough Set na implementação do nosso framework, incorporando conhecimento de especialista, o que aumenta a acurácia de nossos resultados. Em relação ao desempenho, nossa proposta pode ser mais rápida do que outra proposta, uma vez que distribuímos adequadamente os dados sísmicos em um banco de dados de processamento massivamente paralelo. Outra vantagem que nosso framework tem sobre outra abordagem é: nossas regras de classificação podem ser genéricas para qualquer banco de dados sísmico. Assim, a geração de regras ocorre apenas uma vez, que é na implementação do framework. Uma vez que mantemos o mesmo conjunto dessas regras, para garantir flexibilidade ao nosso framework de suporte à interpretação sísmica, fornecemos ao usuário um conjunto de parâmetros de entrada, que permite ajustar a consulta para detectar bright spots. Por fim, experimentos preliminares, realizados com dados sísmicos reais da costa holandesa, demonstram que nosso framework é capaz de identificar bright spots relacionados à acumulação de hidrocarbonetos.",,CIÊNCIA DA COMPUTAÇÃO,WAGNER DOS SANTOS VIEIRA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,31/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'Anlise de Dados;Petrleo & Gs;Interpretao Ssmica;Processamento Paralelo;Rough Set;Processo de Suporte',SISTEMAS DE INFORMAÇÃO,PAULO DE FIGUEIREDO PIRES,66,Análise de Dados;Petróleo & Gás;Interpretação Sísmica;Processamento Paralelo;Rough Set;Processo de Suporte,INFORMÁTICA (31001017110P8),-,"Este trabalho descreve um framework com um processo computacional para auxiliar intérpretes durante a atividade de interpretação sísmica, detectando bright spots como possíveis indicadores de hidrocarbonetos em dados sísmicos. Esse conjunto de dados compreende um conjunto de atributos sísmicos, uma vez que  tais dados podem ser vastos, a interpretação sísmica manual pode ser difícil ou mesmo inviável, assim nosso framework surge para tratar esse problema. Nossa proposta usa a teoria de Rough Set e uma infra-estrutura de processamento massivamente paralelo. Através de Rough Set, podemos tentar reduzir o conjunto de atributos sísmicos e criamos um conjunto de regras de classificação para detectar bright spots. Para gerar as regras de classificação, executamos um processo de treinamento supervisionado via Rough Set na implementação do nosso framework, incorporando conhecimento de especialista, o que aumenta a acurácia de nossos resultados. Em relação ao desempenho, nossa proposta pode ser mais rápida do que outra proposta, uma vez que distribuímos adequadamente os dados sísmicos em um banco de dados de processamento massivamente paralelo. Outra vantagem que nosso framework tem sobre outra abordagem é: nossas regras de classificação podem ser genéricas para qualquer banco de dados sísmico. Assim, a geração de regras ocorre apenas uma vez, que é na implementação do framework. Uma vez que mantemos o mesmo conjunto dessas regras, para garantir flexibilidade ao nosso framework de suporte à interpretação sísmica, fornecemos ao usuário um conjunto de parâmetros de entrada, que permite ajustar a consulta para detectar bright spots. Por fim, experimentos preliminares, realizados com dados sísmicos reais da costa holandesa, demonstram que nosso framework é capaz de identificar bright spots relacionados à acumulação de hidrocarbonetos.",DISSERTAÇÃO,"A Semi-Automatic Framework to Identify Bright Spots as Potential Hydrocarbon Indicators in Seismic Data, using an MPPDB Infrastructure and Rough Set Theory",5204043,01
"Revisões Sistemáticas de Literatura têm sido frequentemente realizadas, mas
poucas vezes atualizadas. Acredita-se que a limitação de tempo e de orçamento
financeiro em projetos financiados seja um dos motivos para a falta de atualização
das revisões. Este estudo propõe um método automatizável, que consiste em
encontrar novos documentos, recomendar a ordem em que eles devem ser lidos e
sugerir novos termos de busca ao revisor, a fim de reduzir o tempo utilizado durante
a atualização de uma RSL. Como resultado, o método se mostrou eficaz ao priorizar
documentos para a pesquisa, permitindo com que o revisor leia mais documentos
relevantes do que irrelevantes, assegurando que haja um menor desperdício de
tempo ao ler documentos que não são úteis para a atualização da Revisão
Sistemática de Literatura.",,CIÊNCIA DA COMPUTAÇÃO,RAMON LEONCIO REGIS,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,31/05/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Reviso Sistemtica;Recuperao da Informao, Sistemas de Recomendao, Palavras-chave, Strings de Busca.'",SISTEMAS DE INFORMAÇÃO,EBER ASSIS SCHMITZ,48,"Revisão Sistemática;Recuperação da Informação, Sistemas de Recomendação, Palavras-chave, Strings de Busca.",INFORMÁTICA (31001017110P8),GESTÃO ESTRATÉGICA DE TI,"Revisões Sistemáticas de Literatura têm sido frequentemente realizadas, mas
poucas vezes atualizadas. Acredita-se que a limitação de tempo e de orçamento
financeiro em projetos financiados seja um dos motivos para a falta de atualização
das revisões. Este estudo propõe um método automatizável, que consiste em
encontrar novos documentos, recomendar a ordem em que eles devem ser lidos e
sugerir novos termos de busca ao revisor, a fim de reduzir o tempo utilizado durante
a atualização de uma RSL. Como resultado, o método se mostrou eficaz ao priorizar
documentos para a pesquisa, permitindo com que o revisor leia mais documentos
relevantes do que irrelevantes, assegurando que haja um menor desperdício de
tempo ao ler documentos que não são úteis para a atualização da Revisão
Sistemática de Literatura.",DISSERTAÇÃO,Suporte Computacional à Atualização de Revisões Sistemáticas,5204341,01
"O presente trabalho analisa o desempenho da rede neural sem peso WiSARD aplicada às ações mais representativas negociadas na bolsa brasileira, BM&FBovespa, em três etapas. Na primeira etapa, a WiSARD é empregada na tarefa de previsão das tendências dos preços das ações baseada em uma estratégia que utiliza médias móveis. Na segunda etapa,épropostoumagentenegociador(AN1)capazdeoperarnomercadodetransações diárias. Nesse agente, as melhores políticas de compra e venda são modeladas por meio de um processo de decisão de Markov (MDP) via programação dinâmica. De maneira a auxiliar a tomada de decisão, adota-se a WiSARD com o objetivo de previsão das tendências das ações com base na mesma estratégia estudada na primeira etapa. Na terceira etapa, um novo agente negociador (AN2) capaz de operar no mercado de alta frequência é apresentado. Este agente compõe-se, assim como o AN1, dos módulos de previsão e decisão. No módulo de previsão, é utilizado o modelo de regressão ridge com o objetivo de prever os valores futuros dos preços dos papéis transacionados. No módulo de decisão, a WiSARD é aplicada com o objetivo de aprender as regras estáticas do índice deforçarelativa(RSI)demaneiraaidentiﬁcaroportunidadesdesubmissõesdeordensde compra e venda neste mercado. Para a avaliação da primeira etapa, a acurácia média na previsão das tendências dos papéis em estudo e do tempo de processamento da WiSARD são comparadas aos desempenhos dos classiﬁcadores SVM e KNN. Nos resultados, a WiSARD apresentou boa acurácia e o menor tempo de processamento dentre todos os classiﬁcadores, sendo este um requisito para classiﬁcação em mercados de frequências mais altas. Os resultados experimentais mostraram que utilizar a estratégia do MDP permite que o agente tenha ganho médio maior e menor volatilidade se comparados aos lucros obtidos por uma estratégia simples. Na terceira etapa, os experimentos em um conjunto de dados reais do mercado de alta frequência revelaram ser possível codiﬁcar as regras do indicador RSI na WiSARD de forma compatível com o tempo de reação exigido pelas atualizações neste tipo de mercado.",,CIÊNCIA DA COMPUTAÇÃO,SAMARA ALVAREZ ALVES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,20/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'WiSARD. negociao de alta frequncia;. ndice de fora relativa. regresso ridge. processo de deciso de Markov.',MODELOS E ARQUITETURAS PARA SISTEMAS INTELIGENTES,PRISCILA MACHADO VIEIRA LIMA,81,WiSARD. negociação de alta frequência;índice de força relativa. regressão ridge. processo de decisão de Markov.,INFORMÁTICA (31001017110P8),-,"O presente trabalho analisa o desempenho da rede neural sem peso WiSARD aplicada às ações mais representativas negociadas na bolsa brasileira, BM&FBovespa, em três etapas. Na primeira etapa, a WiSARD é empregada na tarefa de previsão das tendências dos preços das ações baseada em uma estratégia que utiliza médias móveis. Na segunda etapa,épropostoumagentenegociador(AN1)capazdeoperarnomercadodetransações diárias. Nesse agente, as melhores políticas de compra e venda são modeladas por meio de um processo de decisão de Markov (MDP) via programação dinâmica. De maneira a auxiliar a tomada de decisão, adota-se a WiSARD com o objetivo de previsão das tendências das ações com base na mesma estratégia estudada na primeira etapa. Na terceira etapa, um novo agente negociador (AN2) capaz de operar no mercado de alta frequência é apresentado. Este agente compõe-se, assim como o AN1, dos módulos de previsão e decisão. No módulo de previsão, é utilizado o modelo de regressão ridge com o objetivo de prever os valores futuros dos preços dos papéis transacionados. No módulo de decisão, a WiSARD é aplicada com o objetivo de aprender as regras estáticas do índice deforçarelativa(RSI)demaneiraaidentiﬁcaroportunidadesdesubmissõesdeordensde compra e venda neste mercado. Para a avaliação da primeira etapa, a acurácia média na previsão das tendências dos papéis em estudo e do tempo de processamento da WiSARD são comparadas aos desempenhos dos classiﬁcadores SVM e KNN. Nos resultados, a WiSARD apresentou boa acurácia e o menor tempo de processamento dentre todos os classiﬁcadores, sendo este um requisito para classiﬁcação em mercados de frequências mais altas. Os resultados experimentais mostraram que utilizar a estratégia do MDP permite que o agente tenha ganho médio maior e menor volatilidade se comparados aos lucros obtidos por uma estratégia simples. Na terceira etapa, os experimentos em um conjunto de dados reais do mercado de alta frequência revelaram ser possível codiﬁcar as regras do indicador RSI na WiSARD de forma compatível com o tempo de reação exigido pelas atualizações neste tipo de mercado.",DISSERTAÇÃO,Negociação no mercado financeiro utilizando a rede neural sem peso WiSARD,5204346,01
"No presente trabalho apresenta-se um esquema de veriﬁcação de integridade de software para sistemas embarcados. A veriﬁcação de integridade é uma ferramenta de auditoria que permite a uma entidade Veriﬁcador checar se o software em execução em outra plataforma Disv não foi modiﬁcado de forma não autorizada. É útil para vários cenários domundoreal,emquetaismodiﬁcaçõesadispositivosembarcadospodemtrazerdiversos prejuízos ﬁnanceiros, como em smart grids, e até mesmo riscos à vida, como no caso de software embarcados em automóveis por exemplo.
O esquema proposto visa a facilidade de implementação e é baseado em software. Por ser baseado em software, elimina a necessidade de hardware adicional à plataforma, o que é uma característica interessante para cenários de baixo custo, e também permite a aplicaçãoemsistemaslegados.Ométodoaproveita-sedeumacaracterísticadaplataforma de execução, especiﬁcamente, seus ciclos de clock de forma a compor o resultado da veriﬁcação de integridade.
Por ﬁm, apresenta-se uma implementação de validação do esquema proposto. Ambas as entidades, Veriﬁcador e Disv, são implementados em plataformas amplamente disponíveis. Várias consultas de veriﬁcação de integridade foram obtidas através da interação entre as duas entidades. Os resultados obtidos apontam que o método proposto pode ser utilizado com sucesso em casos reais, já que obteve bons resultados na diferenciação entre um Disv íntegro e um Disv malicioso.",,CIÊNCIA DA COMPUTAÇÃO,CRISTIANO GURGEL DE CASTRO,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,28/06/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'vericaodeintegridade;.atestaodesoftware.seguranaemsistemas embarcados.',REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,LUIZ FERNANDO RUST DA COSTA CARMO,84,veriﬁcaçãodeintegridade;.atestaçãodesoftware.segurançaemsistemas embarcados.,INFORMÁTICA (31001017110P8),-,"No presente trabalho apresenta-se um esquema de veriﬁcação de integridade de software para sistemas embarcados. A veriﬁcação de integridade é uma ferramenta de auditoria que permite a uma entidade Veriﬁcador checar se o software em execução em outra plataforma Disv não foi modiﬁcado de forma não autorizada. É útil para vários cenários domundoreal,emquetaismodiﬁcaçõesadispositivosembarcadospodemtrazerdiversos prejuízos ﬁnanceiros, como em smart grids, e até mesmo riscos à vida, como no caso de software embarcados em automóveis por exemplo.
O esquema proposto visa a facilidade de implementação e é baseado em software. Por ser baseado em software, elimina a necessidade de hardware adicional à plataforma, o que é uma característica interessante para cenários de baixo custo, e também permite a aplicaçãoemsistemaslegados.Ométodoaproveita-sedeumacaracterísticadaplataforma de execução, especiﬁcamente, seus ciclos de clock de forma a compor o resultado da veriﬁcação de integridade.
Por ﬁm, apresenta-se uma implementação de validação do esquema proposto. Ambas as entidades, Veriﬁcador e Disv, são implementados em plataformas amplamente disponíveis. Várias consultas de veriﬁcação de integridade foram obtidas através da interação entre as duas entidades. Os resultados obtidos apontam que o método proposto pode ser utilizado com sucesso em casos reais, já que obteve bons resultados na diferenciação entre um Disv íntegro e um Disv malicioso.",DISSERTAÇÃO,EVINCED: ESQUEMA DE VERIFICAÇÃO DE INTEGRIDADE PARA SISTEMAS DE MEDIÇÃO BASEADOS EM CONTAGEM DE CICLOS E TEMPO,5204462,01
"O presente trabalho caracteriza os tempos de conﬁrmação das transações do Bitcoin, modela a dinâmica de mineração visando analisar os tempos de conﬁrmações, considerando fatores intrínsecos da rede Bitcoin tais como tempos de atividade de blocos, tempos de chegadas de transações nos mineradores através da teoria de ﬁlas. Considerandos as caracterizações, foi proposto um modelo para previsão de conﬁrmação das transações baseadas numa árvore de decisão e rede neural sem peso WiSARD. Além disso, veriﬁcamos dois modelos para detecção de transações anômalas tanto para o Bitcoin quanto para o Ethereum. Tais modelos são baseados em misturas gaussianas.",SAULO RICCI.pdf,CIÊNCIA DA COMPUTAÇÃO,SAULO MARQUES RIBEIRO RICCI,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,17/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Bitcoi;n, Ethereum, Aprendizado de Mquina, Caracterizao.'",REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,DANIEL SADOC MENASCHE,60,"Bitcoin, Ethereum;, Aprendizado de Máquina, Caracterização.",INFORMÁTICA (31001017110P8),-,"O presente trabalho caracteriza os tempos de conﬁrmação das transações do Bitcoin, modela a dinâmica de mineração visando analisar os tempos de conﬁrmações, considerando fatores intrínsecos da rede Bitcoin tais como tempos de atividade de blocos, tempos de chegadas de transações nos mineradores através da teoria de ﬁlas. Considerandos as caracterizações, foi proposto um modelo para previsão de conﬁrmação das transações baseadas numa árvore de decisão e rede neural sem peso WiSARD. Além disso, veriﬁcamos dois modelos para detecção de transações anômalas tanto para o Bitcoin quanto para o Ethereum. Tais modelos são baseados em misturas gaussianas.",DISSERTAÇÃO,CARACTERIZAÇÕES QUANTITATIVAS DE REDES DESCENTRALIZADAS BASEADAS NO BLOCKCHAIN,5204471,01
"Nowadays mobile devices, such as smartphones and tablets, have become vital in our
lives. Consequentially they enabled alternative solutions to communication problems.
However, as the number of mobile devices grow, also grows the amount of exchanged data.
This amount of data is a problem to Internet Service Providers. A way to find a solution
to this problem is to use the mobile devices wireless network capabilities to exchange
data forming mobile P2P networks. These networks should opportunistically collaborate
to exchange information to other devices in their proximity, only requiring users to
specify their interests. One solution that uses this concept is MEK, an acronym that
stands for Mobile Exchange of Knowledge. MEK uses mobile devices to opportunistically
disseminate knowledge among its users, according to the human pattern of movement
and encounter. The solution aims to increase the exchanges of knowledge, while forming
networks composed of users sharing the same interests. To exchange knowledge MEK
compares profiles by matching interests and keywords, assigned by users to their content.
In this approach any match will trigger exchanges, even for users who share at least
one interest. If the network grows, the number of connections among profiles with
little similarity will increase, also increasing the number of exchanges, which in turn
overloads the devices and the networks themselves. In this work we present an extension
of the solution named DMEK, acronym for Decision Mobile Exchange of Knowledge, a
solution where knowledge is exchanged using a decision mechanism to match user profiles
according to their similarities. To develop the decision mechanism we use the theory
of evidence named Dempster-Shafer, in order to combine information from profiles and
determine their similarity. To verify DMEK feasibility and performance, we conducted
three experiments in three different scenarios by means of a computational simulation.
We compared our approach to the traditional profile matching of MEK and an euclidean
distance based approach. Experiments show that our approach was able to keep a
reasonable precision performance of approximately 90% while keeping execution times
that ranged from 1.5 to 4.0 seconds. It successfully lowered the number of exchanges
by 50 to over a 100 if compared to the traditional approach, while maintaining recall
performance by maintaining it at approximately 40% to 60% percent. Considering these
results, we present our conclusions.",JOSÉ GUILHERME MAYWORM.pdf,-,JOSE GUILHERME MAYWORM,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,05/09/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'opportunistic, collaboration, exchange, knowledge, profile, similarity, mobile devices, P2P'",-,JONICE DE OLIVEIRA SAMPAIO,81,"colaboração, oportunística, troca, conhecimento, perfil, similaridade, dispositivos móveis, P2P",INFORMÁTICA (31001017110P8),-,"Dispositivos móveis, tais como smartphones e tablets, se tornaram uma parte importante
de nossas vidas, nos possibilitando a criação de soluções alternativas para problemas
de comunicação. No entanto, à medida que o número de dispositivos móveis cresce,
também aumenta a quantidade de dados trocados. Esse aumento é um problema para os
provedores de Internet. Uma solução é usar os recursos de rede sem fio dos dispositivos
móveis para trocar dados, formando redes P2P móveis. Tais redes podem colaborar
oportunisticamente para trocar informações entre dispositivos fisicamente próximos,
exigindo apenas que os usuários especifiquem seus interesses. Uma solução que usa esse
conceito é o MEK, acrônimo para Mobile Exchange of Knowledge, aplicação que usa
dispositivos móveis para disseminar oportuniscamente o conhecimento entre seus usuários,
de acordo com o padrão de movimento e encontro dos seres humanos. A aplicação visa
aumentar as trocas de conhecimento, ao formar redes de pessoas que compartilham os

mesmos interesses. Para trocar conhecimento, o MEK compara os interesses e palavras-
chave, atribuídos pelos seus usuários ao conteúdo. No entanto, qualquer correspondência

desencadeia trocasv mesmo para usuários que compartilhem somente um interesse. Se a
rede cresce, o número de conexões entre perfis com pouca semelhança também aumenta,
aumentando também o número de trocas, o que pode sobrecarregar os diapositivos e

consequentemente a rede. Neste trabalho, apresentamos uma extensão do MEK denomi-
nada DMEK, acrônimo para Decision Mobile Exchange of Knowledge, uma solução onde

o conhecimento é trocado usando um mecanismo de decisão para obter correspondência
dos perfis de usuários de acordo com sua similaridade. Para desenvolver o mecanismo,
usamos a teoria de evidência de Dempster-Shafer para combinar informações dos perfis
e determinar sua similaridade. Para verificar a viabilidade e o desempenho da nossa
proposta, realizamos três experimentos em três cenários por meio de uma simulação
computacional. Comparamos nossa abordagem com a abordagem tradicional do MEK e
uma abordagem baseada em distância euclidiana. Os experimentos mostram que nossa
DMEK foi capaz de manter uma precisão razoável, de aproximadamente 90%, mantendo
tempos de execução de 1.5 a 4.0 segundos. Nossa abordagem diminuiu com êxito o número
de trocas em 50 a até 100 vezes se comparado com a abordagem tradicional, enquanto
que a revocação(recall) se manteve em aproximadamente 40% a 60 %. Considerando
esses resultados, apresentamos nossa conclusões.",DISSERTAÇÃO,Mobile Exchange of Knowledge: Plataforma de Gerenciamento de Colaborações Oportunísticas,5204510,01
"Uma notícia tendenciosa é, às vezes, bem suave para o interlocutor, e alcança seu objetivo de inuenciar a opinião do leitor no mesmo sentido. Nos dias atuais, devido a quantidade de informações existentes, muitas pessoas sentem diculdades em avaliar a ideia principal do conteúdo de uma notícia ou se existe alguma tendência, no caso deste trabalho, política. Nesta dissertação, buscamos a identicação de polaridade em notícias políticas emportuguêsatravésdoprocessodemineraçãodedadostextuaiscomautilizaçãoda Rede Neural sem Peso WiSARD e de uma derivação, a ClusWiSARD. O WiSARD funciona através de uma estrutura de discriminadores, onde cada discriminador é responsável por identicar uma classe. Realizamos avaliações relacionadas ao corpo da notícia e à manchete da notícia e realizamos uma avaliação de um veículo de mídia amplamente conhecido. Obtivemos acurácia de cerca de 90% ao utilizar o corpo da notícia completo e acurácia de cerca de 75% ao considerar apenas manchetes. Além disso, também fazemos uma análise temporal sobre a dinâmica política das tendências.",RAFAEL DUTRA CAVALCANTI.pdf,CIÊNCIA DA COMPUTAÇÃO,RAFAEL DUTRA CAVALCANTI,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,06/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Minerao de Textos;, Redes Neurais sem Peso, Descoberta de Conhecimento em Dados no Estruturados.'",REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,DANIEL SADOC MENASCHE,81,"Mineração de Textos;, Redes Neurais sem Peso, Descoberta de Conhecimento em Dados não Estruturados.",INFORMÁTICA (31001017110P8),-,"Uma notícia tendenciosa é, às vezes, bem suave para o interlocutor, e alcança seu objetivo de inuenciar a opinião do leitor no mesmo sentido. Nos dias atuais, devido a quantidade de informações existentes, muitas pessoas sentem diculdades em avaliar a ideia principal do conteúdo de uma notícia ou se existe alguma tendência, no caso deste trabalho, política. Nesta dissertação, buscamos a identicação de polaridade em notícias políticas emportuguêsatravésdoprocessodemineraçãodedadostextuaiscomautilizaçãoda Rede Neural sem Peso WiSARD e de uma derivação, a ClusWiSARD. O WiSARD funciona através de uma estrutura de discriminadores, onde cada discriminador é responsável por identicar uma classe. Realizamos avaliações relacionadas ao corpo da notícia e à manchete da notícia e realizamos uma avaliação de um veículo de mídia amplamente conhecido. Obtivemos acurácia de cerca de 90% ao utilizar o corpo da notícia completo e acurácia de cerca de 75% ao considerar apenas manchetes. Além disso, também fazemos uma análise temporal sobre a dinâmica política das tendências.",DISSERTAÇÃO,AUMENTANDO A TRANSPARÊNCIA DE NOTÍCIAS VIA MINERAÇÃO DE TEXTO E REDES NEURAIS SEM PESO,5204556,01
"Devido a deficiências na legislação penal, que está fora do âmbito da Metrologia Legal, o Inmetro vem, nos últimos anos, aumentando o nível de exigência dos requisitos de segurança de software e hardware a serem atendidos pelos instrumentos de medição, de forma que o Brasil está se destacando no cenário mundial nesse aspecto. Assim, uma nova geração de instrumentos de medição está surgindo, portando diversas funcionalidades criptográficas. Vários problemas ou dificuldades já foram resolvidos. Alguns ainda estão em processo de estudo. Este trabalho trata especificamente de estudar um problema relatado por fabricantes de instrumentos de medição. Supondo-se que foi detectada uma falha grave numa versão de software e vários instrumentos já foram vendidos e estão em utilização em diversos clientes. Não é possível fazer um recall de todos os instrumentos para a fábrica e, a partir daí, atualizar os instrumentos. No caso real, o fabricante determina a uma equipe de técnicos para realizar a atualização \emph{in loco}. No final desse processo, a garantia de que todos os instrumentos foram atualizados repousa na alegação dos técnicos. O problema consiste em garantir que uma quantidade qualquer de instrumentos de medição teve seu software atualizado. Várias soluções foram pensadas, mas, para muitas, foram detectadas fragilidades. No final, sobraram três soluções sendo que uma delas se destacou por ser mais robusta, mas também matematicamente mais complexa. A ponto de se extrair duas conjecturas matemáticas interessantes.",,CIÊNCIA DA COMPUTAÇÃO,ANTONIO LACERDA JUNIOR,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,21/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Metrologia Legal;medidores inteligentes, atualizao de firmware, recibos eletrnicos, encriptao homomrfica'",REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,LUIZ FERNANDO RUST DA COSTA CARMO,124,"Metrologia Legal;medidores inteligentes, atualização de firmware, recibos eletrônicos, encriptação homomórfica",INFORMÁTICA (31001017110P8),-,"Devido a deficiências na legislação penal, que está fora do âmbito da Metrologia Legal, o Inmetro vem, nos últimos anos, aumentando o nível de exigência dos requisitos de segurança de software e hardware a serem atendidos pelos instrumentos de medição, de forma que o Brasil está se destacando no cenário mundial nesse aspecto. Assim, uma nova geração de instrumentos de medição está surgindo, portando diversas funcionalidades criptográficas. Vários problemas ou dificuldades já foram resolvidos. Alguns ainda estão em processo de estudo. Este trabalho trata especificamente de estudar um problema relatado por fabricantes de instrumentos de medição. Supondo-se que foi detectada uma falha grave numa versão de software e vários instrumentos já foram vendidos e estão em utilização em diversos clientes. Não é possível fazer um recall de todos os instrumentos para a fábrica e, a partir daí, atualizar os instrumentos. No caso real, o fabricante determina a uma equipe de técnicos para realizar a atualização \emph{in loco}. No final desse processo, a garantia de que todos os instrumentos foram atualizados repousa na alegação dos técnicos. O problema consiste em garantir que uma quantidade qualquer de instrumentos de medição teve seu software atualizado. Várias soluções foram pensadas, mas, para muitas, foram detectadas fragilidades. No final, sobraram três soluções sendo que uma delas se destacou por ser mais robusta, mas também matematicamente mais complexa. A ponto de se extrair duas conjecturas matemáticas interessantes.",DISSERTAÇÃO,Garantia de atualização massiva de firmware de medidores inteligentes através de um recibo agregador,5208041,01
"O tempo de pior caso para inserção de uma chave no esquema clássico de Hashing do Cuco
(Cuckoo Hashing) é uma variável aleatória que pode assumir valores arbitrariamente
altos, dada a probabilidade estritamente positiva de se precisar fazer uma sequência
interminável de realocações.
Propomos uma variante em que o tempo de pior caso da inserção é controlado. Para
isso, aplicamos duas ideias: a primeira é o emprego de um método de Hashing Perfeito
sempre que for necessária uma realocação, fazendo com que no máximo uma realocação
seja necessária por inserção; a segunda ideia é fazer com que o número de tabelas deixe
de ser constante, como de praxe, e passe a crescer em função do número de chaves, para
que a complexidade da realocação seja baixa. O preço pago é piorar a complexidade de
pior caso da operação de busca, que deixa de ser constante. O objetivo, no entanto, é
equilibrar as complexidades de pior caso da inserção e da busca.",,CIÊNCIA DA COMPUTAÇÃO,JUDISMAR ARPINI JUNIOR,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,01/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Hashing do Cuco;, Hashing Perfeito, Tempo de Execuo'",ALGORITMOS E MÉTODOS NUMÉRICOS,VINICIUS GUSMAO PEREIRA DE SA,57,"Hashing do Cuco;, Hashing Perfeito, Tempo de Execução",INFORMÁTICA (31001017110P8),-,"O tempo de pior caso para inserção de uma chave no esquema clássico de Hashing do Cuco
(Cuckoo Hashing) é uma variável aleatória que pode assumir valores arbitrariamente
altos, dada a probabilidade estritamente positiva de se precisar fazer uma sequência
interminável de realocações.
Propomos uma variante em que o tempo de pior caso da inserção é controlado. Para
isso, aplicamos duas ideias: a primeira é o emprego de um método de Hashing Perfeito
sempre que for necessária uma realocação, fazendo com que no máximo uma realocação
seja necessária por inserção; a segunda ideia é fazer com que o número de tabelas deixe
de ser constante, como de praxe, e passe a crescer em função do número de chaves, para
que a complexidade da realocação seja baixa. O preço pago é piorar a complexidade de
pior caso da operação de busca, que deixa de ser constante. O objetivo, no entanto, é
equilibrar as complexidades de pior caso da inserção e da busca.",DISSERTAÇÃO,Hashing do Cuco com Realocação Perfeita,5208201,01
"O Método dos Elementos Discretos (MED) é um método utilizado para a determinação
do comportamento mecânico de um corpo ou meio a partir de parâmetros
micromecânicos de contato. Na grande maioria dos casos, resultados precisos somente
são atingidos se os parâmetros, que são sensíveis ao modelo utilizado, forem
ajustados. A modelagem adotada no contato deve ser feita de modo bem cuidadoso,
pois pois isso pode afetar de modo significativo a busca dos parâmetros que estão
sendo modelados. Desse modo, existe a necessidade de se obter um procedimento
para fazer a calibração dos parâmetros antes de fazer a modelagem do problema.
O programa utilizado para a obtenção dos parâmetros de calibração foi o YADE,
que é um programa de código aberto com foco no MED. Foi realizado um comparativo
entre quatro métodos de otimização sem derivadas (Nelder-Mead, Refinamento
Progressivo, Simulated Annealing e Evolução Diferencial) a partir da realização dos
seguintes ensaios: tração, cisalhamento direto, compressão uniaxial e T-Bar. Neste
trabalho é mostrado que o Nelder-Mead é o melhor método de otimização sem derivadas
a ser utilizado quando comparado o número de chamadas da função a ser
otimizada.",,CIÊNCIA DA COMPUTAÇÃO,ALEX DOS PRAZERES MACHADO,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,08/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Mtodos dos Elementos Discretos;, MED, YADE, Otimizao, Geotecnia, Calibrao, Ensaio de Trao, Cisalhamento Direto , Compresso Uniaxial, T-Bar.'",ALGORITMOS E MÉTODOS NUMÉRICOS,DANIEL GREGORIO ALFARO VIGO,139,"Métodos dos Elementos Discretos;, MED, YADE, Otimização, Geotecnia, Calibração, Ensaio de Tração, Cisalhamento Direto , Compressão Uniaxial, T-Bar.",INFORMÁTICA (31001017110P8),-,"O Método dos Elementos Discretos (MED) é um método utilizado para a determinação
do comportamento mecânico de um corpo ou meio a partir de parâmetros
micromecânicos de contato. Na grande maioria dos casos, resultados precisos somente
são atingidos se os parâmetros, que são sensíveis ao modelo utilizado, forem
ajustados. A modelagem adotada no contato deve ser feita de modo bem cuidadoso,
pois pois isso pode afetar de modo significativo a busca dos parâmetros que estão
sendo modelados. Desse modo, existe a necessidade de se obter um procedimento
para fazer a calibração dos parâmetros antes de fazer a modelagem do problema.
O programa utilizado para a obtenção dos parâmetros de calibração foi o YADE,
que é um programa de código aberto com foco no MED. Foi realizado um comparativo
entre quatro métodos de otimização sem derivadas (Nelder-Mead, Refinamento
Progressivo, Simulated Annealing e Evolução Diferencial) a partir da realização dos
seguintes ensaios: tração, cisalhamento direto, compressão uniaxial e T-Bar. Neste
trabalho é mostrado que o Nelder-Mead é o melhor método de otimização sem derivadas
a ser utilizado quando comparado o número de chamadas da função a ser
otimizada.",DISSERTAÇÃO,DETERMINAÇÃO DE PARÂMETROS DO MÉTODO DOS ELEMENTOS DISCRETOS UTILIZANDO MÉTODOS DE OTIMIZAÇÃO SEM DERIVADAS,5208357,01
"Esta Tese apresenta um ambiente virtual colaborativo  (AVC) para a formação de agentes de segurança para melhorar o seu desempenho ao lidar com emergências envolvendo materiais perigosos. Como a gestão de tais emergências está relacionada com situações dinâmicas e imprevisíveis, um maior esforço cognitivo pode ser requerido. Por esta razão, é muito importante propor formas alternativas para melhorar os métodos aplicados na formação de agentes de segurança que lidam com detecção de materiais perigosos como explosivos e elementos radioativos. Considerando o uso de técnicas de realidade virtual para treinar as questões de segurança física, nos últimos anos, o AVC proposto neste trabalho destina-se a melhorar o desempenho dos agentes de segurança na detecção de materiais perigosos. O AVC foi baseada em cenários modelados com o Autodesk 3ds Max, enquanto Unity 3D foi usado para fazer o terreno e para implementar as características que compõem um ambiente virtual. Para testar o AVC cada cenário foi planejado para estar de acordo com os protocolos de segurança que devem ser seguidos pelos agentes de segurança da Copa de 2014 e dos Jogos Olímpicos Rio 2016. O estádio de futebol brasileiro conhecido como Maracanã foi escolhido como caso particular de estudo, onde foram simuladas possíveis situações de ameaças de materiais perigosos com o objetivo de treinar os agentes de segurança. Os resultados indicaram que o método proposto é capaz de contribuir para aumentar a capacidade de cada membro da equipe de segurança na realização de seu dever.",,CIÊNCIA DA COMPUTAÇÃO,CLAUDIO AZEVEDO PASSOS,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,07/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'realidade virtual;, ambiente virtual colaborativo, treinamento simulado,  avatar, deteco de materiais perigosos.'",SISTEMAS DE INFORMAÇÃO,PAULO VICTOR RODRIGUES DE CARVALHO,139,"realidade virtual;, ambiente virtual colaborativo, treinamento simulado,  avatar, detecção de materiais perigosos.",INFORMÁTICA (31001017110P8),-,"Esta Tese apresenta um ambiente virtual colaborativo  (AVC) para a formação de agentes de segurança para melhorar o seu desempenho ao lidar com emergências envolvendo materiais perigosos. Como a gestão de tais emergências está relacionada com situações dinâmicas e imprevisíveis, um maior esforço cognitivo pode ser requerido. Por esta razão, é muito importante propor formas alternativas para melhorar os métodos aplicados na formação de agentes de segurança que lidam com detecção de materiais perigosos como explosivos e elementos radioativos. Considerando o uso de técnicas de realidade virtual para treinar as questões de segurança física, nos últimos anos, o AVC proposto neste trabalho destina-se a melhorar o desempenho dos agentes de segurança na detecção de materiais perigosos. O AVC foi baseada em cenários modelados com o Autodesk 3ds Max, enquanto Unity 3D foi usado para fazer o terreno e para implementar as características que compõem um ambiente virtual. Para testar o AVC cada cenário foi planejado para estar de acordo com os protocolos de segurança que devem ser seguidos pelos agentes de segurança da Copa de 2014 e dos Jogos Olímpicos Rio 2016. O estádio de futebol brasileiro conhecido como Maracanã foi escolhido como caso particular de estudo, onde foram simuladas possíveis situações de ameaças de materiais perigosos com o objetivo de treinar os agentes de segurança. Os resultados indicaram que o método proposto é capaz de contribuir para aumentar a capacidade de cada membro da equipe de segurança na realização de seu dever.",TESE,AMBIENTE VIRTUAL COLABORATIVO PARA TREINAMENTO DE AGENTES DE SEGURANÇA EM GRANDES EVENTOS,5209172,01
"This dissertation proposes a socio-pedagogical intervention model aimed at the deconstruction of stereotypes detrimental to the cognitive development of individuals, and for the recovery of this lost potential development. In this specific case, such model was applied with the purpose of assisting women affected by the socially constructed assumption that careers linked to technological areas are more suitable for men. Such a supposition underlies a series of coercions which, throughout a woman’s life, may lead to an underdevelopment of certain executive cognitive functions precisely necessary for good performance in those areas.
Thus, a transdisciplinary methodology was used, since the identified problem has a broad character, and cannot be solved if approached in a disciplinary unilateral manner. Through activities that use information technology as paramediating instruments, from the methodological bases of computational neuropsychology, action-research and pedagogy through affection, and a sociohistorical, educational and neuroscientific theoretical framework, a long-term work was carried out with high school and undergraduate students and achieved satisfactory results in terms of cognitive development and the construction of autonomy and empowerment by the participants.",Erica Calil Nogueira.pdf,CIÊNCIA DA COMPUTAÇÃO,ERICA CALIL NOGUEIRA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,18/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'Women empowerment. Information Technology. Education. Neuroscience. Cognitive functions',INFORMÁTICA NA EDUCAÇÃO,CLAUDIA LAGE REBELLO DA MOTTA,146,Empoderamento feminino. Tecnologia da informação. Educação. Neurociência. Funções cognitivas,INFORMÁTICA (31001017110P8),-,"Esta dissertação propõe um modelo de intervenção sociopedagógica voltado para a desconstrução de estereótipos prejudiciais ao desenvolvimento cognitivo de indivíduos, e para o resgate desse desenvolvimento potencial perdido. Neste caso específico, tal modelo foi aplicado com a finalidade de auxiliar mulheres atingidas pela suposição socialmente construída de que carreiras ligadas a áreas tecnológicas são mais adequadas a homens. Tal suposição fundamenta uma série de coerções que, ao longo da vida de uma mulher, podem levar a um subdesenvolvimento de certas funções cognitivas executivas justamente necessárias à boa atuação em tais áreas.
Dessa forma, foi utilizada uma metodologia transdisciplinar, uma vez que o problema identificado tem caráter amplo, não podendo ser solucionado caso abordado de forma disciplinarmente unilateral. Através de atividades que se valem das tecnologias da informação como instrumentos paramediadores, das bases metodológicas da neuropsicologia computacional, da pesquisa-ação e da pedagogia pelo afeto, e de um arcabouço teórico sociohistórico, educacional e neurocientífico, foi realizado um trabalho de longo prazo com estudantes de Ensino Médio e graduação que obteve resultados satisfatórios no tocante ao desenvolvimento cognitivo e à construção de autonomia e empoderamento por parte das participantes.",DISSERTAÇÃO,EMPODERAMENTO FEMININO ATRAVÉS DAS TECNOLOGIAS DA INFORMAÇÃO,5209329,01
"A Ciência das Redes vem caracterizando as dinâmicas estruturas, comportamento e
evolução de sistemas complexos, fazendo predições a respeito das funcionalidades
destes sistemas, ganhando importância pela percepção científica de que muitos
fenômenos importantes podem ser compreendidos se modelados como se ocorressem no
contexto de uma rede. As relações sociais entre os seres humanos é um exemplo desses
sistemas complexos que podem ser modelados como uma rede. No contexto
educacional, a sala de aula é uma pequena sociedade formada pelo professor e seus
alunos. No entanto, o modelo predominante da sala de aula presencial tradicional, onde
o professor é sempre o emissor e o aluno o receptor das informações, não facilita que o
potencial de cada aluno seja identificado, o quanto poderia contribuir para o
enriquecimento da aula e ajudar a turma como um todo, caso fosse estimulado. É
comum alunos não interagirem ou não participarem ativamente das aulas presenciais
por vergonha ou medo de falar em público, por outro lado, é um desafio para o
professor provocar que todos participem tendo em mente como e o quanto cada um de
seus alunos poderia atuar em suas aulas. Sendo assim, este trabalho propõe um ambiente
experimental, que estimule e registre as trocas que acontecem durante uma dinâmica de
aula presencial e, ao mesmo tempo, obtenha um retrato panorâmico da turma, revelando
quais papéis estão presentes no tipo de influência que cada aluno exerce no grupo. As
respostas deste estudo apresentam uma classificação possível para representar os papéis
presentes numa turma de alunos enquanto interagem entre si e com seu professor. Esta
solução pôde auxiliar ao professor e equipe pedagógica no sentido de aproveitar melhor
o potencial de cada aluno enquanto participante da resolução de tarefas em grupo.",,CIÊNCIA DA COMPUTAÇÃO,VIVIANE SOARES RODRIGUES SILVA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,15/08/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Minerao de Dados;, Redes Sociais, Redes Textuais, Educao, Sala de Aula.'",INFORMÁTICA NA EDUCAÇÃO,CLAUDIA LAGE REBELLO DA MOTTA,114,"Mineração de Dados;, Redes Sociais, Redes Textuais, Educação, Sala de Aula.",INFORMÁTICA (31001017110P8),-,"A Ciência das Redes vem caracterizando as dinâmicas estruturas, comportamento e
evolução de sistemas complexos, fazendo predições a respeito das funcionalidades
destes sistemas, ganhando importância pela percepção científica de que muitos
fenômenos importantes podem ser compreendidos se modelados como se ocorressem no
contexto de uma rede. As relações sociais entre os seres humanos é um exemplo desses
sistemas complexos que podem ser modelados como uma rede. No contexto
educacional, a sala de aula é uma pequena sociedade formada pelo professor e seus
alunos. No entanto, o modelo predominante da sala de aula presencial tradicional, onde
o professor é sempre o emissor e o aluno o receptor das informações, não facilita que o
potencial de cada aluno seja identificado, o quanto poderia contribuir para o
enriquecimento da aula e ajudar a turma como um todo, caso fosse estimulado. É
comum alunos não interagirem ou não participarem ativamente das aulas presenciais
por vergonha ou medo de falar em público, por outro lado, é um desafio para o
professor provocar que todos participem tendo em mente como e o quanto cada um de
seus alunos poderia atuar em suas aulas. Sendo assim, este trabalho propõe um ambiente
experimental, que estimule e registre as trocas que acontecem durante uma dinâmica de
aula presencial e, ao mesmo tempo, obtenha um retrato panorâmico da turma, revelando
quais papéis estão presentes no tipo de influência que cada aluno exerce no grupo. As
respostas deste estudo apresentam uma classificação possível para representar os papéis
presentes numa turma de alunos enquanto interagem entre si e com seu professor. Esta
solução pôde auxiliar ao professor e equipe pedagógica no sentido de aproveitar melhor
o potencial de cada aluno enquanto participante da resolução de tarefas em grupo.",TESE,IDENTIFICAÇÃO DE PAPÉIS EM SALA DE AULA COM USO DE MINERAÇÃO DE DADOS EM REDES SOCIAIS E REDES TEXTUAIS,5209356,01
"Dada as mudanças provenientes das novas formas de comunicação baseadas em
tecnologia, um novo indivíduo deve ser estimulado. Mudamos o perfil da sociedade quando a
mesma se viu submetida a uma onda de informação e uma transformação em curto tempo da
sua forma de interagir com o mundo a sua volta. É então necessário incentivar
comportamentos até então não desenvolvidos para que esse indivíduo sobreviva a adaptação
do mundo que vem pela frente. Visando desenvolver o indivíduo desde o início das suas
interações sociais, os olhos se voltam para os processos educativos. Entende-se e esse
trabalho apoia a ideia de que é possível, no âmbito educacional, se desenvolver competências
que apoiem esse sujeito para sua interação com um mundo desconhecido. Com a prática
colaborativa, aprendemos de forma conjunta e com a proximidades entre conhecimentos é
possível avaliarmos as competências dos pares. Para apoiar a avaliação, essa tese propõe um
modelo com base em competências e como premissa a utilização de sistemas colaborativos
como forma de interação. Espera-se como resultado que o modelo apoie a avaliação entre
pares, sendo possível constituir um grande mapa de competências de um grupo e avaliar o
quão resiliente esse indivíduo é perante ao um mundo de situações não previstas.",,CIÊNCIA DA COMPUTAÇÃO,VIVIANE LEITE LUCAS DE AZEVEDO,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,07/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Colaborao;, Competncia, inteligncia coletiva.'",INFORMÁTICA NA EDUCAÇÃO,MARCOS ROBERTO DA SILVA BORGES,130,"Colaboração;, Competência, inteligência coletiva.",INFORMÁTICA (31001017110P8),-,"Dada as mudanças provenientes das novas formas de comunicação baseadas em
tecnologia, um novo indivíduo deve ser estimulado. Mudamos o perfil da sociedade quando a
mesma se viu submetida a uma onda de informação e uma transformação em curto tempo da
sua forma de interagir com o mundo a sua volta. É então necessário incentivar
comportamentos até então não desenvolvidos para que esse indivíduo sobreviva a adaptação
do mundo que vem pela frente. Visando desenvolver o indivíduo desde o início das suas
interações sociais, os olhos se voltam para os processos educativos. Entende-se e esse
trabalho apoia a ideia de que é possível, no âmbito educacional, se desenvolver competências
que apoiem esse sujeito para sua interação com um mundo desconhecido. Com a prática
colaborativa, aprendemos de forma conjunta e com a proximidades entre conhecimentos é
possível avaliarmos as competências dos pares. Para apoiar a avaliação, essa tese propõe um
modelo com base em competências e como premissa a utilização de sistemas colaborativos
como forma de interação. Espera-se como resultado que o modelo apoie a avaliação entre
pares, sendo possível constituir um grande mapa de competências de um grupo e avaliar o
quão resiliente esse indivíduo é perante ao um mundo de situações não previstas.",TESE,MODELO PARA MAPEAMENTO DE COMPETÊNCIAS EM GRUPO APOIADO PELA COLABORAÇÃO EM SISTEMAS,5209376,01
"Avaliação da Conformidade é a demonstração de que os requisitos especificados
relativos a um produto, processo, sistema, pessoa ou organismo são atendidos. Este
método faz com que os produtos e serviços atendam aos objetivos que se pretendem.
Ferramentas de avaliação da conformidade são ensaio, certificação, inspeção e
acreditação. Fraudes em serviços de avaliação da conformidade trazem sérios riscos
à segurança e à saúde da população, bem como ao meio ambiente. Além disso, a
realização de serviços de avaliação da conformidade de forma fraudulenta representa
um tratamento desleal e ilegal por parte daqueles que se valem de produtos adulterados
para ganhos sobre o consumidor. Neste trabalho, propõe-se um arcabouço
para identificar possíveis casos de fraude nos serviços prestados por organismos de
avaliação da conformidade. Este arcabouço compreende uma técnica de monitoramento
de vários centros de avaliação da conformidade e técnicas para detectar
possíveis fraudes em um organismo de avaliação da conformidade. Também, incorporam
esse arcabouço técnicas para análise rápida de vídeos desses organismos de
avaliação da conformidade para monitorar fraudes e uma técnica baseada em assinaturas
para investigação automática de fraudes em organismos de avaliação da
conformidade. Nos experimentos, o arcabouço proposto conseguiu identificar com
sucesso as organizações com comportamento fraudulento.",ROSEMBERGUE PEREIRA.pdf,CIÊNCIA DA COMPUTAÇÃO,ROSEMBERGUE PEREIRA DE SOUZA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,13/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Avaliao da conformidade,;Deteco de fraudes, Aprendizado de mquina, Viso computacional.'",REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,LUIZ FERNANDO RUST DA COSTA CARMO,208,"Avaliação da conformidade;, Detecção de fraudes, Aprendizado de máquina, Visão computacional.",INFORMÁTICA (31001017110P8),-,"Avaliação da Conformidade é a demonstração de que os requisitos especificados
relativos a um produto, processo, sistema, pessoa ou organismo são atendidos. Este
método faz com que os produtos e serviços atendam aos objetivos que se pretendem.
Ferramentas de avaliação da conformidade são ensaio, certificação, inspeção e
acreditação. Fraudes em serviços de avaliação da conformidade trazem sérios riscos
à segurança e à saúde da população, bem como ao meio ambiente. Além disso, a
realização de serviços de avaliação da conformidade de forma fraudulenta representa
um tratamento desleal e ilegal por parte daqueles que se valem de produtos adulterados
para ganhos sobre o consumidor. Neste trabalho, propõe-se um arcabouço
para identificar possíveis casos de fraude nos serviços prestados por organismos de
avaliação da conformidade. Este arcabouço compreende uma técnica de monitoramento
de vários centros de avaliação da conformidade e técnicas para detectar
possíveis fraudes em um organismo de avaliação da conformidade. Também, incorporam
esse arcabouço técnicas para análise rápida de vídeos desses organismos de
avaliação da conformidade para monitorar fraudes e uma técnica baseada em assinaturas
para investigação automática de fraudes em organismos de avaliação da
conformidade. Nos experimentos, o arcabouço proposto conseguiu identificar com
sucesso as organizações com comportamento fraudulento.",TESE,UM ARCABOUÇO TECNOLÓGICO PARA DETECÇÃO DE POSSÍVEIS SERVIÇOS FRAUDULENTOS EM ORGANISMOS DE AVALIAÇÃO DA CONFORMIDADE,5209460,01
"Opinion Mining is the task of labeling a document with a polarity towards a predefined entity. This task is a special case of text categorization.When applying text classification task on streaming environments, such as social medias, this classification must consider the sequence of the data, adapting to possible concept drifts. This work studies the possibility of applying WiSARD in such tasks, exploring the classifier capability of fast learning and classification. To do so, WiSARD had to be modified aiming to overcome some problems in the scenario: (i) data sparsity; (ii) concept drifts; (iii) sequence of the data. The first is usually found in text classification due the use of Bag-of-Words representation. The second is presented in streaming environments and WiSARD had to be adapted to work as a semi-supervised learning classifier. The last was addressed by modifying its architecture, exploring the concepts of Stable Bloom Filters to implement a forgetfulness behavior. For each modification, an experiment was executed, using real and established datasets. The findings of the present work are already being applied in the field of weightless neural networks.",FABIO RANGEL.pdf,CIÊNCIA DA COMPUTAÇÃO,FABIO MEDEIROS RANGEL,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,08/12/2017,INGLES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'WiSARD. Opinion Mining. Machine Learning. Concept Drift. Data Mining. Text Categorization. Semi-Supervised Learning. Online Learning',SISTEMAS DE INFORMAÇÃO,JONICE DE OLIVEIRA SAMPAIO,35,WiSARD. Opinion Mining. Machine Learning. Concept Drift. Data Mining. Text Categorization. Semi-Supervised Learning. Online Learning,INFORMÁTICA (31001017110P8),-,"Opinion Mining is the task of labeling a document with a polarity towards a predefined entity. This task is a special case of text categorization.When applying text classification task on streaming environments, such as social medias, this classification must consider the sequence of the data, adapting to possible concept drifts. This work studies the possibility of applying WiSARD in such tasks, exploring the classifier capability of fast learning and classification. To do so, WiSARD had to be modified aiming to overcome some problems in the scenario: (i) data sparsity; (ii) concept drifts; (iii) sequence of the data. The first is usually found in text classification due the use of Bag-of-Words representation. The second is presented in streaming environments and WiSARD had to be adapted to work as a semi-supervised learning classifier. The last was addressed by modifying its architecture, exploring the concepts of Stable Bloom Filters to implement a forgetfulness behavior. For each modification, an experiment was executed, using real and established datasets. The findings of the present work are already being applied in the field of weightless neural networks.",DISSERTAÇÃO,WiSARD for Opinion Mining in Nonstationary Environments,5209510,01
"A Tecnologia da Informação (TI) é vital para que governos e órgãos públicos
possam divulgar seus serviços aos cidadãos através do e-Government. Selecionar o
projeto correto do portfólio de projetos é uma tarefa que pode impactar no custo,
qualidade e sucesso desse projeto. Essa pesquisa visa fornecer informações para ajudar
os tomadores de decisões na priorização do projeto dentro de um portfólio de
projetos de TI em organizações públicas. Para atingir esse objetivo, foi realizado um
Mapeamento Sistemático de Literatura em busca de processos, métodos e critérios
utilizados nesse tipo de priorização. Foram pesquisados trabalhos publicados pela
comunidade científica entre 2007 e 2017 em cinco repositórios científicos, nos quais
foram considerados apenas os trabalhos que possuíam comprovação empírica. Para
garantir o rigor metodológico, utilizou-se um protocolo previamente testado e aceito
pela comunidade científica. Ele auxiliou na localização, coleta e análise de dados
dos trabalhos relevantes para essa pesquisa e no desenvolvimento de um conjunto
de dados com mais de 2.000 publicações no qual foi possível identificar os estágios
comuns nos processos de priorização de projetos, classificar e tabular estudos, quantificar
métodos e verificar que o Analytic Hierarchy Process (AHP) é o método mais
frequentemente usado. Outro dado extraído do conjunto de publicações foram os
critérios de priorização. A análise dos dados demonstrou, por exemplo, uma diferença
entre o processo de priorização de projetos de TI nos setores público e privado:
o critério. Outra informação interessante foi a quantidade pequena de artigos que
mencionaram a combinação de processo, método de priorização e critério. Com as
informações obtidas no Mapeamento Sistemático, realizou-se uma busca por elas
nas diretrizes do modelo brasileiro de governança e gestão de projetos de TI e esse
modelo foi comparado com o modelo proposto pelo governo dinamarquês.",ALEXANDRE MENDES LIMA.pdf,CIÊNCIA DA COMPUTAÇÃO,ALEXANDRE MENDES LIMA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,06/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Priorizao;Projeto de TI, Tecnologia da Informao, e-Government, Mtodos de priorizao, Processos de priorizao, Critrios de priorizao.'",SISTEMAS DE INFORMAÇÃO,MONICA FERREIRA DA SILVA,93,"Priorização;Projeto de TI, Tecnologia da Informação, e-Government, Métodos de priorização, Processos de priorização, Critérios de priorização.",INFORMÁTICA (31001017110P8),GESTÃO ESTRATÉGICA DE TI,"A Tecnologia da Informação (TI) é vital para que governos e órgãos públicos
possam divulgar seus serviços aos cidadãos através do e-Government. Selecionar o
projeto correto do portfólio de projetos é uma tarefa que pode impactar no custo,
qualidade e sucesso desse projeto. Essa pesquisa visa fornecer informações para ajudar
os tomadores de decisões na priorização do projeto dentro de um portfólio de
projetos de TI em organizações públicas. Para atingir esse objetivo, foi realizado um
Mapeamento Sistemático de Literatura em busca de processos, métodos e critérios
utilizados nesse tipo de priorização. Foram pesquisados trabalhos publicados pela
comunidade científica entre 2007 e 2017 em cinco repositórios científicos, nos quais
foram considerados apenas os trabalhos que possuíam comprovação empírica. Para
garantir o rigor metodológico, utilizou-se um protocolo previamente testado e aceito
pela comunidade científica. Ele auxiliou na localização, coleta e análise de dados
dos trabalhos relevantes para essa pesquisa e no desenvolvimento de um conjunto
de dados com mais de 2.000 publicações no qual foi possível identificar os estágios
comuns nos processos de priorização de projetos, classificar e tabular estudos, quantificar
métodos e verificar que o Analytic Hierarchy Process (AHP) é o método mais
frequentemente usado. Outro dado extraído do conjunto de publicações foram os
critérios de priorização. A análise dos dados demonstrou, por exemplo, uma diferença
entre o processo de priorização de projetos de TI nos setores público e privado:
o critério. Outra informação interessante foi a quantidade pequena de artigos que
mencionaram a combinação de processo, método de priorização e critério. Com as
informações obtidas no Mapeamento Sistemático, realizou-se uma busca por elas
nas diretrizes do modelo brasileiro de governança e gestão de projetos de TI e esse
modelo foi comparado com o modelo proposto pelo governo dinamarquês.",DISSERTAÇÃO,"PRIORIZAÇÃO DE PROJETOS DE TI EM PORTFÓLIO NAS ORGANIZAÇÕES PÚBLICAS: MÉTODOS, PROCESSOS E CRITÉRIOS USADOS NA ÚLTIMA DÉCADA",5209579,01
"The novel paradigm of cloud of sensors (CoS) gained momentum recently, bringing
together the cloud computing and wireless sensor and actuator network (WSAN) paradigms
in the form of a two-tier CoS architecture. With the more recent emergence of the edge
computing paradigm, enabling computation and networking capabilities at the edge of the
network, it became worth considering a three-tier CoS architecture (comprising the sensor,
edge and cloud tiers). To provide a clean decoupling between the three-tier CoS architecture
and applications, we propose the concept of CoS virtualization in our work. By employing
such concept of CoS virtualization, a set of virtual nodes (VNs) is made available to
applications. Assigning VNs to application requests in a timely and efficient way, in order to
meet the requirements of applications, gives rise to the challenge of resource allocation in
CoS. In this work, we formulate the problem of resource allocation in CoS and propose Zeus,
a partly decentralized algorithm for solving it. We choose a heuristic approach to formulate
Zeus, due to its low computation overhead and fast execution. As its key features, Zeus is
capable of (i) performing requests in common for multiple applications only once, sharing
the results of this single execution among these multiple applications, (ii) handling priorities
and (iii) handling precedence relationships among applications. Among its contributions,
Zeus is scalable in terms of the number of VNs and applications in the CoS, provides support
to delay-sensitive applications, and improves the lifetime of WSANs.",,CIÊNCIA DA COMPUTAÇÃO,IGOR LEAO DOS SANTOS,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,05/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Cloud of Sensors, Cloud Computing, Edge Computing, Information Fusion, Virtualization, Wireless Sensor and Actuator Networks, Resource Allocation, Optimization, Mixed Integer Non-linear Programming, Data Provisioning, Data Freshness, Heuristic Algorithm'",REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,LUCI PIRMEZ,175,"Nuvem de Sensores, Computação em Nuvem, Computação em Borda, Fusão de Informação, Virtualização, Redes de Atuadores e Sensores Sem Fio, Alocação de Recursos, Otimização, Programação Não-Linear Inteira Mista, Provisionamento de Dados, Atualidade de dados, Algoritmo Heurístico",INFORMÁTICA (31001017110P8),-,"O novo paradigma da nuvem de sensores (CoS) vem ganhando visibilidade
recentemente, reunindo os paradigmas de computação em nuvem e de redes de atuadores
e sensores sem fio (RASSF) sob a forma de uma arquitetura de dois níveis. Com o surgimento
mais recente do paradigma de computação de borda, disponibilizando recursos de
computação, armazenamento e rede na borda da rede, tornou-se interessante considerar
uma arquitetura de CoS de três camadas (compreendendo os níveis de sensor, borda e
nuvem). Para proporcionar um desacoplamento claro entre a arquitetura de três níveis de
CoS e as aplicações, propomos o conceito de virtualização de CoS em nosso trabalho.Ao
empregar a virtualização de CoS, um conjunto de nós virtuais (VNs) é disponibilizado para as
aplicações. Alocar VNs às solicitações de aplicações de maneira oportuna e eficiente, para
atender aos requisitos das aplicações, dá origem ao desafio da alocação de recursos no CoS.
Neste trabalho, formulamos o problema da alocação de recursos em CoS e propomos Zeus,
um algoritmo parcialmente descentralizado para resolvê-lo. Escolhemos uma abordagem
heurística para formular Zeus, devido à baixa sobrecarga de computação e rápida execução
desse tipo de abordagem. Como características-chave, o Zeus é capaz de (i) executar
solicitações em comum para múltiplas aplicações apenas uma vez, e compartilhar os
resultados dessa execução única entre essas múltiplas aplicações, (ii) lidar com prioridades
de aplicações e (iii) gerenciar relações de precedência entre as aplicações. Entre suas
contribuições, Zeus é escalável em termos de número de VNs e aplicações na CoS, fornece
suporte a aplicações sensíveis ao atraso e melhora a vida útil de WSANs.",TESE,ON THE VIRTUALIZATION AND RESOURCE ALLOCATION IN THE CLOUD  OF SENSORS,5232150,01
"Cyber-Physical Systems (CPSs) are found in many modern applications, e.g.,
power management, water resources, traffic monitoring and life support. CPSs are

defined as the systems that provide integrations of computation, networks and phys-
ical processes, composed of several types of networked devices. In general, some

CPS components are expected to have limited computational, storage, and network

resources to perform specific tasks for monitoring and controlling the physical pro-
cesses. From the possible applications, those requiring real-time response and also

life-critical procedures become an implementation challenge, since the computational
and network constraints conflict directly with the necessary security requirements
and quality of service. So, there is a great need for these systems to operate safely
and reliably, by mitigating the consequences of failures and attacks that may occur
and, also, by investigating and being aware of the factors that have caused undesired
events in the past.
Digital Evidence plays a major role during the investigation process of the cause
and consequences of a failure event or a security incident. Resulting actions from
forensic investigation are important to minimize the negative consequences, as well
as ensuring the security of a system against future threats. Therefore, it is necessary
to use information security methods in order to guarantee the integrity and proof
of origin of the collected evidences. However, the complexity of the cyber-physical

systems scenarios imposes on the used methods some restrictive non-functional re-
quirements, such as scalability, computational performance, data storage and com-
munication network usage, among others.

This thesis aims to propose and evaluate information security methods on the
authentication and validation of digital evidence for storage and transmission in the
Cyber-Physical Systems that comprise resource-constrained devices. We present

three proposals, each one for a specific part of the system, for digital evidence au-
thentication, namely: (i) a secure log structure scheme, (ii) a delayed authentication

protocol for multicast communication, and (iii) a characteristics-based authentica-
tion tag for monitoring data.
We comparatively evaluate our methods to the ones found in the literature,
regarding the reduction of computational cost, internal storage and communication

network overhead. In addition, we also evaluate these methods for constrained-
resource applications in the Cyber-Physical Systems domain. Particularly, we chose

the Smart Grids as our main evaluation scenario.",,CIÊNCIA DA COMPUTAÇÃO,SERGIO DE MEDEIROS CAMARA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,22/03/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Digital Evidence, Cyber-Physical Systems, Resource-constrained, Au- thentication, Digital Forensics, Smart Grids'",REDES DE COMPUTADORES E SISTEMAS DISTRIBUÍDOS,LUIZ FERNANDO RUST DA COSTA CARMO,147,"Evidência Digital, Sistemas Ciberfísicos, Recursos limitados, Au- tenticação, Computação Forense, Redes Elétricas Inteligentes",INFORMÁTICA (31001017110P8),-,"Os Sistemas Ciberfísicos estão presentes em um diverso número de aplicações

modernas, p. ex., gerenciamento de energia elétrica, recursos hídricos, monitora-
mento de tráfego e de assistência à vida. Eles são definidos como os sistemas que

oferecem integrações de computação, redes e processos físicos, sendo compostos por
vários tipos de dispositivos conectados em rede. De maneira geral, é esperado que
alguns componentes de um sistema ciberfísico tenham recursos computacionais, de
memória e de rede limitados para executar tarefas específicas de monitoramento e

controle dos processos físicos. Dentre as aplicações possíveis, as que demandam res-
posta em tempo real e, também, as que envolvem risco à vida tornam-se um desafio

de implementação, uma vez que as restrições computacionais e de rede entram em
conflito com os requisitos de segurança necessários e a qualidade do serviço desejada.

Dessa forma, há uma grande necessidade de que esses sistemas operem de forma se-
gura e confiável, mitigando o impacto de falhas e ataques que possam ocorrer e,

também, investigando e conhecendo os fatores que ocasionaram eventos indesejados
no passado.

A Evidência Digital cumpre um papel principal durante esse processo de investi-
gação da causa e consequências de um evento de falha ou um incidente de segurança.

As ações decorrentes de uma investigação forense são importantes para minimizar
as consequências negativas, assim como também garantir a segurança de um sistema

diante de futuras ameaças. Logo, se faz necessária a utilização de métodos de segu-
rança da informação, a fim de garantir a integridade e a comprovação de origem das

evidências coletadas. No entanto, a complexidade existente nos cenários dos Siste-
mas Ciberfísicos impõe requisitos não-funcionais restritivos aos métodos utilizados,

tais como escalabilidade, desempenho computacional, utilização do armazenamento
de dados e da rede de comunicação, entre outros.
A presente tese tem como objetivo propor e avaliar métodos de segurança da
informação para a autenticação e validação de evidências digitais durante as etapas
de armazenamento e transmissão em Sistemas Ciberfísicos que possuem dispositivos
de recursos limitados. Apresentamos três propostas em relação à autenticação de

v

evidências digitais, cada uma com foco específico em uma parte do sistema, são elas:
(i) um esquema de estruturação de log seguro, (ii) um protocolo de autenticação
atrasada para uma comunicação multicast, e (iii) uma tag de autenticação baseada
em características de dados de monitoramento.

Nossos métodos são avaliados de forma comparativa com outros métodos exis-
tentes na literatura em relação à redução do custo computacional, armazenamento

interno e sobrecarga na rede de comunicação. Além disso, avaliamos esses métodos,

também, para aplicações que apresentam limitações de recursos no domínio dos Sis-
temas Ciberfísicos. Em particular, escolhemos as Redes Elétricas Inteligentes como

o cenário de avaliação principal.",TESE,AUTENTICAÇÃO DE EVIDÊNCIAS DIGITAIS PARA SISTEMAS CIBERFÍSICOS DE RECURSOS  LIMITADOS,5232287,01
"Two agents, starting from distinct points and unaware of the position of one

another, must meet. This general setting is known as the Robot Rendezvous Prob-
lem. This M.Sc. thesis focuses on some of the main variations of the problem among

those in which the agents move along given graph families. After revisiting existing

solutions, we present a novel, randomized solution to one of its most popular vari-
ations, often referred to as the parachuting robots. In such a variation, the space is

unidimensional (an infinite path graph), and the starting positions receive markers,
which may play an important role in the search algorithm; it is also required that
the two agents run exactly the same algortithm, i.e., the variation is symmetric.
The proposed solution presents a better time complexity, on average, than the best
solution known thus far. This text also introduces a variation, not yet considered in
the literature, where the agents must return to their starting points after they have
met, and where the performance improvement attained by the use of randomization
is even more pronounced.",JUAN CARLOS TOLEDO BAPTISTA.pdf,CIÊNCIA DA COMPUTAÇÃO,JUAN CARLOS TOLEDO BAPTISTA,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,22/11/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'Robots Rendezvous, Randomized Algorithms'",ALGORITMOS E MÉTODOS NUMÉRICOS,VINICIUS GUSMAO PEREIRA DE SA,67,"Rendezvous de Robôs, Algoritmos Randomizados",INFORMÁTICA (31001017110P8),-,"Dois agentes, partindo de pontos distintos e desconhecendo a localização um do
outro, precisam se encontrar. Este cenário geral é conhecido como o Problema do

Rendezvous de Robôs. Esta dissertação aborda algumas das principais variantes den-
tre aquelas em que os robôs se movem ao longo de certas famílias de grafos. Após

serem revisitadas algumas soluções conhecidas, é oferecida uma solução randomi-
zada simples e original para uma das variantes mais populares do problema, que é

comumente referida como a variante dos robôs de paraquedas. Nesta variante, o es-
paço considerado é unidimensional (um caminho infinito), e as posições iniciais dos

agentes recebem marcadores que podem ajudá-los a se orientar durante a busca. É

também exigido que os robôs executem exatamente o mesmo algoritmo, isto é, trata-
se de uma variante simétrica. A solução proposta apresenta melhor desempenho,

em média, do que a melhor solução conhecida até então. Esta dissertação introduz,
ainda, uma variante até então não considerada na literatura, a saber, aquela em
que os agentes precisam retornar a suas posições iniciais, e para a qual o ganho em
desempenho obtido pelo uso da randomização torna-se ainda mais acentuado.",DISSERTAÇÃO,RENDEZVOUS SIMÉTRICO EM  GRAFOS,5232587,01
"Economic transformations motivated by the rapid advance of Information Techno-
logy are increasingly focused in debates and research. Currently, changes are con-
stant and knowledge has become even more valuable, resulting in the need for new

approaches and interpretations of the economy. Motivated by the growth in the
quantity and use of mobile software and solutions, this dissertation aims to better
understand the essence of the Ecosystems of Applications Mobile, mapping its most

important components and searching for signs that may indicate new research oppor-
tunities. Thus, two consolidated research methodologies were used simultaneously,

the Systematic Review of Literature (SLR) proposed by Kitchenham, and the Con-
tent Analysis technique proposed by Bardin. The main result of this work, elicitates

the most studied topics about Ecosystem of Applications Mobile that, among the
30 papers selected during the SLR, have concentrated on four components: users,
developers, stores and advertising.",ANTOANNE CHRISTOPHER PONTES WANDERLEY.pdf,CIÊNCIA DA COMPUTAÇÃO,ANTOANNE CHRISTOPHER PONTES WANDERLEY,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,06/12/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,b'App Economy. App Store. Mobile App. Mobile Platform. Software Ecosystem',SISTEMAS DE INFORMAÇÃO,MONICA FERREIRA DA SILVA,88,Aplicativos Mobile. Economia de Aplicativos Mobile. Ecossis- tema de Software. Loja de Aplicativos. Plataforma Mobile,INFORMÁTICA (31001017110P8),GESTÃO ESTRATÉGICA DE TI,"Transformações econômicas, motivadas pelo rápido avanço da Tecnologia da Infor-
mação, estão cada vez mais em foco como tema em debates e pesquisas. Atual-
mente, mudanças são cada vez mais intensas e o conhecimento tem se tornado ainda

mais valioso, resultando na necessidade de novas abordagens e interpretações da
economia pela sociedade. Motivado pelo crescimento em quantidade, utilização e
soluções mobile, esta dissertação tem como objetivo melhor entender a essência dos

Ecossistemas de Aplicativos Mobile, mapeando seus componentes mais importan-
tes e buscando sinais que possam indicar novas oportunidades de pesquisa. Para

isto, duas metodologias de pesquisa já consolidadas foram utilizadas em harmonia,
o protocolo de Revisão Sistemática de Literatura (SLR) aplicado a Engenharia de
Software proposto por Kitchenham e a técnica de Análise de Conteúdo proposta
por Bardin. Como principal resultado desta pesquisa, temos o levantamento dos
temas mais abordados sobre o Ecossistema de Aplicativos Mobile que, dentre os 30
trabalhos selecionados durante a SLR, têm se concentrado em quatro componentes:
usuários, desenvolvedores, lojas e publicidade.",DISSERTAÇÃO,"ECOSSISTEMA DE APLICATIVOS MOBILE E SEUS PILARES: usuários, desenvolvedores, lojas e publicidade",5232756,01
"Mecanismos  dinâmicos de alocação de cache vêm sendo amplamente  utilizados visando melhorar o desempenho de sistemas de distribuição de conteúdo. Tais mecanismos  requerem a determinação da quantidade de recursos que devem ser alocados para o atendimento ótimo de requisições considerando restrições de custo. No presente trabalho é proposto um método para previsão do número de requisições a uma aplicação de vídeo educacional. As  previsões obtidas com o método proposto foram usadas para definir uma políticas de alocação de recursos de cache em rede. Os resultados encontrados nos experimentos mostram que é possível prever o número de acessos futuros dentro do cenário analisado, e que a suavização exponencial dupla apresenta as melhores predições, além de se conseguir ajustar uma política de alocação de recursos.",PRISCILA MELLO ALVES.pdf,CIÊNCIA DA COMPUTAÇÃO,PRISCILA MELLO ALVES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,27/10/2017,PORTUGUES,UNIVERSIDADE FEDERAL DO RIO DE JANEIRO,"b'cache, previso, alocao de recursos'",MODELOS E ARQUITETURAS PARA SISTEMAS INTELIGENTES,DANIEL SADOC MENASCHE,88,"cache, previsão, alocação de recursos",INFORMÁTICA (31001017110P8),-,"Mecanismos  dinâmicos de alocação de cache vêm sendo amplamente  utilizados visando melhorar o desempenho de sistemas de distribuição de conteúdo. Tais mecanismos  requerem a determinação da quantidade de recursos que devem ser alocados para o atendimento ótimo de requisições considerando restrições de custo. No presente trabalho é proposto um método para previsão do número de requisições a uma aplicação de vídeo educacional. As  previsões obtidas com o método proposto foram usadas para definir uma políticas de alocação de recursos de cache em rede. Os resultados encontrados nos experimentos mostram que é possível prever o número de acessos futuros dentro do cenário analisado, e que a suavização exponencial dupla apresenta as melhores predições, além de se conseguir ajustar uma política de alocação de recursos.",DISSERTAÇÃO,Gestão de Recursos em Cache Elástico,5238773,01
"A simulação é uma etapa importante no desenvolvimento de sistemas computacionais; nesta, a corretude, comportamento e desempenho do sistema em desenvolvimento são avaliados. SystemC é uma Linguagem de Descrição de Sistemas (SLDL), uma extensão à linguagem C++ para o suporte a diferentes abstrações.
Ela simula todo o sistema sequencialmente sem aproveitar possível potêncial de processamento paralelo. Essa dissertação propõe uma abordagem genérica para permitir a simulação de componentes SystemC, isolados ou agrupados, em um processo distinto, que podem ser escalanodado em diferentes núcleos em um computador ou num sistema distribuído. A principal vantagem desta abordagem é paralelizar simulações SystemC sem necessitar modificações nos modelos. Para tanto, a comunicação assíncrona em nível de transações é encapsulada em comunicações TCP/IP por modulos que abstraem a necessidade do compartilhamento do espaço de endereçamento de memória.",Versão Final_Tiago Rezende Campos Falcão.pdf,SISTEMAS DE COMPUTAÇÃO,TIAGO REZENDE CAMPOS FALCAO,UNIVERSIDADE ESTADUAL DE CAMPINAS,03/02/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'ArchC;SystemC;Programao Concorrente',PROJETOS DE SISTEMAS COMPUTACIONAIS,RODOLFO JARDIM DE AZEVEDO,97,SystemC;ArchC;Programação Concorrente,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A simulação é uma etapa importante no desenvolvimento de sistemas computacionais; nesta, a corretude, comportamento e desempenho do sistema em desenvolvimento são avaliados. SystemC é uma Linguagem de Descrição de Sistemas (SLDL), uma extensão à linguagem C++ para o suporte a diferentes abstrações.
Ela simula todo o sistema sequencialmente sem aproveitar possível potêncial de processamento paralelo. Essa dissertação propõe uma abordagem genérica para permitir a simulação de componentes SystemC, isolados ou agrupados, em um processo distinto, que podem ser escalanodado em diferentes núcleos em um computador ou num sistema distribuído. A principal vantagem desta abordagem é paralelizar simulações SystemC sem necessitar modificações nos modelos. Para tanto, a comunicação assíncrona em nível de transações é encapsulada em comunicações TCP/IP por modulos que abstraem a necessidade do compartilhamento do espaço de endereçamento de memória.",DISSERTAÇÃO,Concurrent SystemC TLM-2 Simulations,4936881,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,4937119,
"Nos últimos anos, com o grande volume de dados gerados, observou-se a necessidade da representação eficiente de informações. Neste sentido a Visualização de Informações teve especial importância devido a sua natureza de auxiliar o usuário na interpretação de dados. Alguns dados mudam ao longo do tempo, essa característica pode produzir um problema de perda de estabilidade visual perante o contexto. O usuário muitas vezes perde informações por não conseguir acompanhar e distinguir as mudanças que possam acontecer no conjunto de dados analisados. Em nosso mestrado, estudamos diversas técnicas de visualização de informação presentes na literatura e propomos uma abordagem de visualização baseada em árvores filogenéticas. Analizamos coleções de documentos que mudam ao longo do tempo, identificando mudanças que poderiam causar perda de estabilidade visual. Nossa proposta sugere a reconstrução dos dados com árvores filogenéticas combinado com técnicas de sumarização (ThemeriverWordclouds) para auxiliar na interpretação das informações. Implementamos também técnicas de projeção de dados (Redução de dimensionalidade) com o intuito de comparar com as visualizações obtidas pelas árvores, identificando os layouts mais eficientes na análise de dados que mudam ao longo do tempo.",,ENGENHARIA DA INFORMAÇÃO,ACAUAN CARDOSO RIBEIRO,UNIVERSIDADE ESTADUAL DE CAMPINAS,22/02/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Visualizao de informao;rvores filogenticas;Projeo de dados',COMPUTAÇÃO VISUAL,GUILHERME PIMENTEL TELLES,80,Visualização de informação;Árvores filogenéticas;Projeção de dados,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Nos últimos anos, com o grande volume de dados gerados, observou-se a necessidade da representação eficiente de informações. Neste sentido a Visualização de Informações teve especial importância devido a sua natureza de auxiliar o usuário na interpretação de dados. Alguns dados mudam ao longo do tempo, essa característica pode produzir um problema de perda de estabilidade visual perante o contexto. O usuário muitas vezes perde informações por não conseguir acompanhar e distinguir as mudanças que possam acontecer no conjunto de dados analisados. Em nosso mestrado, estudamos diversas técnicas de visualização de informação presentes na literatura e propomos uma abordagem de visualização baseada em árvores filogenéticas. Analizamos coleções de documentos que mudam ao longo do tempo, identificando mudanças que poderiam causar perda de estabilidade visual. Nossa proposta sugere a reconstrução dos dados com árvores filogenéticas combinado com técnicas de sumarização (ThemeriverWordclouds) para auxiliar na interpretação das informações. Implementamos também técnicas de projeção de dados (Redução de dimensionalidade) com o intuito de comparar com as visualizações obtidas pelas árvores, identificando os layouts mais eficientes na análise de dados que mudam ao longo do tempo.",DISSERTAÇÃO,Visualização de Mudanças em Coleções de Textos por Árvores de Similaridade,4937590,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,4937969,
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,4938196,
"A Sociedade Brasileira de Computação definiu para o decênio 2006-2016 grandes desafios para a comunidade brasileira de pesquisa em computação. Entre eles, a “Gestão da Informação em grandes volumes de dados multimídia distribuídos” foi abordada, entre outros, por pesquisadores voltados para o estudo de sistemas de suporte a atividades de pesquisa científica, área conhecida como e-science. O tipo de software assim estudado e desenvolvido trouxe a possibilidade de promover o compartilhamento e reuso de dados científicos, algo benéfico para a ciência sob vários aspectos. Entretanto, o uso destas ferramentas que facilitariam o compartilhamento de subprodutos de pesquisa não foi seguido do aumento esperado na disponibilidade destes dados. Este comportamento inesperado dos usuários perante uma funcionalidade dos sistemas constitui-se claramente um problema de pesquisa para a área de Interação Humano-Computador. Para entender como cientistas interagem com artefatos tecnológicos durante o processo de construção de conhecimento científico, o estudo sociológico de Bruno Latour sobre o trabalho de cientistas alinha-se à necessidade, além de ser o ponto de partida para o desenvolvimento do referencial teórico conhecido como Actor-Network Theory ¬– ANT. Posteriormente, esta teoria foi expandida para compreender a participação de artefatos tecnológicos em fenômenos sociais em geral. Considerando-se também que comunidades científicas podem ser entendidas como grupos de pessoas organizadas para um propósito, buscamos nas bases teóricas da Semiótica Organizacional – SO – e da Socially Aware Computing – SAC – os fundamentos que apoiam a compreensão dos processos internos às organizações e entre estas e as sociedades das quais fazem parte. Esta tese objetiva articular os referenciais teóricos da ANT e da SAC, permitindo, por um lado, compreender uma comunidade científica pelo ponto de vista organizacional, e por outro, entender o papel que diversos elementos não-humanos desempenham na mediação de interesses entre cientistas. Foi proposto um método para clarificar os interesses envolvendo o desenvolvimento e uso de um software, e um mecanismo de visualização de grupos sociais que possibilita uma interpretação qualitativa da participação conjunta de pessoas e elementos não-humanos. A proposta foi aplicada no estudo de um software voltado ao gerenciamento de dados de pesquisa, no mapeamento de diversas comunidades científicas por meio de suas produções bibliográficas, e construindo um sistema para apoio ao processo científico de revisão de literatura – o Quid. Os resultados obtidos demonstram a viabilidade da proposta de união das teorias e métodos. Os caminhos das influências e interesses divergentes no desenvolvimento de sistemas de e-science revelam a não-neutralidade dos desenvolvedores de software – contrariamente a como são tradicionalmente enxergados. A abordagem de rede heterogênea permitiu visualizar comunidades científicas como um fenômeno social coeso, onde os participantes mais importantes são ressaltados, com menor suscetibilidade a vieses comuns em abordagens bibliométricas tradicionais, como a auto-referência e a relevância restrita a sub-comunidades. Como resultado final, acreditamos ter demonstrado a necessidade da compreensão do papel de artefatos digitais como participantes em fenômenos humanos, e validado a possiblidade desta compreensão por meio do ferramental conjunto da ANT com SAC.",Versão Final_Alysson Bolognesi Prado.pdf,ENGENHARIA DA INFORMAÇÃO,ALYSSON BOLOGNESI PRADO,UNIVERSIDADE ESTADUAL DE CAMPINAS,07/03/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Actor-Network Theory;Socially-Aware Computing;redes sociotcnicas',SISTEMAS DE INFORMAÇÃO,MARIA CECILIA CALANI BARANAUSKAS,148,Actor-Network Theory;Socially-Aware Computing;redes sociotécnicas,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A Sociedade Brasileira de Computação definiu para o decênio 2006-2016 grandes desafios para a comunidade brasileira de pesquisa em computação. Entre eles, a “Gestão da Informação em grandes volumes de dados multimídia distribuídos” foi abordada, entre outros, por pesquisadores voltados para o estudo de sistemas de suporte a atividades de pesquisa científica, área conhecida como e-science. O tipo de software assim estudado e desenvolvido trouxe a possibilidade de promover o compartilhamento e reuso de dados científicos, algo benéfico para a ciência sob vários aspectos. Entretanto, o uso destas ferramentas que facilitariam o compartilhamento de subprodutos de pesquisa não foi seguido do aumento esperado na disponibilidade destes dados. Este comportamento inesperado dos usuários perante uma funcionalidade dos sistemas constitui-se claramente um problema de pesquisa para a área de Interação Humano-Computador. Para entender como cientistas interagem com artefatos tecnológicos durante o processo de construção de conhecimento científico, o estudo sociológico de Bruno Latour sobre o trabalho de cientistas alinha-se à necessidade, além de ser o ponto de partida para o desenvolvimento do referencial teórico conhecido como Actor-Network Theory ¬– ANT. Posteriormente, esta teoria foi expandida para compreender a participação de artefatos tecnológicos em fenômenos sociais em geral. Considerando-se também que comunidades científicas podem ser entendidas como grupos de pessoas organizadas para um propósito, buscamos nas bases teóricas da Semiótica Organizacional – SO – e da Socially Aware Computing – SAC – os fundamentos que apoiam a compreensão dos processos internos às organizações e entre estas e as sociedades das quais fazem parte. Esta tese objetiva articular os referenciais teóricos da ANT e da SAC, permitindo, por um lado, compreender uma comunidade científica pelo ponto de vista organizacional, e por outro, entender o papel que diversos elementos não-humanos desempenham na mediação de interesses entre cientistas. Foi proposto um método para clarificar os interesses envolvendo o desenvolvimento e uso de um software, e um mecanismo de visualização de grupos sociais que possibilita uma interpretação qualitativa da participação conjunta de pessoas e elementos não-humanos. A proposta foi aplicada no estudo de um software voltado ao gerenciamento de dados de pesquisa, no mapeamento de diversas comunidades científicas por meio de suas produções bibliográficas, e construindo um sistema para apoio ao processo científico de revisão de literatura – o Quid. Os resultados obtidos demonstram a viabilidade da proposta de união das teorias e métodos. Os caminhos das influências e interesses divergentes no desenvolvimento de sistemas de e-science revelam a não-neutralidade dos desenvolvedores de software – contrariamente a como são tradicionalmente enxergados. A abordagem de rede heterogênea permitiu visualizar comunidades científicas como um fenômeno social coeso, onde os participantes mais importantes são ressaltados, com menor suscetibilidade a vieses comuns em abordagens bibliométricas tradicionais, como a auto-referência e a relevância restrita a sub-comunidades. Como resultado final, acreditamos ter demonstrado a necessidade da compreensão do papel de artefatos digitais como participantes em fenômenos humanos, e validado a possiblidade desta compreensão por meio do ferramental conjunto da ANT com SAC.",TESE,Uma Análise do Papel de Sistemas Computacionais como Mediadores em Redes Sociotécnicas Científicas sob a Ótica da Actor-Network Theory,4968022,1
"Foram testados 16 algoritmos de regressão (random forest, support vector machine - linear, polinomial e radial -, 1-hidden-layer neural network, gradient boosting machine, k-nearest neighbor, generalized linear model com regularização lasso ou elasticnet, multivariate adaptive regression splines, cubist, relevance vector machine, partial least squares, principal component regression, extreme learning machine, RBF network e gaussian process) em 59 datasets reais, com as métricas MAE e MSE. Os algoritmos foram comparados segundo os testes de Friedman com post-hoc Nemenyi e Wilcoxon corrigido por Hommel e por meio de análise bayesiana. Os resultados sugerem que o melhor algoritmo de regressão é o cubist, ainda que para fins práticos, em datasets muito grandes, a melhor opção seja o gradient boosting machine.",,ENGENHARIA DA INFORMAÇÃO,GIOVANI FRONDANA,UNIVERSIDADE ESTADUAL DE CAMPINAS,16/03/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'algoritmos de regresso;comparao;datasets UCI',INFERÊNCIA EM DADOS COMPLEXOS,JACQUES WAINER,63,algoritmos de regressão;comparação;datasets UCI,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Foram testados 16 algoritmos de regressão (random forest, support vector machine - linear, polinomial e radial -, 1-hidden-layer neural network, gradient boosting machine, k-nearest neighbor, generalized linear model com regularização lasso ou elasticnet, multivariate adaptive regression splines, cubist, relevance vector machine, partial least squares, principal component regression, extreme learning machine, RBF network e gaussian process) em 59 datasets reais, com as métricas MAE e MSE. Os algoritmos foram comparados segundo os testes de Friedman com post-hoc Nemenyi e Wilcoxon corrigido por Hommel e por meio de análise bayesiana. Os resultados sugerem que o melhor algoritmo de regressão é o cubist, ainda que para fins práticos, em datasets muito grandes, a melhor opção seja o gradient boosting machine.",DISSERTAÇÃO,Comparação Empírica de 16 Algoritmos de Regressão em 59 Datasets,5007437,1
"Remote sensing is a practice that allows, by means of sensor technologies, to analyze objects at long distances without making physical contact with them. Currently, its contribution for natural sciences is enormous, since it is possible to acquire images of target objects not only in the visible regions of the electromagnetic spectrum, but also in a much wider range. Working with images composed by various spectral bands demands the treatment of huge amounts of data associated with single entities, which affects negatively the performance in prediction tasks, so dimensionality reduction is mandatory. This work introduces a feature extraction approach, based on spectral indices learned by Genetic Programming (GP), to project data from pixel values into new feature spaces aiming to improve classification accuracy. Spectral indices are functions that map the reflectance of remotely sensed objects (in specific wavelength intervals) into a real values that can be interpreted as the abundance of features of interest. Through GP, it is possible to learn indices that maximize the separability of samples from two different classes. After the indices specialized for all the pairs of classes are obtained, the proposed method introduces two different approaches to fuse them into a pixel classification system. Results for the binary and multi-class scenarios show that the proposed method in competitive with respect to traditional dimensionality reduction techniques. Additional experiments in tropical biomes seasonal analysis shows clearly the superiority of GP-based spectral indices over man-made ones for discrimination purposes, regardless its specificity.",,ENGENHARIA DA INFORMAÇÃO,JUAN FELIPE HERNANDEZ ALBARRACIN,UNIVERSIDADE ESTADUAL DE CAMPINAS,17/03/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Genetic Programming;Remote Sensing;Spectral Indices',INFERÊNCIA EM DADOS COMPLEXOS,RICARDO DA SILVA TORRES,80,Genetic Programming;Remote Sensing;Spectral Indices,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Sensoriamento remoto é a prática que permite, por meio de sensores, analisar objetos a longas distâncias sem estabelecer contato físico com eles. Atualmente, sua contribuição em ciências naturais é enorme, dado que é possível adquirir imagens de objetos alvos em mais regiões do espectro eletromagnético além do canal visível. Trabalhar com imagens compostas por múltiplas bandas espectrais requer tratar com grandes quantidades de informação associada a uma única entidade, coisa que afeta negativamente o desempenho de algoritmos de predição. Portanto, o uso de técnicas de redução da dimensionalidade é mandatório. Este trabalho apresenta uma abordagem de extração de características baseada em índices espectrais aprendidos por Programação Genética (GP), que projetam os dados associados aos pixels em novos espaços de características. O objetivo é  aprimorar a acurácia de algoritmos de classificação. Índices espectrais são funções que relacionam a refletância, em canais específicos do espectro, com valores reais que podem ser interpretados como a abundância de características de interesse de objetos captados à distância. Com GP, é possível aprender índices que maximizam a separabilidade de amostras de duas classes diferentes. A partir dos índices especializados para cada par possível de classes são obtidos, propomos duas abordagens diferentes para combiná-los e construir um sistema de classificação de pixels. Os resultados obtidos para os cenários binário e multi-classe mostram que o método proposto é competitivo com respeito a técnicas tradicionais de redução da dimensionalidade. Experimentos adicionais aplicando o método para análise sazonal de biomas tropicais mostram claramente a superioridade de índices aprendidos por GP para propósitos de discriminação, com respeito a índices desenvolvidos por especialistas, independentemente da especificidade do problema.",DISSERTAÇÃO,Genetic-Programming-based Spectral Indices for Remote Sensing Image Classification,5007447,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5007448,
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5007542,
"Image simplification has been proved useful in several image processing applications as an additional step for more complex tasks, such as segmentation and feature extraction. 
In this Master's Thesis, we explore a graph-based simplification method that guarantees a well-behaved suppression of the image extrema (maxima or minima) by taking into account both information of distance and contrast, as well as some interesting aspects of the scale-space theory.
By highlighting some new properties of the method, we define a local update of the graph which implies an interesting bypass in the whole algorithm structure which, originally, is very time-consuming.
Finally, we illustrate how to combine this simplification process with well-known morphological tools to approach problems related mainly to multi-scale image segmentation and homogenization.",,ENGENHARIA DA INFORMAÇÃO,DARWIN DANILO SAIRE PILCO,UNIVERSIDADE ESTADUAL DE CAMPINAS,31/03/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'image simplification;scale-space;mathematical morphology',SISTEMAS DE INFORMAÇÃO,ALEXANDRE XAVIER FALCAO,70,image simplification;scale-space;mathematical morphology,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A simplificação de imagens tem mostrado ser útil em diversas aplicações de processamento de imagens como um passo adicional para tarefas mais complexas, como a segmentação e extração de características. Nesta Dissertação de Mestrado, exploramos um método de simplificação baseado em grafos que garante o bom comportamento da supressão dos nos extremos da imagem (máximas ou mínimas), tendo em conta a informação da distância e contraste, bem como alguns aspectos interessantes da teoria do espaço-escala. Para destacar algumas novas propriedades do método, nós definimos uma atualização local do grafo que implica um desvio interessante em toda a estrutura do algoritmo que, originalmente, é muito custoso. 
Finalmente, nós mostramos como combinar este processo de simplificação com ferramentas morfológicas bem conhecidas para abordar problemas relacionados, principalmente, com segmentação de imagens multi-escala e homogeneização.",DISSERTAÇÃO,Multi-scale Morphological Image Simplification Based on Extrema Relationships: Improvements and Applications,5007590,1
"Um esquema de assinatura digital é uma importante ferramenta para a criptografia de chave pública e uma tecnologia essencial para prover autenticidade, integridade e não-repúdio de dados.
Assinaturas digitais são importantes para muitas aplicações práticas, tais como: eCommerce, eGovernment, distribuição de software, dentre outras aplicações. 
Esquemas de assinaturas digitais muito utilizados como o RSA, o DSA, o ECDSA e o EdDSA, não são imunes aos computadores quânticos, pois sua segurança depende da dificuldade de fatorar grandes números inteiros ou calcular logaritmos discretos. 
O esquema de assinatura digital de Merkle e suas variantes são baseados em funções de resumo e são considerados resistentes aos computadores quânticos. 
Estes esquemas são promissores candidatos a esquemas de assinatura digitais com segurança quântica e tem sido objeto de esforços para padronização. 
Neste trabalho, apresenta-se uma otimizada implementação em software das duas propostas de padrão para o esquema digital de Merkle e suas variantes usando um conjunto de instruções AVX2 nos processadores Haswell e Skylake. 
A implementação usa a abordagem “multi-buffer” para acelerar a função de resumo subjacente e, consequentemente, os algoritmos de geração de chaves, assinatura e verificação. Os parâmetros foram alinhados com os parâmetros definidos nas propostas do LMS e XMSS. A performance dos resultados da implementação é medida em um moderno processador Haswell (Intel Core i7 de 3,4 GHz) e Skylake (Intel Core i7 de 4.20 GHz).
Em particular, a operação de assinatura do esquema XMSS, usando SHA2-256 no processador Skylake, pode ser calculada em 3.841.199 ciclos (1.043 assinaturas por segundo) para o nível de segurança de 128 bits (contra ataques quânticos), usando uma árvore de altura 60 com 12 camadas. Para o LMS, a mesma operação requer 1.307.376 ciclos (3.065 assinaturas por segundo).
Os resultados indicam que ambas as propostas LMS e XMSS, para esquemas de assinaturas baseadas em funções de resumo, têm alto desempenho usando uma implementação vetorial nos modernos processadores da Intel.",,SISTEMAS DE COMPUTAÇÃO,ANA KARINA DOURADO SALINA DE OLIVEIRA,UNIVERSIDADE ESTADUAL DE CAMPINAS,07/04/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'assinatura digital;criptografia ps quntica;assinatura de Merkle',SEGURANÇA E CRIPTOGRAFIA APLICADA,JULIO CESAR LOPEZ HERNANDEZ,110,assinatura digital;criptografia pós quântica;assinatura de Merkle,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Um esquema de assinatura digital é uma importante ferramenta para a criptografia de chave pública e uma tecnologia essencial para prover autenticidade, integridade e não-repúdio de dados.
Assinaturas digitais são importantes para muitas aplicações práticas, tais como: eCommerce, eGovernment, distribuição de software, dentre outras aplicações. 
Esquemas de assinaturas digitais muito utilizados como o RSA, o DSA, o ECDSA e o EdDSA, não são imunes aos computadores quânticos, pois sua segurança depende da dificuldade de fatorar grandes números inteiros ou calcular logaritmos discretos. 
O esquema de assinatura digital de Merkle e suas variantes são baseados em funções de resumo e são considerados resistentes aos computadores quânticos. 
Estes esquemas são promissores candidatos a esquemas de assinatura digitais com segurança quântica e tem sido objeto de esforços para padronização. 
Neste trabalho, apresenta-se uma otimizada implementação em software das duas propostas de padrão para o esquema digital de Merkle e suas variantes usando um conjunto de instruções AVX2 nos processadores Haswell e Skylake. 
A implementação usa a abordagem “multi-buffer” para acelerar a função de resumo subjacente e, consequentemente, os algoritmos de geração de chaves, assinatura e verificação. Os parâmetros foram alinhados com os parâmetros definidos nas propostas do LMS e XMSS. A performance dos resultados da implementação é medida em um moderno processador Haswell (Intel Core i7 de 3,4 GHz) e Skylake (Intel Core i7 de 4.20 GHz).
Em particular, a operação de assinatura do esquema XMSS, usando SHA2-256 no processador Skylake, pode ser calculada em 3.841.199 ciclos (1.043 assinaturas por segundo) para o nível de segurança de 128 bits (contra ataques quânticos), usando uma árvore de altura 60 com 12 camadas. Para o LMS, a mesma operação requer 1.307.376 ciclos (3.065 assinaturas por segundo).
Os resultados indicam que ambas as propostas LMS e XMSS, para esquemas de assinaturas baseadas em funções de resumo, têm alto desempenho usando uma implementação vetorial nos modernos processadores da Intel.",TESE,Implementação Eficiente em Software do Esquema de Assinatura de Merkle e suas Variantes,5007592,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5007595,
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5007598,
"Software cost estimation is a critical step in software project management, and its main driver are requirements. Some algorithmic methods use as inputs only functional requirements and others take also into account non-functional requirements.

The goal of this study is to understand the correlation of using non-functional requirements on the accuracy of software cost estimation algorithmic methods. A systematic literature review was conducted to learn which non-functional requirements are used, how they are used, and their effects on estimation accuracy.

The systematic review shows that only 33% of 39 algorithmic methods use non-functional requirements. However, the investigation on its correlation with estimation accuracy was not conclusive from published results. In order to address this issue, a quasi-experiment was conducted on publicly available datasets. This experiment shows that the use of non-functional requirements results in a reduction of about 30% in the estimation error, with statistically significant confidence.",,SISTEMAS DE COMPUTAÇÃO,STALLIN ESTEFFERSON FERREIRA DA SILVA,UNIVERSIDADE ESTADUAL DE CAMPINAS,26/04/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'systematic review;requirement engineering;software cost estimation;software engineering',PROJETOS DE SISTEMAS COMPUTACIONAIS,MARIO LUCIO CORTES,85,revisão sistemática;requisitos de software;estimativa de esforço de software;engenharia de software,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Em gerenciamento de projetos computacionais, uma etapa bastante complicada e importante é a estimativa de esforço a partir dos requisitos do projeto de software. Diversos métodos de estimativa de esforço foram propostos nas últimas décadas, todos com o objetivo de prever o esforço e custo do projeto com baixas taxas de erro.

No entanto, muitos dos métodos algorítmicos de estimativa de esforço propostos ignoram os requisitos não-funcionais na modelagem de suas variáveis de entrada, outros métodos somente os utilizam parcialmente.

O objetivo deste trabalho é entender o uso dos requisitos não-funcionais nos métodos de estimativa de esforço e reforçar a importância da utilização deles para uma estimativa precisa. Uma revisão sistemática foi conduzida para verificar quais requisitos não-funcionais são usados, como eles são usados e quais seus efeitos sobre o erro da estimativa.

A revisão sistemática mostrou que apenas 33% dos 39 métodos algorítmicos usam requisitos não-funcionais, por sua vez, a correlação entre a precisão da estimativa com o uso deles foi inconclusiva. Para compreender tal correção, um quase-experimento foi realizado em datasets disponíveis publicamente na literatura. Este experimento mostrou que o uso de requisitos não-funcionais resultam em uma redução de aproximadamente 30% no erro da estimativa do software, com garantia estatística.",DISSERTAÇÃO,Uso de requisitos não-funcionais na estimativa de esforço de software: revisão sistemática e resultados experimentais,5007603,1
"In 2004, Google released a paper in which they described how their MapReduce framework worked and how they structured their distributed file system. This innovation made the open software communities to organize themselves in order to develop open source alternatives for the Google's MapReduce framework. In this work, we focused in two of these systems: Hadoop and Disco. The first was developed mainly in Java and the former in Erlang, and we also discuss how these languages influenced the architecture and the behavior of these systems.

Speacilly we looked in how these systems address the problem related to heterogeneous workloads which are composed of research applications (low priority and long duration) and production (high priority and short duration) in the same cluster. We analyzed deeply how these systems work and which are the mechanisms they use to help in the scheduling decision taking. Lastly, we proposed the use of a Fair scheduling policy based in using preemption of tasks to guide the scheduler of Disco to give emphasis in the production application without giving up of being fair. As a consequence the production application quickly gets the resources designated to it in a Fair division. For this to happen, tasks with the lower priority application (research) are affected by preemption.

We also contributed to the communities of Hadoop and Disco: we collaborated in the development of a preemption policy and checkpointing service for Hadoop and developed a Fair scheduling policy based on preemption for Disco to help it prioritize production applications with good results in the execution time for our experimental workload. Another contribution of this work was an Web interface for Disco to help in the reproduction of our experiments.",,SISTEMAS DE COMPUTAÇÃO,AUGUSTO RODRIGUES DE SOUZA,UNIVERSIDADE ESTADUAL DE CAMPINAS,28/04/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'MapReduce;Scheduler;Disco;Hadoop;Fair Scheduling Policy',SISTEMAS DISTRIBUÍDOS,ISLENE CALCIOLARI GARCIA,75,MapReduce;Escalonador;Disco;Hadoop;Política de Escalonamento Fair,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Em 2004, o Google surpreendeu a comunidade de sistemas distribuídos ao divulgar como funcionava seu framework de MapReduce e seu sistema de arquivos distribuído. Tal inovação rapidamente chamou a atenção das comunidades de software livre e diversos sistemas de MapReduce de código aberto foram criados implementando as ideias divulgadas pelo Google. Neste trabalho, focamos nossa atenção em dois deles, chamados Hadoop e Disco. O primeiro é desenvolvido principalmente em Java e o segundo em Erlang e discutimos um pouco sobre como essas linguagens influenciaram a arquitetura e o funcionamento destes sistemas. 

Focamos em especial em como esses sistemas endereçam o problema das cargas heterogêneas, ou seja, compostas por aplicações de pesquisa (baixa prioridade e longa duração) e de produção (alta prioridade e baixa duração) em um mesmo cluster. Analisamos a fundo como esses sistemas funcionam, principalmente, quais são os mecanismos que auxiliam na tomada de decisão sobre agendamento de tarefas aos escalonadores. Por fim, propusemos o uso de uma política de escalonamento Fair e baseada em preempção de tarefas para auxiliar o escalonador do Disco a dar ênfase à aplicação de produção sem deixar de ser justo, ou seja, faz com que rapidamente a aplicação de produção atinja a quantidade de recursos do cluster que por pertence a ela em uma divisão justa dos mesmos. Para isso tarefas da aplicação menos prioritária (de pesquisa) sofrem preempção. 

Contribuímos para as comunidades do Hadoop e do Disco ao longo desse mestrado: colaboramos na codificação de uma política de preempção e checkpointing ao Hadoop e desenvolvemos uma política justa e que se utiliza de preempção no Disco para priorizar as aplicações de produção com resultados significativos no tempo de execução da nossa carga experimental. Outra contribuição deste trabalho é uma interface Web auxiliar ao Disco para reprodução dos nossos experimentos.",DISSERTAÇÃO,Mecanismos para Escalonamento de Aplicações MapReduce de Diferentes Prioridades,5007648,1
"The Alzheimer's disease (AD) is the main cause of dementia in the world, affecting 47.5 million people. In Brazil, it is estimated that 1.2 million people have the disease, with prevalence expected to double by 2030. AD is a chronic neurodegenerative disorder that causes problems in memory, thinking and behavior. Because of its severity, its early diagnosis is crucial to improve the patient's quality of life and reduce the high economic impacts caused by the disease. However, an accurate diagnosis is a complex task that requires cognitive and objective tests, patient records, clinical and laboratory exams. In this context, machine learning techniques have been investigated to aid the diagnosis, aiming the discovery of neuroimaging biomarkers (image descriptors) related to neurodegenerative patterns. This work presents a study about the search for these biomarkers, proposes a new image descriptor, named Residual Center of Mass (RCM), and validates this descriptor in the problem of classifying images of individuals with and without AD. The experiments involved images labeled as belonging to individuals with AD and individuals without AD, acquired by Magnetic Resonance Imaging (MRI) and Fluorodeoxyglucose-Positron Emission Tomography (FDG-PET), provided by the Alzheimer's Disease Neuroimaging Initiative. The RCM descriptor explores image invariant moments and other operations to enhance peripheral regions of the brain. The final description of the images is obtained by selecting the most relevant regions to the problem, determined by Analysis of Variance (ANOVA). The pattern classification uses Support Vector Machine (SVM) and the results show superior or equal performances compared to state-of-the-art methods. Performing 10-folds cross-validation, we achieved correct classification rates of 95.1% and 90.3% for 507 FDG-PET and 1374 MRI scans, respectively. The brain regions, when ordered by relevance for the detection of AD, show that the most discriminative ones coincide with the findings in Medicine.",,ENGENHARIA DA INFORMAÇÃO,ALEXANDRE YUKIO YAMASHITA,UNIVERSIDADE ESTADUAL DE CAMPINAS,28/04/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,"b""image analysis;neuroimaging;machine learning;Alzheimer's disease;biomarker""",COMPUTAÇÃO VISUAL,NEUCIMAR JERONIMO LEITE,90,análise de imagem;neuroimagem;aprendizado de máquina;doença de Alzheimer;biomarcador,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A doença de Alzheimer (DA) é a principal causa de demência no mundo, afetando 47.5 milhões de pessoas. No Brasil, estima-se que a doença atinja cerca de 1.2 milhão de pessoas, com previsão de dobrar a prevalência até 2030. A DA é uma patologia neurodegenerativa crônica que causa problemas de memória, pensamento e comportamento. Face a sua gravidade, seu diagnóstico precoce é crucial para melhorar a qualidade de vida do paciente e reduzir os altos impactos econômicos causados pela doença. No entanto, o diagnóstico preciso é um problema complexo que exige testes cognitivos e objetivos, registro histórico do paciente, exames clínicos e laboratoriais. Neste contexto, técnicas de aprendizado de máquina têm sido investigadas para auxiliar o diagnóstico através da descoberta de biomarcadores em neuroimagem (descritores de imagem) relacionados a padrões neurodegenerativos. Este trabalho apresenta um estudo sobre a busca desses biomarcadores, propõe um novo descritor de imagem, denominado Residual Center of Mass (RCM), e valida este descritor no problema de diferenciar imagens de indivíduos sem e com DA. Os experimentos envolveram imagens anotadas como pertencentes a indivíduos com DA e a indivíduos sem DA, provenientes de Ressonância Magnética (RM) e de Tomografia por Emissão de Pósitrons (TEP), e disponibilizadas pela Alzheimer's Disease Neuroimaging Initiative. O descritor RCM explora momentos invariantes de imagens e outras operações para realçar regiões periféricas do cérebro. A descrição final das imagens é obtida por seleção das regiões mais relevantes ao problema, determinadas por Análise de Variância (ANOVA). A classificação de padrões usa Support Vector Machine (SVM) e os resultados demonstram desempenhos superiores ou equivalentes em comparação aos de métodos do estado da arte. Através de validação cruzada, k-fold com k = 10, foram alcançadas taxas de acerto de 95.1% e 90.3%, sobre 507 imagens de TEP e 1374 imagens de RM, respectivamente. As regiões do cérebro, quando ordenadas por relevância à detecção de DA, demonstram que as mais discriminativas coincidem com os achados da Medicina.",DISSERTAÇÃO,Descoberta de Biomarcadores em Neuroimagem Associados à Doença de Alzheimer,5007649,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5008105,
"Algoritmos de consenso e de difusão totalmente ordenada são centrais para a implementação de aplicações distribuídas tolerantes a falhas. Paxos é um algoritmo assíncrono de consenso comumente empregado para a implementação de difusão totalmente ordenada. De forma breve, Paxos proporciona que mensagens enviadas a um conjunto de processos sejam a eles entregues em uma mesma ordem total. Uma funcionalidade que é simples de se obter sob condições normais de operação de um sistema distribuído, mas que se torna reconhecidamente complicada quando se observa uma mescla de assincronia e falhas. O projeto assíncrono de Paxos visa assegurar um comportamento consistente sob condições particularmente adversas de operação: a robustez é o seu maior atributo. Mas quando se trata da operação regular do sistema, sem falhas e predominantemente síncrona, Paxos peca por não oferecer um desempenho compatível com os recursos disponíveis.
A busca por formas de se melhorar o desempenho de Paxos ganha importância quando o algoritmo passa a compor o núcleo de várias soluções de replicação. De fato, múltiplas variações de Paxos destinadas a obter difusão totalmente ordenada de alto desempenho foram propostas em paralelo ao nosso Doutorado. Elas abordam limitações do algoritmo original e sugerem alterações, que vão desde a reorganização da topologia até otimizações no uso da rede pelos processos. A nossa abordagem difere das existentes, ou paralelas, por envolver a adoção explícita de pressupostos de sincronia como forma de aprimorar o desempenho de Paxos. Trata-se de incorporar a um algoritmo essencialmente assíncrono abordagens empregadas em modelos mais restritivos, como o modelo síncrono.
Esta tese relata os resultados obtidos a partir da abordagem de agregar hipóteses de sincronia a Paxos como forma de aprimorar seu desempenho. Nossas contribuições podem ser sintetizadas da seguinte forma. Primeiramente, mostramos que uma rede local, dado que a ela se apliquem mecanismos de controle de carga, se comporta na maior parte do tempo de forma predominantemente síncrona. Este resultado é atestado por, e propiciou o desenvolvimento de dois algoritmos de difusão totalmente ordenada. O Time Hybrid Total Order Broadcast (THyTOB) representa a possiblidade efetiva de se implementar um algoritmo de difusão totalmente ordenada essencialmente síncrono que opera sobre um sistema tipicamente modelado como assíncrono. THyTOB não apenas apresenta um desempenho, na ausências de falhas, superior a implementações tradicionais de Paxos, como também se destacou por sua reduzida variação de latências: é um algoritmo estável com desempenho bastante previsível. Apresentamos também nosso On-Time Fast Paxos, um algoritmo que usa tempo para gerar uma ordenação total para a as mensagens, que é então ratificada por instâncias de Fast Paxos. On-Time Fast Paxos apresenta altas vazões, da ordem da capacidade da rede, enquanto provê latências baixas e bem condicionadas. Trata-se da comprovação de nossa tese: sincronia melhora o desempenho de Paxos.",Versão Final_Daniel Cason.pdf,SISTEMAS DE COMPUTAÇÃO,DANIEL CASON,UNIVERSIDADE ESTADUAL DE CAMPINAS,31/03/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Sistemas Distribudos;Tolerncia a Falhas;Difuso Totalmente Ordenada',SISTEMAS DISTRIBUÍDOS,LUIZ EDUARDO BUZATO,102,Sistemas Distribuídos;Tolerância a Falhas;Difusão Totalmente Ordenada,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Algoritmos de consenso e de difusão totalmente ordenada são centrais para a implementação de aplicações distribuídas tolerantes a falhas. Paxos é um algoritmo assíncrono de consenso comumente empregado para a implementação de difusão totalmente ordenada. De forma breve, Paxos proporciona que mensagens enviadas a um conjunto de processos sejam a eles entregues em uma mesma ordem total. Uma funcionalidade que é simples de se obter sob condições normais de operação de um sistema distribuído, mas que se torna reconhecidamente complicada quando se observa uma mescla de assincronia e falhas. O projeto assíncrono de Paxos visa assegurar um comportamento consistente sob condições particularmente adversas de operação: a robustez é o seu maior atributo. Mas quando se trata da operação regular do sistema, sem falhas e predominantemente síncrona, Paxos peca por não oferecer um desempenho compatível com os recursos disponíveis.
A busca por formas de se melhorar o desempenho de Paxos ganha importância quando o algoritmo passa a compor o núcleo de várias soluções de replicação. De fato, múltiplas variações de Paxos destinadas a obter difusão totalmente ordenada de alto desempenho foram propostas em paralelo ao nosso Doutorado. Elas abordam limitações do algoritmo original e sugerem alterações, que vão desde a reorganização da topologia até otimizações no uso da rede pelos processos. A nossa abordagem difere das existentes, ou paralelas, por envolver a adoção explícita de pressupostos de sincronia como forma de aprimorar o desempenho de Paxos. Trata-se de incorporar a um algoritmo essencialmente assíncrono abordagens empregadas em modelos mais restritivos, como o modelo síncrono.
Esta tese relata os resultados obtidos a partir da abordagem de agregar hipóteses de sincronia a Paxos como forma de aprimorar seu desempenho. Nossas contribuições podem ser sintetizadas da seguinte forma. Primeiramente, mostramos que uma rede local, dado que a ela se apliquem mecanismos de controle de carga, se comporta na maior parte do tempo de forma predominantemente síncrona. Este resultado é atestado por, e propiciou o desenvolvimento de dois algoritmos de difusão totalmente ordenada. O Time Hybrid Total Order Broadcast (THyTOB) representa a possiblidade efetiva de se implementar um algoritmo de difusão totalmente ordenada essencialmente síncrono que opera sobre um sistema tipicamente modelado como assíncrono. THyTOB não apenas apresenta um desempenho, na ausências de falhas, superior a implementações tradicionais de Paxos, como também se destacou por sua reduzida variação de latências: é um algoritmo estável com desempenho bastante previsível. Apresentamos também nosso On-Time Fast Paxos, um algoritmo que usa tempo para gerar uma ordenação total para a as mensagens, que é então ratificada por instâncias de Fast Paxos. On-Time Fast Paxos apresenta altas vazões, da ordem da capacidade da rede, enquanto provê latências baixas e bem condicionadas. Trata-se da comprovação de nossa tese: sincronia melhora o desempenho de Paxos.",TESE,The role of synchrony on the performance of Paxos,5010922,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5012399,
"Automatic identification of animal species based on their sounds is one of the means to conduct research in bioacoustics. This domain provides, for instance, ways to monitor species, analyze changes in ecological communities, or to understand the real meaning of the animal calls. Identification mechanisms are typically executed in two stages: feature extraction, and pattern matching. Both stages present challenges, in computer science and in bioacoustics. The choice of effective feature extraction and classification techniques is a challenge on any audio recognition system, especially in bioacoustics domain. Given the wide variety of animal groups studied, algorithms are tailored to specific groups. Pattern matching techniques are also sensitive to the features extracted, and conditions surrounding the recordings. As a results, most bioacoustics softwares are not extensible, therefore limiting the kinds of recognition experiments that can be conducted. Given this scenario, this dissertation proposes a software architecture that allows multiple feature extraction, feature fusion and classification algorithms to support scientists on the identification of animal species through their recorded sounds. This architecture was implemented by the WASIS software, freely available on the Web. A number of algorithms were implemented, serving as the basis for a comparative study that recommends sets of feature extraction and classification algorithms for three animal groups.",Versão Final_Leandro Tacioli.pdf,ENGENHARIA DA INFORMAÇÃO,LEANDRO TACIOLI,UNIVERSIDADE ESTADUAL DE CAMPINAS,03/07/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'eScience;Sound Recognition;Feature Extraction;Classification;Bioacoustics',SISTEMAS DE INFORMAÇÃO,CLAUDIA MARIA BAUZER MEDEIROS,51,eScience;Reconhecimento de Sons;Extração de Descritores;Classificação;Bioacústica,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Identificação automática de espécies de animais através de seus sons é um dos meios de conduzir pesquisa em bioacústica. Este domínio fornece, por exemplo, métodos de monitoramento de espécies, análises de mudanças em comunidades ecológicas, ou o entendimento do real significado dos sons dos animais. Mecanismos de identificação são tipicamente executados em dois estágios: extração de descritores e reconhecimento de padrões. Ambos estágios apresentam desafios, tanto em ciência da computação quanto na bioacústica. A escolha de extração de descritores e técnicas de classificação eficientes é um desafio em qualquer sistema de reconhecimento de áudio, especialmente no domínio da bioacústica. Dada a grande variedade de grupos de animais estudados, algoritmos são adaptados a grupos específicos. Técnicas de reconhecimento de padrões também são sensíveis aos descritores extraídos e condições em torno das gravações. Como resultado, muitas aplicações em bioacústica não são expansíveis, limitando os tipos de experimentos de reconhecimentos que podem ser conduzidos. Dado tal cenário, esta dissertação propõe uma arquitetura de software que permita múltiplos algoritmos de extração de descritores, fusão entre descritores e algoritmos de classificação a fim de auxiliar cientistas na identificação de animais através de seus sons. Esta arquitetura foi implementada no software WASIS, gratuitamente disponível na Internet. Diversos algoritmos foram implementados, servindo como base para um estudo comparativo que recomenda conjuntos de algoritmos de extração de descritores e classificação para três grupos de animais.",DISSERTAÇÃO,WASIS - Bioacoustic Species Identification based on Multiple Feature Extraction and Classification Algorithms,5022457,1
"Software for medical training usually follows two main approaches for the representation of its data. Simulation-based training software – e.g., to control simulation mannequins – which have highly structured representations of the clinical data and simulation plans. Moreover, some systems focus on the narrative of a clinical case in free-text format – e.g., some mobile apps as Prognosis and the Jacinto emergency medicine learning environment. In this case, the clinical data mixes with the narrative in unstructured format. Thus, we propose a model that combines both approaches in a hybrid narrative and clinical knowledge base for emergency medicine training. We hypothesize that by connecting narratives with structured clinical information, we can take advantage of the strongest points of each approach. On the one hand, structured clinical data offers flexibility for the production of case variations and alternative plans, which gives machine more autonomy to assess user performance. On the other hand, free-text narratives enable the introduction of real scenario relevant aspects and context, beyond clinical data. In this work, we present a practical experiment involving the database of the Jacinto emergency medicine learning environment.",Versão Final_Francisco José Nardi Filho.pdf,ENGENHARIA DA INFORMAÇÃO,FRANCISCO JOSE NARDI FILHO,UNIVERSIDADE ESTADUAL DE CAMPINAS,19/05/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Medical Training;Emergency Medicine;Clinical Knowledge Base',SISTEMAS DE INFORMAÇÃO,ANDRE SANTANCHE,98,Treinamento Médico;Medicina de Emergência;Base de Conhecimento Clínica,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Software para treinamento médico normalmente segue duas abordagens principais para a representação de seus dados. Software de treinamento baseado em simulação - por exemplo, para controlar manequins de simulação - os quais possuem representações altamente estruturadas dos dados clínicos e de seus planos de simulação. Além deste tipo, alguns sistemas se concentram na narrativa de um caso clínico em formato de texto livre - por exemplo, alguns aplicativos móveis como Prognosis e o ambiente de aprendizado de medicina de emergência Jacinto. Neste caso, os dados clínicos se misturam com a narrativa em formato não estruturado. Assim, propomos um modelo que combina ambas abordagens em uma base de conhecimento narrativa e clínica híbrida para treinamento em medicina de emergência. A hipótese é que ao relacionar as narrativas com informações clínicas estruturadas, podemos tirar proveito dos pontos fortes de cada abordagem. Por um lado, os dados clínicos estruturados oferecem flexibilidade para a produção de variações de casos e planos alternativos, dando mais autonomia à máquina para avaliar o desempenho do usuário. Por outro lado, as narrativas de texto livre permitem a introdução de aspectos e contexto relevantes para o cenário real, além dos dados clínicos. Neste trabalho, apresentamos uma experiência prática envolvendo a base de dados do ambiente de aprendizagem de medicina de emergência Jacinto.",DISSERTAÇÃO,Hybrid Narrative and Clinical Knowledge Base for Emergency Medicine Training,5022488,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5025098,
"Os arcabouços de processamento de big data proveem análise e tratamento de arquivos de maneira mais rápida e eficiente, enquanto as plataformas de nuvens possibilitam a alocação de recursos virtuais para que os usuários possam utilizá-los. Combinar recursos de nuvens para realizar execuções de aplicações dos arcabouços de processamento de big data demanda a realização de tarefas com um certo grau de complexidade, como a criação e a definição das redes virtuais responsáveis pela comunicação das máquinas virtuais da nuvem que irão permitir que as aplicações executem seus processamentos. Além de facilitar o processo de criação, o modelo proposto por nós nesta dissertação permite alocar larguras de banda mínima, modificar e remover redes virtuais, enquanto analisa o consumo dos recursos para criação, modificação e definição de largura de banda das redes virtuais da plataforma de nuvem, visando a melhor utilização dos recursos de redes virtuais na plataforma de nuvem privada.",Versão Final_Luís Guilherme Cordiolli Russi.pdf,SISTEMAS DE COMPUTAÇÃO,LUIS GUILHERME CORDIOLLI RUSSI,UNIVERSIDADE ESTADUAL DE CAMPINAS,14/07/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'networking management;cloud computing;hybrid clouds;big data;mapreduce',REDES DE COMPUTADORES,EDMUNDO ROBERTO MAURO MADEIRA,69,Gerenciamento de redes;computação em nuvem;nuvens híbridas,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Os arcabouços de processamento de big data proveem análise e tratamento de arquivos de maneira mais rápida e eficiente, enquanto as plataformas de nuvens possibilitam a alocação de recursos virtuais para que os usuários possam utilizá-los. Combinar recursos de nuvens para realizar execuções de aplicações dos arcabouços de processamento de big data demanda a realização de tarefas com um certo grau de complexidade, como a criação e a definição das redes virtuais responsáveis pela comunicação das máquinas virtuais da nuvem que irão permitir que as aplicações executem seus processamentos. Além de facilitar o processo de criação, o modelo proposto por nós nesta dissertação permite alocar larguras de banda mínima, modificar e remover redes virtuais, enquanto analisa o consumo dos recursos para criação, modificação e definição de largura de banda das redes virtuais da plataforma de nuvem, visando a melhor utilização dos recursos de redes virtuais na plataforma de nuvem privada.",DISSERTAÇÃO,Um Modelo de Gerenciador de Redes Virtuais para Arcabouços de Processamento de Big Data em Nuvens Privadas e Híbridas,5026523,1
"In the last years, new demands, paradigms, and concepts became available to offer computational resources to end users. Among them, service and applications mobility triggered the development of Fog Computing and Internet of Things. This mobility can be supported through migration of virtual machines between servers that are close to the user, since their applications should be available wherever they are for offering low latency service to these applications. However, the migration process is not straightforward and involves algorithms, data communication, and efficient strategies to avoid quality of ser- vice degradation. In this research, we propose a virtual machine migration architecture and algorithms in fog computing focused in the user’s quality of experience during their mobility. The architecture and algorithms are based on policies and strategies that utilizes the network handoff time to minimize the application’s downtime in the migration. Simulations show that after the virtual machine migration the user’s quality of experience has been maintained or improved. The results show that some policies and strategies are better than others to meet the requirement of these services.",Versão Final_Marcio Moraes Lopes.pdf,SISTEMAS DE COMPUTAÇÃO,MARCIO MORAES LOPES,UNIVERSIDADE ESTADUAL DE CAMPINAS,12/07/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Fog computing;virtual machine migration;algorithms;architecture',SISTEMAS DISTRIBUÍDOS,LUIZ FERNANDO BITTENCOURT,82,Computação em névoa;migração de máquinas virtuais;algoritmos;arquitetura,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Nos últimos anos, novas demandas, paradigmas e conceitos surgiram para o oferecimento de recursos computacionais aos usuários finais. Dentre eles, a mobilidade de serviços e aplicações disparou um gatilho para o desenvolvimento da Fog Computing juntamente com a Internet das Coisas. Essa mobilidade pode ser suportada pela migração de máquinas virtuais entre servidores que estejam próximos ao usuário, uma vez que as aplicações para estes usuários devem estar disponíveis onde eles estiverem, mantendo assim um serviço que ofereça baixa latência para aplicações. No entanto, o processo de migração não é trivial e envolve algoritmos, comunicação de dados, políticas e estratégias eficientes para minimizar o tempo que os usuários percebem uma degradação da qualidade de serviço da aplicação. Neste trabalho propomos uma arquitetura e algoritmos de migração de máquinas virtuais em Fog Computing com o foco na qualidade de experiência do usuário durante sua mobilidade. A arquitetura e os algoritmos são baseados em políticas e estratégias que aproveitam o tempo de handoff da rede para minimizar o tempo de parada da aplicação durante uma migração. Através de simulações, os resultados dos testes mostram que após a migração das máquinas virtuais a qualidade de experiência dos usuários foi mantida ou melhorada. Os testes mostram ainda que há políticas e estratégias melhores para atender os requisitos desses serviços.",DISSERTAÇÃO,Arquitetura e Mecanismos para Migração de Máquinas Virtuais na Computação em Névoa,5031460,
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5032976,
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5033103,
"Cache memories have long been used to reduce problems deriving from the memory-processor performance discrepancy: many levels of on-chip cache reduce the average memory latency at the cost of extra die area and power. To decrease the outlay of these extra components, cache compression techniques are used to store compressed data and allow a cache capacity boost. This project introduces extensions to the Base-Delta-Immediate Compression, many modifications of the original technique that minimize the quantity of padding bits by relaxing the allowed delta sizes for each base and increasing number of bases. The extensions were tested using ZSim, evaluated against state-of-the-art methods, and the performance results were compared and evaluated to determine the validity of the proposed techniques. We verified an improvement of the original BDI compression factor from 1.37x to 1.58x at a energy increase as low as 27%.",Versão Final_Daniel Rodrigues Carvalho.pdf,SISTEMAS DE COMPUTAÇÃO,DANIEL RODRIGUES CARVALHO,UNIVERSIDADE ESTADUAL DE CAMPINAS,20/07/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Compressed Cache;Cache;Memory',PROJETOS DE SISTEMAS COMPUTACIONAIS,RODOLFO JARDIM DE AZEVEDO,82,Cache Comprimida;Cache;Memória,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Memórias cache há muito têm sido utilizadas para reduzir os problemas decorrentes da discrepância de desempenho entre a memória e o processador: muitos níveis de caches on-chip reduzem a latência média de memória ao custo de área e energia extra no die. Para diminuir o dispêndio desses componentes extras, técnicas de compressão de cache são usadas para armazenar dados comprimidos e permitir um aumento de capacidade de cache. Este projeto apresenta extensões para a Compressão Base-Delta-Imediato, várias modificações da técnica original que minimizam a quantidade de bits de preenchimento numa compressão através da flexibilização dos tamanhos de delta permitidos para cada base e do aumento do número de bases. As extensões foram testadas utilizando ZSim, avaliadas contra métodos estado da arte, e os resultados de desempenho foram comparados e avaliados para determinar a validade de utilização das técnicas propostas. Foi constatado um aumento do fator de compressão médio de 1.37x para 1.58x com um aumento de energia tão baixo quanto 27%.",DISSERTAÇÃO,Extensions To The Base-Delta-Immediate Compression,5033153,1
"Today’s world is driven by the usage of computer systems, which are present in all aspects
of everyday life. Therefore, the correct working of these systems is essential to ensure the
maintenance of the possibilities brought about by technological developments. However,
ensuring the correct working of such systems is not an easy task, as many people attempt
to subvert systems working for their own benefit. The most common kind of subversion
against computer systems are malware attacks, which can make an attacker to gain com-
plete machine control. The fight against this kind of threat is based on analysis procedures
of the collected malicious artifacts, allowing the incident response and the development
of future countermeasures. However, attackers have specialized in circumventing analysis
systems and thus keeping their operations active. For this purpose, they employ a series
of techniques called anti-analysis, able to prevent the inspection of their malicious codes.
Among these techniques, I highlight the analysis procedure evasion, that is, the usage of
samples able to detect the presence of an analysis solution and then hide their malicious
behavior. Evasive examples have become popular, and their impact on systems security
is considerable, since automatic analysis now requires human supervision in order to find
evasion signs, which significantly raises the cost of maintaining a protected system. The
most common ways for detecting an analysis environment are: i) Injected code detec-
tion, since injection is used by analysts to inspect applications on their way; ii) Virtual
machine detection, since they are used in analysis environments due to scalability issues;
iii) Execution side effects detection, usually caused by emulators, also used by analysts.
To handle evasive malware, analysts have relied on the so-called transparent techniques,
that is, those which do not require code injection nor cause execution side effects. A
way to achieve transparency in an analysis process is to rely on hardware support. In
this way, this work covers the application of the hardware support for the evasive threats
analysis purpose. In the course of this text, I present an assessment of existing hardware
support technologies, including hardware virtual machines, BIOS support, performance
monitors and PCI cards. My critical evaluation of such technologies provides basis for
comparing different usage cases. In addition, I pinpoint development gaps that currently
exists. More than that, I fill one of these gaps by proposing to expand the usage of
performance monitors for malware monitoring purposes. More specifically, I propose the
usage of the BTS monitor for the purpose of developing a tracer and a debugger. The
proposed framework is also able of dealing with ROP attacks, one of the most common
used technique for remote vulnerability exploitation. The framework evaluation shows no
side-effect is introduced, thus allowing transparent analysis. Making use of this capability,
I demonstrate how protected applications can be inspected and how evasion techniques
can be identified.",Versão Final_Marcus Felipe Botacin.pdf,SISTEMAS DE COMPUTAÇÃO,MARCUS FELIPE BOTACIN,UNIVERSIDADE ESTADUAL DE CAMPINAS,28/07/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'malware;computer security;anti-analysis',SEGURANÇA E CRIPTOGRAFIA APLICADA,PAULO LICIO DE GEUS,139,software malicioso;segurança computacional;anti-forense,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"O mundo atual é impulsionado pelo uso de sistemas computacionais, estando estes pre-
sentes em todos aspectos da vida cotidiana. Portanto, o correto funcionamento destes
é essencial para se assegurar a manutenção das possibilidades trazidas pelos desenvolvi-
mentos tecnológicos. Contudo, garantir o correto funcionamento destes não é uma tarefa
fácil, dado que indivíduos mal-intencionados tentam constantemente subvertê-los visando
benefíciar a si próprios ou a terceiros. Os tipos mais comuns de subversão são os ataques
por códigos maliciosos (malware), capazes de dar a um atacante controle total sobre uma
máquina. O combate à ameaça trazida por malware baseia-se na análise dos artefatos
coletados de forma a permitir resposta aos incidentes ocorridos e o desenvolvimento de
contramedidas futuras. No entanto, atacantes têm se especializado em burlar sistemas
de análise e assim manter suas operações ativas. Para este propósito, faz-se uso de uma
série de técnicas denominadas de “anti-análise”, capazes de impedir a inspeção direta dos
códigos maliciosos. Dentre essas técnicas, destaca-se a evasão do processo de análise, a
qual ocorre quando exemplares capazes de detectar a presença de um sistema de análise
para então esconder seu comportamento malicioso. Exemplares evasivos têm sido cada
vez mais utilizados em ataques e seu impacto sobre a segurança de sistemas é considerá-
vel, dado que análises antes feitas de forma automática passaram a exigir a supervisão de
analistas humanos em busca de sinais de evasão, aumentando assim o custo de se manter
um sistema protegido. As formas mais comuns de detecção de um ambiente de análise se
dão através da detecção de: (i) código injetado, usado pelo analista para inspecionar a
aplicação; (ii) máquinas virtuais, usadas em ambientes de análise por questões de escala;
(iii) efeitos colaterais de execução, geralmente causados por emuladores, também usados
por analistas. Para lidar com malware evasivo, analistas tem se valido de técnicas ditas
transparentes, isto é, que não requerem injeção de código nem causam efeitos colaterais
de execução. Um modo de se obter transparência em um processo de análise é contar com
suporte do hardware. Desta forma, este trabalho versa sobre a aplicação do suporte de
hardware para fins de análise de ameaças evasivas. No decorrer deste texto, apresenta-se
uma avaliação das tecnologias existentes de suporte de hardware, dentre as quais máqui-
nas virtuais de hardware, suporte de BIOS e monitores de performance. A avaliação crítica
de tais tecnologias oferece uma base de comparação entre diferentes casos de uso. Além
disso, são enumeradas lacunas de desenvolvimento existentes atualmente. Mais que isso,
uma destas lacunas é preenchida neste trabalho pela proposição da expansão do uso dos
monitores de performance para fins de monitoração de malware. Mais especificamente, é
proposto o uso do monitor BTS para fins de construção de um tracer e um debugger. O
framework proposto e desenvolvido neste trabalho é capaz, ainda, de lidar com ataques
do tipo ROP, um dos mais utilizados atualmente para exploração de vulnerabilidades. A
avaliação da solução demonstra que não há a introdução de efeitos colaterais, o que per-
mite análises de forma transparente. Beneficiando-se desta característica, demonstramos
a análise de aplicações protegidas e a identificação de técnicas de evasão.",DISSERTAÇÃO,Hardware-Assisted Malware Analysis,5033201,1
"Problems with the specification of software requirements are a common cause of software defects. In domains such as space applications, those defects are very costly, especially when detected after software deployment, when the product is already in the field. It is imperative to ensure that software requirement documents are well written to avoid the introduction of these defects. The quality of software requirements is frequently assessed via reviews guided by checklists, based on standards, and on problems found in previous projects. Given the importance of quality assessment, and the fact that the reviews are performed manually, we propose a framework for assisting a checklist-based review of software requirements, using natural language processing techniques. A tool was developed under this framework, whose objetive is to diminish the reviewer's effort,
and to reduce the amount of uncaught errors during the reviewing process.",Versão Final_Anderson Rossanez.pdf,ENGENHARIA DA INFORMAÇÃO,ANDERSON ROSSANEZ,UNIVERSIDADE ESTADUAL DE CAMPINAS,27/07/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Software Requirements;Quality Assessment;Checklists;Natural Language Processing',SISTEMAS DE INFORMAÇÃO,ARIADNE MARIA BRITO RIZZONI CARVALHO,104,Requisitos de Software;Avaliação de Qualidade;Checklists;Processamento de Língua Natural,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Problemas com a especificação de requisitos são uma causa comum de defeitos de software. Em domínios como o das aplicações espaciais, tais defeitos acarretam um custo muito alto, especialmente quando detectados em campo. É imperativo garantir que os requisitos de software estejam bem escritos, de modo a evitar a introdução desses defeitos no final. A qualidade de documentos de requisitos de software é comumente verificada em revisões baseadas em checklists, gerados a partir de padrões e em problemas encontrados em projetos anteriores. Dada a importância dessa verificação, e do fato de que ela é normalmente efetuada manualmente, nós propusemos um framework para auxiliar nesse processo, utilizando técnicas de processamento de língua natural. Uma ferramenta foi implementada de acordo com esse framework, visando diminuir o esforço dos revisores e reduzir a quantidade de erros não encontrados durante o processo de revisão.",DISSERTAÇÃO,Semi-Automatic Checklist-Based Quality Assessment of Natural Language Requirements,5033215,1
"Modern systems need to be able to self-adapt to changing user needs and system environments. Examples of systems that demand self-adaptive capabilities include mobile devices applications that should deal with environmental changes and service-oriented systems that should replace unreliable services on-the-fly.  In this context, dynamic software product line is an engineering approach for developing self-adaptive systems based on commonalities and variabilities for a  family of similar systems. However, researchers have reported that many DSPL solutions fail to meet all the system’s adaptability requirements, and in many cases, they are developed in ad hoc manner.
The solution proposed in this thesis deals with the study of autonomous and self-adaptive systems development techniques and their application in the improvement of the dynamic capacities of product lines.
Thus, this work encompasses:
(i) the definition of a two-dimension taxonomy, self-adaptation, and variability, to address basic technical issues in the design and development of dynamic product lines;
(ii) a reference model that provides guidelines and development processes for  dynamic software product lines with support to an effective dynamic variability mechanism; and
(iii) a self-adaptive deployment infrastructure extended to meet the reference model, providing tooling support.
We conducted a case study to evaluate our tool-based reference model in the development of a dynamic software product line for the Android platform and also for JavaFX technology. Using the case study, we evaluated the feasibility of the solution by measuring the overhead imposed by the adaptation process. The study presented promising results, which indicate our solution is efficient to support the development of dynamic software product lines.",Versão Final_Jane Dirce Alves Sandim Eleutério.pdf,ENGENHARIA DA INFORMAÇÃO,JANE DIRCE ALVES SANDIM ELEUTERIO,UNIVERSIDADE ESTADUAL DE CAMPINAS,28/07/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Dynamic Software Product Line;Self-Adaptive Systems;Reference Model',SISTEMAS DE INFORMAÇÃO,CECILIA MARY FISCHER RUBIRA,181,Linha de Produto de Software Dinâmica;Sistemas Autoadaptativos;Modelo de referência,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Cada vez mais, os sistemas modernos necessitam ter a capacidade de se autoadaptar às mudanças que ocorrem tanto no seu contexto de execução, quanto nas necessidades dos usuários. Pode-se citar como exemplos de sistemas que exigem tal capacidade as aplicações para dispositivos móveis, que precisam lidar com mudanças no ambiente, e os sistemas orientados a serviços, que precisam substituir serviços não confiáveis rapidamente interferindo minimamente na sua execução.  Neste contexto, Linha de Produtos de Software Dinâmica é uma abordagem de engenharia de software para desenvolver sistemas autoadaptáveis baseados em comunalidades e variabilidades para família de sistemas similares. Entretanto, pesquisas recentes relataram que muitas soluções para linhas de produtos dinâmicas não conseguem cumprir todos os requisitos de adaptabilidade do sistema, e em muitos casos, elas são individualmente desenvolvidas e sem padronização. 
A solução proposta nesta tese aborda o estudo de técnicas de desenvolvimento de sistemas autônomos e autoadaptativos e sua aplicação na melhoria das capacidades dinâmicas de linhas de produtos. Assim, este trabalho envolve: (i) a definição de uma taxonomia em duas dimensões, autoadaptação e variabilidade, para abordar questões técnicas básicas do projeto e desenvolvimento de linhas de produtos dinâmicas; (ii) um modelo de referência que define diretrizes e processos de desenvolvimento para linhas de produtos de software dinâmicas com suporte a um mecanismo de variabilidade dinâmica eficaz; e (iii) uma infraestrutura de implantação autoadaptativa estendida para atender ao modelo de referência, acompanhada de uma ferramenta de apoio. Foi conduzido um estudo de caso para avaliar a aplicação do modelo de referência apoiado por ferramental na construção de uma linha de produto para a plataforma Android e para JavaFX, e para avaliar a viabilidade da solução através da medição da sobrecarga de processamento imposta pelo processo de adaptação. O estudo apresentou resultados promissores indicando que a solução é eficiente para apoiar a construção de linhas de produto de software dinâmica.",TESE,Técnicas de Sistemas Autônomos e Autoadaptativos para Apoiar Linhas de Produtos de Software Dinâmicas,5033254,1
"Redes de Sensores Sem Fio consistem de centenas ou milhares de nós com energia limitada e, portanto, faz-se necessário o uso eficiente de energia nessas redes. Dado que as transmissões são as operações de maior exigência energética, os algoritmos de roteamento devem considerar um uso eficiente das transmissões em seus planejamentos, a fim de prolongar a vida útil da rede. 
Para enfrentar esses desafios, propõe-se um algoritmo centralizado, denominado Roteamento Aprimorado de Melhora Contínua (Improved Continuous Enhacement Routing Algorithm - ICER), para computar árvores de roteamento de qualidade refinada, com base na agregação de dados, considerando o consumo de energia das baterias. 
São realizadas comparações entre o ICER e outros algoritmos conhecidos na literatura. ICER foi capaz de garantir, em média, a sobrevivência de 99,6% e a conectividade de 99,3% dos nós contra 90,2% e 72,4% em relação ao melhor algoritmo comparado em nossos experimentos. Os resultados obtidos indicam que ICER significantemente estende a vida útil da rede enquanto mantém a qualidade da árvore de roteamento.",,TEORIA DA COMPUTAÇÃO,EDSON ARIEL TICONA ZEGARRA,UNIVERSIDADE ESTADUAL DE CAMPINAS,24/08/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Wireless Sensor Networks;Genetic Algorithm;Steiner Tree Problem',ALGORITMOS E OTIMIZAÇÃO,RAFAEL CRIVELLARI SALIBA SCHOUERY,53,Redes de Sensores Sem Fio;Algorítmo Genético;Problema da Árvore de Steiner,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Redes de Sensores Sem Fio consistem de centenas ou milhares de nós com energia limitada e, portanto, faz-se necessário o uso eficiente de energia nessas redes. Dado que as transmissões são as operações de maior exigência energética, os algoritmos de roteamento devem considerar um uso eficiente das transmissões em seus planejamentos, a fim de prolongar a vida útil da rede. 
Para enfrentar esses desafios, propõe-se um algoritmo centralizado, denominado Roteamento Aprimorado de Melhora Contínua (Improved Continuous Enhacement Routing Algorithm - ICER), para computar árvores de roteamento de qualidade refinada, com base na agregação de dados, considerando o consumo de energia das baterias. 
São realizadas comparações entre o ICER e outros algoritmos conhecidos na literatura. ICER foi capaz de garantir, em média, a sobrevivência de 99,6% e a conectividade de 99,3% dos nós contra 90,2% e 72,4% em relação ao melhor algoritmo comparado em nossos experimentos. Os resultados obtidos indicam que ICER significantemente estende a vida útil da rede enquanto mantém a qualidade da árvore de roteamento.",DISSERTAÇÃO,An Energy-Aware Data Aggregation Algorithm for Wireless Sensor Networks,5064322,1
"Executar tarefas analíticas, como agrupamento e classificação, em coleções de dados textuais são objeto de estudo constante em diversas áreas e, em especial, na área de Recuperação de Informação. O enriquecimento semântico é uma forma típica usada para tornar mais representativo a descrição de documentos textuais, visando obter melhores resultados em tarefas de mineração de texto. Este trabalho propõe um arcabouço baseado no uso de workflows para a execução de tarefas de classificação de coleções de texto enriquecidas semanticamente. Umworkflow típico do arcabouço é composto de (i) um extrator de grafos como forma de representação das amostras da coleção, (ii) um enriquecedor semântico dos grafos obtidos a partir do uso de ontologias (por exemplo, a ontologia Wordnet), (iii) do uso do framework Bag of Textual Graphs (BoTG) para representação de grafos em vetores visando permitir a classificação dosgrafos enriquecidos. Os componentes permitem diferentes parametrizações com o objetivo de obter bons resultados em tarefas de classificação de texto. Os resultados obtidos confirmam os benefícios do uso de workflows na especificação e implementação de procedimentos de anotação e classificação de documentos textuais e apontam para resultados promissores na utilização de ontologias em determinados cenários de classificação deste tipo de documento.",Versão Final_Vandalis Giansante.pdf,ENGENHARIA DA INFORMAÇÃO,VANDALIS GIANSANTE,UNIVERSIDADE ESTADUAL DE CAMPINAS,31/08/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Semantic enrichment;Workflow;Bag of Graphs;Text Mining;Wordnet',SISTEMAS DE INFORMAÇÃO,RICARDO DA SILVA TORRES,0,Enriquecimento Semântico;Workflow;Bag of Graphs;Mineração de Textos;Wordnet,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Executar tarefas analíticas, como agrupamento e classificação, em coleções de dados textuais são objeto de estudo constante em diversas áreas e, em especial, na área de Recuperação de Informação. O enriquecimento semântico é uma forma típica usada para tornar mais representativo a descrição de documentos textuais, visando obter melhores resultados em tarefas de mineração de texto. Este trabalho propõe um arcabouço baseado no uso de workflows para a execução de tarefas de classificação de coleções de texto enriquecidas semanticamente. Umworkflow típico do arcabouço é composto de (i) um extrator de grafos como forma de representação das amostras da coleção, (ii) um enriquecedor semântico dos grafos obtidos a partir do uso de ontologias (por exemplo, a ontologia Wordnet), (iii) do uso do framework Bag of Textual Graphs (BoTG) para representação de grafos em vetores visando permitir a classificação dosgrafos enriquecidos. Os componentes permitem diferentes parametrizações com o objetivo de obter bons resultados em tarefas de classificação de texto. Os resultados obtidos confirmam os benefícios do uso de workflows na especificação e implementação de procedimentos de anotação e classificação de documentos textuais e apontam para resultados promissores na utilização de ontologias em determinados cenários de classificação deste tipo de documento.",DISSERTAÇÃO,Um Arcabouço Baseado em Anotações para Enriquecimento Semântico de Documentos Textuais,5064431,1
"Computer memory systems have relied on volatile memories to enhance their performance for quite a time by now. SRAM technology is used at the closest layer to the CPU to accelerate the access time to the main memory, which is traditionally composed by DRAM technology. Non-volatile memories are left as secondary memories, serving as an extension of the main memory and allowing data to be persisted. Persistent data, for residing in the farthest memory layer from the CPU, are commonly not manipulated directly. They are indirectly manipulated with their transient copies, that may differ, in form, from their persistent form. These transient copies will also be scattered throughout the several volatile memories in the memory hierarchy, incurring in data replication. This scenario may change with the usage of emerging non-volatile memories (NVMs), like phase change memory for example, that may allow persistent data to exist in the main memory. This might allow a directly manipulation of persistent data, accelerating their access time and probably reducing the usage of replications. Unfortunately, NVMs are still not broadly available on the market, and research on their usage is still mostly done through simulation. We present a simulator to explore the usage of NVMs in the main memory. We demonstrate the usage of the simulator in two scenarios, the first where DRAM is completely replaced for NVMs, and the second in which a hybrid architecture employing DRAM and NVM is explored. For now, DRAM provides faster access times when compared with NVMs. We show that the use of a main memory composed exclusively of NVMs may incur in slowdowns as high as 5.3, but may be negligible in some cases. In the hybrid main memory scenario, we showed that, although persistent data can be manipulated directly, there are cases in which is still better to work with transient copies, depending on the frequency of usage of the persistent data. To allow programs to make use of the non-volatility presented in main memory, we provide an API, called NVMalloc, that is able to allocate persistent memory in the main memory. We expect the simulator to be a starting point for future researches regarding the usage of NVMs.",,SISTEMAS DE COMPUTAÇÃO,MAURICIO GAGLIARDI PALMA,UNIVERSIDADE ESTADUAL DE CAMPINAS,18/09/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'memory;simulation;simulator;NVM;NVMain;ZSIM;NVMalloc;Storage-Class Memory',PROJETOS DE SISTEMAS COMPUTACIONAIS,RODOLFO JARDIM DE AZEVEDO,0,memória;simulação;simulador;persistência de dados;NVM;NVMain;ZSIM;NVMalloc,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"O sistema de memória dos computadores tem se baseado fortemente no uso de memórias voláteis para prover um bom desempenho. A tecnologia SRAM é utilizada como um intermediário que acelera o acesso à memória principal, comumente composta pela tecnologia DRAM. Memórias não-voláteis são colocadas como memórias secundárias. Pelo fato dos dados persistentes estarem armazenados no nível de memória mais distante do processador, eles normalmente são manipulados de maneira indireta através de cópias transientes. Tais cópias transientes, além de possívelmente estarem presentes em mais de um nível de memória volátil, podem não ter a mesma forma de suas formas persistentes, o que leva à necessidade de uma tradução entre essas formas. Tecnologias emergentes de memórias não-voláteis (NVMs) prometem possibilitar a existência de dados persistentes na memória principal, permitindo que os mesmos sejam manipulados diretamente, e potencialmente reduzindo a quantidade de cópias transientes. Infelizmente, NVMs ainda não estão amplamente disponíveis no mercado, e pesquisas em seu uso são normalmente feitas através de simulação. Neste documento é apresentado um simulador que tem como fim explorar o uso de NVMs na memória principal. Por enquanto, a tecnologia DRAM provê um tempo de acesso inferior ao das NVMs, restringindo o uso de NVMs na memória principal em questão de desempenho. São mostrados aqui dois cenários para o uso do simulador. No primeiro caso, há a utilização de uma memória principal composta apenas de NVM. Como NVM é mais lenta, são observados certos \textit{slowdows} de até 5,3, mas em alguns programas o desempenho é marginalmente afetado. Em um segundo caso, há a exploração da memória híbrida, onde DRAM e NVM coexistem na memória principal. Uma API, chamada NVMalloc, é fornecida para permitir que programas consigam utilizar a não volatilidade presente na memória principal. É mostrado que há casos onde a manipulação direta dos dados persistentes é vantajosa, mas existem outros em que ainda é preferível trabalhar com cópias transientes na DRAM. É esperado que esse simulador seja utilizado como um ponto de partida para futuras pesquisas sobre o uso de NVMs.",DISSERTAÇÃO,Explorando a Substituição de DRAM por NVM na Memória Principal através de Simulação,5064538,1
"Mobile devices, such as smartphones and tablets, had their popularity and affordability greatly increased in recent years. As a consequence of their ubiquity, these devices now carry all sorts of personal data (e.g., photos, text conversations, GPS coordinates, banking information) that should be accessed only by the device owner. Even though knowledge-based procedures, such as entering a PIN or drawing a pattern, are still the main methods to secure the owner's identity, recently biometric traits have been employed for a more secure and effortless authentication.
Among them, face recognition has gained more attention in past years due to recent improvements in image-capturing devices and the availability of images in social networks. In addition to that, the increase in computational resources, with multiple CPUs and GPUs, enabled the design of more complex and robust models, such as deep neural networks. Although the capabilities of mobile devices have been growing in past years, most recent face recognition techniques are still not designed considering the mobile environment's characteristics, such as limited processing power, unstable connectivity and battery consumption.
In this work, we propose a facial verification method optimized to the mobile environment. It consists of a two-tiered procedure that combines hand-crafted features (histogram of oriented gradients and local region principal component analysis) and a convolutional neural network to verify if the person depicted in a picture corresponds to the device owner. We also propose Hybrid-Fire Convolutional Neural Network, an architecture tweaked for mobile devices that process encoded information of a pair of face images. Finally, we expose a technique to adapt our method's acceptance thresholds to images with different characteristics than those present during training, by using the device owner's enrolled gallery.
The proposed solution performs a par to the state-of-the-art face recognition methods, while having a model 16 times smaller and 4 times faster when processing an image in recent smartphone models.
Finally, we have collected a new dataset of selfie pictures comprising 2873 images from 56 identities with varied capture conditions, that hopefully will support future researches in this scenario.",,ENGENHARIA DA INFORMAÇÃO,RAFAEL SOARES PADILHA,UNIVERSIDADE ESTADUAL DE CAMPINAS,01/09/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Facial Recognition;Biometrics;Machine Learning;Mobile Devices',SISTEMAS DE INFORMAÇÃO,JACQUES WAINER,0,Reconhecimento Facial;Biometria;Aprendizado de Máquina;Dispositivos Móveis,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Dispositivos móveis, como smartphones e tablets, se tornaram mais populares e acessíveis nos últimos anos. Como consequência de sua ubiquidade, esses aparelhos guardam diversos tipos de informações pessoais (fotos, conversas de texto, coordenadas GPS, dados bancários, entre outros) que só devem ser acessadas pelo dono do dispositivo. Apesar de métodos baseados em conhecimento, como senhas numéricas ou padrões, ainda estejam entre as principais formas de assegurar a identidade do usuário, traços biométricos tem sido utilizados para garantir uma autenticação mais segura e prática.

Entre eles, reconhecimento facial ganhou atenção nos últimos anos devido aos recentes avanços nos dispositivos de captura de imagens e na crescente disponibilidade de fotos em redes sociais. Aliado a isso, o aumento de recursos computacionais, com múltiplas CPUs e GPUs, permitiu o desenvolvimento de modelos mais complexos e robustos, como redes neurais profundas. Porém, apesar da evolução das capacidades de dispositivos móveis, os métodos de reconhecimento facial atuais ainda não são desenvolvidos considerando as características do ambiente móvel, como processamento limitado, conectividade instável e consumo de bateria.

Neste trabalho, nós propomos um método de verificação facial otimizado para o ambiente móvel. Ele consiste em um procedimento em dois níveis que combina engenharia de características (histograma de gradientes orientados e análise de componentes principais por regiões) e uma rede neural convolucional para verificar se o indivíduo presente em uma imagem corresponde ao dono do dispositivo. Nós também propomos a \emph{Hybrid-Fire Convolutional Neural Network}, uma arquitetura ajustada para dispositivos móveis que processa informação de pares de imagens. Finalmente, nós apresentamos uma técnica para adaptar o limiar de aceitação do método proposto para imagens com características diferentes daquelas presentes no treinamento, utilizando a galeria de imagens do dono do dispositivo.

A solução proposta se compara em performance aos métodos de reconhecimento facial do estado da arte, porém seu modelo é 16 vezes menor e 4 vezes mais rápido ao processar uma imagem em smartphones modernos.

Por último, nós também organizamos uma base de dados composta por 2873 selfies de 56 identidades capturadas em condições diversas, a qual esperamos que ajude pesquisas futuras realizadas neste cenário.",DISSERTAÇÃO,Two-tiered facial verification for mobile devices,5064539,1
"A tecnologia tem evoluído, porém nem sempre ela é acessível para idosos. Fazer parte de uma sociedade que muda tecnologicamente e constantemente é uma tarefa complexa, principalmente para as pessoas mais idosas, particularmente quando eles possuem algum tipo de comprometimento cognitivo. Para que esses e outros problemas possam ser mitigados, uma das maneiras é conhecer melhor as habilidades, e até mesmo as dificuldades, dos potenciais usuários de uma solução de design por meio do design participativo. Porém, as técnicas tradicionais de design participativo não são adequadas para pessoas com comprometimento cognitivo, uma vez que elas não consideram as barreiras cognitivas que essas pessoas possuem. Esta pesquisa de mestrado teve como objetivo, portanto, investigar como idosos com comprometimento cognitivo poderiam ser incluídos em atividades de design participativo. Para isso, foram realizadas 24 atividades com um grupo de idosos do Lar de Velhinhos de Campinas (LVC).  Essas atividades tinham o objetivo de conhecer os partipantes e de abordar as fases de design “entender”, “estudar”, “design”, “prototipar” e “avaliar”. Durante a realização destas atividades foi identificado um objetivo de design, que serviu como estudo de caso de design participativo. Os resultados evidenciaram que é possível incluir idosos com comprometimento cognitivo em um processo de design participativo, todavia é necessário adaptar as técnicas existentes e adotar algumas estratégias para lidar com os desafios que surgem devido aos problemas cognitivos que são características desse público. Assim, este estudo também apresenta e discute esses desafios e as lições aprendidas em lidar com eles durante a adaptação das técnicas utilizadas, planejamento, execução e avaliação das atividades realizadas.",Versão Final_Luã Marcelo Muriana.pdf,ENGENHARIA DA INFORMAÇÃO,LUA MARCELO MURIANA,UNIVERSIDADE ESTADUAL DE CAMPINAS,11/09/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'participatory design;design techniques;older adults;cognitive impairment',SISTEMAS DE INFORMAÇÃO,HEIKO HORST HORNUNG,0,design participativo;técnicas de design;idosos;comprometimento cognitivo,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A tecnologia tem evoluído, porém nem sempre ela é acessível para idosos. Fazer parte de uma sociedade que muda tecnologicamente e constantemente é uma tarefa complexa, principalmente para as pessoas mais idosas, particularmente quando eles possuem algum tipo de comprometimento cognitivo. Para que esses e outros problemas possam ser mitigados, uma das maneiras é conhecer melhor as habilidades, e até mesmo as dificuldades, dos potenciais usuários de uma solução de design por meio do design participativo. Porém, as técnicas tradicionais de design participativo não são adequadas para pessoas com comprometimento cognitivo, uma vez que elas não consideram as barreiras cognitivas que essas pessoas possuem. Esta pesquisa de mestrado teve como objetivo, portanto, investigar como idosos com comprometimento cognitivo poderiam ser incluídos em atividades de design participativo. Para isso, foram realizadas 24 atividades com um grupo de idosos do Lar de Velhinhos de Campinas (LVC).  Essas atividades tinham o objetivo de conhecer os partipantes e de abordar as fases de design “entender”, “estudar”, “design”, “prototipar” e “avaliar”. Durante a realização destas atividades foi identificado um objetivo de design, que serviu como estudo de caso de design participativo. Os resultados evidenciaram que é possível incluir idosos com comprometimento cognitivo em um processo de design participativo, todavia é necessário adaptar as técnicas existentes e adotar algumas estratégias para lidar com os desafios que surgem devido aos problemas cognitivos que são características desse público. Assim, este estudo também apresenta e discute esses desafios e as lições aprendidas em lidar com eles durante a adaptação das técnicas utilizadas, planejamento, execução e avaliação das atividades realizadas.",DISSERTAÇÃO,"Incluindo Idosos com Comprometimento Cognitivo no Design Participativo - Avaliação e Adaptação de Técnicas de Design, Desafios e Lições Aprendidas",5064567,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5064627,
"Soccer match analysis is of paramount importance in the definition of appropriate training programs and game strategies. The increasing availability of sport related data in the recent years, due to the use of modern tracking systems, has allowed advances in sports analytics, providing coaches with valuable information for match and teams analysis. The availability of these data, on the other hand, challenges science to develop tools capable of storing, visualizing, and analyzing this large volume of information. Soccer analyses are usually performed using matches' statistics, events (e.g., passes and shots on goal) and players location data. Related studies have been representing the matches' events as a single graph, where players are vertices and edges are actions performed among them during the match. The graph is then analyzed from a complex network measurements perspective. Although this approach provides interesting insights about the tactical actions occurred during the game, revealing some tactical patterns among players, it disregards the spatio-temporal aspects inherent to the sport, as the positioning of the players on the pitch, and the moment in time when relevant actions occur.

This thesis addresses these shortcomings by presenting a soccer game analysis framework. We propose a new approach for soccer match analysis, based on graphs, that considers the spatio-temporal characteristics, intrinsic to the dynamic of soccer. We propose to represent the match as a temporal graph, by encoding players' location in the pitch into instant graphs, in which vertices represent players in their real location and edges are defined based on their distance in the field and the possibility of short pass exchanges. We demonstrate that this representation, named opponent-aware graph, which takes into account the presence of opponents, and the diversity entropy measurement are effective tools for determining the role of attacking players in a match and the probability of successful passes. Considering different measurements of complex networks in temporal graphs, this study also investigates the feasibility of using complex network measurements and machine learning algorithms to characterize the role of players in a match. The results allow to further characterize the decision-making process of players, providing interesting insights to coaches and researchers for possibly improving training strategies. This study also addresses the visualization of temporal graphs problem by introducing the Graph Visual Rhythm, a novel image-based representation to visualize changing patterns typically found in temporal graphs. This representation is based on the concept of visual rhythms, motivated by its capacity of providing a lot of contextual information about graph dynamics in a compact way. We validate the use of graph visual rhythms through the creation of a visual analytics tool to support the decision-making process based on complex-network-oriented soccer match analysis.",,ENGENHARIA DA INFORMAÇÃO,DANIELE CRISTINA UCHOA MAIA RODRIGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,29/09/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Temporal Graphs;Complex Networks;Soccer Analysis;Visual Analytics',INFERÊNCIA EM DADOS COMPLEXOS,RICARDO DA SILVA TORRES,91,Grafos Temporais;Redes Complexas;Análise de Futebol;Análise Visual,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A análise de partidas de futebol é de suma importância na definição de programas de treinamento apropriados e estratégias de jogo. A crescente disponibilidade de dados relacionados ao esporte nos últimos anos, devido ao uso de sistemas de rastreamento modernos, permitiu avanços em análises esportivas, proporcionando aos treinadores informações valiosas para análise de times e partidas. A disponibilidade desses dados, por outro lado, desafia a Ciência a desenvolver ferramentas capazes de armazenar, visualizar e analisar esse grande volume de informações. Análises de futebol são geralmente realizadas usando estatísticas de partidas, eventos do jogo (por exemplo, passes e finalizações) e os dados de localização dos jogadores. Estudos relacionados têm representado os eventos dos jogos como um único grafo, em que os jogadores são vértices e as arestas são ações realizadas entre eles durante a partida. O grafo é então analisado sob perspectiva de medidas de redes complexas. Embora esta abordagem ofereça informações relevantes sobre as ações táticas ocorridas durante o jogo, revelando alguns padrões táticos entre os jogadores, desconsidera os aspectos espaço-temporais inerentes ao esporte, como o posicionamento dos jogadores no campo e o momento no tempo que ações relevantes ocorrem.

Esta tese aborda estes problemas ao apresentar um framework de análise de jogos de futebol. Propõe-se uma nova abordagem para a análise de partidas de futebol, baseada em grafos que consideram as características espaço-temporais, intrínsecas a esse esporte dinâmico. As partidas de futebol foram representadas como grafos temporais, codificando a localização dos jogadores em grafos instantâneos, nos quais os vértices representam os jogadores em sua localização real e as arestas são definidas com base na distância entre eles no campo e na possibilidade de trocas de passes curtos. Demonstramos que essa representação, denominada opponent-aware graph, que leva em consideração a presença de adversários, e a medida de entropia de diversidade são ferramentas efetivas para determinar o papel dos jogadores atacantes em uma partida e a probabilidade de passes bem-sucedidos. Considerando diferentes medidas de redes complexas em grafos temporais, este estudo também investiga a viabilidade da utilização de medidas de redes complexas e algoritmos de aprendizado de máquina para caracterizar o papel dos jogadores em uma partida. Os resultados permitem caracterizar melhor o processo de tomada de decisão dos jogadores, fornecendo informações relevantes para treinadores e pesquisadores para, possivelmente, melhorar estratégias de treinamento. Este estudo também aborda o problema de visualização de grafos temporais, introduzindo o Ritmo Visual de Grafos (do inglês Graph Visual Rhythm), uma nova representação baseada em imagem para visualizar padrões de mudança tipicamente encontrados em grafos temporais. Esta representação é baseada no conceito de ritmos visuais, motivada pela sua capacidade de codificar uma grande quantidade de informações contextuais sobre a dinâmica de grafos de forma compacta. A utilização dos ritmos visuais de grafos foi realizada através da criação de uma ferramenta de análise visual para apoiar o processo de tomada de decisão com base em análises de partidas de futebol baseadas em redes complexas.",TESE,Complex Network Measurements in Graph-based Spatio-Temporal Soccer Match Analysis,5064642,1
"The Minimum Cost Steiner Tree is a classical optimization problem where, given a graph and a subset of vertices called terminals, one is asked to find a connected subgraph spanning the set of terminals and whose edge weight is minimum. We consider the variant where the graph is complete and the edge weight function is a metric and there is a non-negative weight function on the vertices. The objective is to find a tree spanning the terminals such that the sum of edge and vertex weights is minimum. In this thesis, we observe that the general problem admits a simple 2-approximation. For the special case where the weight of a vertex is at most q times the weight of lightest edge, for a constant q, we obtain a randomized LP rounding algorithm with approximation factor 1.62 and a greedy algorithm with approximation factor 1.55.",,TEORIA DA COMPUTAÇÃO,HUGO KOOKI KASUYA ROSADO,UNIVERSIDADE ESTADUAL DE CAMPINAS,02/10/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Steiner tree;node weighted;approximation algorithm;combinatorial optimization',ALGORITMOS E OTIMIZAÇÃO,LEHILTON LELIS CHAVES PEDROSA,57,árvore de Steiner;peso nos vértices;algoritmo de aproximação;otimização combinatória,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"O Problema da Árvore de Steiner de Custo Mínimo é um problema de otimização clássico em que, dado um grafo e subconjunto de vértices, chamados terminais, procura-se obter um subgrafo conexo com todos os terminais de forma que a soma dos pesos das arestas seja mínima. Consideramos a variante em que o grafo é completo e a função de peso sobre as arestas é uma métrica e, além disso, existe uma função de pesos não negativos sobre os vértices. O objetivo é encontrar uma árvore que contém todos os terminais e que minimiza o peso total de vértices e arestas. Nesta dissertação, observamos que o problema geral admite uma 2-aproximação simples. Para o caso particular em que o peso de um vértice é no máximo q vezes o peso da aresta mais leve, para uma constante q, obtemos um algoritmo aleatorizado baseado em arredondamento de PL com fator de aproximação 1,62 e um algoritmo guloso com fator de aproximação 1,55.",DISSERTAÇÃO,An Approximation Algorithm for the q-Metric Node-Weighted Steiner Tree Problem,5064666,1
"A pesquisa científica tornou-se cada vez mais dependente de dados. Esse novo paradigma de pesquisa demanda técnicas e tecnologias computacionais sofisticadas para apoiar tanto o ciclo de vida dos dados científicos como a colaboração entre cientistas de diferentes áreas. Uma demanda recorrente em equipes multidisciplinares é a construção de múltiplas perspectivas sobre um mesmo conjunto de dados. Soluções atuais cobrem vários aspectos, desde o projeto de padrões de interoperabilidade ao uso de sistemas de gerenciamento de bancos de dados não-relacionais. Entretanto, nenhum desses esforços atende de forma adequada a necessidade de múltiplas perspectivas, denominadas focos nesta tese. Em termos gerais, um foco é projetado e construído para atender um determinado grupo de pesquisa (mesmo no escopo de um único projeto) que necessita manipular um subconjunto de dados de interesse em múltiplos níveis de agregação/generalização. A definição e criação de um foco são tarefas complexas que demandam mecanismos capazes de manipular múltiplas representações de um mesmo fenômeno do mundo real. O objetivo desta tese é prover múltiplos focos sobre dados heterogêneos. Para atingir esse objetivo, esta pesquisa se concentrou em quatro principais problemas. Os problemas inicialmente abordados foram: (1) escolher um paradigma de gerenciamento de dados adequado e (2) elencar os principais requisitos de pesquisas multifoco. Nossos resultados nos direcionaram para a adoção de bancos de dados de grafos como solução para o problema (1) e a utilização do conceito de visões, de bancos de dados relacionais, para o problema (2). Entretanto, não há consenso sobre um modelo de dados para bancos de dados de grafos e o conceito de visões é pouco explorado nesse contexto. Com isso, os demais problemas tratados por esta pesquisa são: (3) a especificação de um modelo de dados de grafos e (4) a definição de um framework para manipular visões em bancos de dados de grafos. Nossa pesquisa nesses quatro problemas resultaram nas contribuições principais desta tese: (i) apontar o uso de bancos de dados de grafos como camada de persistência em pesquisas multifoco -- um tipo de banco de dados de esquema flexível e orientado a relacionamentos que provê uma ampla compreensão sobre as relações entre os dados; (ii) definir visões para bancos de dados de grafos como mecanismo para manipular múltiplos focos, considerando operações de manipulação de dados em grafos, travessias e algoritmos de grafos; (iii) propor um modelo de dados para grafos - baseado em grafos de propriedade - para lidar com a ausência de um modelo de dados pleno para grafos; (iv) especificar e implementar um framework, denominado Graph-Kaleidoscope, para prover o uso de visões em bancos de dados de grafos e (v) validar nosso framework com dados reais em aplicações distintas - em biodiversidade e em recursos naturais",Versão Final_Jaudete Daltio.pdf,ENGENHARIA DA INFORMAÇÃO,JAUDETE DALTIO,UNIVERSIDADE ESTADUAL DE CAMPINAS,04/09/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Data-Driven Science;Graph Databases;Views',SISTEMAS DE INFORMAÇÃO,CLAUDIA MARIA BAUZER MEDEIROS,0,Ciência Orientada a Dados;Bancos de Dados de Grafos;Visões,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A pesquisa científica tornou-se cada vez mais dependente de dados. Esse novo paradigma de pesquisa demanda técnicas e tecnologias computacionais sofisticadas para apoiar tanto o ciclo de vida dos dados científicos como a colaboração entre cientistas de diferentes áreas. Uma demanda recorrente em equipes multidisciplinares é a construção de múltiplas perspectivas sobre um mesmo conjunto de dados. Soluções atuais cobrem vários aspectos, desde o projeto de padrões de interoperabilidade ao uso de sistemas de gerenciamento de bancos de dados não-relacionais. Entretanto, nenhum desses esforços atende de forma adequada a necessidade de múltiplas perspectivas, denominadas focos nesta tese. Em termos gerais, um foco é projetado e construído para atender um determinado grupo de pesquisa (mesmo no escopo de um único projeto) que necessita manipular um subconjunto de dados de interesse em múltiplos níveis de agregação/generalização. A definição e criação de um foco são tarefas complexas que demandam mecanismos capazes de manipular múltiplas representações de um mesmo fenômeno do mundo real. O objetivo desta tese é prover múltiplos focos sobre dados heterogêneos. Para atingir esse objetivo, esta pesquisa se concentrou em quatro principais problemas. Os problemas inicialmente abordados foram: (1) escolher um paradigma de gerenciamento de dados adequado e (2) elencar os principais requisitos de pesquisas multifoco. Nossos resultados nos direcionaram para a adoção de bancos de dados de grafos como solução para o problema (1) e a utilização do conceito de visões, de bancos de dados relacionais, para o problema (2). Entretanto, não há consenso sobre um modelo de dados para bancos de dados de grafos e o conceito de visões é pouco explorado nesse contexto. Com isso, os demais problemas tratados por esta pesquisa são: (3) a especificação de um modelo de dados de grafos e (4) a definição de um framework para manipular visões em bancos de dados de grafos. Nossa pesquisa nesses quatro problemas resultaram nas contribuições principais desta tese: (i) apontar o uso de bancos de dados de grafos como camada de persistência em pesquisas multifoco -- um tipo de banco de dados de esquema flexível e orientado a relacionamentos que provê uma ampla compreensão sobre as relações entre os dados; (ii) definir visões para bancos de dados de grafos como mecanismo para manipular múltiplos focos, considerando operações de manipulação de dados em grafos, travessias e algoritmos de grafos; (iii) propor um modelo de dados para grafos - baseado em grafos de propriedade - para lidar com a ausência de um modelo de dados pleno para grafos; (iv) especificar e implementar um framework, denominado Graph-Kaleidoscope, para prover o uso de visões em bancos de dados de grafos e (v) validar nosso framework com dados reais em aplicações distintas - em biodiversidade e em recursos naturais",TESE,Views over Graph Databases: A Multifocus Approach for Heterogeneous Data,5064743,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5068809,
"Parallelizing loops containing loop-carried dependencies has been considered a very difficult task, mainly due to the overhead imposed by communicating dependencies between iterations. Despite the huge efforts in the past few decades to devise effective parallelization algorithms for such loops, the problem is still far from solved. For many loops, old DOACROSS, and new Decoupled Software Pipeline (DSWP), algorithms have not been able to offer a solution to this problem.

This thesis discuss in detail two of the most prominent algorithms for parallelizing such loops and also present an analyzis of the performance of the parallelized programs across different multicore architectures. Based on insights from this analyze a new algorithm, called Batched DOACROSS, for parallelizing these loops is proposed. Batched DOACROSS (BDX) capitalizes on the advantages of DSWP and DOACROSS, while minimizing their deficiencies. BDX does not require new hardware mechanisms (as DSWP does) and makes use of thread local buffers to reduce DOACROSS synchronization overheads. An extension to the baseline algorithm is proposed, named Parallel-Stage Batched DOACROSS (PS-BDX), and show that in some cases it can considerably improve the performance of the parallel loop.

BDX and PS-BDX are pipelining multithreading algorithms that employs batching to amortize communication overheads. We provide results for a sensibility analysis and show that for small balanced loops (about 40 instructions), a batch size of only 100 iterations
is sufficient to provide good speedups. Our analyze of PS-BDX for seven benchmarks showed an average of 1.85x speedup for 2 threads, 2.95x for 4 threads and 3.11x for 8 threads which was larger than the other best algorithm that we compared.

A qualitative and quantitative analysis of synchronization costs of the three aforementioned loop parallelization algorithms is performed for two modern computer architectures (ARM A9 MPCore and Intel Ivy Bridge). Our results show that at least 30% of the execution time of the programs we parallelized are spent on synchronization/data communication.

We also show that, besides the problem being hard, Intel Ivy Bridge and ARM A9 MPCore, are on opposite endpoints along the axis of commonly accepted requisites for efficient loop parallelization. As a consequence, all three algorithms struggle to effectively
speedup several programs.",Versão Final_Divino César Soares Lucas.pdf,ENGENHARIA DA INFORMAÇÃO,DIVINO CESAR SOARES LUCAS,UNIVERSIDADE ESTADUAL DE CAMPINAS,06/10/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'parallelism;algorithms;multicore',SISTEMAS DE INFORMAÇÃO,GUIDO COSTA SOUZA DE ARAUJO,0,paralelismo;algoritmos;multi núcleo,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A paralelização de laços de programas contendo loop-carried dependencies é considerada uma tarefa bastante difícil, principalmente devido ao sobrecusto imposto pela comunicação de dependências de dados entre iterações do laço paralelizado. Apesar do grande
empenho em décadas recentes para criar algoritmos efetivos para paralelização deste tipo
de laço o problema ainda é considerado longe de estar resolvido. Para muitos laços, antigos
algoritmos, como o DOACROSS, bem como novos algoritmos, como Decoupled Software
Pipeline (DSWP), não foram capazes de oferecer uma solução eficiente para o problema.

Esta tese discute em detalhes dois dos algoritmos mais proeminentes para a paraleliza-
ção de tais laços e também mostra uma análise da performance de programas paralelizados
usando estes algoritmos em diferentes arquiteturas de processadores multi-núcleos. A par-
tir desta análise surgiu o projeto de um novo algoritmo, chamado Batched DOACROSS
(BDX), para fazer a paralelização deste tipo de laço. Este algoritmo foi pensado de forma
a utilizar as melhores características destes algoritmos anteriores e ao mesmo tempo evitar
o uso de propriedades que se mostraram ineficientes no passado. O algoritmo Batched
DOACROSS não requer suporte de hardware (como é exigido por DSWP) e faz uso de
estruturas locais as linhas de execução para reduzir o sobrecusto com sincronização entre
elas. Uma extensão para o algoritmo BDX também é proposta, chamada Parallel Stage
Batched DOACROSS (PS-BDX), e os resultados indicaram que para alguns casos esta
extensão é capaz de produzir aumentos significativos de desempenho.

BDX e PS-BDX são algoritmos que transformam o laço serial para executar paralela-
mente seguindo um formato de pipeline, além disso estes algoritmos empregam a execução
de lotes de iterações para reduzir o custo de comunicação entre núcleos. Resultados de
análise de sensibilidade mostram que estes novos algoritmos são capazes de produzir bons
resultados até mesmo para laços pequenos (contendo em torno de 40 instruções) quando
configurados em lotes de com ao menos 100 iterações.

A análise do algoritmo PS-BDX usando sete programas mostrou uma média de au-
mento de 1.85x no desempenho dos programas quando estes foram paralelizados usando 2
linhas de execução, 2.95x quando paralelizados com 4 linhas de execução e por fim 3.11x
quando estes programas foram paralelizados usando 8 linhas de execução. Em todos estes
casos o desempenho da versão paralelizada com PS-BDX foi melhor que o segundo melhor
algoritmo experimentado (PS-DSWP).

Uma análise quantitativa e qualitativa dos custo de sincronização em programas para-
lelizados usando os algoritmos acima também foi realizada. Os resultados indicaram que
em média 30% do tempo de execução dos programas paralelos é gasto com sincronização
de acesso a regiões críticas e a dados compartilhados. Também são mostrados resulta-
dos que indicam as arquiteturas de computadores Intel Ivy Bridge e ARM A9 MPCore
se encontram em extremos opostos em se tratando de requisitos para a paralelização de
laços. Em consequência disto todos os algoritmos analisados enfretam dificuldades para
melhorar o desempenho dos programas seriais",TESE,The Batched DOACROSS Algorithm,5076960,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5078254,
"Hydrocarbon exploration currently depends on computational techniques that are used to discovery possible hydrocarbon reservoirs through subsurface structure analyzes. CMP (Common Mid Point), CRS (Common Reflection Surface) and CRP (Common Reflection Point) time migration are the used techniques to help specialists predict locations containing hydrocarbon reservoirs. These methods are usually computationally exhaustive, making their execution in some cases prohibitive. Such features makes the use of these techniques expensive. In this work we propose a new parallel programming approach using hardware accelerators to optimize the performance of these techniques. This work proposes a OpenCL (Open Computing Language) implementation that enable the use of hardware accelerators such as GPUs and Xeon Phi to reduce processing time of CMP, CRS, and CRP time migration.",Versão Final_Hércules Cardoso da Silva.pdf,SISTEMAS DE COMPUTAÇÃO,HERCULES CARDOSO DA SILVA,UNIVERSIDADE ESTADUAL DE CAMPINAS,06/11/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'CRS;GPU;HPC;OpenCL;CRP time migration;CMP',PROJETOS DE SISTEMAS COMPUTACIONAIS,EDSON BORIN,0,CRS;GPU;Computação de alto desempenho;OpenCL;migração CRP em tempo;CMP,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A exploração de hidrocarbonetos atualmente depende de técnicas computacionais que são utilizadas para determinar possíveis reservatórios a partir de análises de estruturas do subsolo. CMP (Common Mid Point), CRS (Common Reflection Surface) e migração CRP (Common Reflection Point) em tempo são técnicas utilizadas para auxiliar especialistas a prever se em um determinado local é possível encontrar uma fonte. Esses métodos normalmente são computacionalmente exaustivos, tornando a execução deles em alguns casos proibitivos. Tais características dificultam o uso dessas técnicas. Neste trabalho é proposta uma nova abordagem de programação paralela utilizando aceleradores de hardware para otimizar o desempenho dessas técnicas. Este trabalho propõe uma implementação em OpenCL (Open Computing Language) que possibilite o uso de aceleradores de hardware como GPUs e o Xeon Phi, com o objetivo de reduzir o tempo de execução dos métodos CMP, CRS e migração CRP em tempo. Resultados são apresentados com dados reais e sintéticos para validar as propostas desse trabalho.",DISSERTAÇÃO,Aceleração de Métodos de Processamento Sísmico com OpenCL,5086077,1
"Atualmente, doenças respiratórias afetam uma grande parcela da população mundial. A melhor maneira para detecção e análise desse tipo de doença é o diagnóstico por imagem, principalmente a tomografia computadorizada (CT). Sistemas de apoio ao diagnóstico foram desenvolvidos para auxiliar especialistas clínicos na análise de imagens de CT e na obtenção de diagnósticos precisos e rápidos. Em tais sistemas, a segmentação dos pulmõesé um passo primordial a ser realizado, sendo fundamental para análises quantitativas. Ao longo dos últimos anos, muitos métodos de segmentação dos pulmões foram propostos. Entretanto, eles sofrem de pelo menos uma das seguintes limitações: alto tempo computacional, fracas condições para separação da traqueia e de cada pulmão, e um número limitado de amostras para validação. Endereçando tais limitações, este projeto de mestrado propõe um método rápido e automático chamado ALTIS, para segmentação dos pulmões e da traqueia em imagens de CT do tórax. O método ALTIS se fundamenta na competição ótima de sementes para segmentar os pulmões e a traqueia em tempo proporcional ao tamanho da imagem. Ele consiste de uma rápida sequência de operações de processamento de imagem baseadas em características anatômicas que são robustas para a maioria das variações de forma e aparência dos pulmões. Isto é, a partir da premissa de que o sistema respiratório em uma imagem de CT representa o maior objeto cercado por tecido mais claro, os pulmões e a traqueia são extraídos criando o volume de interesse. Considerando que os pulmões direito e esquerdo são mais largos do que a traqueia e que a traqueia é um objeto longo e distante dos pulmões, sementes com rótulos diferentes dentro de cada um desses componentes são estimadas por meio de transformadas de distância. A competição ótima de sementes propaga esses rótulos para o resto do volume de interesse, realizando o delineamento dos objetos. O delineamento é feito nos volumes de ar da traqueia e dos pulmões até a pleura visceral. Assim, a segmentação da cavidade pleural está fora do escopo deste projeto.
O método ALTIS foi extensivamente avaliado em um conjunto de aproximadamente 1.750 imagens de tomografia, unindo tanto bases de dados internas como públicas. Até onde sabemos, esse é o maior conjunto de imagens para validação dentre os trabalhos reportados na literatura. Além do método ALTIS, outros dois métodos baseados em modelos de forma, MALF e SOSM-S, foram quantitativamente avaliados em 250 imagens desse conjunto. Essa avaliação foi feita através da análise de sobreposição e distância até a borda das segmentações interativas consideradas corretas. Os experimentos realizados indicaram que o método ALTIS é estatisticamente superior e consideravelmente mais rápido que ambos os métodos comparados. As 1.500 imagens restantes foram utilizadas para verificação da robustez do método proposto.",,ENGENHARIA DA INFORMAÇÃO,AZAEL DE MELO E SOUSA,UNIVERSIDADE ESTADUAL DE CAMPINAS,09/11/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Automatic CT image segmentation of the thorax;image separation of the lungs and trachea;image foresting transform',SISTEMAS DE INFORMAÇÃO,ALEXANDRE XAVIER FALCAO,0,Segmentação automática de imagens de CT do tórax;separação dos pulmões e da traqueia;transformada imagem floresta,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Atualmente, doenças respiratórias afetam uma grande parcela da população mundial. A melhor maneira para detecção e análise desse tipo de doença é o diagnóstico por imagem, principalmente a tomografia computadorizada (CT). Sistemas de apoio ao diagnóstico foram desenvolvidos para auxiliar especialistas clínicos na análise de imagens de CT e na obtenção de diagnósticos precisos e rápidos. Em tais sistemas, a segmentação dos pulmõesé um passo primordial a ser realizado, sendo fundamental para análises quantitativas. Ao longo dos últimos anos, muitos métodos de segmentação dos pulmões foram propostos. Entretanto, eles sofrem de pelo menos uma das seguintes limitações: alto tempo computacional, fracas condições para separação da traqueia e de cada pulmão, e um número limitado de amostras para validação. Endereçando tais limitações, este projeto de mestrado propõe um método rápido e automático chamado ALTIS, para segmentação dos pulmões e da traqueia em imagens de CT do tórax. O método ALTIS se fundamenta na competição ótima de sementes para segmentar os pulmões e a traqueia em tempo proporcional ao tamanho da imagem. Ele consiste de uma rápida sequência de operações de processamento de imagem baseadas em características anatômicas que são robustas para a maioria das variações de forma e aparência dos pulmões. Isto é, a partir da premissa de que o sistema respiratório em uma imagem de CT representa o maior objeto cercado por tecido mais claro, os pulmões e a traqueia são extraídos criando o volume de interesse. Considerando que os pulmões direito e esquerdo são mais largos do que a traqueia e que a traqueia é um objeto longo e distante dos pulmões, sementes com rótulos diferentes dentro de cada um desses componentes são estimadas por meio de transformadas de distância. A competição ótima de sementes propaga esses rótulos para o resto do volume de interesse, realizando o delineamento dos objetos. O delineamento é feito nos volumes de ar da traqueia e dos pulmões até a pleura visceral. Assim, a segmentação da cavidade pleural está fora do escopo deste projeto.
O método ALTIS foi extensivamente avaliado em um conjunto de aproximadamente 1.750 imagens de tomografia, unindo tanto bases de dados internas como públicas. Até onde sabemos, esse é o maior conjunto de imagens para validação dentre os trabalhos reportados na literatura. Além do método ALTIS, outros dois métodos baseados em modelos de forma, MALF e SOSM-S, foram quantitativamente avaliados em 250 imagens desse conjunto. Essa avaliação foi feita através da análise de sobreposição e distância até a borda das segmentações interativas consideradas corretas. Os experimentos realizados indicaram que o método ALTIS é estatisticamente superior e consideravelmente mais rápido que ambos os métodos comparados. As 1.500 imagens restantes foram utilizadas para verificação da robustez do método proposto.",DISSERTAÇÃO,Segmentação Automática dos Pulmões e da Traqueia em Imagem de Tomografia Computadorizada do Tórax,5149216,1
"Triangular meshes are spatial data representations commonly used in the manipulation and visualization of complex surfaces. This work proposes and evaluates a static out-of-core representation of three-dimensional triangle meshes, which allows the query of adjacent vertices and triangles in constant time and the random access to the vertices and triangles of the mesh, requiring little space in main memory. The out-of-core feature of the representation consists in the fact that only the necessary information for the application is loaded into primary memory, such that the remainder is stored in secondary memory. The representation is static in the sense that when any topological change occurs, it must be rebuilt again. Experiments are conducted on several volumetric models to demonstrate the efficacy of the proposed methodology.",Versão Final_Thiago Franco de Moraes.pdf,ENGENHARIA DA INFORMAÇÃO,THIAGO FRANCO DE MORAES,UNIVERSIDADE ESTADUAL DE CAMPINAS,17/11/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Out-of-Core representation;Triangle Meshes;Rendering',SISTEMAS DE INFORMAÇÃO,HELIO PEDRINI,0,Representações Out-of-Core;Malha de Triângulos;Renderização,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Malhas triangulares são representações de dados espaciais comumente utilizadas na manipulação e visualização de superfícies complexas. Este trabalho apresenta e avalia uma proposta de representação out-of-core estática de malhas de triângulos tridimensionais, a qual permite a consulta de vértices e triângulos adjacentes em tempo constante e o acesso aleatório aos vértices e triângulos da malha, requerendo pouco espaço em memória principal. A característica out-of-core da representação consiste no fato de que apenas as informações necessárias para a aplicação são carregadas em memória primária, ficando o restante armazenado em memória secundária. A representação é estática no sentido de que, ao ocorrer qualquer alteração topológica, ela deverá ser reconstruída novamente. Experimentos são realizados em vários modelos volumétricos para demonstrar a eficácia da metodologia proposta.",DISSERTAÇÃO,Representação Out-of-Core de Malhas Triangulares Tridimensionais para Renderização de Grandes Volumes de Dados,5170580,1
"Software product lines (SPL) are gaining interest because of the increasing demand for customizable products. This is partly because SPLs are an efficient and effective means of delivering products with higher quality at a lower cost. In SPL, products have common requirements and also, specific features for each one. Testing whether a product implements common and specific requirements is an important step to ensure good quality of the derived products. However, testing a SPL is a complex task, since the variety of products that can be derived from the combination of common and specific features is huge. Even if only a few specific products are selected, the effort to test them is still great, since the products vary in terms of the specific features that are selected. Therefore, reusing test cases from one product to another to determine whether they satisfy the functional requirements may not be possible. Model-based testing (MBT) may be useful in this case, in which a behavior model can be obtained from the requirements and this model can be used for automatic test cases generation.

This work presents an approach in which SPL requirements are centered on use cases. Use Cases (UC) are a popular format for representing requirements. From the use case descriptions written in the form of a semi-structured format and containing the variability specification, the behavior models are automatically generated for a product under test, in the form of a state machine model. Building a state machine is not a trivial task for most practitioners, who are more familiarized with textual and informal descriptions of requirements. In general, the manual creation of state machine models from UCs can be time-consuming and prone to errors. The goal is to provide to the test engineers with a method that guides them in the creation of artifacts necessary to extract a preliminary version of a state model from the requirements. This preliminary model can be refined to become suitable for a test case generation tool.

Some guidelines are also provided for the refinement process. As proof of concept, a prototype of a tool was developed, MARITACA, which uses natural language processing techniques to extract state machines from the use case descriptions. The text presents the use of the method and the tool in an illustrative example, obtained from the literature, and in a family of distributed fault-tolerant applications. This study showed the applicability of the proposed method. One of the concerns in SPL testing is the generation of redundant test cases from one product to another. The results, though preliminary, showed that most of the test cases generated for a new product are not redundant because they involve specific features of each product.",Aguardando Versão Final.pdf,ENGENHARIA DA INFORMAÇÃO,LEYDI ROCIO ERAZO PARUMA,UNIVERSIDADE ESTADUAL DE CAMPINAS,11/12/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Software product line testing;Model based testing;State machines',SISTEMAS DE INFORMAÇÃO,ELIANE MARTINS,0,Testes em linhas de produto software;Testes baseados em modelos;Máquinas de estados,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"As linhas de produtos de software (LPS) estão ganhando interesse devido à crescente demanda por produtos personalizáveis. Tal se deve, em parte, por que as LPS são um meio eficiente e efetivo de entregar produtos com maior qualidade a um custo menor.
Em uma LPS, produtos têm requisitos em comum e também, características específicas a cada um. Testar se um produto implementa os requisitos comuns e específicos é um importante passo para garantir uma boa qualidade. No entanto, o teste de uma LPS é uma tarefa complexa, uma vez que a variedade de produtos que podem ser derivados a partir da combinação de características comuns e específicas é enorme. Mesmo que se escolha apenas alguns produtos selecionados, o esforço para testá-los ainda assim é grande, dado que os produtos variam em termos das características específicas selecionadas. Portanto, reutilizar casos de teste de um produto para o outro para determinar se satisfazem os requisitos funcionais, pode não ser possível. Os testes baseados em modelos (MBT) podem ser úteis neste caso, nos quais um modelo de comportamento pode ser obtido a partir dos requisitos e este modelo pode ser usado para a geração automática de casos de teste.

Neste trabalho é apresentada uma abordagem em que os requisitos SPL são centrados em casos de uso. Casos de uso (UC) são um formato popular para representar os requisitos. A partir das descrições de casos de uso escritas em um formato semi-estruturado e contendo a especificação de variabilidade, os modelos de comportamento são gerados automaticamente para um produto sob teste, na forma de um modelo de máquina de estado. Construir uma máquina de estado não é trivial para a maioria dos profissionais, que estão mais habituados com descrições textuais e informais dos requisitos. Em geral, a criação manual de modelos de máquinas de estado a partir de UCs pode ser demorado e propenso a erros. O objetivo é fornecer aos engenheiros de teste um método que os guie na criação dos artefatos necessários para que uma versão preliminar de um modelo de estado seja extraída automaticamente dos requisitos. Este modelo preliminar pode ser refinado para tornar-se adequado para uma ferramenta de geração de casos de teste.

Para esse processo de refinamento também são fornecidas algumas diretrizes. Como prova de conceito, desenvolveu-se um protótipo de uma ferramenta, MARITACA, que utiliza técnicas de processamento de língua natural para extrair as máquinas de estado a partir das descrições dos casos de uso. O texto apresenta o uso do método e da ferramenta em um exemplo ilustrativo, obtido da literatura, e em uma família de aplicações distribuídas tolerantes a falhas. Este estudo mostrou a aplicabilidade do método proposto. Uma das preocupações nos testes de SPL é a geração de casos de teste redundantes de um produto para outro. Os resultados, embora preliminares, mostraram que a maioria dos casos de teste gerados para um novo produto não são redundantes, pois envolvem características específicas de cada produto.",DISSERTAÇÃO,A method and a tool for model-based testing of software product lines,5170681,1
"In Brazil, access to education is guaranteed by law. Thus, education professionals seek to use different materials and technologies to enable their students, regardless of their abilities and needs, to be inserted into the school environment. In this way, technologies are increasingly used as mediators of the teaching and learning process. However, not all technologies promote inclusive education. In this scenario, the Tangible User Interfaces stand out, allowing the user to interact with physical objects to carry out their activities, promoting a sensorial engagement of their users. The Tangible Interfaces can promote greater accessibility for its users, being a powerful tool to promote inclusion in the school environment. However, many of the works seen in the literature are aimed at a very specific target audience, making it difficult to use them in the classroom, considering that each student has their needs. Another factor that should be considered is the ability of the teacher to use the technology. In our research, we work as the hypothesis that the use of participatory design techniques, with professionals of inclusive education, during all phases of the project can help the creation of technologies aimed at inclusive education. We developed a series of workshops using different techniques to instigate the participation of teachers and observe how they influence the creation process.",Versão Final_Leonara de Medeiros Braz.pdf,ENGENHARIA DA INFORMAÇÃO,LEONARA DE MEDEIROS BRAZ,UNIVERSIDADE ESTADUAL DE CAMPINAS,11/12/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Participatory Design;Design for all;Inclusive Education',SISTEMAS DE INFORMAÇÃO,HEIKO HORST HORNUNG,0,Design Participativo;Design para Todos;Educação Inclusiva,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"No Brasil, o acesso à educação é garantido por lei. Assim, profissionais de educação buscam utilizar diferentes materiais e tecnologias para possibilitar que seus alunos, independentemente de suas habilidades e necessidades, possam ser inseridos dentro do ambiente escolar. Desse modo, tecnologias são cada vez mais utilizadas como mediadoras do processo de ensino e aprendizagem. Porém, nem todas as tecnologias promovem uma educação inclusiva. Diante desse cenário, destacam-se as Interfaces de Usuário Tangíveis, permitindo ao usuário interagir com objetos físicos para a realização de suas atividades, promovendo um engajamento sensorial de seus usuários. As Interfaces Tangíveis podem promover uma maior acessibilidade para seus usuários, sendo uma poderosa ferramenta para promover inclusão no ambiente escolar. Porém, muitos dos trabalhos vistos na literatura são voltados para um público-alvo muito específico, dificultado sua utilização em sala de aula, considerando que cada aluno tem suas necessidades. Outro fator que deve ser considerado é a habilidade do professor que utilizará a tecnologia. Em nossa pesquisa, trabalhamos como a hipótese de que a utilização de técnicas de design participativo, com profissionais da educação inclusiva, durante todas as fases do projeto pode auxiliar a criação de tecnologias voltadas para a educação inclusiva. Elaboramos uma série de oficinas utilizando diferentes técnicas para instigar a participação dos professores e observar como as mesmas influenciam no processo de criação.",DISSERTAÇÃO,Design para todos e Educação Inclusiva: Envolvendo Professores na Criação de Tecnologias,5170755,1
"To derive efficient schedules for the tasks of scientific applications modelled as workflows, schedulers need information on the application demands as well as on the resource availability, especially those regarding the available bandwidth. However, the lack of precision of bandwidth estimates provided by monitoring/measurement tools should be considered by the scheduler to achieve near-optimal schedules. Uncertainties of available bandwidth can be a result of imprecise measurement and monitoring network tools and/or their incapacity of estimating in advance the real value of the available bandwidth expected for the application during the scheduling step of the application. Schedulers specially designed for hybrid clouds simply ignore the inaccuracies of the given estimates and end up producing misleading, low-performance schedules, making them non-robust schedulers under uncertainties stemming from using these networking tools.

This thesis introduces a proactive mechanism to provide robustness to schedulers not designed to be robust in the face of uncertainties stemming from using imprecise networking tools. To make non-robust schedulers into robust schedulers, the mechanism deflates uncertain estimates before being used as input to the non-robust schedulers. By proposing the use of refined estimates of the available bandwidth, non-robust schedulers initially sensitive to these uncertainties started to produce robust schedules under these inaccuracies. The effectiveness and efficiency of the mechanism in providing robustness to non-robust schedulers are evaluated through simulation. Schedules generated by induced-robustness schedulers through the use of the mechanism is compared to that of produced by uncertainty-sensitive schedulers. In addition, this thesis also introduces a flexible scheduler for a special case of scientific applications modelled as a set of workflows grouped into ensembles. Although the novelty of this scheduler is the replacement of objective functions according to the user's needs, it is a non-robust scheduler as well. However, the mechanism was able to provide the necessary robustness for this flexible scheduler be able to produce robust, high-performance schedules under uncertain bandwidth estimates.

It is shown in this thesis that the proposed mechanism enhanced the effectiveness of workflow schedulers designed especially for hybrid clouds as they started to produce more robust schedules in the presence of uncertainties stemming from using monitoring/measurement networking tools. The proposed mechanism is an important tool to furnish robustness to non-robust schedulers that are originally designed to work in a computational environment where bandwidth estimates are very likely to vary and cannot be estimated precisely in advance, bringing, therefore, improvements in the execution of scientific applications in hybrid clouds.",Aguardando Versão Final.pdf,SISTEMAS DE COMPUTAÇÃO,THIAGO AUGUSTO LOPES GENEZ,UNIVERSIDADE ESTADUAL DE CAMPINAS,13/12/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'scheduling;workflows;hybrid clouds',SISTEMAS DISTRIBUÍDOS,EDMUNDO ROBERTO MAURO MADEIRA,154,escalonamento;workflows;nuvens híbridas,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Para que escalonadores de aplicações científicas modeladas como workflows derivem escalonamentos eficientes em nuvens híbridas, é necessário que se forneçam, além da descrição da demanda computacional desses aplicativos, as informações sobre o poder de computação dos recursos disponíveis, especialmente aqueles dados relacionados com a largura de banda disponível. Entretanto, a imprecisão das ferramentas de medição fazem com que as informações da largura de banda disponível fornecida aos escalonadores difiram dos valores reais que deveriam ser considerados para se obter escalonamentos quase ótimos. Escalonadores especialmente projetados para nuvens híbridas simplesmente ignoram a existência de tais imprecisões e terminam produzindo escalonamentos enganosos e de baixo desempenho, o que os tornam sensíveis às informações incertas.

A presente Tese introduz um mecanismo pró-ativo para fornecer robustez a escalonadores não projetados para serem robustos frente às incertezas decorrentes do uso de ferramentas de medições imprecisas. Para tentar tornar os escalonadores sensíveis às incertezas em escalonadores robustos às essas imprecisões, o mecanismo propõe o refinamento das estimativas dadas por essas ferramentas de rede antes de serem utilizadas pelo escalonador. Ao propor o uso de estimativas refinadas da largura de banda disponível, escalonadores inicialmente sensíveis às incertezas passaram a produzir escalonamentos robustos às essas imprecisões. A eficácia e a eficiência do mecanismo em prover robustez a escalonadores não robustos são avaliadas através de simulação. Comparam-se, portanto, os escalonamentos gerados pelos escalonadores com robustez induzida pelo mecanismo com aqueles produzidos originalmente pelos escalonadores sensíveis às incertezas. Além disso, esta Tese também propõe um escalonador de aplicações científicas especialmente compostas por um conjunto de workflows. A novidade desse escalonador é que ele é flexível, ou seja, permite o uso diferentes categorias de funções objetivos. Embora a flexibilidade proposta seja uma novidade no estado da arte, esse escalonador também é sensível às incertezas da largura de banda estimada. Entretanto, o mecanismo mostrou-se capaz de provê-lo robustez frente às tais incertezas.

É mostrado nesta Tese que o mecanismo proposto aumentou a eficácia e a eficiência de escalonadores de workflows não robusto projetados para nuvens híbridas, já que eles passaram a produzir escalonamentos com um alto desempenho na presença de estimativas incertas da largura de banda disponível. Dessa forma, o mecanismo proposto nesta Tese é uma importante ferramenta para aprimorar os escalonadores sensíveis às estimativas incertas da banda dispo",TESE,Provendo Robustez a Escalonadores de Workflows Sensíveis às Incertezas da Largura de Banda Disponível,5170814,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_1.html,,,,,,,,,,,5170895,
"Service Oriented Architecture (SOA) is a popular design pattern to build web services be- cause of the interoperability, scalability, and reuse of software solutions that it promotes. The services using this architecture need to operate in a highly dynamic environment, but as the complexity of these services grows, traditional validation processes become less feasible. SOA applications can evolve and change during their execution, and offline tests do not completely assure the correct behavior of the system during its execution. There- fore there is a need of techniques to validate the proper behaviour of SOA applications during the SOA lifecycle. Because of that, in this project online testing will be used.

The project goal is to employ model-based testing techniques to generate and execute relevant test cases to SOA applications during runtime. In order to achieve this goal a self-adaptive model-based online testing framework was designed.

Tests based on models can be generated offline and online. Offline test are generated before the system execution. Online tests are generated and performed concomitantly, and the output produced by the application under test defines the next step to be performed. when our solution detects that a monitored service evolves, the model of the target service is updated, and online test case generation and execution is performed.

More specifically, four components were integrated in a self-adaptive loop: a mon- itoring service, a model generator service, a model based testing service and a testing platform. The testing framework had its features tested in three scenarios that were performed in a SOA application orchestrated by BPEL, called jSeduite.

This work is an effort to understand the constraints and limitations of the software testing on SOA applications, and present analysis and solutions to some of the problems found during the research.",Aguardando Versão Final.pdf,ENGENHARIA DA INFORMAÇÃO,LUCAS CARVALHO LEAL,UNIVERSIDADE ESTADUAL DE CAMPINAS,14/12/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'SOA;regression testing;Model-based;Self-adaptive',SISTEMAS DE INFORMAÇÃO,ELIANE MARTINS,0,SOA;teste de regressão;baseado em modelo;auto adaptativo,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Arquitetura orientada a serviços (SOA) é um padrão de design popular para implemen- tação de serviços web devido à interoperabilidade, escalabilidade e reuso de soluções de software que promove. Os serviços que usam essa arquitetura precisam operar em um am- biente altamente dinâmico, entretanto quanto mais a complexidade desses serviços cresce menos os métodos tradicionais de validação se mostram viáveis.

Aplicações baseadas em arquitetura orientada a serviços podem evoluir e mudar du- rante a execução. Por conta disso testes offline não asseguram completamente o compor- tamento correto de um sistema em tempo de execução. Por essa razão, a necessidade de tecnicas diferentes para validar o comportamento adequado de uma aplicação SOA du- rante o seu ciclo de vida são necessárias, por isso testes online serão usados nesse projeto.

O objetivo do projeto é de aplicar técnicas de testes baseados em modelos para gerar e executar casos de testes relevantes em aplicações SOA durante seu tempo de execu- ção. Para alcançar esse objetivo uma estruta de teste online autoadaptativa baseada em modelos foi idealizada.

Testes baseados em modelos podem ser gerados de maneira offline ou online. Nos testes offline, os casos de teste são gerados antes do sistema entrar em execução. Já nos testes online, os casos de teste são gerados e aplicados concomitantemente, e as saídas produzidas pela aplicação em teste definem o próximo passo a ser realizado. Quando uma evolução é detectada em um serviço monitorado uma atualização no modelo da aplicação alvo é executada, seguido pela geração e execução de casos de testes online.

Mais precisamente, quatro componentes foram integrados em um circuito autoadap- tativo: um serviço de monitoramento, um serviço de criação de modelos, um serviço de geração de casos de teste baseado em modelos e um serviço de teste. As caracteristicas da estrutura de teste foram testadas em três cenários que foram executados em uma aplicação SOA orquestrada por BPEL, chamada jSeduite.

Este trabalho é um esforço para entender as restrições e limitações de teste de soft- ware para aplicações SOA, e apresenta análises e soluções para alguns dos problemas encontrados durante a pesquisa.",DISSERTAÇÃO,Self-Adaptive Model-Based Online Testing for Dynamic SOA,5436014,1
"Heterogeneous Networks (HetNets) come as a clever approach to increase the capacity and the coverage of cellular networks in which low power base stations can share the load of high power ones. However, such strategy also brought new challenges. For instance, user association techniques used on homogeneous networks are no longer efficient when the amount of served users and the load balancing among the BSs are considered.

In this work, the user association in HetNets problem is modeled as an integer linear programming (ILP) problem aiming to balance the traffic load among short and long range cells. In addition, two heuristics are introduced: a centralized solution based on a greedy algorithm and a distributed, probabilistic load-aware solution. These heuristics produce good results of load balancing among the cells and in terms of number of accepted users in comparison with the optimal solution and better results than some of the main strategies presented in the literature.",,TEORIA DA COMPUTAÇÃO,ALEXANDRE TOSHIO HIRATA,UNIVERSIDADE ESTADUAL DE CAMPINAS,15/12/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'HetNet;Load Balancing;User Association',ALGORITMOS E OTIMIZAÇÃO,JULIANA FREITAG,87,HetNet;Balanceamento de Carga;Associação de Usuários,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Redes Heterogêneas (HetNets) apareceram como um modo inteligente de aumentar a capacidade e cobertura de redes de celular nas quais estações rádio-base (BSs) de baixa potência podem dividir a carga de estações de alta potência. Entretanto, essa estratégia também trouxe novos desafios. Por exemplo, técnicas para associação de usuários usadas em redes homogêneas não são eficientes neste tipo de rede quando se considera a quantidade de usuários servidos e o balanço de carga entre as BSs.

Neste trabalho, o problema de associação de usuários em HetNets é modelado como um problema de programação linear inteira (ILP) com o intuito de balancear a carga entre células de curto e longo alcance. Além disso, duas heurísticas são introduzidas: uma solução centralizada baseada em um algoritmo guloso e uma estratégia distribuída, probabilística e ciente de carga. Estas heurísticas produzem bons resultados de balanceamento de carga entre as células e em termos de número de usuários aceitos quando comparados à solução ótima e melhores resultados que algumas das principais estratégias apresentadas na literatura.",DISSERTAÇÃO,Load Balancing and User Association in HetNets,5436276,1
"Lung cancer, which is characterized by the presence of nodules, is the most common type of cancer around the world, as well as one of the most aggressive and deadliest cancer, with 20% of total cancer mortality. Lung cancer screening can be performed by radiologists analyzing chest X-ray (CXR) images. However, the detection of lung nodules is a difficult task due to their wide variability, human limitations of memory, distraction and fatigue, among other factors. These difficulties motivate the development of computer-aided diagnosis (CAD) systems for supporting radiologists in detecting lung nodules. Lung nodule classification is one of the main topics related to CAD systems. Although convolutional neural networks (CNN) have been demonstrated to perform well on many tasks, there are few explorations of their use for classifying lung nodules in CXR images. In this work, we proposed and analyzed a pipeline for detecting lung nodules in CXR images that includes lung area segmentation, potential nodule localization, and nodule candidate classification. We presented a method for classifying nodule candidates with a CNN trained from the scratch. The effectiveness of our method relies on the selection of data augmentation parameters, the design of a specialized CNN architecture, the use of dropout regularization on the network, inclusive in convolutional layers, and addressing the lack of nodule samples compared to background samples balancing mini-batches on each stochastic gradient descent iteration. All model selection decisions were taken using a CXR subset of the Lung Image Database Consortium and Image Database Resource Initiative (LIDC/IDRI) dataset separately. Thus, we used all images with nodules in the Japanese Society of Radiological Technology (JSRT) dataset for evaluation. Our experiments showed that CNNs were capable of achieving competitive results when compared to state-of-the-art methods. Our proposal obtained an area under the free-response receiver operating characteristic (AUC) curve of 7.51 considering 10 false positives per image (FPPI), and a sensitivity of 71.4% and 81.0% with 2 and 5 FPPI, respectively.",Aguardando Versão Final.pdf,ENGENHARIA DA INFORMAÇÃO,JULIO CESAR MENDOZA BOBADILLA,UNIVERSIDADE ESTADUAL DE CAMPINAS,18/12/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Lung Nodule;Chest Radiography;Computer-Aided Diagnosis',COMPUTAÇÃO VISUAL,HELIO PEDRINI,0,Nódulo pulmonar;Radiografia de tórax;Sistemas de diagnóstico por computador,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"O câncer de pulmão, que se caracteriza pela presença de nódulos, é o tipo mais comum de câncer em todo o mundo, além de ser um dos mais agressivos e fatais, com 20% da mortalidade total por câncer. A triagem do câncer de pulmão pode ser realizada por radiologistas que analisam imagens de raios-X de tórax (CXR). No entanto, a detecção de nódulos pulmonares é uma tarefa difícil devido a sua grande variabilidade, limitações humanas de memória, distração e fadiga, entre outros fatores. Essas dificuldades motivam o desenvolvimento de sistemas de diagnóstico por computador (CAD) para apoiar radiologistas na detecção de nódulos pulmonares. A classificação do nódulo do pulmão é um dos principais tópicos relacionados aos sistemas de CAD. Embora as redes neurais convolucionais (CNN) tenham demonstrado ter um bom desempenho em muitas tarefas, há poucas explorações de seu uso para classificar nódulos pulmonares em imagens CXR. Neste trabalho, propusemos e analisamos um arcabouço para a detecção de nódulos pulmonares em imagens de CXR que inclui segmentação da área pulmonar, localização de nódulos e classificação de nódulos candidatos. Apresentamos um método para classificação de nódulos candidatos com CNN treinadas a partir do zero. A eficácia do nosso método baseia-se na seleção de parâmetros de aumento de dados, no projeto de uma arquitetura CNN especializada, no uso da regularização de dropout na rede, inclusive em camadas convolucionais, e no tratamento da falta de amostras de nódulos em comparação com amostras de fundo, balanceando mini-lotes em cada iteração da descida do gradiente estocástico. Todas as decisões de seleção do modelo foram tomadas usando-se um subconjunto de imagens CXR da base Lung Image Database Consortium and Image Database Resource Initiative (LIDC/IDRI) separadamente. Então, utilizamos todas as imagens com nódulos no conjunto de dados da Japanese Society of Radiological Technology (JSRT) para avaliação. Nossos experimentos mostraram que as CNNs foram capazes de alcançar resultados competitivos quando comparados com métodos da literatura. Nossa proposta obteve uma curva de operação (AUC) de 7.51 considerando 10 falsos positivos por imagem (FPPI) e uma sensibilidade de 71.4% e 81.0% com 2 e 5 FPPI, respectivamente.",DISSERTAÇÃO,Lung Nodule Classification Based on Deep Convolutional Neural Networks,5437226,1
,,,,,,,UNICAMP_CIENCIA_DA_COMPUTACAO_33003017005P8_7_2017_2.html,,,,,,,,,,,5437643,
"Video on Demand (VoD) is a service that enables the subscriber to choose different kinds of content, such as movies, TV shows, concerts etc and receive them, possibly in high quality, through an on-line device without the need to acquire or rent physical media. Currently, VoD is vastly deployed on peer-to-peer and cloud computing networks. In this dissertation, we are going to study how to minimize operational costs of a VoD service implemented over Hybrid Cloud, taking into consideration several aspects, such as content distribution in virtualized environment, storage for big portions of data and required network bandwidth. We first describe online heuristics that aim to be not only fast, but also economically efficient. Different policies are designed in an attempt to cover most real-world use-cases. Later on, we also propose a similar model using integer linear programming. Finally, our heuristics and ILP model will be extensively evaluated under small and large scale simulations and the results will be compared.",,TEORIA DA COMPUTAÇÃO,THIAGO FERNANDES CREPALDI,UNIVERSIDADE ESTADUAL DE CAMPINAS,19/12/2017,PORTUGUES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'Computao em nuvem;Heurstica computacional;Sistemas multimdia',ALGORITMOS E OTIMIZAÇÃO,NELSON LUIS SALDANHA DA FONSECA,0,Computing cloud;Computational heuristic;Multimedia systems,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"Vídeo sob Demanda é um serviço no qual assinantes escolhem diferentes vídeos em um catálogo digital e recebem fluxo de vídeo através da Internet em seus dispositivos sem a necessidade de adquirir ou alugar mídia física. Atualmente, este tipo de serviço é vastamente implantado em redes peer-to-peer e computação em nuvem. Nesta dissertação, introduzem-se heurísticas para tomada de decisão sobre alocação dinâmica de recursos em nuvens híbridas, bem como sobre a distribuição de vídeo em datacenters geograficamente distribuídos a fim de se prover serviço de vídeo sob demanda. Diferentes políticas e critérios
de seleção de servidores são propostos. Propõe-se, também, um modelo utilizando Programação Linear Inteira para avaliar a efetividade das diferentes heurísticas. Os resultados mostram que as heurísticas ajudam a oferecer serviço de VoD com baixo custo e alta qualidade de serviço. Ao se comparar os resultados dados pelas soluções heurísticas com
os resultados dados pelo PLI, verificou-se que a precisão das heurísticas tende a melhorar conforme o tamanho das instâncias simuladas aumentam.",DISSERTAÇÃO,Heurísticas para Alocação de Servidores em Serviços de Vídeo sob Demanda sobre Nuvens Híbridas,5437817,1
"A sabedoria popular diz que qualquer sistema é tão seguro quanto o seu elo mais fraco. Com o uso generalizado de bibliotecas criptográficas de boa reputação, baseadas em padrões confiáveis de algoritmos de qualidade, implementações inseguras ou falhas na matemática subjacente não são os elos fracos mais prováveis. Porém, o comprometimento da criptografia ou da sua implementação são eventos raros de impacto catastrófico.
Atualmente, a investigação sistemática de problemas práticos associados ao mau uso de criptografia está ganhando força. À medida que a segurança torna-se transparente para os usuários e a criptografia de boa reputação está disponível para todos os desenvolvedores, o elo fraco mais provável deixa de ser a infra-estrutura criptográfica para se tornar o software em torno da criptografia, escrito por desenvolvedores não especialistas no assunto. Hoje, o mau uso generalizado da criptografia em software é a fonte mais freqüente de problemas de segurança relacionados à criptografia. O efeito cumulativo desse mau uso generalizado também tem um impacto catastrófico, embora difuso.
Esta tese investiga o papel da criptografia na segurança de software e propõe a segurança de software criptográfico como um novo campo de estudo, preocupado com o desenvolvimento sistemático de software criptográfico seguro.
Esta tese alcançou os seguintes resultados:
Primeiramente, uma revisão da literatura sobre programação e verificação de software criptográfico descobriu que apenas um quarto das ferramentas identificadas poderia ser usado por programadores não especialistas em criptografia.
Em segundo lugar, uma metodologia para o desenvolvimento de software criptográfico seguro consolidou as práticas casuais empregadas pela segurança de software na construção de software criptográfico. Adicionalmente, um estudo empírico sobre comunidades on-line para programação descobriu que o mau uso de criptografia é muito freqüente, com padrões recorrentes de mau uso.
Em quarto lugar, uma avaliação de ferramentas de análise estática descobriu que estas ferramentas detectam pouco mais de um terço dos maus usos criptográficos.
Além deste, um estudo longitudinal e retrospectivo sobre o mau uso da criptografia descobriu que desenvolvedores podem aprender a usar APIs criptográficas sem realmente aprender criptografia, enquanto alguns maus usos persistem ao longo do tempo.
Finalmente, uma classificação dos maus usos da criptografia, voltada para a área de segurança de software, e uma metodologia para o desenvolvimento de software criptográfico compõem o corpo de conhecimento para o seu desenvolvimento seguro.
Há uma grande lacuna entre o que os especialistas em criptografia vêem como maus usos de criptografia, os maus usos que as ferramentas de segurança atuais são capazes de detectar e aquilo que os desenvolvedores vêem como uso inseguro da criptografia.O arcabouço conceitual proposto nesta tes",Aguardando Versão Final.pdf,SISTEMAS DE COMPUTAÇÃO,ALEXANDRE MELO BRAGA,UNIVERSIDADE ESTADUAL DE CAMPINAS,19/12/2017,INGLES,UNIVERSIDADE ESTADUAL DE CAMPINAS,b'cryptography;software security;secure software development;secure cryptographic software',SEGURANÇA E CRIPTOGRAFIA APLICADA,RICARDO DAHAB,0,criptografia;segurança de software;desenvolvimento de software seguro;segurança de software criptográfico,CIÊNCIA DA COMPUTAÇÃO (33003017005P8),-,"A sabedoria popular diz que qualquer sistema é tão seguro quanto o seu elo mais fraco. Com o uso generalizado de bibliotecas criptográficas de boa reputação, baseadas em padrões confiáveis de algoritmos de qualidade, implementações inseguras ou falhas na matemática subjacente não são os elos fracos mais prováveis. Porém, o comprometimento da criptografia ou da sua implementação são eventos raros de impacto catastrófico.
Atualmente, a investigação sistemática de problemas práticos associados ao mau uso de criptografia está ganhando força. À medida que a segurança torna-se transparente para os usuários e a criptografia de boa reputação está disponível para todos os desenvolvedores, o elo fraco mais provável deixa de ser a infra-estrutura criptográfica para se tornar o software em torno da criptografia, escrito por desenvolvedores não especialistas no assunto. Hoje, o mau uso generalizado da criptografia em software é a fonte mais freqüente de problemas de segurança relacionados à criptografia. O efeito cumulativo desse mau uso generalizado também tem um impacto catastrófico, embora difuso.
Esta tese investiga o papel da criptografia na segurança de software e propõe a segurança de software criptográfico como um novo campo de estudo, preocupado com o desenvolvimento sistemático de software criptográfico seguro.
Esta tese alcançou os seguintes resultados:
Primeiramente, uma revisão da literatura sobre programação e verificação de software criptográfico descobriu que apenas um quarto das ferramentas identificadas poderia ser usado por programadores não especialistas em criptografia.
Em segundo lugar, uma metodologia para o desenvolvimento de software criptográfico seguro consolidou as práticas casuais empregadas pela segurança de software na construção de software criptográfico. Adicionalmente, um estudo empírico sobre comunidades on-line para programação descobriu que o mau uso de criptografia é muito freqüente, com padrões recorrentes de mau uso.
Em quarto lugar, uma avaliação de ferramentas de análise estática descobriu que estas ferramentas detectam pouco mais de um terço dos maus usos criptográficos.
Além deste, um estudo longitudinal e retrospectivo sobre o mau uso da criptografia descobriu que desenvolvedores podem aprender a usar APIs criptográficas sem realmente aprender criptografia, enquanto alguns maus usos persistem ao longo do tempo.
Finalmente, uma classificação dos maus usos da criptografia, voltada para a área de segurança de software, e uma metodologia para o desenvolvimento de software criptográfico compõem o corpo de conhecimento para o seu desenvolvimento seguro.
Há uma grande lacuna entre o que os especialistas em criptografia vêem como maus usos de criptografia, os maus usos que as ferramentas de segurança atuais são capazes de detectar e aquilo que os desenvolvedores vêem como uso inseguro da criptografia.O arcabouço conceitual proposto nesta tes",TESE,Towards the Safe Development of Cryptographic Software,5439784,1
"Although soundtracks play an essential role in the experience delivered by digital games, there
are a number of design restrictions it suffers from due to technology limitations. This is specially
true for real-time effects, a natural demand in the interactive media of games. Developers
may either implement their own solutions each time, rely on proprietary software, or neglect
the soundtrack altogether. Besides, even the best commercial tools support only sample-based
audio, which is one of the main causes for the aforementioned design restrictions. Thus, this
thesis proposes VORPAL, a free software game audio middleware implementation that focuses
on procedural audio instead – while maintaining the possibility of sample-based audio – as a
more accessible and adequate tool for composing real-time soundtracks for digital games. The
middleware, inspired by its commercial predecessors, is divided in two main pieces of software: an
audio engine and a soundtrack creation kit. The audio engine comprises a native C++ programming
library, which games and game engines can be linked to to play and control in real-time
soundtrack pieces created using the soundtrack creation kit, which consists of building blocks
provided as Pure Data abstractions. We have interviewed and partnered with professional sound
designers to validate our technology, and came to develop a proof of concept game called Sound
Wanderer, which showcases the possibilities and limitations of the VORPAL middleware.",,CIÊNCIA DA COMPUTAÇÃO,WILSON KAZUO MIZUTANI,UNIVERSIDADE DE SÃO PAULO,24/01/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'digital games, dynamic audio, adaptive music, real-time soundtrack, audio middleware, game soundtrack'",-,RONALDO FUMIO HASHIMOTO,149,"jogos digitais, áudio dinâmico, música adaptativa, trilhas sonoras em tempo real, middleware de áudio, trilhas sonoras para jogos, games",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Muito embora trilhas sonoras desempenhem um papel essencial na experiência criada por jogos
digitais, elas sofrem de diversas restrições de projeto causadas por limitações tecnológicas.
Isso afeta principalmente efeitos em tempo real, que são uma demanda natural na mídia interativa
dos jogos. Desenvolvedores precisam optar entre implementar uma solução própria caso a
caso, investir em software proprietário, ou simplesmente negligenciar a trilha sonora por falta
de opção melhor. Além disso, mesmo as melhores ferramentas comerciais trabalham apenas com
áudio baseado em amostras, o que é uma das principais causas das ditas restrições de projeto.
Portanto, esta dissertação propõe VORPAL, um middleware livre para áudio em jogos que foca
em áudio procedural – mas mantém compatibilidade com áudio baseado em amostras – como
uma ferramenta mais acessível e adequada para a composição de trilhas sonoras em tempo real
para jogos digitais. O middleware, inspirado em seus antecessores comerciais, é dividido em duas
principais componentes de software: um motor de áudio e um kit de criação de trilhas sonoras. O
primeiro é constituído por uma biblioteca de programação nativa em C++, com a qual jogos e motores
de jogos podem se ligar para reproduzir e controlar, em tempo real, peças da trilha sonora
criadas usando a outro componente, que é um kit de blocos de construção providos como abstrações
de Pure Data. Projetistas de som profissionais foram entrevistados e depois trabalharam
em parceria com os autores para validar a tecnologia proposta, o que levou ao desenvolvimento
de um jogo de prova conceitual chamado Sound Wanderer, que demonstra as possibilidades e
limitações do middleware VORPAL.",DISSERTAÇÃO,VORPAL: Um middleware de trilhas sonoras em tempo real para jogos digitais,4964880,1
"The kissing number of Rn is the maximum number of pairwise-nonoverlapping
unit spheres that can simultaneously touch a central unit sphere. In this thesis
we study methods to bound from above the size of such configurations using optimization
techniques, like duality and semidefinite programming. The main result
achieved is the computation of better bounds for the kissing number in dimensions
9 to 23; a result possible due to the exploitation of symmetries in the polynomials
present in the bound proposed by Bachoc and Vallentin (2008), leading to the consideration
of smaller semidefinite programs. Finally, the studied bound is extended
to a bigger class of problems.",Dissertacao_FabricioCaluzaMachadoFinal_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,FABRICIO CALUZA MACHADO,UNIVERSIDADE DE SÃO PAULO,21/02/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'kissing number, spherical codes, packing, semidefinite programming.'",-,FERNANDO MARIO DE OLIVEIRA FILHO,94,"número de contato, códigos esféricos, empacotamento, programação semidefinida.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"de esferas de raio unitário e interiores dois-a-dois disjuntos que podem tocar simultaneamente
uma esfera de raio unitário central. Nesta dissertação estudamos
métodos que limitam o tamanho de tais configurações através de técnicas de otimização,
como dualidade e programação semidefinida. O principal resultado obtido
foi o cálculo de melhores limitantes para o número de contato nas dimensões 9 a
23; o que foi possível graças à exploração de simetrias dos polinômios presentes no
limitante proposto por Bachoc e Vallentin (2008), levando à consideração de programas
semidefinidos menores. Por fim, o limitante estudado é estendido para uma
classe mais geral de problemas.",DISSERTAÇÃO,Limitantes de programação semidefinida para o número de contato,5010694,1
"Software startups face a very demanding market: they must deliver high innovative solutions in
the shortest possible period of time. Resources are limited and time to reach market is short. Then,
it is extremely important to gather the right requirements and that they are precise. Nevertheless,
software requirements are usually not clear and startups struggle to identify what they should build.
This context aects how requirements engineering activities are performed in these organizations.
This work seeks to characterize the state-of-practice of requirements engineering in software startups.
Using an iterative approach, seventeen interviews were conducted during three stages with
founders and/or managers of dierent Brazilian software startups operating in dierent market
sectors and with dierent maturity levels. Data was analyzed using grounded theory techniques
such open and axial coding through continuous comparison. As a result, a conceptual model of
requirements engineering state-of-practice in software startups was developed consisting of its context
inuences (founders, software development manager, developers, business model, market and
ecosystem) and activities description (product team; elicitation; analysis, validation and prioritization;
verication and documentation). Software development and startup development techniques
are also presented and their use in the startup context is analyzed. Finally, using a bad smell analogy
borrowed from software development literature, some bad practices and behaviors identied in
software startups are presented and solutions to avoid them proposed.",Dissertacao_JorgeAMelegatiGoncalves_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,JORGE AUGUSTO MELEGATI GONCALVES,UNIVERSIDADE DE SÃO PAULO,06/03/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'software startups, requirements engineering, empirical software engineering'",-,ALFREDO GOLDMAN VEL LEJBMAN,86,"startups de software, engenharia de requisitos, engenharia de software experimental",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Startups de software enfrentam um mercado muito exigente: elas devem entregar soluções altamente
inovativas no menor período de tempo possível. Recursos são limitados e tempo para alcançar
o mercado é pequeno. Então, é extremamente importante coletar os requisitos certos e que eles sejam
precisos. Entretanto, os requisitos de software geralmente não são claros e as startups fazem um
grande esforço para identicar quais serão implementados. Esse contexto afeta como as atividades
de engenharia de requisitos são executadas nessas organizações. Este trabalho procura compreender
o estado-da-prática da engenharia de requisitos em startups de software. Usando uma abordagem
iterativa, dezessete entrevistas foram realizados em três diferentes estágios com fundadores e/ou
gestores de diferentes startups de software brasileiras operando em diferentes setores e com diferentes
estágios de maturidade. Os dados foram analisados usando técnicas de teoria fundamentada
como codicação aberta e axial através da comparação contínua. Como resultado, um modelo conceitual
do estado-da-prática da engenharia de requisitos em startups de software foi desenvolvido
consistindo da suas inuências do contexto (fundadores, gerente de desenvolvimento de software,
desenvolvedores, modelo de negócio, mercado e ecossistema) e descrição das atividades (time de
produto; levantamento; análise, validação e priorização; e documentação). Técnicas oriundas de
metodologias de desenvolvimento de software e desenvolvimento de startups também são apresentadas
e seu uso em no contexto de startups é analisado. Finalmente, a partir de uma analogia de
maus cheiros presente na literatura de desenvolvimento de software, algumas más práticas e maus
comportamentos identicados em startups de software são apresentados e algumas sugestões de
solução são propostas.",DISSERTAÇÃO,Requirements Engineering in Software Startups: a Qualitative Investigation,5010697,1
"Resulting from the technological revolution over the last few decades, many software startup
ecosystems have emerged around the globe. Boosted by the Internet, the omnipresence of mobile
devices, and the abundance of cloud-based services, software companies with scalable business
models, known as startups, became all the hype.
With tech entrepreneurs as their main agents, some of these ecosystems have existed for over 50
years, while others are newly born. This difference in evolution and maturity makes comparing tech
hubs a challenge. Moreover, if they are to evolve towards fruitful and sustainable environments,
nascent ecosystems need a clear vision of how to develop their community.
This thesis presents a multiple-case study research in three different ecosystems, and it was
divided in three phases. During the first phase, we analyzed the Israeli entrepreneurship ecosystem
and, using grounded theory, created a conceptual generalized framework to map ecosystems.We also
developed a methodology and a systematic interview protocol to be used to analyze any ecosystem.
The second phase was performed in São Paulo, with the objective of refining and validating
both the methodology and the conceptual framework. The second phase resulted in the discovery
of how important it is to analyze ecosystem dynamics and evolution process, leading us to create a
maturity model for software startup ecosystems. The maturity model was based on the conceptual
model we created, mapping the most important factors that define an ecosystem.
To validate and refine the Maturity Model created in the second phase, we ran a third case-study
iteration in New York City. Based on the feedback from over a dozen experts, we generated the
final model and a practical guide to determine an ecosystem’s maturity level. With this model, it is
possible not only to compare different ecosystems, but also to identify gaps and propose customized
practical actions that can yield meaningful improvements and lead ecosystems to the next level of
development.",Tese_DanielCukier_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,DANIEL CUKIER,UNIVERSIDADE DE SÃO PAULO,02/05/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'Software Startups, Startup Ecosystems, Ecosystem, Maturity Model, Entrepreneurship, Innovation, Qualitative Methods, Grounded Theory.'",-,FABIO KON,96,"Startups de Software, Ecossitemas de Startups, Ecossistemas, Modelo de Maturidade, Empreendedorismo, Inovação, Métodos Qualitativos, Teoria Fundamentada em Dados.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Resultado da revolução tecnológica das últimas décadas, vários ecossistemas de startups de
software surgiram ao redor do globo. Acelerados pela Internet, pela onipresença dos dispositivos
móveis e pela abundância de serviços de nuvem, empresas de software com modelos de negócio
escalável, conhecidas como startups, se tornaram o assunto da moda.
Com empreendedores de tecnologia como seus principais agentes, alguns desses ecossistemas já
existem há mais de 50 anos, enquanto outros são apenas recém-nascidos. Essa diferença no grau de
evolução e maturidade torna a comparação de aglomerados de tecnologia um desafio. Mais ainda, se
alguns ecossistemas querem evoluir para um estágio próspero e sustentável, ecossitemas nascentes
precisam de uma visão clara de como desenvolver suas comunidades.
Esta tese apresenta nossa pesquisa baseada em um estudo de caso múltiplo em três diferentes
ecossistemas, e foi dividade em três fases. Durante a primeira fase, nós analisamos o ecossistema
empreendedor de Israel e, utilizando teoria fundamentada em dados, criamos um arcabouço conceitual
que provê uma versão generalizada para mapear ecossistemas. Desenvolvemos, também, uma
metodologia e um protocolo sistemático para entrevistas a serem usadas na análise de ecossistemas
específicos.
A segunda fase da pesquisa foi realizada em São Paulo, com o objetivo de refinar e validar
a metodologia e o arcabouço conceitual. Esta fase resultou na descoberta de como é importante
analisar a dinâmica e o processo de evolução dos ecossistemas, nos levando a criar um modelo
de maturidade para ecossistemas de startups de software. O modelo de maturidade foi baseado no
modelo conceitual que criamos, mapeando os fatores mais importantes que definem as características
de um ecossistema.
Para validar e refinar o modelo de maturidade criado na segunda fase, realizamos um terceiro
estudo de caso em Nova Iorque que contou com o feedback de mais de uma dezena de especialistas.
Geramos um modelo de maturidade final, um guia prático para determinar o nível de maturidade
de cada ecossistema. Com esse modelo, é possível não somente comparar diferentes ecossistemas,
como também identificar lacunas e propor ações práticas e personalizadas que podem resultar em
melhorias significativas e levar ecossistemas ao próximo nível de desenvolvimento.",TESE,Software Startup Ecosystems Evolution: A Maturity Model,5010706,1
"Low structural coupling is a design principle at the heart of software engineering. A recurrent
claim is that classes with high coupling are prone to undergo forced local changes as a consequence of
changes made in the classes they are connected to. This claim can be found in almost every Software
Engineering book, in fundamental papers of the area, in whitepapers written by industry experts,
and even in Wikipedia. Despite the popularity and credibility of the claim, very little research effort
has been put on its understanding. In other words, if a class A depends on another class B, then is
A more likely to co-change with B as compared to the case where A does not depend on B? In other
words, is the existence of dependencies statistically associated with the occurrence of co-changes? To
what extent? Answering this question is crucial step in understanding the link between dependencies
and software changes. Hence, in this paper, we set out to empirically investigate the link between
structural dependencies and co-changes. In a preliminary study with 4 open-source systems, we
discovered that structural dependencies do not instantly make artifacts co-change. However, the
rate with which an artifact co-cochanges with another is indeed higher when the former structurally
depends on the latter. We confirmed this finding in a new study where we extracted structural
dependencies and co-changes from 77,286 code snapshots of 45 Java projects randomly sampled from
the Apache Software Foundation. Our results indicated that, when A depends on B and B changes,
the chances of A changing together with B is around 32% in average, with a standard deviation
of 13.6%. We also built classification models using Random Forests to investigate which kinds of
dependency best explain co-changes. We found that the length of transitive dependencies, number of
type imports, number of method calls, and number of references were the most important variables
in the model. However, the classifiers were often inaccurate, thus implying that dependencies are
not good predictors for co-changes. In fact, we also found that a substantial number of commits
involve classes that are not connected via dependencies, reinforcing our belief that co-changes are
more frequently induced by other forms of connascence, such as conceptual coupling. In summary,
on the one hand, we found empirical evidence connecting the existence of structural dependencies to
the occurrence co-changes. On the other hand, we found that the majority of co-changes do not correlate
with structural dependencies, meaning that structural dependencies might be responsible for
a small portion of all software changes. As a practical consequence, developers should still embrace
the low coupling principle by managing software dependencies while designing and evolving their
systems. However, our findings also imply that IDEs should take into account additional sources
of information to support developers in successfully propagating software changes, as structural
iii
iv
dependencies seem not to be the main player. Finally, the data and tools produced during this
research might be leveraged to bootstrap follow-up investigations, such as the influence of structural
anti-patterns on change propagation.",Tese_GustavoAnsaldiOliva_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,GUSTAVO ANSALDI OLIVA,UNIVERSIDADE DE SÃO PAULO,16/02/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'structural dependencies, syntactic dependencies, co-changes, change propagation, change coupling, evolutionary coupling, static analysis, historical analysis, classification models, random forests.'",-,MARCO AURELIO GEROSA,78,"dependências estruturais, dependências sintáticas, mudanças casadas, propagação de mudanças, acoplamento de mudança, acoplamento evolucionário, análise estática, análise histórica, modelos de classificação, florestas aleatórias.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Baixo acoplamento estrutural é um princípio de design que está no coração da engenharia de
software. Um discurso recorrente é de que classes com alto acoplamento são mais propensas a
sofrerem mudanças forçadas locais por consequência de mudanças realizadas nas classes as quais
estão conectadas. Essa asserção pode ser encontrada em quase todos os livros de Engenharia de
Software, em artifos fundamentais da área, em whitepapers escritos por especialistas da indústria
e até na Wikipedia. Apesar da popularidade e credibilidade do princípio, pouquíssimo esforço de
pesquisa foi colocado na direção de seu entendimento. Em outras palavras, se uma classe A depende
de outra classe B, então A é mais propenso a mudar conjuntamente com B comparado à situação em
que A não depende de B? Ou seja, a existência de dependências é estatisticamente associada com a
ocorrência de mudanças casadas? Até que ponto? Responder essa questão é um passo crucial para
um entendimento da conexão entre dependências estruturais e mudanças casadas. Em um estudo
preliminar com 4 sistemas de código aberto, descobrimos que dependências estruturais não fazem
com que artefatos fatalmente mudem de forma casada. Contudo, a taxa com a qual um artefato
muda conjuntamente com outro é, de fato, maior quando o primeiro estruturalmente depende do
segundo. Nós confirmamos essa descoberta em um novo estudo em que extraímos dependências
estruturais e mudanças casadas de 77.286 snapshots de código de 45 projetos Java aleatoriamente
selecionados da população de projetos da Apache Software Foundation. Nossos resultados indicaram
que, quando A depende de B e B muda, a chance de A mudar junto com B é de 32% em média,
com um desvio padrão de 13.6%. Também construímos modelos de classificação usando Florestas
Aleatórias para investigar quais tipos de dependência melhor explicam mudanças casadas. Descobrimos
que o comprimento formado do caminho formado por dependências transitivas, o número
de importações de tipe, o número de chamadas de métodos e o números de referências foram as
variáveis mais importantes no modelo. Contudo, os classificadores foram frequentemente imprecisos,
implicando assim que dependências não são bons preditores para mudanças casadas. De fato,
também concluímos que um número substancial de commits envolvem classes que não estão conectadas
por dependências, reforçando nossa crença de que mudanças casadas são mas frequentemente
induzidas por outras formas de conascença, tais como acoplamento conceitual. Em resumo, por
um lado encontramos evidência empírica conectando a existência de dependências estruturais com
a ocorrência de mudanças casadas. Por outro lado, descobrimos que uma quantidade substancial
de mudanças casadas não está correlacionada com dependências estruturais, implicando que essas
dependências são provavelmente responsáveis por uma porção de pequena de todas as mudanças
v
vi
no software. Como consequência prática, desenvolvedores devem continuar a abraçar o princípio
do baixo acoplamento por meio da gerência de dependências durante o design e evolução de seus
sistemas. Contudo, os resultados também implicam que IDEs devem passar a considerar adicionais
fontes de informação para dar suporte ao desenvolvedores na tarefa de propagação das mudanças, já
que dependências estruturais não parecem ser o ator principal nesse cenário. Finalmente, os dados e
ferramentas produzidos durante essa pesquisa podem ser aproveitados para alavancar investigações
seguintes, tal como a influência de anti-padrões estruturais em propagação de mudanças.",TESE,On the Link between Structural Dependencies and Software Changes,5010713,1
"Several natural systems such as protein-protein interactions, genetic regulation, functional
connectivity of the brain, and social relationships can be modeled as graphs where its
vertices represent the entities under study and the edges represent which pair of entities are
associated. It is known that much of these systems are modular, i.e., they can be clustered
into sub-systems, which interact and influence each other. However, from a computational
statistical viewpoint, little is known about statistical methods to analyze graphs. For
example, how can one identify whether a graph “causes” another graph? In this context, we
propose a method to identify Granger causality among time series of graphs in the frequency
domain. This method is based on the idea of spectral analysis of random graphs and also on
the Partial Directed Coherence. We present the model, a method to estimate the parameters
of the model, and a statistical test. We demonstrate the usefulness of the method in intensive
Monte Carlo simulations. Results show that the method effectively controls the type I error
and also present high statistical power to identify Granger causality in five different random
graph models. Finally, we illustrate an application of the method in an electrocorticography
data collected from a macaque under anesthesia.",Tese_GustavoPintoVilela_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,GUSTAVO PINTO VILELA,UNIVERSIDADE DE SÃO PAULO,14/02/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'graph theory;Granger causality;spectral radius;spectral analysis;partial directed coherence.',-,ANDRE FUJITA,57,teoria dos grafos;causalidade de Granger;raio espectral;análise espectral;coerência parcial direcionada.,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Diversos sistemas naturais, como a malha aeroviária, interações proteína-proteína, regulação
genética, conectividade funcional do cérebro e relações sociais podem sem modeladas
por grafos onde os vértices são as entidades sob estudo e as arestas representam quais pares
de entidades se relacionam. Também é sabido que muitos desses sistemas são modulares,
ou seja, podem ser particionados de alguma maneira em sub-sistemas que interagem ou se
influenciam. No entanto, do ponto de vista estatístico-computacional, pouco se é conhecido
sobre métodos de análise estatística em grafos. Por exemplo, como identificar que um grafo
“causa” outro grafo? Dentro deste contexto, propomos um método de identificação de causalidade
de Granger entre séries temporais de grafos no domínio da frequência. Este método se
baseia tanto na análise espectral dos grafos aleatórios como também no método da Coerência
Parcial Direcionada. Apresentamos o modelo, uma forma de estimação, um teste estatístico
e resultados sobre o efetivo controle da taxa de falsos positivos, bem como seu poder estatístico
em simulações de Monte Carlo. Finalmente, ilustramos uma aplicação do método em
dados de eletrocorticografia coletados de um macaco sob estado de alerta e posteriormente
em estado anestésico.",TESE,Causalidade de Granger entre grafos no domínio da frequência,5010714,1
"Games are a very old human interest and video games are still one of the most important forms
of modern entertainment. The reason why games are so interesting is because they are fun. Due to
its subjectivity and context dependency, fun can not be designed. Instead, designers employ best
known practices and techniques to plan for good experiences and then evaluate their design choices
iteratively with users in order to improve the product. The evaluation of user experience in games
has been traditionally performed by means of observation, questionnaires and interviews, but it has
becoming very common to capture performance and physiological data for quantitative and qualitative
analysis. Pure quantitative analysis are known to not be enough to properly characterize fun
and body sensors are still intrusive, increasing the risk of tampering the experience. In that sense,
the analysis from facial expressions seems a good alternative. Facial expressions are important in
the human communication, carrying a lot of contextual and emotional information. In the context
of video games the face seems more accessible because players keep the eyes at the screen most of
the time and the face close enough to diminish problems with Computer Vision algorithms. Also,
most of the and game-enabled devices already have embedded cameras that can be used to monitor
facial expressions as game are played. This work presents a proposal on how to assess fun from a
low-cost webcam with the purpose of supporting playtesting. The solution proposed is based on the
detection and tracking of the player's face, followed by the extraction of features for the classication
immersion and prototypic emotional and then the detection of immersion and fun. The face
tracking is performed with the Viola-Jones and a variation of the Active Appearance Model (AAM)
algorithms, the classication of emotions is done with a Support Vector Machine (SVM), and the
detection of immersion and fun use Structured Perceptrons and Hidden Markove Models (HMM).
The system evaluation is performed with comparison of self-reported responses to the Game Experience
Questionnaire.",Tese_LuizCarlosVieira.pdf,CIÊNCIA DA COMPUTAÇÃO,LUIZ CARLOS VIEIRA,UNIVERSIDADE DE SÃO PAULO,16/05/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'Fun, Assessment, Immersion, Emotions, Facial Expressions.'",-,FLAVIO SOARES CORREA DA SILVA,114,"Fun, Assessment, Immersion, Emotions, Facial Expressions.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Games are a very old human interest and video games are still one of the most important forms
of modern entertainment. The reason why games are so interesting is because they are fun. Due to
its subjectivity and context dependency, fun can not be designed. Instead, designers employ best
known practices and techniques to plan for good experiences and then evaluate their design choices
iteratively with users in order to improve the product. The evaluation of user experience in games
has been traditionally performed by means of observation, questionnaires and interviews, but it has
becoming very common to capture performance and physiological data for quantitative and qualitative
analysis. Pure quantitative analysis are known to not be enough to properly characterize fun
and body sensors are still intrusive, increasing the risk of tampering the experience. In that sense,
the analysis from facial expressions seems a good alternative. Facial expressions are important in
the human communication, carrying a lot of contextual and emotional information. In the context
of video games the face seems more accessible because players keep the eyes at the screen most of
the time and the face close enough to diminish problems with Computer Vision algorithms. Also,
most of the and game-enabled devices already have embedded cameras that can be used to monitor
facial expressions as game are played. This work presents a proposal on how to assess fun from a
low-cost webcam with the purpose of supporting playtesting. The solution proposed is based on the
detection and tracking of the player's face, followed by the extraction of features for the classication
immersion and prototypic emotional and then the detection of immersion and fun. The face
tracking is performed with the Viola-Jones and a variation of the Active Appearance Model (AAM)
algorithms, the classication of emotions is done with a Support Vector Machine (SVM), and the
detection of immersion and fun use Structured Perceptrons and Hidden Markove Models (HMM).
The system evaluation is performed with comparison of self-reported responses to the Game Experience
Questionnaire.",TESE,Assessment of Fun from the Analysis of Facial Images,5010732,1
"Plankton are microscopic organisms that constitute the basis of the food chain of
aquatic ecosystems. They have an important role in the carbon cycle as they are responsible
for the absorption of carbon in the ocean surfaces. Detecting, estimating and monitoring
the distribution of plankton species are important activities for understanding the role
of plankton and the consequences of changes in their environment. Part of these type
of studies is based on the analysis of water volumes by means of imaging techniques.
Due to the large quantity of generated images, computational methods for helping
the process of image analysis are in demand. In this work we address the problem of
species identication. We follow the conventional pipeline consisting of target detection,
segmentation (contour delineation), feature extraction, and classication steps. In the
rst part of this work we address the problem of choosing an appropriate segmentation
algorithm. Since evaluating segmentation results is a subjective and time consuming task,
we propose a method to evaluate segmentation algorithms by evaluating the classication
results at the end of the pipeline. Experiments with this method showed that distinct
segmentation algorithms might be appropriate for identifying species of distinct classes.
Therefore, in the second part of this work we propose a classication method that takes into
consideration multiple segmentations. Specically, multiple segmentations are computed
and classiers are trained individually for each segmentation, which are then combined to
build the nal classier. Experimental results show that the accuracy obtained with the
combined classier is superior in more than 2% to the accuracy obtained with classiers
using a xed segmentation. The proposed methods can be useful to build plankton identi-
cation systems that are able to quickly adjust to changes in the characteristics of the images.",Dissertacao_MarielaAtausinchiFernandez.pdf,CIÊNCIA DA COMPUTAÇÃO,MARIELA ATAUSINCHI FERNANDEZ,UNIVERSIDADE DE SÃO PAULO,27/03/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'detection of plankton, feature extraction, plankton image segmentation, plankton image classication, segmentation algorithms assessment.'",-,NINA SUMIKO TOMITA,94,"Detecção de plâncton, extração de características, segmentação de imagens de plâncton, classicação de imagens de plâncton, avaliação de algoritmos de segmentação.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Plâncton são organismos microscópicos que constituem a base da cadeia alimentar de
ecossistemas aquáticos. Eles têm importante papel no ciclo do carbono pois são os respons
áveis pela absorção do carbono na superfície dos oceanos. Detectar, estimar e monitorar a
distribuição das diferentes espécies são atividades importantes para se compreender o papel
do plâncton e as consequências decorrentes de alterações em seu ambiente. Parte dos estudos
deste tipo é baseada no uso de técnicas de imageamento de volumes de água. Devido
à grande quantidade de imagens que são geradas, métodos computacionais para auxiliar
no processo de análise das imagens estão sob demanda. Neste trabalho abordamos o problema
de identicação da espécie. Adotamos o pipeline convencional que consiste dos passos
de detecção de alvo, segmentação (delineação de contorno), extração de características, e
classicação. Na primeira parte deste trabalho abordamos o problema de escolha de um
algoritmo de segmentação adequado. Uma vez que a avaliação de resultados de segmenta-
ção é subjetiva e demorada, propomos um método para avaliar algoritmos de segmentação
por meio da avaliação da classicação no nal do pipeline. Experimentos com esse método
mostraram que algoritmos de segmentação distintos podem ser adequados para a identica-
ção de espécies de classes distintas. Portanto, na segunda parte do trabalho propomos um
método de classicação que leva em consideração múltiplas segmentações. Especicamente,
múltiplas segmentações são calculadas e classicadores são treinados individualmente para
cada segmentação, os quais são então combinados para construir o classicador nal. Resultados
experimentais mostram que a acurácia obtida com a combinação de classicadores
é superior em mais de 2% à acurácia obtida com classicadores usando uma segmentação
xa. Os métodos propostos podem ser úteis para a construção de sistemas de identicação
de plâncton que sejam capazes de se ajustar rapidamente às mudanças nas características
das imagens.",DISSERTAÇÃO,Classicação de imagens de plâncton usando múltiplas segmentações,5010739,1
"The vertex coloring problem is a classic problem in graph theory that asks for a partition of the vertex
set into a minimum number of stable sets. This thesis presents our studies on three vertex (re)coloring
problems on graphs and on a problem related to a long-standing conjecture on subdivision of digraphs.
Firstly, we address the convex recoloring problem in which an arbitrarily colored graph G is given and
one wishes to find a minimum weight recoloring such that each color class induces a connected subgraph
of G. We show inapproximability results, introduce an integer linear programming (ILP) formulation that
models the problem and present some computational experiments using a column-generation approach.
The k-fold coloring problem is a generalization of the classic vertex coloring problem and consists in
covering the vertex set of a graph by a minimum number of stable sets in such a way that every vertex
is covered by at least k (possibly identical) stable sets. We present an ILP formulation for this problem
and show a detailed polyhedral study of the polytope associated with this formulation. The last coloring
problem studied in this thesis is the proper orientation problem. It consists in orienting the edge set of a
given graph so that adjacent vertices have different in-degrees and the maximum in-degree is minimized.
Clearly, the in-degrees induce a partition of the vertex set into stable sets, that is, a coloring (in the
conventional sense) of the vertices. Our contributions in this problem are on hardness and upper bounds
for bipartite graphs. Finally, we study a problem related to a conjecture of Mader from the eighties on
subdivision of digraphs. This conjecture states that, for every acyclic digraph H, there exists an integer
f(H) such that every digraph with minimum out-degree at least f(H) contains a subdivision of H as
a subdigraph. We show evidences for this conjecture by proving that it holds for some particular classes
of acyclic digraphs.",Tese_PhabloFernandoMoura_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,PHABLO FERNANDO SOARES MOURA,UNIVERSIDADE DE SÃO PAULO,30/03/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'graph coloring, convex recoloring, graph orientation, digraph subdivision, polyhedral study, column generation, inapproximability, computational complexity. iii'",-,YOSHIKO WAKABAYASHI,129,"coloração de grafos, recoloração convexa, orientação de grafos, subdivisão de digrafos, poliedro, geração de colunas, inaproximabilidade, complexidade computacional.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"O problema de coloração de grafos é um problema clássico em teoria dos grafos cujo objetivo é particionar
o conjunto de vértices em um número mínimo de conjuntos estáveis. Nesta tese apresentamos nossas
contribuições sobre três problemas de coloração de grafos e um problema relacionado à uma antiga conjectura
sobre subdivisão de digrafos. Primeiramente, abordamos o problema de recoloração convexa no qual
é dado um grafo arbitrariamente colorido G e deseja-se encontrar uma recoloração de peso mínimo tal
que cada classe de cor induza um subgrafo conexo de G. Mostramos resultados sobre inaproximabilidade,
introduzimos uma formulação linear inteira que modela esse problema, e apresentamos alguns resultados
computacionais usando uma abordagem de geração de colunas. O problema de k-upla coloração é uma
generalização do problema clássico de coloração de vértices e consiste em cobrir o conjunto de vértices de
um grafo em uma quantidade mínima de conjuntos estáveis de tal forma que cada vértice seja coberto por
pelo menos k conjuntos estáveis (possivelmente idênticos). Apresentamos uma formulação linear inteira
para esse problema e fazemos um estudo detalhado do politopo associado à essa formulação. O último
problema de coloração estudado nesta tese é o problema de orientação própria. Ele consiste em orientar o
conjunto de arestas de um dado grafo de tal forma que vértices adjacentes possuam graus de entrada distintos
e o maior grau de entrada seja minimizado. Claramente, os graus de entrada induzem uma partição
do conjunto de vértices em conjuntos estáveis, ou seja, induzem uma coloração (no sentido convencional)
dos vértices. Nossas contribuições nesse problema são em complexidade computacional e limitantes
superiores para grafos bipartidos. Finalmente, estudamos um problema relacionado à uma conjectura de
Mader, dos anos oitenta, sobre subdivisão de digrafos. Esta conjectura afirma que, para cada digrafo
acíclico H, existe um inteiro f(H) tal que todo digrafo com grau mínimo de saída pelo menos f(H)
contém uma subdivisão de H como subdigrafo. Damos evidências para essa conjectura mostrando que
ela é válida para classes particulares de digrafos acíclicos.",TESE,Graph colorings and digraph subdivisions,5010741,1
"Cloud computing is a new paradigm in which virtual resources are leased in the short-term.
Cloud providers publish an API though which users can request, use, and release those resources.
Thus, a properly architected system can be quickly deployed and their infrastructure can be quickly
updated to better accommodate to workload uctuations and limit expenses. Many services running
in clouds comprise an automated resource management unit, that is in charge of requesting and
releasing resources without human intervention, as demand changes. Those are knows as elastic
systems. The distinguishing architectural characteristic that enables elastic systems is scalability.
It means that, by adding more resources to the system infrastructure, its capacity grows. The
standard approach for resource management relies on utilization-based rules. Service resources are
periodically monitored with respect to the portion of the available hardware capacity in use and
thresholds are preestablished, dening triggers to add or remove resources. In this approach is possible
to identify the need for change, but not the extent. Thus, the quantity of resources to add or
remove is dened a priory, for each rule. Also, the benet of the changes in resource availability may
vary with scale. The eect of adding one VM to a system, when there are already ten other VMs
in use, may be less signicant than if there was only one VM in use. In addition, there no standard
procedure to dene the rules that trigger the elasticity actions. The rule based approach is specially
problematic in cases of load surge. When of a quick and drastic increase of the workload, the system
may take many cycles of infrastructural redimensioning until achieve an adequate state. In this case,
the system remains overloaded during all those cycles, aecting user experience. In this research,
we investigate how we can properly understand what are the eects, in system capacity, incurred
by variations in resource availability, and how this knowledge can be applied to improve elasticity.
We investigate the eorts regarding scalability evaluation of distributed systems and automated
system elasticity. Then, we propose a strategy that comprises performing scalability tests to model
scalability and apply the model to estimate resource need, according to the arriving workload. This
approach enables dynamic updates to the resource pool in terms that, when a need to update the
pool size is detected, the extent is estimated based on the model. We introduce a framework for
automated scalability evaluation of distributed systems and experimentally evaluate the proposed
strategy. We compare the allocation and performance obtained using our strategy with a rule based
strategy in a trace-driven simulation and with synthetic workloads. We also evaluate six variations
of the model-based approach. Generally, our approach can deliver better performance, while increasing
resource allocation and, consequently, cost. The extent of the performance improvement is
larger than the cost increment, though.",Tese_PauloBittencourtMoura_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,PAULO BITTENCOURT MOURA,UNIVERSIDADE DE SÃO PAULO,17/03/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'scalability, scalability testing, scalability modeling, cloud computing, elasticity'",-,FABIO KON,91,"escalabilidade, teste de escalabilidade, modelagem de escalabilidade, computação em nuvem, elasticidade.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Provedores de serviços de nuvem disponibilizam uma interface através da qual seus clientes
podem solicitar, usar e liberar estes recursos. Muitos serviços implantados em nuvens incluem um
componente para gerenciamento automatizado de recursos, encarregado de requisitar e librar recursos
sem intervenção humana, à medida que a demanda varia. Eles são chamados de sistemas
elásticos. Uma característica marcante desses sistemas é a escalabilidade. Ela permite que a capacidade
do sistema seja aumentada disponibilizando mais recursos ao mesmo. A técnica padrão para o
gerenciamento de recursos se baseia em regras sobre utilização de recursos. Os recursos disponíveis
ao sistema são monitorados e regras denem níveis de utilização que, quando alcançados, disparam
ações para requisitar ou liberar recursos. A quantidade de recursos adicionados ou removidos é de-
nida a priori, em cada regra. Porém, o benefício das atualizações pode mudar com a escala. O efeito
de adicionar um máquina virtual, quando o sistema já usa dez, pode não ser tão signicante quanto
se ele está usando apenas uma. Além disso, não há um procedimento padrão para denição das
regras. Quando ocorre um aumento signicativo na carga em um curto espaço de tempo, o sistema
pode levar vários ciclos de monitoramento e ação até alcançar uma conguração adequada. Neste
período, o sistema permanece sobrecarregado. Nesta pesquisa, investigamos como compreender adequadamente
os efeitos da variação na disponibilidade de recursos sobre a capacidade de um sistema
e como aplicar este conhecimento para melhorar sua elasticidade. Investigamos as iniciativas relacionadas
à avaliação de escalabilidade de sistemas distribuídos e à automatização da elasticidade.
Então, propomos uma estratégia que abrange avaliação da escalabilidade do sistema, visando sua
modelagem, e a aplicação deste modelo nas estimativas de necessidade por recursos com base na
carga de trabalho. Isso possibilita atualizações dinâmicas da infraestrutura de recursos. Quando detectada
uma necessidade de atualização, a quantidade de recursos envolvida é estimada com base no
modelo. Introduzimos um arcabouço para automatizar a avaliação de escalabilidade de sistemas distribu
ídos e efetuamos uma validação experimental da estratégia proposta. Comparamos a alocação
de recursos e o desempenho obtido usando nossa estratégia e estratégia baseada em regras, fazendo
a reprodução de carga real e usando cargas sintéticas. De forma geral, nossa proposta foi capaz de
prover melhor desempenho, ao ponto que o uso de recursos cresceu, e consequentemente o custo
de utilização. No entanto, a melhora de desempenho foi mais signicativa que o aumento dos custos.",TESE,Dynamic resource allocation for elastic systems based on scalability modeling,5010742,1
"The diffusion magnetic resonance images portray the diffusivity of water molecules present on
biological tissues. High organized and compact biological structures like neuronal fibres and muscles
present higher diffusivity parallel to the fibres than perpendicular to those. This property allows
the digital reconstruction of fibres trajectories, technique named tractography, being one of the
few non invasive ways of investigation of the anatomical connectivity and structural organization
of the brain and heart. The most common tractography methodology uses numerical integration
following the main diffusion direction in order to reconstruct trajectories. Yet, this technique raises
issues regarding the intrinsic error to numeric integration and the error associated to uncertainty
regions on diffusion data. A methodology considered more robust to such problems consists on
modelling tractography as a particle system simulation. However, such methodology has several
parameters that require fine tuning for each case and has a high computational complexity. This
dissertation shows presents a global tractography methodology based on particle system but at
lower computational cost because of the avoidance of unnecessary optimization steps on trajectories
reconstruction. We evaluate its accuracy on on crescent difficulty on diffusion datasets built
digitally; phantom images acquired with magnetic resonance scanners; and with myocardium and
human brain diffusion images. This methodology has the potential to evidence the organization and
architecture of several tissues from the human body with better fidelity e lower reconstruction time.",Dissertacao_RafaelReggianiManzo_FINAL.pdf,CIÊNCIA DA COMPUTAÇÃO,RAFAEL REGGIANI MANZO,UNIVERSIDADE DE SÃO PAULO,13/03/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'diffusion magnetic resonance images, tractography, particle simulation.'",-,MARCEL PAROLIN JACKOWSKI,69,"imagens de ressonância magnética ponderadas por difusão, tractografia, simulação de partículas. i",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"As imagens de ressonância magnética ponderadas por difusão retratam a difusividade de moléculas
de água presentes em tecidos biológicos. Em estruturas biológicas altamente organizadas e
compactas como fibras nervosas e musculares, a difusividade é maior na direção paralela às fibras do
que perpendicularmente às mesmas. Essa propriedade permite a reconstrução digital das trajetórias
das fibras, técnica denominada tractografia, representando uma das poucas formas não invasivas
de investigação da conectividade anatômica e organização estrutural do cérebro e do coração. A
metodologia de tractografia mais difundida faz uso da integração numérica da direção principal de
difusividade para reconstruir essas trajetórias. Porém, esta técnica apresenta problemas como o erro
intrínseco a métodos de integração numérica e o erro associado a regiões de incerteza nos dados
de difusividade. Uma metodologia considerada mais robusta consiste da modelagem da tractografia
como a simulação de um sistema de partículas. No entanto, tal metodologia possui diversos parâmetros
que precisam ser otimizados para cada caso e apresenta alta complexidade computacional. Esta
dissertação apresenta uma metodologia de tractografia global baseada em sistema de partículas, mas
com custo computacional reduzido pois evita passos desnecessários da otimização para reconstrução
das trajetórias. Avaliamos sua acurácia em conjuntos de dados com graus de complexidade crescentes
utilizando imagens sintéticas de difusão construídas digitalmente; imagens de fantomas físicos
amostrados em escâner de ressonância magnética; e em imagens reais de difusão do miocárdio e do
cérebro humano. Esta metodologia possui o potencial de evidenciar a organização e arquitetura de
diversos tecidos do corpo humano com maior fidelidade e menor tempo de reconstrução.",DISSERTAÇÃO,Um Método de Tractografia Global Usando Imagens de Ressonância Magnética Ponderadas por Difusão,5010743,1
"Little is known about the causes of autism spectrum disorder (ASD) and its effects on
brain functions. Several researches point that it may be related to a differentiated connection
between brain regions. The cortico-cerebellar connectivity has been theme of research over
the last decade given the new discoveries suggesting that this connection is associated with
learning and tuning of diverse brain functionalities. It is believed that cortico-cerebellar
connectivity impairment may be related to impairments on sensorimotor, cognitive, and
emotional functions. For a better understanding of the condition, we aim at identifying brain
regions that present a impaired cortico-cerebellar connectivity between TD and ASD. Thus,
our goal is to identify brain regions where the functional connectivity with the cerebellum
is different between subjects with typical development (TD) and ASD. We used functional
magnetic resonance images (fMRI) of 708 subjects under resting state protocol (432 TD
and 276 ASD) with ages between 6 and 58 years old collected by the ABIDE Consortium.
Data was pre-processed, splited in anatomical brain regions, which were adopted as regions
of interest (ROIs). To establish the functional connectivity of each cortical ROI to the
cerebellum, first we applied the principal component analysis (PCA) on cerebellar ROIs.
Next, we used a linear regression model to each cortical ROI time series as response variable
and the cerebellum principal components (PCs) as the predictive variables. After that, we
identified regions of different connections between TD and ASD subjects applying a linear
model including age, gender, and data collection site as covariables.We identified five cortical
vi
vii
regions with reduced functional connectivity with cerebellum on the ASD subjects, namely:
(i) right fusiform gyrus, (ii) right postcentral gyrus, (iii) right superior temporal gyrus, (iv)
right middle temporal gyrus, and (v) left middle temporal gyrus. All five regions are part of
the sensorimotor system, and contribute to functions typically associated with ASD, such
as: sensitivity to external stimulus, dyslexia, prosopagnosia (difficulty to recognize faces),
language comprehension impairments, and recognition of emotional expressions impairment.
Our results show that there are brain regions with atypical cortico-cerebellar connectivity
impairment on ASD, in accordance with results from previous studies of tactography and
task driven fMRI. We believe that the decreased cortico-cerebellar connectivity of these
regions are affecting the learning process of sensorimotor functionalities, leading to typical
ASD symptoms.",Dissertacao_TaianeCoelhoRamos_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,TAIANE COELHO RAMOS,UNIVERSIDADE DE SÃO PAULO,15/03/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'autism spectrum disorder, fMRI, functional conectivity, cortico-cerebellar conectivity.'",-,ANDRE FUJITA,65,"transtorno do espectro autista, fMRI, conectividade funcional, conectividade córtico-cerebelar.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Ainda pouco se sabe sobre as causas do transtorno do espectro autista (TEA) e seus efeitos
na funcionalidade cerebral, porém, diversas pesquisas apontam que a condição esteja relacionada
à uma conectividade diferenciada entre regiões do cérebro. A conectividade córticocerebelar
tem sido tema de pesquisas nas últimas décadas em decorrência de novos achados
que indicam que esta conectividade está relacionada ao aprendizado e refinamento de diversas
funcionalidades do córtex. Acredita-se que uma falha na conectividade córtico-cerebelar
poderia estar relacionada à falhas em funções sensorimotoras, cognitivas e emocionais. A
investigação de regiões cuja conectividade córtico-cerebelar está alterada no TEA contribui
para uma melhor compreensão deste transtorno. Assim, o objetivo deste trabalho é identificar
regiões do cérebro cuja conectividade funcional com o cerebelo seja diferente entre indivíduos
com desenvolvimento típico (DT) e diagnosticados com TEA. Para isto, utilizamos imagens
de ressonância magnética funcional (fMRI) de 708 indivíduos em estado de repouso (432 DT
e 276 TEA) com idades entre 6 e 58 anos coletados pelo consórcio ABIDE. Os dados foram
pré-processados e divididos conforme regiões anatômicas do cérebro que foram adotadas
como regiões de interesse (ROIs). Para determinar a conectividade funcional de cada região
do córtex com o cerebelo, aplicamos o método de análise de componentes principais (PCA)
nas ROIs do cerebelo e utilizamos um modelo regressão linear para cada ROI do córtex,
sendo a série temporal da ROI do córtex a variável resposta e as componentes principais
(PCs) do cerebelo as variáveis preditoras. Em seguida, identificamos as regiões com conectividade
funcional diferente entre indivíduos com DT e diagnosticados com TEA através
de um modelo linear que inclui como covariáveis, idade, gênero e local de coleta do dado.
Identificamos cinco regiões do córtex que apresentam reduzida conectividade funcional com
o cerebelo nos indivíduos com TEA, sendo elas: (i) giro fusiforme direito, (ii) giro pós-central
iv
v
direito, (iii) giro temporal superior direito e (iv) giro temporal médio direito e (v) esquerdo.
Todas as cinco regiões são parte do sistema sensorimotor, e estão relacionadas à funções ligadas
à sintomas característicos do quadro de TEA, como: sensibilidade à estímulos sensoriais,
dislexia, prosopagnosia (dificuldade para reconhecer faces), dificuldade de compreensão de
linguagem e dificuldade de reconhecimento de emoções em faces. Nossos resultados mostram
que existem regiões do sistema sensorimotor que apresentam conectividade funcional com
o cerebelo atipicamente reduzida em TEA, como corroborado por estudos de imageamento
com tarefa específica e como hipotetizado por estudos de conectividade estrutural. Nós acreditamos
que a conectividade córtico-cerebelar reduzida dessas regiões esteja prejudicando o
processamento e aprendizado de funções sensorimotoras, levando ao surgimento de sintomas
típicos do TEA.",DISSERTAÇÃO,Identificação de Alterações em Conectividades Funcionais Córtico-Cerebelares no Transtorno do Espectro Autista,5010751,1
"Probabilistic planning deals with sequential decision making in stochastic environments and is
modeled by a Markovian Decision Process (MDP). An MDP models the interaction between an
agent and its environment: at each stage, the agent decides to execute an action, with probabilistic
effects and a certain cost which produces a future state. The purpose of the MDP agent is
to minimize the expected cost along a sequence of choices. The number of stages that the agent
acts in the environment is called horizon, which can be finite, infinite or undefined. An example of
MDP with undefined horizon is the Stochastic Shortest Path MDP, which extends the definition
of MDP by adding a set of goal states (the agent stops acting after reaching a goal state). In an
SSP MDP the assumption is made that it is always possible to achieve a goal state from every
state of the world. However, this is a very strong assumption and cannot be guaranteed in practical
applications. States from which it is impossible to reach the goal are called dead-ends. A dead-end
may be avoidable or unavoidable (when no policy leads from the initial state to the goal with probability
one). Recent work has proposed extensions to SSP MDP that allow the existence of different
types of dead-ends as well as algorithms to solve them. However, the detection of dead-end is done
using: (i) heuristics that may fail to detect implicitly dead-ends or (ii) more reliable methods that
require a high computational cost. In this project we make a formal characterization of probabilistic
planning models with dead-ends. In addition, we propose a new technique for dead-end detection
based on this characterization. Finally, we adapt probabilistic planning algorithms to use the new
detection method, allowing them to solve both problem with avoidable dead-ends and unavoidable
dead-ends. The empirical results show that the proposed method is able to detect all dead-ends
of a given set of states and, when used with probabilistic planners, can make these planners more
efficient in domains with difficult to detect dead-ends.",Dissertacao_ThiagoDiasSimao_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,THIAGO DIAS SIMAO,UNIVERSIDADE DE SÃO PAULO,06/03/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'Probabilistic Planning, MDP, SSP MDP, Dead-ends.'",-,LELIANE NUNES DE BARROS,113,"Planejamento Probabilístico, MDP, SSP MDP, Becos-sem-saída.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Planejamento probabilístico lida com a tomada de decisão sequencial em ambientes estocásticos
e geralmente é modelado por um Processo de Decisão Markoviano (Markovian Decision Process -
MDP). Um MDP modela a interação entre um agente e o seu ambiente: em cada estágio, o agente
decide executar uma ação, com efeitos probabilísticos e um certo custo, que irá produzir um estado
futuro. O objetivo do agente MDP é minimizar o custo esperado ao longo de uma sequência de
escolhas de ação. O número de estágios que o agente atua no ambiente é chamado de horizonte,
o qual pode ser finito, infinito ou indefinido. Um exemplo de MDP com horizonte indefinido é
o Stochastic Shortest Path MDP (SSP MDP), que estende a definição de MDP adicionando um
conjunto de estados meta (o agente para de agir ao alcançar um estado meta). Num SSP MDP é
feita a suposição de que é sempre possível alcançar um estado meta a partir de qualquer estado do
mundo. No entanto, essa é uma suposição muito forte e que não pode ser garantida em aplicações
práticas. Estados a partir dos quais é impossível atingir a meta são chamados de becos-sem-saída.
Um beco-sem-saída pode ser evitáve ou inevitável (se nenhuma política leva do estado inicial para a
meta com probabilidade um). Em trabalhos recentes foram propostas extensões para SSP MDP que
permitem a existência de diferentes tipos de beco-sem-saída, bem como algoritmos para resolvê-los.
No entanto, a detecção de becos-sem-saída é feita utilizando: (i) heurísticas que podem falhar para
becos-sem-saída implícitos ou (ii) métodos mais confiáveis, mas que demandam alto custo computacional.
Neste projeto fazemos uma caracterização formal de modelos de planejamento probabilístico
com becos-sem-saída. Além disso, propomos uma nova técnica para detecção de becos-sem-saída baseada
nessa caracterização. Finalmente, adaptamos algoritmos de planejamento probabilístico para
utilizarem o novo método de detecção, permitindo assim que os mesmos resolvam tanto problemas
com becos-sem-saída evitáveis quanto problemas com becos-sem-saída inevitáveis. Os resultados
empíricos mostram que o método proposto é capaz de detectar todos os becos-sem-saída de um
dado conjunto de estados e, quando usado com planejadores probabilísticos, pode tornar esses planejadores
mais eficientes em domínios com becos-sem-saída difíceis de serem detectados.",DISSERTAÇÃO,Planejamento Probabilístico com Becos sem Saída,5010753,1
"Business processes play a very important role in the industry, especially by the evolution of information
technologies. Cloud computing platforms, for example, with the allocation of on-demand
computing resources enable the execution of highly requested processes.
Therefore, it is necessary to define the execution environment of the processes in such a way
that the resources are used optimally and the correct functionality of the process is guaranteed.
In this context, different methods have already been proposed to model business processes and
analyze their quantitative and qualitative properties. There are, however, a number of challenges
that may restrict the application of these methods, especially for high-demanded processes (such
as workflows of numerous instances) and that rely on resources that are limited.
The analysis of the performance of workflows of numerous instances through analytical modeling
is the object of study of this work. Generally, for the accomplishment of this type of analysis,
mathematical models based on Markovian techniques (stochastic systems) are used, which suffer
the problem of the state space explosion. However, the Mean Field Theory, indicates that the behavior
of a stochastic system, under certain conditions, can be approximated by that of a deterministic
system, avoiding the explosion of the state space.
In this work we use such a strategy, based on the formal definition of deterministic approximation
and its conditions of existence, we elaborate a method to represent the workflows, and
their resources, as ordinary differential equations, which describe a deterministic system. Once the
deterministic approximation has been defined, we perform the performance analysis in the deterministic
model, verifying that the obtained results are a good approximation for the stochastic
solution.",Dissertacao_WaldirEdsonFarfanCaro_deposito.pdf,CIÊNCIA DA COMPUTAÇÃO,WALDIR EDISON FARFAN CARO,UNIVERSIDADE DE SÃO PAULO,17/04/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'business workflows, analytical modeling, performance analysis, mean field approximation.'",-,KELLY ROSA BRAGHETTO,98,"workflows de negócio, modelagem analítica, análise de desempenho, aproximação de campo médio.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Os processos de negócio desempenham um papel muito importante na indústria, principalmente
pela evolução das tecnologias da informação. As plataformas de computação em nuvem,
por exemplo, com a alocação de recursos computacionais sob demanda, possibilitam a execução
de processos altamente requisitados. Para tanto, é necessário definir o ambiente de execução dos
processos de tal modo que os recursos sejam utilizados de forma ótima e seja garantida a correta
funcionalidade do processo. Nesse contexto, diferentes métodos já foram propostos para modelar
os processos de negócio e analisar suas propriedades quantitativas e qualitativas. Há, contudo, vários
desafios que podem restringir a aplicação desses métodos, especialmente para processos com
alta demanda (como os workflows de numerosas instâncias) e que dependem de recursos limitados.
A análise de desempenho de workflows de numerosas instâncias via modelagem analítica é o
objeto de estudo deste trabalho. Geralmente, para a realização desse tipo de análise usa-se modelos
matemáticos baseados em técnicas Markovianas (sistemas estocásticos), que sofrem do problema
da explosão do espaço de estados. Entretanto, a Teoria do Campo Médio indica que o comportamento
de um sistema estocástico, sob certas condições, pode ser aproximado por o de um sistema
determinístico, evitando a explosão do espaço de estados.
Neste trabalho usamos tal estratégia e, com base na definição formal de aproximação determinística
e suas condições de existência, elaboramos um método para representar os workflows, e
seus recursos, como equações diferenciais ordinárias, que descrevem um sistema determinístico.
Uma vez definida a aproximação determinística, realizamos a análise de desempenho no modelo
determinístico, verificando que os resultados obtidos são uma boa aproximação para a solução estocástica.",DISSERTAÇÃO,Análise preditiva de desempenho de workflows usando Teoria do Campo Médio,5010754,1
"Bayesian networks are widely used graphical models for reasoning under uncertainty on complex
domains. A Bayesian network is a directed acyclic graph where nodes represent random variables
and the arcs represent (in)dependence relationships. Manually specifying a Bayesian network over a
large and complex domain is a time-consuming and error-prone task. This justifies the development
of methods for learning Bayesian network structures from data.
A successful approach to Bayesian network structure learning is to use a score function which
assigns a value for each structure based on how well the structure represents the data. This way the
problem of learning a Bayesian network becomes a combinatorial optimization of finding structures.
Score-based structure learning is a computationally demanding task (in fact, NP-Hard), which
justifies the development of approximate methods. Even though there are some methods which
provide quality guarantees (convergence, consistence or error estimative), they scale poorly to large
domains (hundreds and thousands of variables).
An effective approach for learning Bayesian network structures is to perform a local search
on the space of topological orderings using a restricted space of parent sets. While this approach
has no performance guarantee, it is computationally efficient and performs empirically better than
other approaches, especially on large domains. Typically, the search is initialized with a randomly
generated ordering. This can lead to poor local optima, slow convergence and ultimately degrade
the performance of the method as the number of variables increases.
This work aims at studying and improving order-based local search methods for score-based
Bayesian network structure learning on large domains. Specifically, we aim at improving solutions
obtained by order-based local searches using state-of-the-art parent set selection methods, and at
developing new informed heuristics that allow for learning better large Bayesian networks. The new
heuristics were evaluated on the scores obtained from real-world data sets. Results show that the
new initialization heuristics improve the obtained Bayesian networks significantly.",Dissertacao_WalterPerezUrcia_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,WALTER PEREZ URCIA,UNIVERSIDADE DE SÃO PAULO,10/01/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'Bayesian networks, Machine Learning, Probabilistic Model, Local Search.'",-,LELIANE NUNES DE BARROS,79,"Redes Bayesianas, Aprendizagem de máquina, Modelos probabilísticos, Busca local.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Redes Bayesianas são modelos gráficos amplamente utilizados para automatizar o raciocínio
probabilístico em domínios complexos. Uma rede Bayesiana é um grafo direcionado acíclico no
qual os nós representam variáveis aleatórias e os arcos representam relações de dependência entre
variáveis. Especificar manualmente uma rede Bayesiana sobre um domínio grande e complexo é
uma tarefa altamente custosa e propensa a erros. Isto justifica o desenvolvimento de métodos para
aprender estruturas de redes Bayesianas a partir de observações.
Uma abordagem bem sucedida para aprendizado de redes Bayesianas é especificar uma função
de pontuação (score), que associa cada estrutura a um número que representa a adequação do
modelo aos dados e ao conhecimento prévio. O aprendizado então consiste em selecionar uma
estrutura com alta pontuação. A aprendizagem estrutural baseada em pontuação é uma tarefa
computacionalmente custosa (NP-difícil), o que cria a necessidade de desenvolvimento de técnicas
aproximadas. Embora existam muitas técnicas com alguma garantia de qualidade (convergência,
consistência ou estimativa de erro), elas possuem um custo computacional alto e não são aplicáveis
em domínios muito grandes (centenas ou milhares de variáveis).
Uma técnica simples e eficaz para a aprendizagem aproximada de estruturas de redes Bayesianas
consiste em realizar uma busca local no espaço de ordenações topológicas de variáveis utilizando
um espaço restrito de conjuntos de pais. Embora essa abordagem não possua garantias de desempenho,
ela é computacionalmente eficiente, e empiricamente superior a outros métodos, especialmente
quando o número de variáveis é grande. Geralmente, a busca local é inicializada com uma
ordenação das variáveis gerada uniformemente no espaço de ordenações. Isso pode levar a busca
a obter soluções de baixa qualidade e a requerer um número alto de iterações, o que prejudica o
desempenho do método.
Esse trabalho tem como objetivo estudar e aprimorar as técnicas de aprendizagem de redes
Bayesianas em domínios muito grandes. Em particular, pretende-se melhorar a qualidade das
soluções encontradas pelo algoritmo de busca por geração de ordenações topológicas, empregando
técnicas do estado-da-arte na geração de conjuntos de pais e desenvolvendo heurísticas informadas
para geração de ordenações topológicas. A qualidade das soluções encontradas foi avaliada pela pontuação.
Os resultados mostram que as novas heurísticas de inicialização melhoram as redes obtidas
com uma diferencia significativa.",DISSERTAÇÃO,Aprendizado de Redes Bayesianas para domínios grandes,5010755,1
"Mobile Music applications are becoming commonplace around the world, and mobile devices are
used as digital instruments everywhere. Controlling, performing or composing music in real time
with these devices encourages collaboration and interaction in that telecommunication advances
allow many numbers of people to cooperate through local networks or the Internet. In this context,
the aim of this thesis is to evaluate mobile technologies that might be suitable for mobile musicians
and their audiences while performing or composing. Specifically, the main goal is to explore technologies
for collaborative mobile music and obtain quantitative and qualitative results regarding
these technologies and their settings, so that composers might take full advantage of the available
options for mobile applications. This evaluation focuses on message exchange using Multicast,
Unicast, and Cloud Services, using academic networks as the main pathway. With these services,
messages are organized as streams of packages, characterized by several package sizes and time intervals
between packages. Evaluation also includes the development of several applications making
use of these technologies, on both Android devices and Web browsers, which were used in actual
performances, serving as both evaluation tools and experimental music instruments. I analyze results
relating delay, jitter and data loss under very different configuration scenarios, demonstrating
that although some obvious impediments cannot be circumvented (e.g. high delay in international
settings), it is possible to choose appropriately in terms of technology and achieve interesting results
under most music application scenarios. I argue that although Multicast appears by far to
be the best technology to use in theory, it is the most difficult to implement due to the burden of
configuring every step of the network pathway. On the other hand, Cloud Services is slower but turn
out to be the most compatible and easiest to set up, and definitely suitable for many collaborative
music experiences. To conclude I discuss how Mobile Music practitioners can take advantage of
these results for composition and performance by considering specific technological advantages or
drawbacks that are inherent by each technology and setting.",Tese_AntonioDeusanydeCarvalhoJunior_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,ANTONIO DEUSANY DE CARVALHO JUNIOR,UNIVERSIDADE DE SÃO PAULO,24/05/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'mobile music, computer music, computer networks, cloud services'",-,MARCELO GOMES DE QUEIROZ,98,"música móvel, computação musical, redes de computadores, serviços das nuvens",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Aplicações de Música Móvel estão se tornando populares ao redor do mundo e dispositivos
móveis estão sendo utilizados como instrumentos em diversos lugares. Controlar, performar ou
compor música em tempo real com estes dispositivos estimula colaboração e a interação por meio
destes avanços nas telecomunicações permite um grande número de pessoas cooperar musicalmente
através de redes locais ou pela Internet. Neste contexto, o objetivo desta tese é avaliar as tecnologias
móveis que podem ser úteis para os músicos e sua audiência durante performance ou
composição. De maneira mais específica, o objetivo principal é explorar as tecnologias para Música
Móvel colaborativa e obter resultados quantitativos e qualitativos referentes a estas tecnologias e
suas configurações, de modo que compositores possam usufruir de todas as vantagens das opções
para aplicações móveis. Esta avaliação foca na troca de mensagens através de Multicast, Unicast, e
Serviços em Nuvem utilizando redes de computadores acadêmicas como principal caminho. Através
destes serviços as mensagens foram organizadas como fluxos de pacotes caracterizados por diversos
tamanhos e intervalos entre envios. A avaliação também inclui o desenvolvimento de diversas
aplicações fazendo uso destas tecnologias para dispositivos com o sistema operacional Android e
navegadores Web, que foram utilizados em performances reais, servindo como ferramentas de avaliação
e instrumentos para música experimental. Os dados são analisados com relação ao atraso,
jitter e perda de pacotes em diferentes configurações de cenário, demonstrando que apesar de alguns
impedimentos óbvios não terem como ser contornados (como longo atraso em configurações
internacionais, por exemplo), é possível escolher tecnologias adequadamente e alcançar resultados
interessantes em muitos cenários de aplicações musicais. Argumenta-se que apesar de o Multicast
se apresentar de longe como a melhor tecnologia para estes casos em teoria, ele é o mais complicado
de ser implementado devido à grande dificuldade de configurar cada parte da rede para seu uso.
Por outro lado, Serviços em Nuvem são lentos, porém se apresentam como os mais compatíveis de
fáceis de configurar, sendo definitivamente os mais adequados para muitas experiências de música
colaborativa. Como conclusão, é discutido como profissionais de Música Móvel podem se aproveitar
dos resultados apresentados, considerando as vantagens e desvantagens tecnológicas específicas que
são inerentes a cada tecnologia ou configuração ao ser utilizada em performances e composições
musicais.",TESE,Mobile Technologies for Music Interaction,5026284,1
"The analysis of large-scale datasets is one of the major current computational challenges and it
is present not only in fields of modern science domain but also in the industry and public sector.
In these scenarios, the data processing is usually modeled as a set of activities interconnected through
data flows – as known as workflows. Due to their high computational cost, several strategies
were proposed to improve the efficiency of data-intensive workflows, such as activities clustering to
minimize data transfers and parallelization of data processing for reducing the runtime, in which
two or more activities are performed at same time on different computational resources. The parallelism,
in this case, is defined by the structure described in their model of activities composition.
In general, Workflow Management Systems are responsible for the coordination and execution of
these activities in a distributed environment. However, they are not aware of the type of processing
that will be performed by each one. Thus, they are not able to automatically explore strategies for
parallel execution. Parallelizable activities are defined by user at workflow design time and creating
a structure that makes an efficient use of a distributed environment is not a trivial task. This work
aims to provide more efficient executions for data intensive workflows and, for that, proposes a
method for automatic parallelization of these applications, with focus on non-specialists users in
high performance computing. This method defines nine semantic annotations to characterize how
data is accessed and consumed by activities and thus, taking into account the available computational
resources, to create automatically strategies that exploit data parallelism. The proposed method
generates replicas of annotated activities. It also defines a workflow data indexing and distribution
scheme that allows greater parallel access. Its efficiency was evaluated in two workflow models with
real data, executed in Amazon cloud platform. A relational (PostgreSQL) and a NoSQL (MongoDB)
DBMS were used to manage up to 20.5 million of data objects in 21 scenarios with different partitioning
and data replication settings. The obtained results showed that the parallelization of the
execution of the activities promoted by the method achieved an up to 66.6 % makespan reduction
without increasing its monetary cost.",dissertacao_ElaineWatanabe_primeiraversao.pdf,CIÊNCIA DA COMPUTAÇÃO,ELAINE NAOMI WATANABE,UNIVERSIDADE DE SÃO PAULO,22/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'Data Parallelism, NoSQL, Data-intensive Workflows'",-,KELLY ROSA BRAGHETTO,85,"Paralelismo de Dados, NoSQL, Workflows Intensivos em Dados",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"A análise de dados em grande escala é um dos grandes desafios computacionais atuais e está
presente não somente em áreas da ciência moderna mas também nos setores público e industrial.
Nesses cenários, o processamento dos dados geralmente é modelado como um conjunto de atividades
interligadas por meio de fluxos de dados – os workflows. Devido ao seu alto custo computacional,
diversas estratégias já foram propostas para melhorar a eficiência da execução dos workflows intensivos
em dados, tais como o agrupamento de atividades para a minimizar a transferência de dados e
a paralelização do processamento, de modo que duas ou mais atividades sejam executadas ao mesmo
tempo em diferentes recursos computacionais. O paralelismo nesse caso é definido pela estrutura
descrita em seu modelo de composição de atividades. Em geral, os Sistema de Gerenciamento de
Workflows, responsáveis pela coordenação e execução dessas atividades em um ambiente distribuído,
desconhecem o tipo de processamento a ser realizado e por isso não são capazes de explorar
automaticamente estratégias para execução paralela. As atividades paralelizáveis são definidas pelo
usuário em tempo de projeto e criar uma estrutura que faça uso eficiente de um ambiente distribuído
não é uma tarefa trivial. Este trabalho tem como objetivo prover execuções mais eficientes
de workflows intensivos em dados e propõe para isso um método para a paralelização automática
dessas aplicações, voltado para usuários não-especialistas em computação de alto desempenho. Este
método define nove anotações semânticas para caracterizar a forma como os dados são acessados
e consumidos pelas atividades e, assim, levando em conta os recursos computacionais disponíveis
para a execução, criar automaticamente estratégias que explorem o paralelismo de dados. O método
proposto gera réplicas das atividades anotadas e define também um esquema de indexação e
distribuição dos dados do workflow que possibilita maior acesso paralelo. Avaliou-se sua eficiência
em dois modelos de workflows com dados reais, executados na plataforma de nuvem da Amazon.
Usou-se um SGBD relacional (PostgreSQL) e um NoSQL (MongoDB) para o gerenciamento de até
20,5 milhões de objetos de dados em 21 cenários com diferentes configurações de particionamento e
replicação de dados. Os resultados obtidos mostraram que a paralelização da execução das atividades
promovida pelo método reduziu o tempo de execução do workflow em até 66,6% sem aumentar
o seu custo monetário.",DISSERTAÇÃO,Um método para paralelização automática de workflows intensivos em dados,5027353,'
"Adaptivity is a capability that enables a system to choose amongst various alternatives to satisfy
or maintain the satisfaction of certain requirements. The criteria of requirements satisfaction
could be pragmatic and context-dependent. Contextual Goal Models (CGM) capture the power of
context on banning or allowing certain alternatives to reach requirements (goals) and also deciding
the quality of those alternatives with regards to certain quality measures (softgoals). It is used to
depict facets of the decision making strategy and rationale of an adaptive system at the preliminary
level of requirements. In this thesis we argue the case for pragmatic requirements and extend the
CGM with additional constructs to capture them and allow their analysis. We also develop an automated
analysis which aids the planning and scheduling of tasks execution to meet pragmatic goals.
Moreover, we evaluate our modelling and analysis regarding correctness and performance. Such an
evaluation showed the applicability of the approach and its usefulness in aiding sensible decisions.
It has also shown its capability to do so in a time short enough to suit run-time adaptation decision
making",Tese_FelipePontesGuimaraes_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,FELIPE PONTES GUIMARAES,UNIVERSIDADE DE SÃO PAULO,26/05/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'Service Composition, Fault Tolerance, Context Aware'",-,DANIEL MACEDO BATISTA,84,"Service Composition, Fault Tolerance, Context Aware",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Adaptivity is a capability that enables a system to choose amongst various alternatives to satisfy
or maintain the satisfaction of certain requirements. The criteria of requirements satisfaction
could be pragmatic and context-dependent. Contextual Goal Models (CGM) capture the power of
context on banning or allowing certain alternatives to reach requirements (goals) and also deciding
the quality of those alternatives with regards to certain quality measures (softgoals). It is used to
depict facets of the decision making strategy and rationale of an adaptive system at the preliminary
level of requirements. In this thesis we argue the case for pragmatic requirements and extend the
CGM with additional constructs to capture them and allow their analysis. We also develop an automated
analysis which aids the planning and scheduling of tasks execution to meet pragmatic goals.
Moreover, we evaluate our modelling and analysis regarding correctness and performance. Such an
evaluation showed the applicability of the approach and its usefulness in aiding sensible decisions.
It has also shown its capability to do so in a time short enough to suit run-time adaptation decision
making",TESE,IMPROV: An Architecture for Reliable Execution of Dynamic Service Compositions,5027355,1
"The knowledge society, comprehended as the result of the development of digital technologies,
creates possibilities for new understandings of the world, whose representations take a myriad of
multiple forms. In addition to the multiplicity of forms, current society is changing at exponential
rates due to these technological advances that remark this century.
Access to information transforms our status quo. But this process was enhanced by the digital
revolution until the point of world division between digital immigrants and digital natives. In a time
of transition experienced these days, both groups have the challenge to understand and reframe their
roles and identities. Both are challenged to build knowledge for the continuous development of the
humanity, while learning to appropriate the cyberspace and take advantage of its potentialities.
The Digital Age gave birth to multiple forms of literacy: digital literacy, technology literacy
and media literacy, among others. They interconnect, overlap and converge into and within each
other, increasing their unique power in a dynamic process. This process represents a fundamental
paradigm shift to educators - most of them, digital immigrants.
How do they overcome their technical limitations and prejudices concerning these new tools?
How do they become comfortable moving through multiple media interfaces? How do they acquire
digital literacy and computational thinking in order to explore new learning experiences for the
benefit of their students? In which tools and frameworks are they using to teach?
Considering this context, the objective of this research is to create a new framework for the
development of new 21st century competences, focusing on computational thinking and programming.
In order to validate the proposed framework a lean curriculum has been developed on the
basis of which. Thus, educators with knowledge or not can, quickly, take advantage of this scientific
production for the scalable development of digital and technological literacy",Dissertacao_CamilaFernandezAchutti-deposito_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,CAMILA FERNANDEZ ACHUTTI,UNIVERSIDADE DE SÃO PAULO,09/06/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'digital literacy, technology literacy, computational thinking, programming'",-,ANA CRISTINA VIEIRA DE MELO,221,"literacia digital, literacia tecnológica, pensamento computacional, programação",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"A sociedade do conhecimento, compreendida como o resultado do desenvolvimento das tecnologias
digitais, cria novas possibilidades de percepção do mundo, cujas representações podem ter as
mais diversas formas. Além da multiplicidade de formas, a sociedade atual está mudando em taxas
exponenciais devido a esses avanços tecnologicos que marcam este século.
O acesso à informação transforma o nosso status quo. E esse processo foi ainda mais reforçado
pela revolução digital, a ponto de dividir o mundo entre os imigrantes e nativos digitais. No dias de
hoje, um momento de transição é vivido, ambos os grupos têm o desafio de entender e reformular
seus papéis e identidades no mundo. Ambos são obrigados a criar conhecimento para o contínuo
desenvolvimento humano, enquanto aprendem a se apropriar do ciberespaço tirando partido das
suas potencialidades. Cada um no seu nível e tempo.
A era digital deu origem a múltiplas formas de alfabetização: literacia digital, tecnológica, mídia,
entre outros. Eles se interconectam, se sobrepõem e convergem, aumentando seu poder em um
processo dinâmico. Este processo representa uma mudança de paradigma fundamental para os
educadores - a maioria deles, imigrantes digitais.
Como é que eles podem superar suas limitações técnicas e preconceitos relativos a estas novas
ferramentas? Como é que eles podem se sentir confortáveis se movendo através de múltiplas interfaces
de mídia? Como é que eles adquirem literacia digital e pensamento computacional, a fim de
explorar novas experiências de aprendizagem para o benefício de seus alunos?Em quais ferramentas
e framework se baseiam?
Considerando este contexto, o objetivo desta pesquisa é criar um novo framework de desenvolvimento
de novas competências para o século 21 com foco em pensamento computacional e
programação.Para validação do framework proposto, criou-se um currículo enxuto como instância
do mesmo. Desta forma, educadores, independente do conhecimentáo na área, podem se valer rapidamente
da produção científica deste trabalho no desenvolvimento de literacia digital e tecnologica
de maneira escalável.",DISSERTAÇÃO,Tree Bark framework: competences and mindset rearrangements for Digital and Technology Literacy in times of exponential rate of changes.,5027359,1
"belonging to desired objects in the image domain, which is an important step for computer
vision, medical image processing and other applications. Many times automatic segmentation
generates results with imperfections. The user can correct them by editing manually, interactively
or can simply discard the segmentation and try to automatically generate another result
by a different method. Interactive methods combine benefits from manual and automatic ones,
reducing user effort and using its high-level knowledge. In seed-based methods, to continue or
repair a prior segmentation (presegmentation), avoiding the user to start from scratch, it is necessary
to solve the Reverse Interactive Segmentation Problem (RISP), that is, how to automatically
estimate the seeds that would generate it. In order to achieve this goal, we first divide the segmented
object into its composing cores. Inside a core, two seeds separately always produce the
same result, making one redundant. With this, only one seed per core is required. Cores leading
to segmentations which are contained in the result of other cores are redundant and can also be
discarded, further reducing the seed set, a process called Redundancy Analysis. A minimal set
of seeds for presegmentation is generated and the problem of interactive repair can be solved by
adding new seeds or removing seeds. Within the framework of the Image-Foresting Transform
(IFT), new methods such as Oriented Image-Foresting Transform (OIFT) and Oriented Relative
Fuzzy Connectedness (ORFC) were developed. However, there were no known algorithms for
computing the core of these methods. This work develops such algorithms, with proof of correctness.
The cores also gives an indication about the degree of robustness of the methods on
the positioning of the seeds. Therefore, a hybrid method that combines GraphCut and the ORFC
cores, as well as the Robustness Coefficient (RC), have been developed. In this work, we present
another developed solution to repair segmentations, which is based on IFT-SLIC, originally used
to generate supervoxels. Experimental results analyze, compare and demonstrate the potential
of these solutions.",Tese_AndersonCarlosMoreiraTavares_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,ANDERSON CARLOS MOREIRA TAVARES,UNIVERSIDADE DE SÃO PAULO,02/06/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'graph-based segmentation, image-foresting transform, seed robustness, supervoxels'",-,CARLOS HITOSHI MORIMOTO,65,"segmentação baseada em grafos, transformada imagem-floresta, robustez de sementes, supervoxels",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Segmentação de imagem consiste no seu particionamento em regiões, tal como para isolar
os pixels pertencentes a objetos de interesse em uma imagem, sendo uma etapa importante para
visão computacional, processamento de imagens médicas e outras aplicações. Muitas vezes a
segmentação automática gera resultados com imperfeições. O usuário pode corrigi-las editandoa
manualmente, interativamente ou simplesmente descartar o resultado e gerar outro automaticamente.
Métodos interativos combinam os benefícios dos métodos manuais e automáticos, reduzindo
o esforço do usuário e utilizando seu conhecimento de alto nível. Nos métodos baseados
em sementes, para continuar ou reparar uma segmentação prévia (presegmentação), evitando o
usuário começar do zero, é necessário resolver o Problema da Segmentação Interativa Reversa
(RISP), ou seja, estimar automaticamente as sementes que o gerariam. Para isso, este trabalho
particiona o objeto da segmentação em núcleos. Em um núcleo, duas sementes separadamente
produzem o mesmo resultado, tornando uma delas redundante. Com isso, apenas uma semente
por núcleo é necessária. Núcleos contidos nos resultados de outros núcleos são redundantes e
também podem ser descartados, reduzindo ainda mais o conjunto de sementes, um processo denominado
Análise de Redundância. Um conjunto mínimo de sementes para a presegmentação
é gerado e o problema da reparação interativa pode então ser resolvido através da adição de novas
sementes ou remoção. Dentro do arcabouço da Transformada Imagem-Floresta (IFT), novos
métodos como Oriented Image-Foresting Transform (OIFT) e Oriented Relative Fuzzy Connectedness
(ORFC) foram desenvolvidos. Todavia, não há algoritmos para calcular o núcleo destes
métodos. Este trabalho desenvolve tais algoritmos, com prova de corretude. Os núcleos também
nos fornecem uma indicação do grau de robustez dos métodos sobre o posicionamento das
sementes. Por isso, um método híbrido do GraphCut com o núcleo do ORFC, bem como um
Coeficiente de Robustez (RC), foram desenvolvidos. Neste trabalho também foi desenvolvida
outra solução para reparar segmentações, a qual é baseada em IFT-SLIC, originalmente utilizada
para gerar supervoxels. Resultados experimentais analisam, comparam e demonstram o potencial
destas soluções.",TESE,"Interactive 3D Segmentation Repair with Image-Foresting Transform, Supervoxels and Seed Robustness",5027361,1
"Online communities provide a fertile ground for analyzing people's behavior and improving
our understanding of social processes. For instance, when modeling social interaction online, it
is important to understand when people are reacting to each other. Also, since both people and
communities change over time, we argue that analyses of online communities that take time into
account will lead to deeper and more accurate results. In many cases, however, users' behavior can
be easily missed: users react to content in many more ways than observed by explicit indicators
(such as likes on Facebook or replies on Twitter) and poorly aggregated temporal data might hide,
misrepresent and even lead to wrong conclusions about how users are evolving.
In order to address the problem of detecting non-explicit responses, we present a new approach
that uses tf-idf similarity between a user's own tweets and recent tweets by people they follow. Based
on a month's worth of posting data from 449 ego networks in Twitter, this method demonstrates
that it is likely that at least 11% of reactions are not captured by the explicit reply and retweet
mechanisms. Further, these uncaptured reactions are not evenly distributed between users: some
users, who create replies and retweets without using the ocial interface mechanisms, are much
more responsive to followees than they appear. This suggests that detecting non-explicit responses
is an important consideration in mitigating biases and building more accurate models when using
these markers to study social interaction and information diusion.
We also address the problem of users evolution in Reddit based on comment and submission
data from 2007 to 2014. Even using one of the simplest temporal dierences between usersyearly
cohortswe nd wide dierences in people's behavior, including comment activity, eort, and survival.
Furthermore, not accounting for time can lead us to misinterpret important phenomena. For
instance, we observe that average comment length decreases over any xed period of time, but
comment length in each cohort of users steadily increases during the same period after an abrupt
initial drop, an example of Simpson's Paradox. Dividing cohorts into sub-cohorts based on the
survival time in the community provides further insights; in particular, longer-lived users start at
a higher activity level and make more and shorter comments than those who leave earlier. These
ndings both give more insight into user evolution in Reddit in particular, and raise a number of
interesting questions around studying online behavior going forward.",Tese_Samuel Martins Barbosa Neto_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,SAMUEL MARTINS BARBOSA NETO,UNIVERSIDADE DE SÃO PAULO,29/05/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b""users' behavior, social networks, reaction, simpson's paradox, twitter, reddit""",-,ROBERTO MARCONDES CESAR JUNIOR,66,"comportamento de usuários, redes sociais, reações, paradoxo de simpson, twitter, reddit",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Comunidades online proporcionam um ambiente fértil para a análise do comportamento de
indivíduos e avanços na compreensão de processos sociais. Por exemplo, quando modelamos intera
ções sociais online, é importante compreender quando indivíduos estão reagindo a outros indiv
íduos. Além disso, pessoas e comunidades mudam com o passar do tempo, e nós argumentamos
que análises de comunidades online que levam em consideração a evolução temporal fornecem resultados
mais precisos e profundos. Entretanto, em muitos casos, o comportamento de usuários
pode facilmente ser perdido: muitas das reações dos usuários ao conteúdo que são expostos não
são capturadas por indicadores explicitos (como likes no Facebook ou replies no Twitter). Aggrega
ções temporais de dados pouco criteriosas podem ocultar, enviesar ou mesmo levar a conclusões
equivocadas sobre a forma como os usuários evoluem.
Para tratar o problema de detectar respostas não-explicitas, nós apresentamos uma nova abordagem
que utiliza a similaridade medida por tf-idf entre os tweets de um usuário e os tweets recentes
que este usuário recebeu das pessoas que ele segue. Nos baseando em dados de postagens de um
mês para 449 redes egocêntricas no Twitter, este método evidencia que temos um provável volume
de ao menos 11% de reações que não são capturadas pelos mecanismos explicitos de reply e retweet.
Mais do que isso, essas reações que não são capturadas não estão uniformemente distribuídas entre
os usuários: alguns usuários que criam replies e retweets sem utilizar os mecanismos formais da interface
são muito mais responsivos a quem eles seguem do que aparentam. Isso sugere que detecetar
respostas não-explicitas é importante quando consideramos mitigar viéses e construir modelos mais
precisos baseados nestes identicadores de respostas a m de estudar iterações sociais e difusão de
informação.
Nós também abordamos o problema de evolução dos usuários no Reddit, nos baseando em
dados de comentários e submissões no período de 2007 a 2014. Mesmo utilizando um dos métodos
mais simples de diferenciação temporal dos usuários-cohorts anuais-nós encontramos amplas
diferenças entre o comportamento dos indivíduos, que incluem criação de comentários, métricas
de esforço e sobrevivência. Além disso, não considerar a evolução temporal pode levar a equívocos
a repeito de fenômenos importantes. Por exemplo, uma de nosssas observações é que o tamanho
médio dos comentários na rede decresce ao longo de qualquer intervalo de tempo arbitrário, mas
o tamanho médio dos comentários em cada uma das cohorts de usuários é crescente no mesmo
intervalo de tempo, salvo de uma queda inicial, que caracteriza uma observação do Paradoxo de
Simpson. Dividindo as cohorts de usuários em sub-cohorts baseadas em anos de sobrevivência na
iii
iv
rede nos fornece uma perspectiva melhor; usuários que sobrevivem por mais tempo apresentam um
maior nível de atividade inicial, fazendo mais comentários mais curtos do que aqueles usuários que
sobrevivem menos. Estes elementos nos fornecem uma compreensão melhor de como usuários no
Reddit evoluem e levantam uma série de questões interessantes a respeito de futuros desdobramentos
do estudo de comportamento online.",TESE,Revealing social networks' missed behavior: detecting reactions and time-aware analyses,5027373,1
"Image Processing techniques can be used to solve a broad range of problems,
such as medical imaging, document processing and object segmentation.
Image operators are usually built by combining basic image operations
and tuning their parameters. This requires both experience in Image Processing
and trial-and-error to get the best combination of parameters.
An alternative approach to design image operators is to estimate them
from pairs of training images containing examples of the expected input and
their processed versions. By restricting the learned operators to those that
are translation invariant and locally dened (W-operators) we can apply
Machine Learning techniques to estimate image transformations. The shape
that denes which features are used is called a window.
W-operators trained with large windows usually overt due to the lack of
training data. This issue is even more present when training operators with
gray-level inputs. Although approaches such as the two-level design partly
mitigates these problems, they also required more complicated parameter
determination to achieve good results.
In this work we present techniques that aim to increase the window sizes
we can use and decrease the number of manually dened parameters in W-
operator learning. The rst one, KA, is based on Support Vector Machines
and employs kernel approximations to estimate image transformations. We
also present adequate kernels for processing binary and gray-level images.
The second technique, NILC, automatically nds small subsets of operators
that can be successfully combined using the two-level approach. It does so
by examining all subwindows of a domain window D.
Both methods achieve competitive results with methods from the literature
in two dierent application domains. The rst one is a binary document
processing problem common in Optical Music Recognition, while the second
is a segmentation problem in gray-level images. The same techniques were
applied without modication in both domains.",Tese_IgordosSantosMontagner_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,IGOR DOS SANTOS MONTAGNER,UNIVERSIDADE DE SÃO PAULO,12/06/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,b'W-operator learning;Support Vector Machines;Linear classi\x0ccation methods;Machine Learning;Image Processing',-,ROBERTO HIRATA JUNIOR,66,Projeto automatico de W-operadores;Maquinas de Suporte Vetorial;Metodos lineares de classicac~ao;Aprendizado de Maquine;Processamento de imagens,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Processamento de imagens pode ser usado para resolver problemas em
diversas areas, como imagens medicas, processamento de documentos e segmenta
c~ao de objetos. Operadores de imagens normalmente s~ao construdos
combinando diversas operac~oes elementares e ajustando seus par^ametros.
Uma abordagem alternativa e a estimac~ao de operadores de imagens a partir
de pares de exemplos contendo uma imagem de sada e o resultado esperado.
Restringindo os operadores considerados para o que s~ao invariantes a
translac~ao e localmente denidos (W-operadores), podemos aplicar tecnicas
de Aprendizagem de Maquina para estima-los. O formato que dene quais
caractersticas s~ao usadas e chamado de janela.
W-operadores treinados com janelas grandes frequentemente tem problemas
de generalizac~ao, pois necessitam de grandes conjuntos de treinamento.
Este problema e ainda mais grave ao treinar operadores em nveis de cinza.
Apesar de tecnicas como o projeto de operadores dois nveis mitigar em parte
estes problemas, uma determinac~ao de par^ametros complexa e necessaria.
Neste trabalho apresentamos duas tecnicas que permitem o treinamento
de operadores usando janelas grandes. A primeira, KA, e baseada em
Maquinas de Suporte Vetorial (SVM) e utiliza tecnicas de aproximac~ao de
kernels para realizar o treinamento deW-operadores. Uma escolha adequada
de kernels permite o treinamento de operadores nveis de cinza e binarios.
A segunda tecnica, NILC, permite a criac~ao automatica de combinac~oes
de operadores de imagens. Este metodo utiliza uma tecnica de otimizac~ao
especca para casos em que o numero de caractersticas e muito grande.
Ambos metodos obtiveram resultados competitivos com algoritmos da
literatura em dois domnio de aplicac~ao diferentes. O primeiro, Sta Removal,
e um processamento de documentos binarios frequente em sistemas
de reconhecimento otico de partituras. O segundo e um problema de segmenta
c~ao de vasos sanguneos em imagens em nveis de cinza.",TESE,W-operator learning using linear models for both gray-level and binary inputs,5027396,1
"Image representation based on superpixels has become indispensable for improving efficiency
in Computer Vision systems. Object recognition, segmentation, depth estimation, and body model
estimation are some important problems where superpixels can be applied. However, superpixels
can influence the quality of the system results in a positive or negative manner, depending on
how well they respect the object boundaries in the image. In this work, we propose an iterative
method for superpixels generation, known as IFT-SLIC, which is based on sequences of Image
Foresting Transforms, starting with a regular grid for seed sampling. A seed pixel recomputation
procedure is applied per each iteration, generating connected superpixels with a better adherence to
objects borders present in the image. The superpixels obtained by IFT-SLIC structurally correspond
to spanning trees rooted at those seeds, that naturally define superpixels as regions of strongly
connected pixels. Compared to Simple Linear Iterative Clustering (SLIC), IFT-SLIC considers
minimum path costs between pixel and cluster centers rather than their direct distances. Nonmonotonically
increasing connectivity functions are explored in our IFT-SLIC approach leading to
improved performance. Experimental results indicate better superpixel extraction by the proposed
approach in comparation to that of SLIC. We also analyze the effectiveness of IFT-SLIC, according
to efficiency, and accuracy on a high level application – namely sky segmentation. The results show
that IFT-SLIC can be competitive to the best state-of-the-art methods and superior to many others,
which motivates it’s futher development for different applications.",Dissertacao_EduardoBarretoAlexandre_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,EDUARDO BARRETO ALEXANDRE,UNIVERSIDADE DE SÃO PAULO,29/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'Simple Linear Iterative Clustering, Image Foresting Transform, Superpixel, Unsupervisioned Segmentation'",-,PAULO ANDRE VECHIATTO DE MIRANDA,99,"Agrupamento Iterarivo Linear Simples, Transformada Imagem-Floresta, Superpixel, Segmentação não supervisionada",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"A representação de imagem baseada em superpixels tem se tornado indispensável na melhoria
da eficiência em sistemas de Visão Computacional. Reconhecimento de objetos, segmentação, estimativa
de profundidade e estimativa de modelo corporal são alguns importantes problemas nos
quais superpixels podem ser aplicados. Porém, superpixels podem influenciar a qualidade dos resultados
do sistema positiva ou negativamente, dependendo de quão bem eles respeitam as fronteiras
dos objetos na imagem. Neste trabalho, nós propomos um método iterativo para geração de superpixels,
conhecido por IFT-SLIC, baseado em sequências de Transformadas Imagem-Floresta,
começando com uma grade regular de sementes. Um procedimento de recomputação de pixels sementes
é aplicado a cada iteração, gerando superpixels conexos com melhor aderência às bordas dos
objetos presentes na imagem. Os superpixels obtidos via IFT-SLIC correspondem, estruturalmente,
à árvores de espalhamento enraizadas nessas sementes, que naturalmente definem superpixels como
regiões de pixels fortemente conexas. Comparadas ao Agrupamento Iterativo Linear Simples (SLIC),
o IFT-SLIC considera os custos do caminho mínimo entre pixels e os centros dos clusters, em vez de
suas distâncias diretas. Funções de conexidade não monotonicamente incrementais são exploradas
em nosso método resultando em melhor desempenho. Resultados experimentais indicam resultados
de extração de superpixels superiores pelo método proposto em comparação com o SLIC. Nós também
analisamos a efetividade do IFT-SLIC, em termos de medidas de eficiência e acurácia, em uma
aplicação de alto nível de segmentação do céu em fotos de paisagens. Os resultados mostram que
o IFT-SLIC é competitivo com os melhores métodos do estado da arte e superior a muitos outros,
motivando seu desenvolvimento para diferentes aplicações.",DISSERTAÇÃO,IFT-SLIC: Geração de Superpixels com Base em Agrupamento Iterativo Linear Simples e Transformada Imagem-Floresta,5027399,1
"Multi-label classification (MLC) consists of learning a function that is capable of mapping an
object to a set of relevant labels. It has applications such as the association of genes with biological
functions, semantic classification of scenes and text categorization. Traditional classification
(ie, single-label) is therefore a particular case of multi-label classification in which each object is
associated with exactly one label.
A successful approach to constructing classifiers is to obtain a probabilistic model of the relation
between object attributes and labels. This model can then be used to classify objects by computing
the most probable explanation (MPE) or computing the marginal probability of one or more labels
given the attributes. Depending on the family of probabilistic models chosen, such inferences may
be intractable when the number of labels is large.
Sum-Product Networks (SPN) are deep probabilistic models, that allow tractable marginal
inference. Nevertheless, as with many other probabilistic models, performing MPE inference is
NP-hard. Although SPNs have already been used successfully for traditional classification tasks
(single-label), there is no in-depth investigation on MLC.
In this work we investigate the use of SPNs for multi-label classification. We compare several
algorithms for learning SPNs combined with different proposed approaches for approximate MPE
inference. We show that SPN-based multi-label classifiers are competitive against state-of-the-art
classifiers, such as Random k-Labelsets with Support Vector Machine and MPE inference on Cut-
Nets, in a collection of benchmark datasets.",,CIÊNCIA DA COMPUTAÇÃO,JULISSA GIULIANA VILLANUEVA LLERENA,UNIVERSIDADE DE SÃO PAULO,06/09/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'Multi-label classification, probabilistic graphical models, sum-product networks'",-,DENIS DERATANI MAUA,51,"Classificação Multi-Rótulo, modelos probabilísticos, redes suma-produto",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Uma abordagem bem sucedida para construir classificadores é obter um modelo probabilístico
da relação entre atributos do objeto e rótulos. Esse modelo pode então ser usado para classificar
objetos calculando a explicação mais provável (MPE) ou computando a probabilidade marginal de
um ou mais rótulos dados os atributos. Dependendo da família de modelos probabilísticos escolhidos,
tais inferências podem ser intratáveis quando o número de rótulos é grande.
As redes Soma-Produto (SPN) são modelos probabilísticos profundos, que permitem inferência
marginal tratável. No entanto, como em muitos outros modelos probabilísticos, a inferência da
MPE é NP-dificil. Embora as SPNs já tenham sido usadas com sucesso para tarefas de classificação
tradicionais (rótulo único), não há uma investigação aprofundada no MLC.
Neste trabalho, investigamos o uso de SPNs para classificação de Multi-Rótulo. Comparamos
vários algoritmos para aprender SPNs combinados com diferentes abordagens propostas para inferência
aproximada de MPE. Mostramos que os classificadores Multi-Rótulos baseados em SPN são
competitivos contra classificadores de estado-da-arte, como Random k-Labelsets com Máquinas de
Soporte Vectorial e inferência exacta de MPE em CutNets, em uma coleção de conjuntos de dados
de referência.",DISSERTAÇÃO,Multi-label classification based on sum-product networks,5051276,1
"In the present work, we studied and developed algorithm with evaluation complexity analysis for
nonlinear programming problems. For the unconstrained optimization case, we estabilished two
similar algorithms that explore high-order regularization models. We proposed a computational implementation
that preserves the good properties of the evaluation complexity theory, and we made
numerical experiments with classical problems from the literature, in order to check the implementation
and certify the practical applicability of methods that employ high-order models. For the
constrained optimization case, we estabilished a two phases algorithm that converges to points that
meet the unscaled first-order optimallity condition for the nonlinear programming problem.",Tese_JohnLenonCardosoGardenghi_deposito.pdf,CIÊNCIA DA COMPUTAÇÃO,JOHN LENON CARDOSO GARDENGHI,UNIVERSIDADE DE SÃO PAULO,09/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'nonlinear programming, optimization, evaluation complexity.'",-,ERNESTO JULIÁN GOLDBERG BIRGIN,67,"programação não linear, otimização, complexidade de avaliação.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"avaliação
para problemas de programação não linear. Para minimização irrestrita, estabelecemos dois
algoritmos semelhantes que exploram modelos de ordem superior com estratégia de regularização.
Propusemos uma implementação computacional que preserva as boas propriedades teóricas de complexidade,
e fizemos experimentações numéricas com problemas clássicos da literatura, a fim de
atestar a implementação e avaliar a aplicabilidade prática de métodos que empreguem modelos de
ordem superior. Para minimização com restrições, estabelecemos um algoritmo de duas fases que
converge a pontos que satisfazem condições de otimalidade de primeira ordem não escaladas para o
problema de programação não linear.",TESE,Complexidade em programação não linear,5058157,1
.,Disertacao_Julio Cesar Delgado Vazquez deposito.pdf,CIÊNCIA DA COMPUTAÇÃO,JULIO CESAR DELGADO VASQUEZ,UNIVERSIDADE DE SÃO PAULO,28/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'.',-,ERNESTO JULIÁN GOLDBERG BIRGIN,52,"flow shop, permutação, heurística, adiantamento, atraso, data de entrega comum, algoritmo de timing.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"No presente trabalho, estudamos e desenvolvemos algoritmos com análise de complexidade de avaliação
para problemas de programação não linear. Para minimização irrestrita, estabelecemos dois
algoritmos semelhantes que exploram modelos de ordem superior com estratégia de regularização.
Propusemos uma implementação computacional que preserva as boas propriedades teóricas de complexidade,
e fizemos experimentações numéricas com problemas clássicos da literatura, a fim de
atestar a implementação e avaliar a aplicabilidade prática de métodos que empreguem modelos de
ordem superior. Para minimização com restrições, estabelecemos um algoritmo de duas fases que
converge a pontos que satisfazem condições de otimalidade de primeira ordem não escaladas para o
problema de programação não linear.",DISSERTAÇÃO,Programação de tarefas emum ambiente flow shop comm máquinas para a minimização do desvio absoluto total de uma data de entrega comum,5058224,1
"capable of applying user defined strategies to guide the behavior of the agents in the simulation.
With this objective in mind we created a formal strategy model to describe complex team behavior
and developed methods of using that model to calculate collective plans. We defined both the
strategy model and the planning methods in a broad manner that can be applied in many different
domains. Then we defined a basketball simulation domain and implemented our methodology to
develop a simulator. We also present a control system architecture that is compatible with our
proposed planner and show how we implemented it to create the basketball simulator.
The formal strategy model we developed can be used to represent team behavior, analyze real
world events and create simulations. We developed a strategy design tool that allows the end user
to create and visualize team strategies for basketball. Finally, we developed a system that interprets
the user generated strategies and creates a basketball match simulation of the described behavior.
We also proposed a methodology for the development of simulation systems involving multiple
intelligent agents. Our recommended control system architecture separates the many layers of
control, which simplifies the development process and results in a naturally expansible system.",Tese_GuilhermefernandesOtranto Deposito _pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,GUILHERME FERNANDES OTRANTO,UNIVERSIDADE DE SÃO PAULO,13/09/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'planning, behavior simulation, artificial intelligence'",-,JUNIOR BARRERA,101,"planejamento, simulação de comportamento, inteligência artificial",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"A motivação que deu origem a esse trabalho foi um desejo de criarmos um simulador de esportes
de invasão coletivos capaz de aplicar estratégias definidas pelo usuário para guiar o comportamento
de agentes na simulação. Com esse objetivo em mente nós criamos um modelo formal de estratégia
para descrever comportamentos complexos em equipe e desenvolvemos métodos para usar esse
modelo no cálculo de planos coletivos. Definimos o modelo e os métodos de planejamento de uma
forma abrangente que pode ser aplicada em muitos domínios diferentes. Definimos um domínio
para a simulação de partidas de basquete e implementamos nossa metodologia para desenvolver um
simulador. Também apresentamos uma arquitetura de controle que é compatível com o planejador
proposto e mostramos como implementá-la na criação de um simulador de basquete.
O modelo formal que desenvolvemos pode ser usado para representar comportamento coletivo,
analisar eventos reais e criar simulações. Desenvolvemos um desenhador de estratégia que permite
que o usuário final desenhe e visualize estratégias de equipes de basquete. Finalmente, desenvolvemos
um sistema que interpreta o conteúdo gerado pelo usuário e cria uma simulação de basquete
usando o comportamento descrito.
Propusemos também uma metodologia para o desenvolvimento de sistemas de simulação envolvendo
múltiplos agentes inteligentes. Nossa arquitetura de controle separa as várias camadas de
controle, simplificando o processo de desenvolvimento e resultando em um sistema naturalmente
expansível.",TESE,A formal model for strategic planning in cooperative and competitive environments Case study: design and implementation of a basketball simulator,5058244,1
"This thesis investigates the mediation of digital games in the context of the classroom and teacher
training of a full-time state school located in Cotia - São Paulo. It bases its results of interactive
mediator acting from the categories studied by the psychologist and educator Reuven Feuerstein,
in what relates to the changes in the cognitive structure of Elementary School students, in the
learning of mathematical knowledge. This research was developed with XXX students and three
teachers of Mathematics of the Curricular Workshops denominated of Mathematical Experiences.
The project elaborated collectively (management team, Mathematics teachers, Elementary School
students II and Alpha Research Group - FEUSP) lasted for two years. Qualitative research, such as
action research, involved total immersion, with participant observation, semi-structured interviews,
informal interviews, focus groups, audio and video recording, photos, eld diaries, chosen games
activities and more three social networks ? Moodle, FaceBook and Whatsapp. The results point out
that: i) the school context represents a privileged space of systematization and comprehension of
the complex notational register of Mathematics with the mediation of digital games; ii) These give
meaning and meaning to the mathematics studied in the classroom by students; iii) Digital games,
many times, help the development of the competencies and cognitive abilities often unintentional
and not planned in a static, linear and hierarchical way, mediation criteria pointed out by Feuerstein,
with varying frequency and intensity; iv) As for the professors of Mathematics, the research
carried out a pedagogical work of ongoing formation, helping them to understand the innovative
methodological strategies of evaluation; v) The alternatives of mobile-learning, ipped-classroom
and Bring your own device have made it possible to reduce the precariousness of existing infrastructure
in public schools.",Tese_AdalbertoBoscoCastroPereira.pdf,CIÊNCIA DA COMPUTAÇÃO,ADALBERTO BOSCO CASTRO PEREIRA,UNIVERSIDADE DE SÃO PAULO,16/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'Elementary School students II, Digital Games, Mobile Learning, Mathematics, Feuerstein.'",-,LELIANE NUNES DE BARROS,162,"Ensino Fundamental II, Jogos Digitais, Mobile Learning, Matemática, Feuerstein.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Este doutorado investiga a mediação dos jogos digitais em conjunto com um instrutor no contexto
da sala de aula e da formação do professor, com base em experimentos em uma escola estadual
de tempo integral, situada em Cotia - São Paulo. O trabalho fundamenta seus resultados de atuação
mediadora interativa a partir das categorias estudadas pelo psicólogo e educador Reuven Feuerstein
no que se relaciona com as mudanças na estrutura cognitiva (EAM) dos alunos do Ensino
Fundamental II durante a aprendizagem dos conhecimentos matemáticos. Esta investigação foi desenvolvida
com 60 alunos e três professoras de Matemática em Ocinas Curriculares denominadas
de Experiências Matemáticas. O projeto elaborado coletivamente (equipe gestora, professores de
Matemática, alunos do Ensino Fundamental II e Grupo Alpha de Pesquisa - FEUSP) teve a dura-
ção de dois anos. A investigação, de cunho qualitativo, é caracterizada como pesquisa-ação e contou
com a imersão total do pesquisador, com observação participante, entrevistas semiestruturadas,
entrevistas informais, grupos focais, gravação de áudio e vídeos, fotos, diários de campo, atividades
com os jogos digitais escolhidos, Moodle e mais duas redes sociais, FaceBook e Whatsapp. Os
resultados apontam que: i) o contexto escolar representa espaço privilegiado de sistematização e
compreensão do complexo registro notacional da Matemática com a mediação dos jogos digitais;
ii) Estes conferem sentido e signicado à Matemática estudada em sala de aula pelos alunos; iii)
Os jogos digitais, muitas vezes, desenvolvem competências e habilidades cognitivas, de forma não
intencional e planejada, com frequência e intensidade diversas, de forma cooperativa, com exibilidade,
autonomia, transcendência e construção de signicados, critérios de mediação apontados por
Feuerstein; iv) Quanto aos professores de Matemática, a pesquisa realizou um trabalho pedagógico
de formação permanente, ajudando-os a compreender as estratégias metodológicas inovadoras de
avaliação; v) As alternativas de perspectivas didáticas como mobile-learning, Flipped-classroom e
Bring Your Own Device, além do uso de smartphones, tornaram possível reduzir a precariedade de
infraestrutura existente nas escolas públicas.",TESE,USO DE JOGOS DIGITAIS NO DESENVOLVIMENTO DE COMPETÊNCIAS CURRICULARES DA MATEMÁTICA,5058329,1
"In different Machine Learning’s applications we can be interest in the minimization of the expected
value of some loss function. For the resolution of this problem, Stochastic optimization and Sample
size selection has an important role. In the present work, it is shown the theoretical analysis of
some algorithms of these two areas, including some variations that considers variance reduction.
In the practical examples we can observe the advantage of Stochastic Gradient Descent in relation
to the processing time and memory, but considering accuracy of the solution obtained and the
cost of minimization, the metodologies of variance reduction has the best solutions. In the algorithms
Dynamic Sample Size Gradient and Line Search with variable sample size selection, despite
of obtaining better solutions than Stochastic Gradient Descent, the disadvantage lies in their high
computational cost.",Disertacao_Jessica De Sousa Fernandes deposito.pdf,CIÊNCIA DA COMPUTAÇÃO,JESSICA KATHERINE DE SOUSA FERNANDEZ,UNIVERSIDADE DE SÃO PAULO,23/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,"b'Machine Learning, stochastic optimization, sample size approximation, dynamic sample size selection, variance reduction methods'",-,ERNESTO JULIÁN GOLDBERG BIRGIN,62,"chave: Aprendizado de máquina, otimização estocástica, Sample size approximation, dynamic sample size selection, métodos de redução de variância.",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Em diferentes aplicações de Aprendizado de Máquina podemos estar interessados na minimização
do valor esperado de certa função de perda. Para a resolução desse problema, Otimização
estocástica e Sample Size Selection têm um papel importante. No presente trabalho se apresentam
as análises teóricas de alguns algoritmos destas duas áreas, incluindo algumas variações que
consideram redução da variância. Nos exemplos práticos pode-se observar a vantagem do método
Stochastic Gradient Descent em relação ao tempo de processamento e memória, mas, considerando
precisão da solução obtida juntamente com o custo de minimização, as metodologias de redução da
variância obtêm as melhores soluções. Os algoritmos Dynamic Sample Size Gradient e Line Search
with variable sample size selection apesar de obter soluções melhores que as de Stochastic Gradient
Descent, a desvantagem se encontra no alto custo computacional deles.",DISSERTAÇÃO,Estudo de algoritmos de Otimização Estocástica aplicados em Aprendizado de Máquina,5058359,1
"Let G be a connected graph and k be a positive integer. A vertex subset D of G is a
k-hop connected dominating set if the subgraph of G induced by D is connected, and for
every vertex v in G there is a vertex u in D such that the distance between v and u in G
is at most k. We study the problem of finding a minimum k-hop connected dominating
set of a graph (Mink-CDS). We prove that Mink-CDS is NP-hard on planar bipartite
graphs of maximum degree 4. We also prove that Mink-CDS is APX-complete on bipartite
graphs of maximum degree 4.We present inapproximability thresholds for Mink-CDS
on bipartite and on (1; 2)-split graphs. Interestingly, one of these thresholds is a parameter
of the input graph which is not a function of its number of vertices. We also discuss
the complexity of computing this graph parameter. On the positive side, we show an
approximation algorithm for Mink-CDS. Finally, when k = 1, we present two new approximation
algorithms for the weighted version of the problem restricted to graphs with
a polynomially bounded number of minimal separators.",Tese_RafaelSantosCoelho_pdf.pdf,CIÊNCIA DA COMPUTAÇÃO,RAFAEL SANTOS COELHO,UNIVERSIDADE DE SÃO PAULO,13/06/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,"b'approximation algorithms, computational complexity, k-hop connected dominating set, k-disruptive separator, inapproximability threshold'",-,YOSHIKO WAKABAYASHI,56,"algoritmos de aproximação, complexidade computacional, conjunto dominante conexo, separador minimal, poliedro",CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"um conjunto dominante conexo de k-saltos se o subgrafo de G induzido por D é conexo e
todo vértice de G está a uma distância não maior que k de um vértice em D. Estudamos
o problema de se encontrar um conjunto dominante conexo de k-saltos com cardinalidade
mínima de um dado grafo (Mink-CDS).
Provamos que Mink-CDS é NP-difícil em grafos planares com grau máximo 4. Também
provamos que Mink-CDS é APX-completo em grafos bipartidos com grau máximo
4. Apresentamos limiares de inaproximabilidade para Mink-CDS para grafos bipartidos
e (1; 2)-split, sendo que um desses é expresso em função de um parâmetro independente
da ordem do grafo de entrada. Também discutimos a complexidade computacional do
problema de se computar tal parâmetro.
No lado positivo, mostramos um algoritmo de aproximação para Mink-CDS que aprimora
o melhor algoritmo de aproximação que se conhece para esse problema. Finalmente,
quando k = 1, apresentamos dois novos algoritmos de aproximação para o problema restrito
a classes de grafos com um número polinomial de separadores minimais.",TESE,The k-hop connected dominating set problem: approximation algorithms and hardness results,5058433,1
"Component tree is a full image representation which uses the connected components of the level
sets of the image and these connected components’ inclusion relationship. This information can be
used in various image processing and computational vision applications, e.g. connected filters, segmentation,
feature extraction, among others. In general, applications which uses component trees
compute attributes that describe the connected components represented by the tree nodes. Attributes
such as area, perimeter and Euler number, can be used directly or indirectly (when they are
used to compute others attributes) to describe the component tree nodes in various applications.
The bit-quads are binary patterns of size 2  2 that are grouped in determined sets and counted in
binary images to compute area, perimeter (also theirs continuous approximation) and Euler number
. Even though the bit-quads usage can yield an efficient method to compute binary image attributes,
they cannot be used efficiently to compute attributes for all component tree nodes, since some
bit-quads can be counted more than once over the level sets. An adaptation of the bit-quads has
been proposed to compute efficiently and incrementally the number of holes for all component tree
nodes. This adaptation uses the fact that each component tree node represents a unique connected
component and one of Euler number definitions to compute the number of holes. Even though this
adaptation can compute Euler number, it cannot compute other attributes derived from the bitquads
(area and perimeter). In this work, an extension of this adaptation is proposed to efficiently
and incrementally count all bit-quads sets in a component tree. Moreover it yields a method to
compute all attributes which can be computed by the bit-quads in binary images in the component
tree using an incremental strategy.",,CIÊNCIA DA COMPUTAÇÃO,DENNIS JOSE DA SILVA,UNIVERSIDADE DE SÃO PAULO,26/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'component tree;bit-quads;incremental attributes',APRENDIZADO DE MÁQUINA,RONALDO FUMIO HASHIMOTO,0,árvore de componentes;bit-quads;atributos incrementais,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Árvore de componentes é uma representação completa de imagens que utiliza componentes conexos
dos conjuntos de níveis de uma imagem e a relação de inclusão entre esses componentes. Essas
informações possibilitam diversas aplicações em processamento de imagens e visão computacional,
e.g. filtros conexos, segmentação, extração de características entre outras. Aplicações que utilizam
árvore de componentes geralmente computam atributos que descrevem os componentes conexos
representados pelos nós da árvore. Entre esses atributos estão a área, o perímetro e o número de
Euler, que podem ser utilizados diretamente ou indiretamente (para o cálculo de outros atributos).
Os bit-quads são padrões de tamanho 22 binários que são agrupados em determinados conjuntos e
contados em imagens binárias. Embora o uso de bit-quads resulte em um método rápido para calcular
atributos em imagens binárias, o mesmo não ocorre para o cálculo de atributos dos nós de uma
árvore de componentes, porque os padrões contados em um nó podem se repetir nos conjuntos de
níveis da imagem e serem contados mais de uma vez. Uma adaptação dos bit-quads é proposta para
o cálculo incremental e eficiente do número de buracos na árvore de componentes. Essa adaptação
utiliza o fato de cada nó da árvore de componentes representar um único componente conexo e uma
das definições do número de Euler para o cálculo do número de buracos. Embora essa adaptação
possa calcular o número de Euler, os outros atributos (área e perímetro) não podem ser computados.
Neste trabalho é apresentada uma extensão dessa adaptação de bit-quads que permite a contagem
de todos os agrupamentos de bit-quads de maneira incremental e eficiente na árvore de componentes.
De forma que o método proposto possa calcular todos os atributos que podem ser obtidos pelos
bit-quads (além do número de buracos) em imagens binárias na árvore de componentes de maneira
incremental.",DISSERTAÇÃO,Contagem incremental de padrões locais em árvores de componentes para cálculo de atributos,5093053,1
"In this work, two new shape descriptors are proposed for tasks in Content-Based Image Retrieval
(CBIR) and Shape Analysis tasks, which are built upon an extended tensor scale based on the
Euclidean Distance Transform (EDT). First, the tensor scale algorithm is applied to extract shape
attributes from its local structures (thickness, orientation, and anisotropy) as represented by the
largest ellipse within a homogeneous region centered at each image pixel. In the new descriptors,
the upper limit of the interval of local orientation of tensor scale ellipses is extended from  to
2, in order to better discriminate the description of local structures. Then, the new descriptors
are built based on different sampling approaches, aiming to summarize the most relevant features.
In the first proposed descriptor, Tensor Scale Sector descriptor (TSS), the local distributions of
relative orientations within circular sectors are used to compose a fixed-length feature vector, for a
region-based shape characterization. For the second method, the Tensor Scale Band (TSB) descriptor,
histograms of relative orientations are considered for each circular concentric band, to also compose a
fixed-length feature vector, with linear time distance function for matching. Experimental results for
different shape datasets (MPEG-7 and MNIST) are presented to illustrate and validate the methods.
TSS can achieve high retrieval values comparable to state-of-the-art methods, which usually rely
on time-consuming correspondence optimization algorithms, but uses a simpler and faster distance
function, while the even faster linear complexity of TSB leads to a suitable solution for very large
shape collections.",,CIÊNCIA DA COMPUTAÇÃO,ANDERSON MEIRELLES FREITAS,UNIVERSIDADE DE SÃO PAULO,24/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'content based image retrieval;CBIR;shape descriptors;shape analysis;tensor scale;computer vision',-,PAULO ANDRE VECHIATTO DE MIRANDA,0,recuperação de imagens baseada em conteúdo;CBIR;descritores de forma;análise de formas;escala tensorial;tensor scale;visão computacional,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Neste trabalho são apresentados dois novos descritores de forma para tarefas de recuperação
de imagens por conteúdo (CBIR) e análise de formas, que são construídos sobre uma extensão
do conceito de tensor scale baseada na Transformada de Distância Euclidiana (EDT). Primeiro,
o algoritmo de tensor scale é utilizado para extrair informações da forma sobre suas estruturas
locais (espessura, orientação e anisotropia) representadas pela maior elipse contida em uma região
homogênea centrada em cada pixel da imagem. Nos novos descritores, o limite do intervalo das orientações
das elipses do modelo de tensor scale é estendido de  para 2, de forma a melhor discriminar
a descrição das estruturas locais. Então, com base em diferentes abordagens de amostragem, visando
resumir informações mais relevantes, os novos descritores são construídos. No primeiro descritor
proposto, Tensor Scale Sector (TSS), a distribuição das orientações relativas das estruturas locais
em setores circulares é utilizada para compor um vetor de características de tamanho fixo, para uma
caracterização de formas baseada em região. No segundo descritor, o Tensor Scale Band (TSB),
foram considerados histogramas das orientações relativas extraídos de bandas concêntricas, formando
também um vetor de características de tamanho fixo, com uma função de distância de tempo linear.
Resultados experimentais com diferentes bases de formas (MPEG-7 e MNIST) são apresentados
para ilustrar e validar os métodos. TSS demonstra resultados comparáveis aos métodos estado da
arte, que geralmente dependem de algoritmos custosos de otimização de correspondências. Já o
TSB, com sua função de distância em tempo linear, se demonstra como uma solução adequada para
grandes coleções de formas.",DISSERTAÇÃO,TSS e TSB: Novos descritores de forma baseados em Tensor Scale,5114254,1
"The Principal Component Analysis (PCA) technique has as the main goal the description of the
variance and covariance between a set of variables. This technique is used to mitigate redundancies
in the set of variables and as a mean of achieving dimensional reduction in various applications in
the scientific, technological and administrative areas. On the other hand, the multidimensional data
model is composed by fact and dimension relations (tables) that describe an event using metrics
and the relationship between their dimensions. However, the volume of data stored and the complexity
of their dimensions usually involved in this model, specially in data warehouse environment,
makes the correlation analyses between dimensions very difficult and sometimes impracticable. In
this work, we propose the development of an Application Programming Interface (API) on multidimensional
data model in order to facilitate the characterization task and dimension reduction. For
verifying the effectiveness of this API, a case study was carried out using the scientific production
data obtained from the Lattes Platform, the Web of Science, Google Scholar and Scopus, provided
by the IT Superintendence at University of São Paulo.",,CIÊNCIA DA COMPUTAÇÃO,RAFAEL GERMANO ROSSI,UNIVERSIDADE DE SÃO PAULO,07/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'PCA;multidimensional model;data warehouse;bibliometric analysis',-,JOAO EDUARDO FERREIRA,0,PCA;modelo multidimensional;data warehouse;análise bibliométrica,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"A técnica de Análise de Componentes Principais (PCA) tem como objetivo principal a descrição
da variância e covariância entre um conjunto de variáveis. Essa técnica é utilizada para mitigar
redundâncias no conjunto de variáveis e para redução de dimensionalidade em várias aplicações nas
áreas científica, tecnológica e administrativa. Por outro lado, o modelo de dados multidimensionais
é composto por relações de fato e dimensões (tabelas) que descrevem um evento usando métricas e
a relação entre suas dimensões. No entanto, o volume de dados armazenados e a complexidade de
suas dimensões geralmente envolvidas neste modelo, especialmente no ambiente de data warehouse,
tornam a tarefa de interpretar a correlação entre dimensões muito difícil e às vezes impraticável.
Neste trabalho, propomos o desenvolvimento de uma Interface de Programação de Aplicação (API)
no modelo de dados multidimensionais para facilitar a tarefa de caracterização e redução de dimensionalidade.
Para verificar a eficácia desta API, um estudo de caso foi realizado utilizando dados de
produção científica e suas citações obtidas das Plataformas Lattes, Web of Science, Google Scholar
e Scopus, fornecidas pela Superintendência de Tecnologia da Informação da Universidade de São
Paulo.",DISSERTAÇÃO,Análise de Componentes Principais em Data Warehouses,5116522,1
"For centuries, cosmetics have been used in the most diverse societies. However, when it comes to face
makeup, the process of choosing a product is still a challenge because it is a manual work that takes time, in addition to consume the makeup itself and other materials for application and cleaning. This manual process also makes it difficult to experiment a number of different products due to the need to clean the skin to remove a previously applied product. Thus, a makeup simulation system using augmented reality can facilitate this process, allowing experimentation with the combination of products and the comparison of the results, as well as allowing to experience the products virtually, through the internet for example. Existing works on this theme allow the user to apply the makeup on a photo or even video of the user himself by means of a mouse or touch of a finger on a touch-sensitive monitor, as if the user applied makeup on a third person.
In this dissertation we propose the development of textit SelfMakeup, an augmented reality system that
allows the self-application of virtual makeup by means of touches made directly on the face, rather than
touches on the monitor. Our hypothesis is that this form of interaction is more natural and gives the user a better experience when testing virtual make-up products. The first step in enabling SelfMakeup was the development of a method to estimate the position of the touch of an applicator on the face using an RGBD camera. We performed tests to evaluate the performance of this method and verified that its accuracy and precision was adequate for the purpose of this research. Next, we designed the graphical interface of the system for applying virtual makeup. The interface allows highlighting and shading effects that simulate the effects brought about by the application of real makeup products. Results from a pilot test of our prototype with 32 users suggest that SelfMakeup, by using touches directly on the face, provides a better user experience to try on virtual makeup products.",,CIÊNCIA DA COMPUTAÇÃO,ALINE DE FATIMA SOARES BORGES,UNIVERSIDADE DE SÃO PAULO,23/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'human computer interaction;augmented reality;facial touch;virtual makeup',-,CARLOS HITOSHI MORIMOTO,0,interação humano computador;realidade aumentada;toque na face;maquiagem virtual,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Durante séculos, cosméticos têm sido utilizados nas mais diversas sociedades. Entretanto, quando se
trata de maquiagem facial, o processo de escolha de um produto ainda é um desafio pois se trata de um trabalho
manual que demanda tempo, além de consumir a maquiagem em si e outros materiais para aplicação
e limpeza. Esse processo manual também dificulta a experimentação de vários produtos diferentes devido a
necessidade de limpeza da pele para retirada de um produto aplicado anteriormente. Assim, um sistema de
simulação de maquiagem utilizando realidade aumentada pode facilitar esse processo, permitindo a experimentação
com a combinação de produtos e a comparação dos resultados, além de permitir experimentar os
produtos virtualmente, pela internet por exemplo. Trabalhos existente sobre esse tema permitem ao usuário
aplicar a maquiagem sobre uma foto ou mesmo vídeo do próprio usuário por meio de mouse ou toque de um
dedo sobre um monitor sensível a toques, como se o usuário aplicasse maquiagem em uma terceira pessoa.
Nessa dissertação propomos o desenvolvimento do SelfMakeup, um sistema de realidade aumentada que
permite a auto-aplicação de maquiagem virtual por meio de toques feitos diretamente na face, ao invés de
toques no monitor. A nossa hipótese é que essa forma de interação seja mais natural e forneça ao usuário
uma melhor experiência ao testar produtos virtuais de maquiagem.
O primeiro passo para viabilizar o SelfMakeup foi o desenvolvimento de um método para estimar a
posição do toque de um aplicador na face utilizando uma câmera RGBD. Realizamos testes para avaliar o
desempenho desse método e verificamos que a sua acurácia e precisão se mostrou adequada para o propósito
dessa pesquisa. Em seguida, projetamos a interface gráfica do sistema para aplicação de maquiagem virtual.
A interface permite efeitos de destaque e sombreamento que simulam os efeitos provocados pela aplicação
de produtos reais de maquiagem. Resultados de um teste piloto do nosso protótipo com 32 usuários sugerem
que o SelfMakeup, por utilizar toques diretamente na face, oferece uma melhor experiência ao usuário na
experimentação de produtos virtuais de maquiagem.",DISSERTAÇÃO,SelfMakeup: um Sistema de Realidade Aumentada para Auto-Aplicação de Maquiagem Virtual,5116607,1
"In this work, we study the multi-period two-dimensional cutting problem with usable leftover,
which consists of cutting objects to produce a set of items. There is a finite planning horizon with
a finite amount of periods between the initial and final time. The work is divided into two parts.
In the first one, we consider a deterministic version in which we know, a priori, the demand of the
items and the cost of the objects at each period. Some of the leftovers generated during the cutting
process of the demanded items in an period may be used as objects in the future. The leftovers that
can be used in the future are called usable leftovers. In general, a leftover is considered usable if
it has dimensions equal to or greater than that of some item from a predefined list for the period.
The goal is to minimize the total cost of the objects used to satisfy the demand of the entire horizon
considered. If there are solutions with the same cost, we wish to find one that, at the end of
the considered time horizon, maximizes the value of the remaining usable leftovers. We present a
mathematical model of the problem using a bilevel formulation, which is transformed into a mixed
integer linear programming model, due to the characteristics of the problem. Considering the
difficulty in solving the developed model, we propose a heuristic approach based on approximate
dynamic programming (ADP) to deal with the proposed problem. Other options based on the rolling
horizon and relax-and-fix strategies are also considered. The second part considers the scenario
where we do not know in advance the demand of the items and the cost of the objects, but we
have information about the probability distributions of both. In this case, we present an approach
based on approximate dynamic programming to estimate the best strategy to be followed at each
period. We compared the results obtained by the ADP with the results found by a greedy method.
In suitable scenarios, the results show that the ADP achieves superior solutions to the greedy method.",,CIÊNCIA DA COMPUTAÇÃO,OBERLAN CHRISTO ROMAO,UNIVERSIDADE DE SÃO PAULO,18/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'Multi-period cutting problems;usable leftover;optimization;rolling horizon;relax-andfix;approximate dynamic programming',-,ERNESTO JULIÁN GOLDBERG BIRGIN,0,Problemas de corte multiperíodo;sobras aproveitáveis;otimização;horizonte rolante;relax-and-fix;programação dinâmica aproximada,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Neste trabalho, estudamos o problema de corte bidimensional multiperíodo com sobras aproveitáveis,
que consiste em cortar objetos grandes visando a produção de um conjunto de itens menores.
Há um horizonte de planejamento finito com uma quantidade finita de períodos entre o tempo inicial
e o final. O trabalho é dividido em duas partes. Na primeira parte consideramos uma versão determinística
em que conhecemos, à priori, a demanda dos itens e o custo dos objetos a cada período.
Algumas das sobras geradas durante o processo de corte dos itens demandados em um período podem
ser utilizadas como objetos no futuro. As sobras que podem ser usadas no futuro são denominadas
sobras aproveitáveis. De forma geral, uma sobra é considerada aproveitável se possui dimensões
iguais ou superiores as de algum item de uma lista pré-definida para o período. O objetivo é minimizar
o custo total dos objetos utilizados para satisfazer a demanda de todo o horizonte considerado.
Havendo soluções com o mesmo custo, desejamos encontrar aquela que, no fim do horizonte de
tempo considerado, maximize o valor das sobras aproveitáveis remanescentes. Apresentamos uma
modelagem matemática do problema usando uma formulação em dois níveis, que é transformada em
um modelo de programação linear inteira mista, devido às características do problema. Considerando
a dificuldade em resolver o modelo desenvolvido, apresentamos uma proposta de uma abordagem
heurística baseada em programação dinâmica aproximada (PDA) para lidar com o problema proposto.
Outras opções baseadas em estratégias do tipo horizonte rolante e relax-and-fix também são consideradas.
A segunda parte considera o cenário onde não conhecemos de antemão a demanda dos itens
e o custo dos objetos, mas temos informações das distribuições de probabilidade de ambos. Nesse
caso, apresentamos uma abordagem baseada em programação dinâmica aproximada para estimar a
melhor estratégia a ser seguida em cada período. Comparamos os resultados obtidos pela PDA com
os resultados encontrados por um método guloso. Em cenários adequados, os resultados mostram
que a PDA consegue soluções superiores ao método guloso.",TESE,O problema de corte não-guilhotinado multiperíodo com sobras aproveitáveis,5117360,1
"Ontology-based data access (OBDA) has gained attention in recent years by allowing access to large
volumes of data by using an ontology as a conceptual layer and exploring its ability to describe
domains and deal with data incompleteness. This is done through mappings that connect the data
in the database to the vocabulary of the ontology. The first OBDA studies were about data stored
in relational databases, but the variety and volume of available data led to the emergence of a
new group of database management systems known as NoSQL (Not Only SQL) systems. Recent
studies have begun to extend the use of OBDA to NoSQL databases. The objective of this work is
to propose a less verbose mapping from the current literature and use it in a case study. We have
developed an OBDA system for document-oriented NoSQL databases using an access interface with
an intermediate conceptual layer that is extensible and flexible, capable of providing access to different types of database management systems. This OBDA system was evaluated in two applications.
One that uses MongoDB, document-oriented NoSQL DBMS most used today and one that uses the
relational database model.",,CIÊNCIA DA COMPUTAÇÃO,BARBARA TIEKO AGENA,UNIVERSIDADE DE SÃO PAULO,27/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'Ontology-Based Data Access;NoSQL;MongoDB;OBDA;ontology',-,RENATA WASSERMANN,0,Acesso a dados baseado em ontologias;NoSQL;MongoDB;OBDA;ontologia,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"O acesso a dados baseado em ontologias (OBDA, de Ontology-Based Data Access) tem atraído
atenção por possibilitar o acesso a grandes volumes de dados utilizando uma ontologia como camada
conceitual e explorando sua capacidade de descrever o domínio e lidar com a incompletude dos dados.
A comunicação entre ontologia e sistema de banco de dados é possível através de mapeamentos
que conectam os dados na base de dados com o vocabulário da ontologia. Os primeiros estudos
com OBDA tiveram como objetivo o acesso a dados armazenados em bancos de dados relacionais,
porém, a variedade de formatos e o volume em que atualmente são disponibilizados os dados motivaram
o surgimento de um novo grupo de sistemas gerenciadores de banco de dados conhecido
como sistemas NoSQL (Not Only SQL). Estudos recentes já começam a estender a utilização de
OBDA para bancos de dados NoSQL. O objetivo desta pesquisa é propor um mapeamento menos
verboso que o apresentado na literatura atual para bancos de dados NoSQL e seu uso em um estudo
de caso. É apresentado um sistema OBDA para sistemas gerenciadores de banco de dados NoSQL
orientado a documentos através de uma interface de acesso dotada de uma camada conceitual intermediária,
generalizável e extensível, o que torna mais flexível o acesso a diferentes tipos de sistemas
gerenciadores. Esse sistema OBDA foi avaliado em duas aplicações. Uma que utiliza o MongoDB, o
SGBD NoSQL orientado a documentos mais empregado na atualidade e outra que utiliza o modelo
de bancos de dados relacional.",DISSERTAÇÃO,Acesso a Dados Baseado em Ontologias com NoSQL,5121227,1
"Translucent materials, such as milk or marble, are characterized by their soft and smooth appearance,
as well as their bleed through effect when illuminated from behind. Diffusion based models
are currently the best approximation of the real physical process that takes place underneath the
surface of this kind of material. This process, dubbed subsurface scattering, is the one one responsible
for the blurring effect that generates that soft appearance. The success of these models is due
to the similarity between energy and light propagation. Since diffusion theory solves the problem
of energy propagation, its use returns a good approximation of subsurface scattering. Aiming to
produce a better approximation for translucent materials we developed an extension of a diffusion
based subsurface scattering model called directional multipole. In our model we extend the
directional dipole to achieve a better solution for thin slabs, i.e. shallow depths. The directional
dipole is a model that uses a diffusion theory solution for a ray in an infinite medium, which comes
closer to the representation of light rays since they have a magnitude and a direction. This is what
differentiates it from the other diffusion based models, which are based on a solution for a point
in an infinite medium. This model, however, fails to represent thin slabs because it assumes light
steps are infinitely smaller than the depth of the object. Our model solves this problem by merging
the directional dipole model with the multipole model, which was the first and only model that
addresses this issue. By doing this, we created a model that achieves a closer approximation to the
real for thin slabs than those of the multipole.",,CIÊNCIA DA COMPUTAÇÃO,DIANA ESTEFANIA NARANJO POMALAYA,UNIVERSIDADE DE SÃO PAULO,24/11/2017,INGLES,UNIVERSIDADE DE SÃO PAULO,b'Subsurface scattering;BSSRDF;light transport;diffusion theory;diffusion dipole;diffusion multipole;directional dipole',-,MARCEL PAROLIN JACKOWSKI,0,...,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,...,DISSERTAÇÃO,Directional multipole light transport model for rendering translucent materials,5121435,1
"This study objective is the computational performance improvement of credal network inference
algorithms by applying computational parallel and distributed system techniques of sparse matrix
factorization algorithms. Roughly, computational parallel techniques are used to transform systems
in systems with algorithms that can be executed concurrently. And the matrix factorization is a
group of mathematical techniques to decompose a matrix in a product of two or more matrixes. The
sparse matrixes are matrixes which have most of their values equal to zero. And credal networks
are similar to bayesian networks, which are acyclic graphs representing a joint probability through
conditional probabilities and their independence relations. Credal networks can be considered as a
bayesian network extension because of their manner of leading to uncertainty and the poor data
quality. To apply parallel techniques of sparse matrix factorization in credal network inference the
variable elimination method was used, where the credal network acyclic graph is associated to a
sparse matrix and every eliminated variable is analogous to an eliminated column.",,CIÊNCIA DA COMPUTAÇÃO,RAMON FORTES PEREIRA,UNIVERSIDADE DE SÃO PAULO,25/04/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'credal network;credal network inference;variables elimination;sparse matrix factorization',-,JULIO MICHAEL STERN,0,rede credal;inferência em redes credais;eliminação de variáveis;fatoração de matrizes esparsas,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Este estudo tem como objetivo melhorar o desempenho computacional dos algoritmos de inferência
em redes credais, aplicando técnicas de computação paralela e sistemas distribuidos em algoritmos
de fatoração de matrizes esparsas. Rusticamente, técnicas de computação paralela são técnicas para
transformar um sistema em um sistema com algoritmos que possam ser executados concorrentemente.
E a fatoração de matrizes são técnicas da matemática para decompor uma matriz em um
produto de duas ou mais matrizes. As matrizes esparsas são matrizes que possuem a maioria de
seus valores iguais a zero. E as redes credais são semelhantes as redes bayesianas, que são grafos
acíclicos que representam uma probabilidade conjunta através de probabilidades condicionais e suas
relações de independência. As redes credais podem ser consideradas como uma extensão das redes
bayesianas para lidar com incertezas ou a má qualidade dos dados. Para aplicar a técnica de paraleliza
ção de fatoração de matrizes esparsas na inferência de redes credais, a inferência utiliza-se
da técnica de eliminação de variáveis onde o grafo acíclico da rede credal é associado a uma matriz
esparsa e cada variável eliminada é análoga a eliminação de uma coluna.",DISSERTAÇÃO,Paralelização de inferência em redes credais utilizando computação distribuída para fatoração de matrizes esparsas,5121774,1
"An important indicator in the academic area is related to the degree of impact of a publication that can help in evaluating the quality and degree of internationalization in academic institutions. One approach to better understand the aforementioned indicator is analyzing the collaboration network formed by each researcher. In order to analyze this network, several alternatives use the well known relational data model which is predominant in most databases used today. Even though this model is widely used, it has a performance drawback when some types of queries are performed. For overcoming this drawback, certain alternatives are using a graph-oriented database model which is similar to a collaboration network model. However, it is unclear what parameters can be used to define when to use a relational or graph-oriented model. In this work, we propose an analysis of queries that, from the syntax of a query and the execution environment, can point to the most suitable data model for the execution given a specific query. With this query analysis, it is possible to delimit in which scenarios an integration between the relational and the graph-oriented models is more appropriate.",,CIÊNCIA DA COMPUTAÇÃO,MARINO HILARIO CATARINO,UNIVERSIDADE DE SÃO PAULO,10/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'graph database;integration;NoSQL;internationalization;collaboration networks',-,JOAO EDUARDO FERREIRA,0,banco de dados orientado a grafos;integração;NoSQL;internacionalização;redes de colaboração,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Um indicador importante na área acadêmica está relacionado ao grau de impacto de uma publicação, o que pode auxiliar na avaliação da qualidade e do grau de internacionalização de uma instituição. Para melhor delimitar esse indicador torna-se necessária a realização de uma análise das redes de colaboração dos autores envolvidos. Considerando que o modelo de dados relacional é o modelo predominante dos bancos de dados atuais, observa-se que a análise das redes de colaboração é prejudicada pelo fato desse modelo não atender, com o mesmo desempenho, a todos os tipos de consultas realizadas. Uma alternativa para executar as consultas que perdem desempenho no modelo de banco de dados relacional é a utilização do modelo de banco de dados orientado a grafos. Porém, não é claro quais parâmetros podem ser utilizados para definir quando utilizar cada um dos modelos de bancos de dados. Assim, neste trabalho é proposta uma análise de consultas que, a partir da sintaxe da consulta e do ambiente de execução, possa apontar o modelo de dados mais adequado para execução da referida consulta. Com esta análise, é possível delimitar em que cenários uma integração entre o modelo relacional e o orientado a grafos é mais adequada.",DISSERTAÇÃO,Integrando banco de dados relacional e orientado a grafos para otimizar consultas com alto grau de indireção,5121908,1
"A lot of content is produced and published every day on the Internet. Those documents are published by diﬀerent people, organizations and in many formats without any type of established standards. For this reason, relevant information about a domain of interest is spread through the Web in various portals, which hinders a broad, centralized and objective view of this information. In this context, the integration of the data scattered in the network becomes a relevant research problem, to enable smarter queries, in order to obtain richer results of meaning and closer to the user’s interest. However, such integration is not trivial, and is often costly because of the reliance on the development of specialized systems by professionals, since there are few reusable and easily integrable models. Thus, the existence of a standardized model for data integration and access to the information produced by these diﬀerent entities reduces the eﬀort in the construction of speciﬁc systems. In this work we propose an architecture based on ontologies and data mining techniques for the integration of data published on the Internet. Its use is illustrated through experimental cases for the integration of information on the Internet, showing how the use of ontologies can bring more relevant results.",,CIÊNCIA DA COMPUTAÇÃO,FELIPE LOMBARDI PIERIN,UNIVERSIDADE DE SÃO PAULO,05/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'data mining;semantic Web;data-integration',-,JAIME SIMAO SICHMAN,0,mineração de dados;Web semântica;integração de informação,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Atualmente uma grande quantidade de conteúdo é produzida e publicada todos os dias na Internet. São documentos publicados por diferentes pessoas, por diversas organizações e em inúmeros formatos sem qualquer tipo de padronização. Por esse motivo, a informação relevante sobre um mesmo domínio de interesse acaba espalhada pela Web nos diversos portais, o que diﬁculta uma visão ampla, centralizada e objetiva sobre esta informação. Nesse contexto, a integração dos dados espalhados na rede torna-se um problema de pesquisa relevante, para permitir a realização de consultas mais inteligentes, de modo a obter resultados mais ricos de signiﬁcado e mais próximos do interesse do usuário. No entanto, tal integração não é trivial, sendo por muitas vezes custosa devido à dependência do desenvolvimento de sistemas e mão de obra especializados, visto que são poucos os modelos reaproveitáveis e facilmente integráveis entre si. Assim, a existência de um modelo padronizado para a integração dos dados e para o acesso à informação produzida por essas diferentes entidades reduz o esforço na construção de sistemas especíﬁcos. Neste trabalho é proposta uma arquitetura, baseada em ontologias e técnicas de mineração de dados, para a integração de dados publicados na Internet. O seu uso é ilustrado através de casos de uso reais para a integração da informação na Internet, evidenciando como o uso de ontologias pode trazer resultados mais relevantes.",DISSERTAÇÃO,IntegraWeb: uma proposta de arquitetura baseada em mapeamentos semânticos e técnicas de mineração de dados,5147906,1
"A brain-computer interface is a communication system that generates signals from the user’s brain activity. The use of these interfaces outside the medical and bioengineering ﬁeld were conducted in the last decade and showed that is possible to use this technology for personal use, with applications in arts and entertainment. The hardware advance has allowed the utilization of these interfaces for domestic usage by creating lighter interfaces, due the number of electrodes reduction, and reducing the time needed to wear it, by using the dry electrode technology. In despite of the portability brought by the hardware simpliﬁcation, new questions emerge regarding the measured signal quality and the interface’s usability. The main goal of this work is to seek answers for these questions by performing a critical evaluation focused in the “Do it yourself” brain-computer interfaces. This evaluation will be conducted in two fronts: the evaluation of the interface itself, by analyzing the restrictions imposed by the hardware, and the investigation of applications, especially the musical ones, that can be implemented in this setup.",,CIÊNCIA DA COMPUTAÇÃO,GUILHERME FEULO DO ESPIRITO SANTO,UNIVERSIDADE DE SÃO PAULO,05/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'EEG;Brain-Computer Interface;Music Interface',-,MARCELO GOMES DE QUEIROZ,0,EEG;Interface cérebro-computador;Interface musical,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Uma interface cérebro-computador é um sistema de comunicação que gera sinais de e utilizando a atividade cerebral de um indivíduo. Na última década pesquisas visando a utilização de tais interfaces fora da área médica e de bioengenharia têm aumentado e mostrado ser possível a aplicação dessa tecnologia no desenvolvimento de interfaces para o uso pessoal, com aplicações em artes e entretenimento. A utilização destas interfaces por usuários domésticos é possibilitada pelos avanços de hardware que, ao trabalhar com um número menor de eletrodos, tornou as interfaces de aquisição mais leves e portáteis e, ao utilizar eletrodos sem a necessidade de gel condutor, reduziram o tempo de montagem . Porém, em face desta portabilidade obtida pela simpliﬁcação do hardware, são gerados novos questionamentos quanto a ﬁdelidade do sinal medido e a usabilidade dessas interfaces. O principal objetivo deste trabalho é buscar respostas para tais questionamentos realizando uma análise critica sobre interfaces cérebro-computador, focando em interfaces de baixo custo do tipo ""faça você mesmo"". Esta avaliação foi realizada em duas frentes: avaliação da interface em si, visando quantiﬁcar as limitações impostas pelo hardware, e avaliação das aplicações, em especial as musicais, passíveis de serem recriadas neste setup.",DISSERTAÇÃO,Controle em Interfaces Cérebro-Computador de Baixo Custo para Aplicações Musicais,5147981,1
"Our work in the thesis describes the implementation and experiments of a software system that obtains text from historical document images, manages a database of such documents, and performs aligment on them. The document text show OCR noise besides word variations introduced by diﬀerent editionsorcopyists,sooursoftwaresystempossess,accordingly,asetapproximatesearch algorithms bundled into an Information Retrieval engine able to cope with this kind of text. The alignment algorithms implemented in this system use a BLAST-type strategy, to align a sequence against a list of other sequences from a library of documents, so we can obtain a list of local alignments involving a reference document. Finally, our system also oﬀers the possibility of displaying a multiple alignment that expands local alignments so we can investigate diﬀerent documents as potential sources or visualize how much a reference document is covered by such documents. The system was implemented into a web server with a HTML/Javascript user interface that allows visualization of similarities and diﬀerences among diﬀerent documents.",,CIÊNCIA DA COMPUTAÇÃO,GUSTAVO ENRIQUE SALAZAR TORRES,UNIVERSIDADE DE SÃO PAULO,06/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'Image Processing;Approximate Search;Multiple Alignment;Historical Documents',-,ALAIR PEREIRA DO LAGO,0,Processamento de Imagens;Busca Aproximada;Alinhamento Múltiplo;Documentos Historicos,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"O corrente trabalho relata o projeto, implementação e testes de um sistema que se presta a extrair textos de imagens de documentos históricos,gerencia um banco de dados com tais tipos de documentos, e faz o alinhamento destes textos. Os textos obtidos são sujeitos a erros de OCR, além das variantes introduzidas por diferentes edições ou diferentes copistas,de forma que os algoritmos implementam alguma forma de busca inexata e um sistema de apoio com Recuperação de Informação dotado de busca aproximada integra o sistema. O alinhamento realizado implementa uma estratégia tipo BLAST, para alinhar uma sequência a ser analisada contra múltiplas sequências de uma biblioteca de documentos, de forma a obter listas de alinhamentos locais signiﬁcativos envolvendo o documento analisado. Ao ﬁnal, oferece-se a possibilidade de também exibir um alinhamento múltiplo que expande os diversos alinhamentos locais obtidos,de forma a estudar as eventuais origens do documento em questão,como também o quanto dele é coberto por alinhamentos locais signiﬁcativos com outros textos históricos. Foi implementado um sistema que executa num servidor web e uma interface de usuário HTML/JavaScript que facilita a visualização das semelhanças e das diferenças entre os diversos documentos.",TESE,"Um sistema de reconhecimento, busca aproximada, e alinhamento múltiplo de documentos históricos",5148205,1
"The objective of this work is to join the procces of belief revision and model checking to evaluate project speciﬁcations looking for inconsistences, using Computation Tree Logic (CTL), and revise them generating changes suggestions in the speciﬁcation. Our approach will translate the model (software speciﬁcation) and the property to be revised to classical logic. Then we will apply a SAT solver to verify the generated formula’s satsifability. From the SAT solver answer, we will create changes valid suggestions to the speciﬁcation making the translation back from classical logic to the original model. To generate the changes suggestions, we proposed a framework based on heuristics where different approaches and decisions can be implemented, aiming a better application for each project scope. We implemented a basic heuristic as an example and used it to test the implementation to analise the proposed algorithm.",,CIÊNCIA DA COMPUTAÇÃO,BRUNO VERCELINO DA HORA,UNIVERSIDADE DE SÃO PAULO,03/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'belief revision;CTL;bounded model checking',-,MARCELO FINGER,0,Revisão de Crenças;CTL;Veriﬁcação de Modelos Limitada,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Uma importante etapa do desenvolvimento de software é o de levantamento e análise dos requisitos. Porém, durante esta etapa podem ocorrer inconsistências que prejudicarão o andamento do projeto. Além disso, após ﬁnalizada a especiﬁcação, o cliente pode querer acrescentar ou modiﬁcar as funcionalidades do sistema. Tudo isso requer que a especiﬁcação do software seja revista, mas isso é altamente custoso, tornando necessário um processo automatizado para simpliﬁcar tal revisão. Para lidar com este problema, uma das abordagens utilizadas tem sido o processo de Revisão de Crenças, juntamente com o processo de Veriﬁcação de Modelos. O objetivo deste trabalho é utilizar o processo de Revisão de Crenças e Veriﬁcação de Modelos para avaliar especiﬁcações de um projeto procurando inconsistências, utilizando Computation Tree Logic (CTL), e revisá-las gerando sugestões de mudanças na especiﬁcação. A nossa proposta é traduzir para lógica clássica tanto o modelo (especiﬁcação do software) quanto a propriedade a ser revisada, e então aplicar um resolvedor SAT para veriﬁcar a satisfazibilidade da fórmula gerada. A partir da resposta do resolvedor SAT, iremos gerar sugestões válidas de mudanças para a especiﬁcação, fazendo o processo de tradução reversa da lógica clássica para o modelo original.",DISSERTAÇÃO,Revisão de Crenças no fragmento universal da CTL usando Verificação de Modelos Limitada,5148541,1
"The project of images W-operators requires the estimation of a local operator using training examples and the induction of a classifier based on machine learning to classify examples that are seldom or never seen during training.
In the last years, the area of machine learning advanced enormously due to the use of convolutional neural networks (CNN). This advance is caused mainly due to the power of representation of the neural networks and due to the fact that the convolutional neural networks are effective in the extraction of local characteristics. Consequently, they are present in many state of the art solutions for problems of computer vision [MPGC17, HGC + 17, FTM + 17, MZY + 17, CGW + 17].
In this work we study and explore the power of representation of the CNN’s in the context of images w-operators project. We integrate public implementations and very mature libraries of CNN and w-operators developed by our group (TRIOS) and test several strategies to segment gray-level images or to classify the gray-level intensity patterns observed through a window w in a few labels (in general, 2 labels, either 0 or 1).
To validate this proposal we use 2 data sets of retinal images, called DRIVE and STARE which are commonly used for vessel segmentation of the retina and also in a data set called STAFF [KFV + 13, VKFJ13], which is a variation of the database CVC-MUSCIMA [FDGL12] and has the objective of segmenting musical notes in partitures. The results have shown that, for a big window, the results are satisfactory when compared to specific state of the art solutions which use pre and post-processing.",,CIÊNCIA DA COMPUTAÇÃO,ANDRE VINICIUS LOPES,UNIVERSIDADE DE SÃO PAULO,08/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'neural networks;convolutional neural networks;deep-learning;image operators',-,ROBERTO HIRATA JUNIOR,0,redes neurais;redes neurais convolucionais;deep-learning;operadores de imagem,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"O projeto de W-operadores de imagens requer a estimação de um operador local a partir de exemplos de treinamento e da indução de um classificador baseado em aprendizado de máquina para a classificação de exemplos pouco, ou nunca, observados no treinamento.
Nos últimos anos, a área de aprendizado de máquina passou por um avanço muito grande devido às redes neurais convolucionais (CNN). Esse avanço é principalmente devido ao poder de representação das redes neurais e pelo fato das redes convolucionais serem efetivas na extração de características locais. Devido a isso, elas estão presentes em muitas soluções do estado da arte de diversos problemas de visão computacional [MPGC17, HGC + 17, FTM + 17, MZY + 17, CGW + 17].
Neste trabalho, estudamos e exploramos o poder de representação das CNNs no contexto do projeto de W-operadores de imagens. Integramos implementações públicas e bastante maduras de CNN a uma biblioteca de projeto de W-operadores desenvolvida pelo nosso grupo (TRIOS) e testamos diversas estratégias para segmentar imagens de níveis de cinza ou, ainda, classificar os padrões de intensidades em níveis de cinza observados através de
uma janela W em poucos rótulos (em geral, dois rótulos, ou 0, ou 1).
Para validar a proposta, usamos 2 conjuntos de dados de imagens de fundo de olho, chamados de DRIVE e STARE, os quais já são um padrão na área de imagens para a segmentação das veias da retina e também em um conjunto de dados chamado de STAFF [KFV + 13, VKFJ13], o qual é uma variação do banco de dados CVC-MUSCIMA [FDGL12]
e tem o objetivo de segmentar notas musicais em partituras. Os resultados obtidos mostram que, para uma janela razoavelmente grande, os resultados são satisfatórias ao se comparar com soluções específicas do estado da arte, as quais utilizam heurísticas de pré e pós-processamento.",DISSERTAÇÃO,Redes Neurais Convolucionais Aplicadas ao Projeto de Operadores de Imagens,5157683,1
...,,CIÊNCIA DA COMPUTAÇÃO,FERNANDO OMAR ALUANI,UNIVERSIDADE DE SÃO PAULO,08/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'...',-,CARLOS HITOSHI MORIMOTO,0,rastreamento do olhar;computação vestível;consumo de energia,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Cada vez mais o rastreamento do olhar tem sido usado para interação humano-computador em diversos cenários, como forma de interação (usualmente substituindo o mouse, principalmente para pessoas com de?ciências físicas) ou estudo dos padrões de atenção de uma pessoa (em situações como fazendo compras no mercado, olhando uma página na internet ou dirigindo um carro).
Ao mesmo tempo, dispositivos vestíveis tais quais pequenas telas montadas na cabeça e sensores para medir dados relativos à saúde e exercício físico realizado por um usuário, também têm avançado muito nos últimos anos, ?nalmente chegando a se tornarem acessíveis aos consumidores. Essa forma de tecnologia se caracteriza por dispositivos que o usuário usa junto de seu corpo, como uma peça de roupa ou acessório. O dispositivo e o usuário estão em constante interação e tais sistemas são feitos para melhorar a execução de uma ação pelo usuário (por exemplo dando informações sobre a ação em questão) ou facilitar a execução de várias tarefas concorrentemente. O uso de rastreadores de olhar em computação vestível permite uma nova forma de interação para tais dispositivos, possibilitando que o usuário interaja com eles enquanto usa as mãos para realizar outra ação.
Em dispositivos vestíveis, o consumo de energia é um fator importante do sistema que afeta sua utilidade e deve ser considerado em seu design. Infelizmente, rastreadores oculares atuais ignoram seu consumo e focam-se principalmente em precisão e acurácia, seguindo a ideia de que trabalhar com imagens de alta resolução e frequência maior implica em melhor desempenho. Porém tratar mais quadros por segundo ou imagens com resolução maior demandam mais poder de processamento do computador, consequentemente aumentando o gasto energético. Um dispositivo que seja mais econômico tem vários benefícios, por exemplo menor geração de calor e maior vida útil de seus componentes eletrônicos. Contudo, o maior impacto é o aumento da duração da bateria para dispositivos vestíveis. Pode-se economizar energia diminuindo resolução e frequência da câmera usada, mas os efeitos desses parâmetros na precisão e acurácia da estimação do olhar não foram investigados até o presente.
Neste trabalho propomos criar uma plataforma de testes, que permita a integração de alguns algoritmos de rastreamento de olhar disponíveis, tais como Starburst, ITU Gaze Tracker e Pupil, para estudar e comparar o impacto da variação de resolução e frequência na acurácia e precisão dos algoritmos. Por meio de um experimento com usuários analisamos o desempenho e consumo desses algoritmos sob diversos valores de resolução e frequência. Nossos resultados indicam que apenas a diminuição da resolução de 480 para 240 linhas (mantendo a proporção da imagem) já acarreta em ao menos 66% de economia de energia em alguns rastreadores sem perda signi?cativa de acurácia.",DISSERTAÇÃO,Avaliação de Desempenho de Algoritmos de Estimação do Olhar para Interação com Computadores Vestíveis,5157779,1
"The QC-MDPC McEliece scheme was considered one of the most promising public key encryption schemes for efficient post-quantum secure encryption. As a variant of the McEliece scheme, it is based on the syndrome decoding problem, an NP -hard problem from Coding Theory. The key sizes are competitive with the ones of the widely used RSA cryptosystem, and it came with an apparently strong security reduction. For three years, the scheme has not suffered major threats, until the end of 2016, when Guo, Johansson, and Stankovski presented at Asiacrypt
a reaction attack on the QC-MDPC that exploits one aspect that was not considered in the security reduction: the probability of a decoding failure to occur is lower when the secret key and the error used for encryption share certain properties, which they called spectrums. By detecting decoding failures, the attacker can obtain information on the spectrum of the secret key and then use this information to reconstruct the key. Guo et al. presented an algorithm for key reconstruction for which we can point three weaknesses. The first one is that it cannot deal efficiently with partial information on the spectrum of the secret key, resulting in the attacker having to send a great number of decoding trials. The second one is that it does not scale well for higher security levels. The third one is that the algorithm, which is based on a depth-first search, cannot be trivially parallelized. To improve the efficiency of the attack, we propose two different key reconstruction algorithms that are more efficient, use less information on the secret key, and can be trivially parallelized. The first algorithm, which is a simple probabilistic extension of Guo’s et al. algorithm, is more efficient and runs increasingly faster, for higher security levels, than the original one. However, for security levels higher than 80 bits, the probabilistic algorithm cannot run efficiently without too much information on the spectrum of the secret key, even though it needs less information than the original algorithm. The second algorithm is based on a linear relation between the blocks of the secret key. It can run efficiently with around 50% less information than needed by Guo’s et al. key reconstruction algorithm. This makes it possible for an attacker to recover the
secret key sending approximately 20% of the of the number of decoding trials needed by Guo’s et al. algorithm, for the security level of 80 bits. The performance of each presented algorithm is analyzed and compared with that of the original one. The analysis are made theoretically, considering a probabilistic analysis of the algorithms, and in practice, considering the corresponding implementations in C language.",,CIÊNCIA DA COMPUTAÇÃO,THALES ARECO BANDIERA PAIVA,UNIVERSIDADE DE SÃO PAULO,11/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'QC-MDPC;post-quantum cryptography;reaction attack',-,ROUTO TERADA,0,QC-MDPC;criptografia pós-quântica;ataque de reação,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"O QC-MDPC McEliece foi considerado um dos mais promissores esquemas criptográficos de chave pública que oferecem segurança contra ataques por computadores quânticos. Esse esquema, por ser uma variante do esquema de McEliece, é baseado no problema da decodificação por síndrome, que é um problema NP - completo e bem estudado em Teria de Códigos. O tamanho das chaves públicas do QC-MDPC McEliece é competitivo com o das chaves do RSA, e o esquema
tem uma redução de segurança aparentemente forte. Por três anos, o esquema não sofreu ataques críticos, até que na Asiacrypt de 2016 Guo, Johansson, e Stankovski mostraram um ataque de reação contra o QC-MDPC McEliece que explora um aspecto não considerado em sua redução de segurança: a probabilidade de o algoritmo de decriptação falhar é menor quando a chave secreta e o vetor usado para encriptar a mensagem compartilham certas propriedades, chamadas de espectros. Dessa forma, uma atacante pode, ao detectar falhas de decriptação, obter informação sobre
o espectro da chave secreta. Então, depois de recuperada informação suficiente, a atacante pode reconstruir a chave. Guo et al. apresentaram um algoritmo para a reconstrução da chave a partir do espectro recuperado, para o qual é possível apontar três problemas. O primeiro é que seu algoritmo não é eficiente quando o espectro da chave não foi recuperado quase completamente, o que resulta na atacante tendo que enviar um grande número de testes de decriptação à portadora da chave secreta. O segundo problema é que o desempenho de seu algoritmo não escala bem para níveis de segurança mais altos. O terceiro e último problema é que, por ser baseado numa busca em profundidade, seu algoritmo não pode ser paralelizado trivialmente. Para aumentar a eficiência do ataque, dois novos algoritmos de reconstrução são propostos neste trabalho. Estes algoritmos são mais eficientes, usam menos informação sobre a chave secreta, e podem ser paralelizados trivialmente. O primeiro algoritmo, que é uma simples extensão probabilística do algoritmo de Guo et al., tem complexidade assintótica ligeiramente melhor do que a do original, o que faz com que seu desempenho escale melhor. No entanto, para níveis de segurança mais altos do que 80 bits, o algoritmo probabilístico não é eficiente sem muita informação sobre o espectro da chave, sofrendo, similarmente ao algoritmo original, com um número muito grande de nós em sua árvore de busca. O segundo algoritmo explora uma relação linear entre os blocos da chave secreta. Este é mais eficiente, tanto assintoticamente quanto na prática, que os dois outros algoritmos, e é eficiente mesmo com em torno de 50% menos informação sobre o espectro do que o necessário para o algoritmo original. Isso permite que a atacante encontre a chave secreta fazendo apenas em torno de 20% do número de testes necessários pelo algoritmo de Guo’s et al., considerando-se o nível de segurança de 80 bits. O desempenho de ambos os algoritmos são analisados e comparados com o do algoritmo original, e as análises são feitas tanto para a complexidade teórica quanto para o desempenho na prática, considerando a implementação dos algoritmos em linguagem C.",DISSERTAÇÃO,Melhorando o ataque de reação contra o QC-MDPC McEliece,5157984,1
"Image segmentation is a problem of great relevance in computer vision, where an image is divided into relevant regions, such as to isolate an object of interest from a given application. Segmentation methods with monotonically incremental connectivity functions (MI) based on Image Foresting Transform (IFT) have achieved great success in several contexts. In interactive segmentation of images, where the user is allowed to specify the desired object, new seeds can be added and/or removed to correct the labeling until achieving the expected segmentation. This process generates a sequence of IFTs that can be calculated more efficiently by Differential Image Foresting Transform (DIFT). Recently, non-monotonically incremental connectivity functions (NMI) have been used successfully in the IFT framework in the context of image segmentation, allowing the incorporation of shape, boundary polarity and connectivity constraints, in order to customize the segmentation for a given target object. Non-monotonically incremental functions were also successfully exploited in the generation of superpixels, via sequences of IFT executions.
In this work, we present a study of the Differential Image Foresting Transform in the case of NMI functions. Our research indicates that the original DIFT algorithm presents a series of inconsistencies for non-monotonically incremental functions. This work extends the DIFT algorithm to NMI functions in directed graphs, and shows its application in the context of the generation of superpixels. Another application that is presented to spread the relevance of NMI functions is the Bandeirantes algorithm for curve tracing and boundary tracking.",,CIÊNCIA DA COMPUTAÇÃO,MARCOS ADEMIR TEJADA CONDORI,UNIVERSIDADE DE SÃO PAULO,11/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'image foresting transform;interactive segmentation;non-monotonically incremental functions;differential image foresting transform;curve tracing;boundary tracking;superpixels',-,PAULO ANDRE VECHIATTO DE MIRANDA,0,transformada imagem-floresta;segmentação interativa;funções não monotonicamente incrementais;transformada imagem-floresta diferencial;rastreamento de curvas;perseguição de bordas;superpixels,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"A segmentação de imagens é um problema de muita relevância em visão computacional, onde uma imagem é dividida em regiões relevantes, tal como para isolar objetos de interesse de uma dada aplicação. Métodos de segmentação baseados na transformada imagem-floresta (IFT, Image Foresting Transform), com funções de conexidade monotonicamente incrementais (MI) têm alcançado um grande sucesso em vários contextos. Em segmentação interativa de imagens, onde é permitido ao usuário especificar o objeto desejado, novas sementes podem ser adicionadas e/ou removidas para corrigir a rotulação até conseguir a segmentação esperada. Este processo gera uma sequência
de IFTs que podem ser calculadas de modo mais eficiente pela DIFT (Differential Image Foresting Transform). Recentemente, funções de conexidade não monotonicamente incrementais (NMI) têm sido usadas com sucesso no arcabouço da IFT no contexto de segmentação de imagens, permitindo incorporar informações de alto nível, tais como, restrições de forma, polaridade de borda e restrição de conexidade, a fim de customizar a segmentação para um dado objeto desejado. Funções não monotonicamente incrementais foram também exploradas com sucesso na geração de superpixels, via sequências de execuções da IFT.
Neste trabalho, apresentamos um estudo sobre a Transformada Imagem-Floresta Diferencial no caso de funções NMI. Nossos estudos indicam que o algoritmo da DIFT original apresenta uma série de inconsistências para funções não monotonicamente incrementais. Este trabalho estende a DIFT, visando incorporar um subconjunto das funções NMI em grafos dirigidos, e mostra sua aplicação no contexto da geração de superpixels. Outra aplicação que é apresentada para difundir a relevância das funções NMI é o algoritmo Bandeirantes para perseguição de bordas e rastreamento de curvas.",DISSERTAÇÃO,Extensão da Transformada Imagem-Floresta Diferencial para funções de conexidade com aumentos baseados na raiz e sua aplicação para geração de superpixels,5158596,1
"Angiographic images acquired by Magnetic Resonance (angio-MR) and Computed Tomography (angio-CT) are widely used tools in vascular quantification processes and into the diagnosis of cardiovascular diseases, which are considered one of the main causes of death. However, the blood vessel analyses from angiographic images is difficult because of the natural human vessel variability and the large amount of available data. Additionally, the current quantification methods usually characterize blood vessels analysis the skeleton or the complete image. That process, may turn necessary to reanalyze the image many times.
With the aim of reduce these problems and to provide an aide tool for diagnosis, in this work, we propose a textual representation model for vascular networks and an automatic vascular quantification methodology, which is performed over our representation. The representation is obtained from volume tric image segmentation of angio-MR and angio-TC, followed by the extraction of the trajectory and the blood vessel diameters. Such representation is hybrid, combining graphs and a sequence of textual instructions, and allows the extraction of morphological features, the image compression, and the reconstruction of similar images to the original ones.
Using those extracted features, comparative studies of vascular architecture were performed, it was made using synthetic and real images, in which was possible to find structural differences that make feasible to haracterize the aneurysm of an individual. Also, a vessel similarity identification method was developed, which in turn makes possible the vessel segments recognition and labeling in a set of vessel networks. The proposed methodology should aid in the developing of blood vessel classification process, to aid in the automatic diagnosis of cardiovascular diseases, and to allow the future development of methods and applications that could be used int the clinical practice.",,CIÊNCIA DA COMPUTAÇÃO,MIGUEL ANGEL GALARRETA VALVERDE,UNIVERSIDADE DE SÃO PAULO,12/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'angiography;quantification;characterization;vascular morphology',-,MARCEL PAROLIN JACKOWSKI,0,angiografia;quantificação;caracterização;morfologia vascular,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"As imagens de Angiografia por Ressonância Magnética (angio-RM) e Tomografia Computadorizada (angio-TC) são ferramentas amplamente usadas em processos de quantificação vascular e no diagnóstico de doenças ardiovasculares. As quais, são consideradas entre as principais causas de morte. Contudo, a análise dos vasos em larga escala a partir das imagens é dificultada, tanto pela variabilidade natural dos vasos no corpo humano, quanto pela grande quantidade de dados disponíveis. Além disso, os métodos de quantificação existentes, usualmente extraem características dos vasos sanguíneos a partir de seus esqueletos, ou até mesmo das próprias imagens de angiografia, razão pela qual tais métodos podem fazer necessária a reanalise das imagens repetidas vezes.
Com o intuito de facilitar a análise e de fornecer uma ferramenta de apoio ao diagnóstico, neste trabalho são apresentados um modelo de representação textual de redes vasculares e uma metodologia de quantificação vascular automática, que é feita a partir dessa representação. A representação é obtida a partir da segmentação de imagens volumétricas de angio-RM e angio-TC, seguida da extração de trajetórias e diâmetros de redes vasculares. Tal representação é híbrida, combinando grafos e uma sequência textual de instruções, e permite não apenas a extração de caraterísticas morfológicas da rede vascular, como também a compressão das imagens, e ainda a reconstrução de imagens similares às imagens originais.
A partir das características extraídas, realizamos estudos comparativos entre arquiteturas vasculares, o que é feito tanto por meio do uso de imagens sintéticas, como por meio de imagens reais, imagens nas quais foi possível encontrar diferenças entre arquiteturas, além de viabilizar a caracterização de aneurismas em um individuo. Paralelamente, desenvolvemos um método que permite identificar similaridade entre segmentos vasculares, o que por sua vez possibilita o reconhecimento e rotulação de segmentos em um conjunto de redes vasculares.
A metodologia por nós desenvolvida deve também auxiliar no desenvolvimento de processos de classificação de vasos sanguíneos, de ferramentas para o diagnóstico automático de doenças vasculares, e para a melhora de técnicas utilizadas na prática clínica.",TESE,Representação e Quantificação de Redes Vasculares a Partir de Imagens de Angiografia Tridimensional,5165868,1
"Ontology-based data access (OBDA) is a data access method that connects database entities to the vocabulary of an ontology, using it as a conceptual layer and exploring its ability to describe domains and deal with data incompleteness. This is done through mappings that connect the database entities to the vocabulary of the ontology. OBDA studies were about data stored in relational databases, but the exponential growth of the volume and the velocity of data nowadays and the development of NoSQL databases has brought the need to extend the use of OBDA to NoSQL databases. In this paper, we present a novel approach for OBDA with document-oriented
NoSQL databases. Our approach uses an access interface with an intermediate conceptual layer that is extensible and flexible, capable of providing access to different types of database management systems. To validate the approach, we have implemented a prototype for MongoDB, using a real-world application domain as a case study to access data and generate reports for Plataforma Lattes, a scientific collaboration network.",,CIÊNCIA DA COMPUTAÇÃO,THIAGO HENRIQUE DIAS ARAUJO,UNIVERSIDADE DE SÃO PAULO,27/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'ontology-based data access;NoSQL;ontology;Plataforma Lattes',-,RENATA WASSERMANN,0,acesso a dados baseado em ontologias;NoSQL;ontologia;Plataforma Lattes,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Acesso a Dados Baseado em Ontologias é uma forma de acesso a dados que utiliza uma ontologia como camada conceitual, uma base de dados para persistência, e um método de tradução de consultas utilizando um mapeamento que conecta os dados armazenados no banco de dados com o vocabulário da ontologia. A vantagem dessa abordagem é sua capacidade de descrever e representar domínios de maneira coesa, lidar com inconsistências e acessar diferentes tipos de informação de maneira padronizada. Os trabalhos nessa área se concentraram em bases de dados relacionais, mas com o aumento do volume de dados disponível e com o surgimento dos grupos de bancos de dados
NoSQL, foi necessário adaptar a técnica para dar suporte aos bancos de dados NoSQL. Entretanto, essas propostas possuem algumas limitações na forma de construção do mapeamento e na flexibilidade da solução. Propomos um novo método de acesso OBDA utilizando uma camada conceitual intermediária extensível e capaz de prover acesso a diferentes tipos de Sistemas de Gerenciamento de Banco de Dados NoSQL, e validamos nossa proposta através da criação de uma aplicação em um domínio real.",DISSERTAÇÃO,OntoMongo - Um Método de Acesso a Dados da Plataforma Lattes Baseado em Ontologias e Sistemas NoSQL,5167265,1
"Geometrical image alignment, commonly known as image registration, is an essential step in a number of image processing applications, ranging from populational studies in medical imaging to map reconstruction based on satellite images. Non-rigid deformations represent a big slice of all possible deformations in images that need to be registered. Presently, studies that involve thousands of images are becoming commonplace, suggesting that acceleration techniques should be developed to diminish the analysis or processing time. One of the most used methods for solving non-rigid registration is the Thin Plate Splines (TPS). Non-rigid registration techniques, TPS included, are usually very computational demanding, hence the use of graphic processing units (GPUs) creates
a possibility to speedup several stages of the registration process, such as extraction of features, etermination of feature correlation and calculaton of the deformation function. This project targets the parallelization of the Thin Plate Splines in modern GPU architecture and evaluate their efficiency.",,CIÊNCIA DA COMPUTAÇÃO,THIAGO DE GOUVEIA NUNES,UNIVERSIDADE DE SÃO PAULO,14/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'Image Registration;Thin Plate Splines;Graphic Processing Units',-,MARCEL PAROLIN JACKOWSKI,0,Registro de Imagens;Thin Plate Splines;Graphic Processing Units,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"O alinhamento geométrico, comumente conhecido como registro de imagens, é uma etapa essencial em várias aplicações na área de processamento de imagens, que incluem desde análises populacionais na área médica até reconstrução de mapas utilizando imagens de satélite. Deformações não rígidas representam uma ampla gama das possíveis deformações presentes nas várias categorias de imagens que são registradas. Atualmente, estudos que envolvem milhares de imagens estão se tornando cada vez mais comuns, sugerindo que técnicas de aceleração sejam desenvolvidas de forma a minimizar o tempo de análise e processamento. Um dos métodos mais usados para resolver casos de registro não-rigido é o Thin Plate Splines (TPS). Como as técnicas de registro não-rígido, o TPS incluso, são tradicionalmente custosas computacionalmente, o uso de graphic processing units (GPUs) possibilita a aceleração das diversas etapas do processo via hardware, tais como a extração de características, determinação de correspondências e cálculo das funções de deformação. Este trabalho tem como objetivo a paralelização do TPS aplicado à arquiteturas modernas de GPU, e avaliação de sua eficiência.",DISSERTAÇÃO,Aceleração de Registro Não-Rígido de Imagens Tomográficas utilizando GPU,5169787,1
"Organizations seeking to support their business processes in a more flexible way are increasingly
interested in replacing existing information systems centered on data with process-aware informa-
tion systems (PAISs), in which the control-flow logic of processes is specified in executable models
separate from the application code. Even though traditional process modeling approaches have been
largely used to build enterprise PAISs, most lack the flexibility needed to: perform business process
reengineering; manage dependencies among interacting, parallel processes; and keep the code base
for handling exceptions and unforeseen process states small and manageable. WED-flow, in turn, is
a transactional, event-based, and data-driven process modeling approach that addresses these chal-
lenges. However, WED-flow cannot model the temporal behavior and constraints of time-critical
processes found in real-time systems. For most real-time systems, failing to meet time constraints
could result in catastrophic damage. Hence, the ability to decide whether or not a time-critical
process can meet its deadlines is essential to at least alleviate potentially hazardous side effects.
Moreover, if time-critical processes compete for shared resources, early detecting that some of them
are likely to miss their deadlines could increase the efficiency of the resource allocation policy in use.
The main contributions of this work are: (1) to present Time WED-flow, extending WED-flow with
the notion of time; (2) to present a method for mapping a WED-flow process model to a Petri net –
a graphical and mathematical formalism for modeling and reasoning about the functional behavior
of concurrent systems; (3) and to present a time dependent Petri net model suitable for describing
the temporal semantics of Time WED-flow process models. Thus, we can check the control-flow
logic of WED-flow process models through their equivalent Petri nets, and check their timeliness
as well. As an example of a real-time system modeled using WED-flow, we present SISAUT – an
autopsy management system that coordinates interacting, parallel, time-critical processes to collect
and process organs for research projects.",,CIÊNCIA DA COMPUTAÇÃO,RODRIGO ALVES LIMA,UNIVERSIDADE DE SÃO PAULO,18/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'WED-flow;Time WED-flow;real-time systems;Petri net;time dependent Petri net',-,JOAO EDUARDO FERREIRA,0,WED-flow;WED-flow Temporal;sistemas de tempo real;rede de Petri;rede de Petri dependente de tempo,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Organizações que buscam apoiar seus processos de negócio de forma mais flexível estão cada vez mais
interessadas em substituir os sistemas de informação existentes centrados em dados por sistemas
de informação cientes de processo (PAISs), nos quais a lógica de controle de fluxo dos processos é
especificada em modelos executáveis separados do código da aplicação. Ainda que as abordagens
tradicionais de modelagem de processos sejam amplamente utilizadas para o desenvolvimento de
PAISs empresariais, muitas não possuem a flexibilidade necessária para: realizar reengenharia de
processos de negócios; gerenciar dependências entre processos interativos e paralelos; e manter a
base de código para tratar exceções e estados de processo imprevistos pequena e gerenciável. WED-
flow, por sua vez, é uma abordagem de modelagem de processos transacional, baseada em eventos,
e orientada a dados que atende a esses desafios. No entanto, WED-flow não pode modelar o com-
portamento e as restrições temporais dos processos sensíveis ao tempo encontrados em sistemas de
tempo real. Para a maioria dos sistemas de tempo real, o não cumprimento de restrições de tempo
pode resultar em catástrofes. Assim, a capacidade de decidir se um processo sensível ao tempo pode
ou não cumprir seus prazos é essencial para ao menos aliviar seus efeitos colaterais potencialmente
perigosos. Além disso, se processos sensíveis ao tempo competirem por recursos compartilhados, a
detecção antecipada de que alguns deles não atenderão aos seus prazos pode aumentar a eficiên-
cia da política de alocação de recursos em uso. As principais contribuições deste trabalho são: (1)
apresentar o modelo WED-flow Temporal, estendendo WED-flow com uma noção de tempo; (2)
apresentar um método para mapear um modelo de processo WED-flow para uma rede de Petri – um
formalismo gráfico e matemático para modelagem e análise de sistemas concorrentes; (3) e apresen-
tar um modelo de rede Petri dependente de tempo adequado para descrever a semântica temporal
dos modelos de processo de WED-flow Temporal. Assim, podemos verificar a lógica de controle de
fluxo dos modelos de processo WED-flow através de suas redes de Petri equivalentes, assim como
verificar sua pontualidade. Como um exemplo de um sistema de tempo real modelado usando WED-
flow, apresentamos SISAUT – um sistema de gerenciamento de autópsias que coordena processos
interativos, paralelos e sensíveis ao tempo para coletar e processar órgãos para projetos de pesquisa.",DISSERTAÇÃO,Modeling of Time WED-flow,5727251,1
"Real world problemas in areas such as machine learning are known for the huge number of deci-
sion variables (> 10 6 ) and data volume. For such problems working with second order derivatives
is prohibitive. These problems have properties that benefits the application of coordinate des-
cent/minimization methods. These kind of methods are defined by the change of a single, or small
number of, decision variable at each iteration. In the literature, the commonly found description
of this type of method is based on the cyclic change of variables. Recent papers have shown that
randomized versions of this method have better convergence properties. This version is based on
the change of a single variable chosen randomly at each iteration, based on a fixed, but not neces-
sarily uniform, distribution. In this work we present some theoretical aspects of such methods, but
we focus on practical aspects.",,-,LUIZ GUSTAVO DE MOURA DOS SANTOS,UNIVERSIDADE DE SÃO PAULO,22/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'Convex optimization;coordinate minimization;random coordinate descent;pagerank;support vector machine',-,ERNESTO JULIÁN GOLDBERG BIRGIN,0,Otimizaçãoo convexa;busca em coordenada;busca em coordenada aleatória;pagerank;máquina de suporte vetorial,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"Problemas reais em áreas como aprendizado de máquina tˆ em chamado aten¸ c˜ ao pela enorme quan-
tidade de variáveis (> 10 6 ) e volume de dados. Em problemas dessa escala o custo para se obter e
trabalhar com informa¸ c˜ oes de segunda ordem s˜ ao proibitivos. Esse problemas apresentam carac-
ter´ısticas que podem ser aproveitadas por m´ etodos de busca em coordenada. Essa classe de m´ etodos
´ e caracterizada pela altera¸ c˜ ao de apenas uma ou poucas variáveis a cada itera¸ c˜ ao. A variante do
m´ etodo comumente descrita na literatura ´ e a minimiza¸ c˜ ao c´ıclica de variáveis. Por´ em, resultados
recentes sugerem que variantes aleatórias do m´ etodo possuem melhores garantias de convergˆ encia.
Nessa variante, a cada itera¸ c˜ ao, a variável a ser alterada ´ e sorteada com uma probabilidade prees-
tabelecida n˜ ao necessariamente uniforme.
Neste trabalho estudamos algumas varia¸ c˜ oes do m´ etodo de busca em coordenada. S˜ ao apre-
sentados aspectos teóricos desses m´ etodos, por´ em focamos nos aspectos práticos de implementa¸ c˜ ao
e na compara¸ c˜ ao experimental entre varia¸ c˜ oes do m´ etodo de busca em coordenada aplicados a
diferentes problemas com aplica¸ c˜ oes reais.",DISSERTAÇÃO,Métodos de Busca em Coordenada,6100107,1
...,,-,JEFFERSON SERAFIM ASCANEO,UNIVERSIDADE DE SÃO PAULO,15/03/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO,b'...',-,ROBERTO HIRATA JUNIOR,0,processamento de imagens;holografia;plâncton;visão computacional;processamento de sinais digitais,CIÊNCIA DA COMPUTAÇÃO (33002010176P0),-,"O estudo de plâncton é importante para estimar a biodiversidade em ambientes marinhos,
auxiliando na compreensão de sua dinâmica e do impacto da atividade humana. Neste trabalho, es-
tudamos as etapas necessárias para a observação de organismos planctônicos utilizando um sistema
holográfico digital. A holografia digital permite que as imagens sejam numericamente focalizadas
após sua aquisição através de algoritmos de reconstrução holográfica, o que remove a limitação da
baixa profundidade de foco presente em métodos mais tradicionais, e permite a observação de um
volume maior de água. É feita uma análise das limitações teóricas deste sistema holográfico em ter-
mos das frequências presentes no holograma e na sua reconstrução, observando-se as consequências
para a escolha e implementação dos algoritmos de reconstrução. Também são considerados alguns
detalhes de implementação que decorrem da natureza finita e discreta dos algoritmos. No proces-
samento dos hologramas, os organismos são automaticamente localizados segmentando-se regiões
de interesse por meio de uma reconstrução volumétrica. Estas regiões são reconstruídas a diversas
distâncias e é feita a escolha de um plano focal utilizando uma medida de foco. Diversas medidas
são comparadas em imagens obtidas experimentalmente para determinar a mais adequada na fo-
calização de organismos planctônicos. Por fim, descrevemos a aplicação web desenvolvida para o
processamento distribuído de imagens holográficas digitais de plâncton.",DISSERTAÇÃO,Processamento de imagens de holografia digital para o estudo de organismos planctônicos,6100583,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,4991900,
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5048500,
"Software architecture and software testing are important areas with the common goal of providing
the means to develop high-quality software systems. It was found through a systematic mapping
that there are few studies relating these two areas so that one can contribute to the other, especially
in reference architectures and software testing. This project aims at extending the Reference
Architecture Model (RAModel), so that reference architectures are established with software
test information. Thus, in addition to promoting standardization, standardization, and reuse
of information related to software architectures, it is also promoted the reuse, standardization,
and standardization of software test information in the development of solutions for a given
application domain. For this, test elements related to the test planning were extracted from the
TMMi and the ISO/IEC/IEEE 29119 standard and inserted into the RAModel, thus defining
the RAModelT . In this way, the test activity is incorporated into the early stages of software
development. A feasibility study performed to evaluate the extent of the RAModelT and an
application example are presented.",Versão revisada - Nilton Mendes de Souza.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,NILTON MENDES DE SOUZA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),12/09/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Software Test;Software Architecture;Reference Arhitecture Model',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,JOSE CARLOS MALDONADO,165,Teste de Software;Arquitetura de Software;Modelo para Arquiteturas de Referência,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Processos, Métodos, Tecnologias e Ferramentas para o Reuso de Software","Arquitetura de software e teste de software são importantes áreas com o objetivo comum de
prover meios para que sistemas de software sejam desenvolvidos com alta qualidade. Foi constatado,
por meio de um mapeamento sistemático, que existem poucos estudos relacionando essas
duas áreas de modo que uma possa contribuir para a outra, principalmente em arquiteturas de
referência e teste de software. Esse projeto tem por objetivo estender o modelo para arquiteturas
de referência (RAModel), para que arquiteturas de referência sejam estabelecidas com
informações de teste de software. Assim, além de promover padronização, uniformização e
reúso a informações relacionadas a arquiteturas de software, promove-se também o reúso, a
padronização e a uniformização das informações de teste de software no desenvolvimento das
soluções para um dado domínio de aplicação. Para isso, elementos de teste relacionados ao
planejamento de teste foram extraídos do TMMi e da norma ISO/IEC/IEEE 29119 e inseridos
no RAModel, definindo, assim, o RAModelT . Dessa forma, a atividade de teste é incorporada
aos estágios iniciais do desenvolvimento de software. Um estudo de viabilidade realizado para
avaliar a extensão do RAModelT e um exemplo de aplicação são apresentados.",DISSERTAÇÃO,RAModelTI: agregando informações de teste de software no Modelo para Arquiteturas de Referência (RAModel),5403300,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,4991852,
"Eucalyptus have played an important economic roll worldwide. It has rapid growth, high yield,
wide species diversity, great adaptability and it is used in different industrial processes such as
cellulose and paper production. However, Eucalyptus are susceptible to diseases and plagues,
which could bring heavy damage to the plantations. In view of the importance of the production
of Eucalyptus, the dimensions of the planted areas and of the possibility to increase yield, there is
a need to detect and identify the pathologies affecting the trees. Since Eucalyptus plantations can
cover a very extensive area, the use of UAVs (Unmanned Aerial Vehicles) can drastically speed
up the process of monitoring the crop, since they can survey very large areas in very little time.
Thus, this project aims to develop a system that automatically diagnoses eucalyptus diseases.
Using techniques of digital intrusion detection, the diagnostic is made by comparing the spectral
signature of diseased plants with known signatures stored in a database following a signature
model also proposed in this project. The system was developed and validated by using data from
spetroradiometers, showing a accuracies as high as 96% in some cases.",,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,ARTHUR AVELAR CHAVES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),23/02/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'UAVs;ADDSIGN;Diagnose;Pathologies;Eucaliptus',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,KALINKA REGINA LUCAS JAQUIE CASTELO BRANCO,98,VANTs;ADDSIGN;Diagnóstico;Patologias;Eucalipto,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Arquiteturas de Referência para Sistemas Embarcados,"Apresentando um papel de destaque no cenário nacional e internacional, o eucalipto possui
rápido crescimento, alta produtividade, ampla diversidade de espécies, grande capacidade de
adaptação e é aplicado em diferentes processos industriais, como por exemplo, produção de
madeira, celulose e papel. No Brasil existem extensas áreas plantadas, principalmente nos
estados de Minas Gerais, São Paulo e Paraná. Entretanto, eucaliptos são suscetíveis a doenças
e pragas, o que pode trazer grandes prejuízos aos produtores. Tendo em vista esse contexto,
surge a necessidade de detectar e diagnosticar doenças prematuramente, permitindo um combate
mais eficaz e preciso a essas patologias. Visto que as plantações de eucalipto cobrem áreas
muito extensas, o uso de VANTs (Veículos Aéreos Não-Tripulados) pode agilizar o processo
de monitoramento, uma vez que podem sobrevoar grandes distâncias em pouco tempo. Sendo
assim, esse trabalho desenvolveu um sistema de diagnóstico automático de doenças de eucalipto.
Baseando-se em técnicas de detecção de ataques digitais, o diagnóstico é feito comparando
assinaturas espectrais de plantas doentes com assinaturas conhecidas armazenadas em uma base
de dados seguindo um modelo de assinaturas espectrais inspirado em um modelo de assinaturas
de ataque. O sistema foi desenvolvido e validado utilizando dados de espectroradiômetros,
apresentando precisão de até 96% em alguns casos.",DISSERTAÇÃO,Uso de assinaturas espectrais e veículos aéreos não tripulados para o diagnóstico automático de doenças de eucaliptos,4991997,1
"Systems-of-Systems (SoS) have performed an important and even
essential role to the whole society and refer to complex softwareintensive
systems, resulted from interoperability of independent constituent
systems that work together to achieve more complex missions.
SoS have emerged specially in critical application domains
and, therefore, high level of quality must be assured during their development
and evolution. However, dealing with quality of SoS still
presents great challenges, as SoS present a set of unique characteristics
that can directly affect the quality of such systems. Moreover,
there are not comprehensive models that can support the quality
evaluation of SoS. Motivated by this scenario, the main contribution
of this Master’s project is to present a SoS Evaluation Model, more
specifically, addressing the crisis/emergency management domain,
built in the context of a large international research project. The
proposed model covers important evaluation activities and considers
all SoS characteristics and challenges not usually addressed by other
models. This model was applied to evaluate a crisis/emergency management
SoS and our results have shown it viability to the effective
management of the SoS quality.",Versão de Defesa - Daniel Soares Santos.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,DANIEL SOARES SANTOS,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),13/03/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Software Quality;Evaluation Model;Crisis and Emergency Management;Systems-of-Systems',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ELISA YUMI NAKAGAWA,118,Qualidade de Software;Modelo de Avaliação;Gerenciamento de Crises e Emergências;Sistemas-de-Sistemas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),-,"Sistemas-de-Sistemas (SoS, do inglês Systems-of-Systems) realizam
um importante e até essencial papel na sociedade. Referem-se a complexos
sistemas intensivos em software, resultado da interoperabilidade
de sistemas constituintes independentes que trabalham juntos
para realizar missões mais complexas. SoS têm emergido especialmente
em domínios de aplicação crítica, portanto, um alto nível de
qualidade deve ser garantido durante seu desenvolvimento e evolução.
Entretanto, lidar com qualidade em SoS ainda apresenta grandes desafios,
uma vez que possuem um conjunto de características únicas
que podem diretamente afetar a qualidade desses sistemas. Além
disso, não existem modelos abrangentes para o suporte à avaliação de
qualidade de SoS. Motivado por este cenário, a principal contribuição
deste projeto de mestrado é apresentar um modelo de avaliação para
SoS, especialmente destinado ao domínio de gerenciamento de crises
e emergências. Este modelo foi construído no contexto de um grande
projeto de pesquisa internacional, e cobre as mais importantes atividades
de avaliação, considerando as principais características e desafios
de SoS geralmente não abordados por outros modelos. Este
modelo foi aplicado na avaliação de um SoS de gerenciamento de
crises e emergência, e nossos resultados têm mostrado sua viabilidade
para o efetivo gerenciamento da qualidade de SoS.",DISSERTAÇÃO,Modelo de avaliação de qualidade para Sistemas-de-Sistemas de gerenciamento de crises e emergências,4993133,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5002197,
"Visualization methods make use of interactive graphical representations embedded on a display
area in order to enable data exploration and analysis. These typically rely on geometric primitives
for representing data or building more sophisticated representations to assist the visual analysis
process. One of the most challenging tasks in this context is to determinate an optimal layout
of these primitives which turns out to be effective and informative. Existing algorithms for
building layouts from geometric primitives are typically designed to cope with requirements
such as orthogonal alignment, overlap removal, optimal area usage, hierarchical organization,
dynamic update among others. However, most techniques are able to tackle just a few of those
requirements simultaneously, impairing their use and flexibility. In this dissertation, we propose
a set of approaches for building layouts from geometric primitives that concurrently addresses
a wider range of requirements. Relying on multidimensional projection and optimization
formulations, our methods arrange geometric objects in the visual space so as to generate
well-structured layouts that preserve the semantic relation among objects while still making an
efficient use of display area. A comprehensive set of quantitative comparisons against existing
methods for layout generation and applications on text, image, and video data set visualization
prove the effectiveness of our approaches.",Versão de defesa - Erick Mauricio Gomez Nieto .pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,ERICK MAURICIO GOMEZ NIETO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),24/02/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Overlap Removal;Similarity Preserving;Structured Layouts;Area Optimization;Semantic Layout;High-Dimensional Data',-,LUIS GUSTAVO NONATO,129,Remoção de sobreposição;Preservação da similaridade;Layouts Estruturados;Otimização de Área;Layout Semântico;Dados em Alta Dimensão,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Métodos Espectrais e Topológicos para o Processamento, Modelagem e Visualização a partir de Dados Massivos","Métodos de visualização fazem uso de representações gráficas interativas embutidas em uma área
de exibição para exploração e análise de dados. Esses recursos visuais usam primitivas geométricas
para representar dados ou compor representações mais sofisticadas que facilitem a extração
visual de informações. Uma das tarefas mais desafiadoras é determinar um layout ótimo visando
explorar suas capacidades para transmitir informação dentro de uma determinada visualização.
Os algoritmos existentes para construir layouts a partir de primitivas geométricas são tipicamente
projetados para lidar com requisitos como alinhamento ortogonal, remoção de sobreposição,
área usada, organização hierárquica, atualização dinâmica entre outros. No entanto, a maioria
das técnicas são capazes de lidar com apenas alguns desses requerimentos simultaneamente,
prejudicando sua utilização e flexibilidade. Nesta tese, propomos um conjunto de abordagens
para construir layouts a partir de primitivas geométricas que simultaneamente lidam com uma
gama mais ampla de requerimentos. Baseando-se em projeções multidimensionais e formulações
de otimização, os nossos métodos organizam objetos geométricos no espaço visual para gerar
layouts bem estruturados que preservam a relação semântica entre objetos enquanto ainda fazem
um uso eficiente da área de exibição. Um conjunto detalhado de comparações quantitativas com
métodos existentes para a geração de layouts e aplicações em visualização de conjunto de dados
de texto, imagem e vídeo comprova a eficácia das técnicas propostas.",TESE,Geração de layouts semânticos para a visualização interativa de dados multidimensionais,5002214,01
"Context: Accurate decision-making requires updated and precise information to establish
the reality of an overall situation. New data sources (e.g., wearable technologies) have been
increasing the amount of available and useful data, which is now called “big data”. This has
a great potential for transforming the entire business process and improving the accuracy of
decisions. In this context, disaster management represents an interesting scenario that relies
on “big data” to enhance decision-making. This is because it must cope with data provided
not only by traditional sources (e.g., stationary sensors) but also by emerging sources - for
instance, information shared by local volunteers, i.e., volunteered geographic information (VGI).
When combined, these data sources can be regarded as large in volume, with different velocities,
and a variety of formats. Furthermore, an analysis is required to confirm their veracity is
required since these data sources are disconnected and prone to various errors. These are the
“4Vs” that characterize “big data”. Gap: However, although all these data open up further
opportunities, their huge volume, together with an inappropriate data integration and unsuitable
visualization, can result in information being overlooked by decision-makers. This problem
arises because the integration of the available data is hampered by the intrinsic heterogeneity
of their features (e.g., their occurrence in different formats). When integrated, this information
also often fails to reach the decision-makers in a suitable way (e.g., in appropriate visualization
formats). Moreover, there is not a clear understanding of the decision-makers’ needs or how the
available data can meet these needs. Objective: In light of this, this thesis presents an approach
for improving decision-making with heterogeneous geospatial big data based on spatial decision
support systems and volunteered geographic information in disaster management. Methods:
Systematic mapping studies were conducted to identify gaps in research studies with regard to
the use of volunteered information and spatial decision support systems in disaster management.
On the basis of these studies, two design science projects were carried out. The first of these
aimed at defining the elements that are essential for ensuring the integration of heterogeneous
data, whereas the second project aimed at obtaining a better understanding of decision-makers’
needs. A cross-organizational action research project was also conducted to define the design
principles that should be observed for a spatial decision support system to effectively support
decision-making with heterogeneous geospatial big data. A series of empirical case studies
was undertaken to evaluate the outcomes of these projects. Results: The overall approach
thus consists of the three significant outcomes that were derived from these projects. The first
outcome was the conceptual architecture that defines the integration of heterogeneous data
sources. The second outcome was a model-based framework that describes the connection of
decision-making with appropriate data sources. The third outcome is based on the framework
and comprises a set of design principles for guiding the development of spatial decision support
systems for decision-making with heterogeneous geospatial big data. Conclusion: This thesis
has made a useful contribution to both practice and research. In short, it defines ways of
integrating heterogeneous data sources, provides a better understanding of decision-makers’
needs, and supports the development of a spatial decision support system to effectively assist
decision-making with heterogeneous geospatial big data.",,-,FLAVIO EDUARDO AOKI HORITA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),10/03/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Decision-making;Heterogeneous Geospatial Data;Spatial Decision Support Systems;Volunteered Geographic Information;Disaster Management',-,JOAO PORTO DE ALBUQUERQUE PEREIRA,208,Tomada de Decisão;Dados Espaciais Heterogêneos;Sistemas de Suporte à Decisão Espacial;Informações Geográficas Voluntárias;Gestão de Desastres,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),-,"Contexto: Uma tomada de decisão precisa exige informações mais precisas e atualizadas para
estabelecer a realidade da situação geral. Novas fontes de dados (e.g, tecnologias vestíveis) tem
aumentado a quantidade de dados úteis disponíveis, que agora é chamado de “big data”. Isso
tem grande potencial para transformar todo o processo de negócio e melhorar a precisão na
tomada de decisão. Neste contexto, a gestão de desastres representa um interessante cenário que
depende de “big data” para aprimorar a tomada de decisão. Isso porque, ela tem que lidar com
dados fornecidos não apenas por fontes tradicionais (e.g., sensores estáticos), mas também por
fontes emergentes – por exemplo, informações compartilhadas por voluntários locais, i.e., as
informações geográficas de voluntários (VGI). Quando combinadas, estas fontes de dados podem
ser consideradas grandes em volume, com diferentes velocidades e uma variedade de formatos.
Além disso, uma análise com relação à sua veracidade é necessaria uma vez que estas fontes de
dados são desconectadas e propensas à erros. Estes são os “4Vs” que caracterizam “big data”.
Problema: No entanto, embora todos estes dados abrem novas oportunidades, seu grande volume
em conjunto com uma integração inapropriada e uma visualização inadequada, podem tornar
as informações ignoradas por tomadores de decisão. Isso ocorre, pois, a integração dos dados
disponíveis torna-se complicada devido a heterogeneidade intrínseca nas suas características (e.g.,
dados em formatos diferentes). Quando integradas, estas informações frequentemente também
não chegam aos tomadores de decisão em uma condição apropriada (por exemplo, no formato de
visualização adequado). Além disso, não existe uma clara compreensão sobre as necessidades
dos tomadores de decisão ou sobre como os dados disponíveis podem ser usados para atender
essas necessidades. Objetivo: Dessa forma, esta tese de doutorado apresenta uma abordagem
para melhorar a tomada de decisões com grande volume de dados espaciais heterogêneos baseada
em sistemas de suporte à decisão espacial e informações geográficas de voluntários na gestão
de desastres. Métodos: Mapeamentos sistemáticos foram conduzidos para identificar lacunas
de pesquisa no uso de dados voluntários e sistemas de suporte à decisão na gestão de desastres.
Com base nestes estudos, dois projetos de design science foram conduzidos. O primeiro deles
buscou definir elementos essências para entender a integração de dados heterogêneos, enquanto
o segundo projeto buscou fornecer um melhor entendimento das necessidades dos tomadores
de decisão. Também foi conduzido um projeto de pesquisa-ação interinstitucional para definir
princípios de projeto que deveriam ser observados para um sistema de suporte à decisão espacial
ser efetivo no apoio a tomada de decisão com grande volume de dados espaciais heterogêneos.
Uma série de estudos de caso empíricos foram conduzidos para avaliar os resultados destes
projetos. Resultados: A abordagem geral então é composta pelos três resultados significantes
que foram derivados destes projetos. Em primeiro lugar, uma arquitetura conceitual que especifica
a integração de fontes de dados heterogêneas. O segundo elemento é uma estrutura baseada
em modelo que descreve a conexão entre a tomada de decisão com as fontes de dados mais
adequadas. Com base nesta estrutura, o terceiro elemento consiste em um conjunto de princípios
de design que guiam o desenvolvimento de um sistema de suporte à decisão espacial para tomada
de decisão com grande volume de dados espaciais heterogêneos. Conclusão: Esta tese de
doutorado realizou importantes contribuições para a prática e pesquisa. Em resumo, ela define
formas para integrar fontes de dados heterogêneos, fornece uma melhor compreensão sobre as
necessidades dos tomadores de decisão e ajuda no desenvolvimento de sistemas de suporte à
decisão espacial para tomada de decisão com grande volume de dados espaciais heterogêneos.",TESE,Uma abordagem para melhorar a tomada de decisão com grande volume de dados espaciais heterogêneos: Uma aplicação usando sistemas de suporte à decisão espacial e informações geográficas voluntárias na gestão de desastres,5002219,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5003001,
"software development, product quality is directly related to the quality of the development
process. Therefore, activities of Verification, Validation & Testing (VV&T) performed by
methods, techniques and tools are urgently required to increase productivity, quality and cost
reduction in software development. Similarly, testing technique and criteria contribute to the
productivity of test activities. A crucial point for the software testing automation is making
the most reliable activities and significantly reducing development costs. Regarding software
testing automation, test oracles are essential, representing an mechanism (program, process
or data) to indicate whether the actual output for a given test case is correct. This master’s
thesis aims to explore concepts of mutation testing to create alternative implementations of the
oracle procedure and thus assess their quality. Mutation testing refers to the creation of system
development versions with minor syntactic code changes. It has high efficiency on detecting
of defects and it is very flexible in its application being used in various types of artifacts. This
work also proposes specific mutation operators for oracles, implements an useful support tool
for using oracle mutation operators and conducts an empirical study of operators, highlighting
benefits and challenges associated with their use.",Versão de defesa - Ana Claudia Maciel.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,ANA CLAUDIA MACIEL,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),19/04/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'test oracles;mutation testing;mutation operators;software testing',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,MARCIO EDUARDO DELAMARO,98,oráculos de teste;teste de mutação;operadores de mutação;teste de software,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Apoio à sistematização e automatização da atividade de teste de software,"No desenvolvimento de software, a qualidade do produto está diretamente relacionada à qualidade
do processo de desenvolvimento. Diante disso, atividades de Verificação, Validação & Teste
(VV&T) realizadas por meio de métodos, técnicas e ferramentas são de extrema necessidade para
o aumento da produtividade, qualidade e diminuição de custos no desenvolvimento de software.
Do mesmo modo, técnicas e critérios contribuem para a produtividade das atividades de teste.
Um ponto crucial para o teste de software é sua automatização, tornando as atividades mais
confiáveis e diminuindo significativamente os custos de desenvolvimento. Na automatização dos
testes, os oráculos são essenciais, representando um mecanismo (programa, processo ou dados)
que indica se a saída obtida para um caso de teste está correta. Este trabalho de mestrado utiliza
a ideia de mutação para criar implementações alternativas de oráculos de teste e, assim, avaliar a
sua qualidade. O teste de mutação se refere à criação de versões do sistema em desenvolvimento
com pequenas alterações sintáticas de código. A mutação possui alta eficácia na detecção
de defeitos e é bastante flexível na sua aplicação, podendo ser utilizado em diversos tipos de
artefatos. Adicionalmente este trabalho propõe operadores de mutação específicos para oráculos,
implementa uma ferramenta de suporte à utilização dos operadores para oráculos, e também
descreve um estudo empírico dos operadores, destacando benefícios e desafios associados ao seu
uso.",DISSERTAÇÃO,Avaliação da qualidade de oráculos de teste utilizando mutação,5009816,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5009848,
"Decision Support Systems (DSS) organize and process data and information to generate results
to support decision making in a specific domain. They integrate knowledge from domain
experts in each of their components: models, data, mathematical operations (that processes
the data) and analysis results. In traditional development methodologies, this knowledge
must be interpreted and used by software developers to implement DSSs. That is ,
because domain experts cannot formalize this knowledge in a computable model that can
be integrated into DSSs. The knowledge modeling process is carried out, in practice, by the
developers, biasing domain knowledge and hindering the agile development of DSSs (as domain
experts cannot modify code directly). To solve this problem, a method and web tool is
proposed that uses ontologies, in the Web Ontology Language (OWL), to represent expert’s
knowledge, and a Domain Specific Language (DSL), to model DSS behavior. Ontologies, in
OWL, are a computable knowledge representation, which allows the definition of DSSs in
a format understandable and accessible to humans and machines. This method was used
to create the Decisioner Framework for the instantiation of DSSs. Decisioner automatically
generates DSSs from an ontology and and a description in its DSL, including the DSS interface
(using aWeb Components library). An online ontology editor, using a simplified format,
allows that domain experts to change the ontology and immediately see the consequences
of their changes in the in the DSS. A validation of this method was done through the instantiation
of the SustenAgro DSS using the Decisioner Framework. The SustenAgro DSS
evaluates the sustainability of sugarcane production systems in the center-south region of
Brazil. Evaluations, done by by sustainability experts from Embrapa Environment (partners
in this project), showed that domain experts are capable of change the ontology and DSL
program used, without the help of software developers, and that the system produced correct
sustainability analysis.",Versão de Defesa - John Freddy Garavito Suárez.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,JOHN FREDDY GARAVITO SUAREZ,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),03/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),"b'Ontologies;Domain-Specific Language;Semantic Web;Knowledge Representation, Decisioner Framework;Decision Support System;SustenAgro'",ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,DILVAN DE ABREU MOREIRA,147,Ontologias;Linguagem de Domínio Específico;Web Semântica;Representação de Conhecimento;Framework Decisioner;Sistema de apoio à decisão;SustenAgro,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),-,"Os Sistemas de Apoio à Decisão (SAD) organizam e processam dados e informações para
gerar resultados que apoiem a tomada de decisão num domínio especifico. Eles integram
conhecimento de especialistas de domínio em cada um de seus componentes: modelos,
dados, operações matemáticas (que processamos dados) e resultado de análises. Nas metodologias
de desenvolvimento tradicionais, esse conhecimento deve ser interpretado e usado
por desenvolvedores de software para implementar os SADs. Isso porque especialistas de
domínio não conseguem formalizar esse conhecimento em um modelo computável que
possa ser integrado aos SADs. O processo de modelagem de conhecimento é realizado, na
prática, pelos desenvolvedores, parcializando o conhecimento do domínio e dificultando
o desenvolvimento ágil dos SADs (já que os especialistas não modificam o código diretamente).
Para solucionar esse problema, propõe-se um método e ferramenta web que usa
ontologias, naWeb Ontology Language (OWL), para representar o conhecimento de especialistas,
e uma Domain Specific Language (DSL), para modelar o comportamento dos SADs.
Ontologias, em OWL, são uma representação de conhecimento computável, que permite
definir SADs num formato entendível e accessível a humanos e máquinas. Esse método foi
usado para criar o framework Decisioner para a instanciação de SADs. O Decisioner gera
automaticamente SADs a partir de uma ontologia e uma descrição na sua DSL, incluindo a
interface do SAD (usando uma biblioteca deWeb Components). Umeditor online de ontologias,
que usa um formato simplificado, permite que especialistas de domínio possam modificar
aspectos da ontologia e imediatamente ver as consequência de suas mudanças no SAD.
Uma validação dessemétodo foi realizada, por meio da instanciação do SAD SustenAgro no
Framework Decisioner. O SAD SustenAgro avalia a sustentabilidade de sistemas produtivos
de cana-de-açúcar na região centro-sul do Brasil. Avaliações, conduzidas por especialistas
em sustentabilidade da Embrapa Meio ambiente (parceiros neste projeto), mostraram que
especialistas são capazes de alterar a ontologia e DSL usadas, sem a ajuda de programadores,
e que o sistema produz análises de sustentabilidade corretas.",DISSERTAÇÃO,"Ontologias e DSLs na geração de sistemas de apoio à decisão, caso de estudo SustenAgro",5010359,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5010373,
"In order to perform attenuation correction in single photon emission computed tomography
(SPECT), we need to measure and reconstruct the attenuation coeffients map using a transmission
tomography scan, performed either sequentially or simultaneously with a emission scan.
This approach increases the cost required to produce the image and, in some cases, increases
considerably the scanning time, therefore the pacient immobility is a important factor to the
reconstruction success. An alternative that dispense the transmission scan is reconstruct, both
the activity image and the attenuation map, only from the emission scan data. In this approch
we proposed a method based on the Censor’s algorithm, which objective is solve mixed convexconcave
feasibility problem to reconstruct simultaneously all images. The method proposed is
formulated as a minimization problem, where the objective function is given by images total
variation subject to Censor’s mixed feasibility. In the simulations artificial images were used and
the obtained results without noised data, even for small quantity of data, were satisfactory. The
method was unstable in presence of noise with Poisson distribution, because tolerance choice is
an open problem yet.",Versão de defesa - João Guilherme Vicente de Araujo.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,JOAO GUILHERME VICENTE DE ARAUJO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),13/04/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'SPECT;CSP;Simultaneous reconstruction;Total variation',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,ELIAS SALOMAO HELOU NETO,77,SPECT;CSP;Reconstrução simultanea;Variação total,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Para realizar a correção de atenuação em uma tomografia computadorizada por emissão de fóton
único (SPECT, em inglês) é necessário medir e reconstruir o mapa dos coeficientes de atenuação
utilizando uma leitura de um tomógrafo de transmissão, feita antes ou simultaneamente à leitura
de emissão. Essa abordagem encarece a produção da imagem e, em alguns casos, aumenta
consideravelmente a duração do exame, sendo a imobilidade do paciente um fator importante para
o sucesso da reconstrução. Uma alternativa que dispensa a leitura de transmissão é reconstruir,
tanto a imagem de atividade quanto o mapa de atenuação, somente através dos dados de uma
leitura de emissão. Dentro dessa abordagem propusemos um método baseado no algoritmo criado
por Censor, cujo objetivo é resolver um problema misto de viabilidade côncavo-convexo para
reconstruir simultaneamente as imagens. O método proposto é formulado como um problema de
minimização, onde a função objetivo é dada pela variação total das imagens sujeita à viabilidade
mista de Censor. Os testes foram feitos em imagens simuladas e os resultados obtidos na ausência
de ruídos, mesmo para uma pequena quantidade de dados, foram satisfatórios. Na presença de
dados ruidosos com distribuição de Poisson o método foi instável, pois a escolha das tolerâncias
ainda é um problema aberto.",DISSERTAÇÃO,Reconstrução tomográfica de imagens SPECT a partir de poucos dados utilizando variação total,5010376,1
"Dynamic texture analysis has been an area of research increasing and in potential in recent years
in computer vision. Dynamic textures are sequences of texture images (i.e. video) that represent
dynamic objects. Examples of dynamic textures are: evolution of the colony of bacteria, growth
of body tissues, moving escalator, waterfalls, smoke, process of metal corrosion, among others.
Although there are researches related to the topic and promising results, most literature methods
have limitations. Moreover, in many cases the dynamic textures are the result of complex
phenomena, making a characterization task even more challenging. This scenario requires the
development of a paradigm of methods based on complexity. The complexity can be understood
as a measure of irregularity of the dynamic textures, allowing to measure the structure of the
pixels and to quantify the spatial and temporal aspects. In this context, this masters aims to study
and develop methods for the characterization of dynamic textures based on methodologies of
complexity from the area of complex systems. In particular, two methodologies already used in
computer vision problems are considered: complex networks and deterministic walk partially
self-repulsive. Based on these methodologies, three methods of characterization of dynamic
textures were developed: (i) based on diffusion in networks − (ii) based on deterministic walk
partially self-repulsive − (iii) based on networks generated by deterministic walk partially
self-repulsive. The developed methods were applied in problems of nanotechnology and vehicle
traffic, presenting potencial results and contribuing to the development of both areas.",Versão de defesa - Lucas Correia Ribas.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,LUCAS CORREIA RIBAS,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),27/04/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Dynamic texture;Complex system;Deterministic walk partially self-repulsive;Complex networks;Complexity',BANCO DE DADOS/COMPUTAÇÃO GRÁFICA E PROCESSAMENTO DE IMAGENS,ODEMIR MARTINEZ BRUNO,147,Textura dinâmica;Sistemas complexos;Redes complexas;Caminhada determinística parcialmente auto-repulsiva;Complexidade,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Padrões e desordem em ciência não-linear com enfoque em aplicações em computação, física e biologia.","A análise de texturas dinâmicas tem se apresentado como uma área de pesquisa crescente e
em potencial nos últimos anos em visão computacional. As texturas dinâmicas são sequências
de imagens de textura (i.e. vídeo) que representam objetos dinâmicos. Exemplos de texturas
dinâmicas são: evolução de colônia de bactérias, crescimento de tecidos do corpo humano, escada
rolante em movimento, cachoeiras, fumaça, processo de corrosão de metal, entre outros. Apesar
de existirem pesquisas relacionadas com o tema e de resultados promissores, a maioria dos
métodos da literatura possui limitações. Além disso, em muitos casos as texturas dinâmicas são
resultado de fenômenos complexos, tornando a tarefa de caracterização um desafio ainda maior.
Esse cenário requer o desenvolvimento de um paradigma de métodos baseados em complexidade.
A complexidade pode ser compreendida como uma medida de irregularidade das texturas
dinâmicas, permitindo medir a estrutura dos pixels e quantificar os aspectos espaciais e temporais.
Neste contexto, o objetivo deste mestrado é estudar e desenvolver métodos para caracterização
de texturas dinâmicas baseado em metodologias de complexidade advindas da área de sistemas
complexos. Em particular, duas metodologias já utilizadas em problemas de visão computacional
são consideradas: redes complexas e caminhada determinística parcialmente auto-repulsiva.
A partir dessas metodologias, três métodos de caracterização de texturas dinâmicas foram
desenvolvidos: (i) baseado em difusão em redes − (ii) baseado em caminhada determinística
parcialmente auto-repulsiva − (iii) baseado em redes geradas por caminhada determinística
parcialmente auto-repulsiva. Os métodos desenvolvidos foram aplicados em problemas de
nanotecnologia e tráfego de veículos, apresentando resultados potenciais e contribuindo para o
desenvolvimento de ambas áreas.",DISSERTAÇÃO,Análise de texturas dinâmicas baseada em sistemas complexos,5010638,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5010652,
"Efforts to make government data open and widely accessible on the Internet are now widespread
over many countries, generally referred to as Open Government Data. In Brazil, in parallel to
similar initiatives the Information Access Law has been published to regulate the constitutional
rights of citizens to access relevant public information. Moreover, social media channels such
as Twitter and Facebook often contribute to disseminate initiatives that seek to inform and
empower citizens concerned with government actions. On the other hand, certain actions and
statements by governmental institutions, or parliament members and political journalists that
appear on the conventional media tend to reverberate on the social media. This scenario produces
a lot of textual data that can reveal relevant information on governmental actions and policies.
Nonetheless, the target audience still lacks appropriate tools capable of supporting the acquisition,
correlation and interpretation of potentially useful information embedded in such text sources.
Accordingly, this work introduces a framework for analyzing the temporal evolution of topics in
Twitter in connection with political debates. The association is performed by a clustering process
based on a domain-independent text segmentation method and a simplified version of MONIC is
employed to track the temporal evolution of the debates/topics. Finally, a novel visualization is
introduced and embedded into a visual interface.",Versão de defesa - Eder José de Carvalho.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,EDER JOSE DE CARVALHO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),04/05/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Visualization;Visual analytics;Social data analisys',BANCO DE DADOS/COMPUTAÇÃO GRÁFICA E PROCESSAMENTO DE IMAGENS,MARIA CRISTINA FERREIRA DE OLIVEIRA,77,Visualização;Análise visual;Análise de dados sociais,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Desafios e Aplicações em Visualização Exploratória de Dados Multidimensionais,"Iniciativas por todo o mundo vêm ocorrendo no sentido de disponibilizar dados governamentais
em formato aberto (Open Government Data) na internet. No Brasil, em paralelo a essas iniciativas,
foi publicada a Lei de Acesso à Informação para acesso público, que regulamenta o direito
constitucional de acesso dos cidadãos às informações públicas. Além disso, mídias sociais como
o Twitter e o Facebook atuam, em diversas situações, como canais de iniciativas que buscam
ampliar as ações de cidadania. Por outro lado, certas ações e manifestações na mídia convencional
por parte de instituições governamentais, ou de jornalistas e políticos como deputados e senadores,
tendem a repercutir nas mídias sociais. Como resultado, gera-se uma enorme quantidade de dados
em formato textual que podem ser muito informativos sobre ações e políticas governamentais.
No entanto, o público-alvo continua carente de boas ferramentas que o ajudem a levantar,
correlacionar e interpretar as informações potencialmente úteis associadas a esses textos. Nesse
sentido, este trabalho apresenta um framework para a análise da evolução temporal de tópicos
discutidos no Twitter em conexão com debates políticos. A associação é realizada por um
processo de clusterização baseado em um método de segmentação independente de domínio
e uma versão simplificada do MONIC é empregada para a análise da evolução temporal dos
debates/tópicos. Por último, uma nova visualização é introduzida e embutida em uma interface
visual.",DISSERTAÇÃO,Análise visual de tópicos no Twitter em conexão com debates políticos,5010670,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5010685,
"The Poisson-Boltzmann equation has a wide range of applications, from Colloidal and microfluidic
science to biochemistry and biophysics. The electrical potential in Electric double layer
leads to a Force potential in terms of the Navier-Stokes equations that is then used To simulate the
resulting flow. In biphasic flows a simplification Of this equation is used to obtain the pressure
field.
The present study has as main objective to study the problem of Poisson-Boltzmann Addressed
in cite HEL and propose a solution through Implementation of the immersed interfaces method
using high order finite difference schemesand thus get high order numerical accuracy.",Versão de defesa - Miguel Angel Rojas Meza.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,MIGUEL ANGEL ROJAS MEZA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),05/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Fluid Mechanics and Applications;Poisson-Boltzmann;Immersed Interfaces Method',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,JOSE ALBERTO CUMINATO,72,Mecânica dos Fluidos e Aplicações;Poisson-Boltzmann;Método das Interfaces Imersas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Solução Numérica das Equações de Navier-Stokes - Escoamentos Tridimensionais,"A equação de Poisson-Boltzmann tem uma vasta gama de aplicações, desde a ciência coloidal e
microfluídica até bioquímica e biofísica. O potencial elétrico na dupla camada elétrica leva a um
potencial de força, em termos das equações de Navier-Stokes que é então usado para simular o
fluxo resultante. Em escoamentos bifásicos uma simplificação desta equação é usada para se
obter o campo de pressão.
O presente trabalho tem como principal objetivo estudar o problema de Poisson-Boltzmann
abordado em (HELGADÓTTIR; GIBOU, 2011) e propor uma solução através da implementação
do método das interfaces imersas utilizando diferenças finitas de altas ordens de precisão
numérica.",DISSERTAÇÃO,O método das interfaces imersas para a solução da equação de Poisson-Boltzmann,5010704,1
"The testing activity supports the development of high quality software, providing techniques
and testing criteria to improve the detection of defects in the software. In a robotic systems
development context, the validation is normally performed using simulation tools or ad-hoc tests.
These approaches may be efficient but, in general, they cannot test all the situations (or all of the
behavior) of the system, and some defects may be unrevealed. This project explores this gap and
it proposes an approach for integration testing applied to robotic embedded systems, looking for
defects mainly related to the communication among components of these systems. This study
was initially conducted by mapping coverage criteria, defined for concurrent program testing and
exploring problems related to non-determinism, communication, deadlock and other undesired
situations. During the project was defined an approach of integration testing and coverage criteria
for mobile robotic systems. The case studies indicate that the proposed approach is effective to
reveal defects do not found using simulations or traditional testing of mobile robots.",Versão de Defesa - Maria Adelina Silva Brito.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,MARIA ADELINA SILVA BRITO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),11/04/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Software testing;Integration testing;Mobile robotic;Testing criteria;Testing scenarios',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,SIMONE DO ROCIO SENGER DE SOUZA,155,Teste de software;Teste de integração;Robótica móvel;Critérios de teste;Cenários de teste,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Estudo Teórico e Aplicado de Critérios de Teste e Validação na  Produção de Software,"A atividade de teste contribui com o desenvolvimento de software confiável por meio da aplicação
de técnicas e critérios de teste. Esses instrumentos viabilizam a detecção de defeitos,
minimizando a quantidade de falhas no produto final. Durante o desenvolvimento de sistemas
robóticos móveis a validação normalmente consiste em simulações durante o desenvolvimento
e testes ad-hoc no ambiente de desenvolvimento e plataforma final. Essa abordagem, apesar
de ser eficiente em alguns casos, não permite que todas as situações que levam a falhas sejam
testadas. Este projeto de doutorado contribui nessa direção, explorando essa lacuna e propondo
uma abordagem de teste de integração para sistemas robóticos que permita explorar defeitos
relacionados à comunicação entre os módulos do sistema. Este estudo foi conduzido com base
em conceitos explorados no teste de programas concorrentes, dado que os sistemas robóticos
podem apresentar problemas similares aos encontrados nos programas concorrentes, como não
determinismo, diferentes possibilidades de comunicação, deadlocks, entre outros. Durante o
tabalho foi definida uma abordagem de teste de integração e critérios de cobertura aplicados a
sistemas robóticos reais. Os estudos indicaram que a abordagem proposta é efetiva em revelar
defeitos não explorados pela simulação e técnicas de teste utilizadas anteriormente.",TESE,Estudo e definição do teste de integração de software para o contexto de sistemas robóticos móveis,5010746,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5010750,
"In this research project we present a problem of production scheduling in parallel lines, motivated
by a food industry characterized by the perishability of the products, sequencing the production
of the lots and by the need to synchronize scarce resources for the operation of the production
lines. In industries of this branch, there are high costs related to the storage of the products, in
order to avoid their loss, so that the good management of industrial processes and inventory
is essential. Initially mathematical models were proposed for the problem of lot sizing and
scheduling and made a study of restrictions proposed in the literature for the treatment of
perishability. Computational tests were performed for the validation of the proposed model,
however, due to the high complexity of the problem, heuristic methods were proposed based
on the mathematical formulation for the efficient resolution of the problem. With the objective
of demonstrating the competitiveness of the proposed methods of resolution, we compare their
results with instances of the literature and with examples based on the industrial production
scenario.",Versão de defesa - Rafael Soares Ribeiro.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,RAFAEL SOARES RIBEIRO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),02/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Lot sizing;Scheduling;Parallel Machines;Perishability;ALNS',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,MARISTELA OLIVEIRA DOS SANTOS,109,Dimensionamento de lotes;Sequenciamento;Máquinas paralela;Perecibilidade;ALNS,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Nesse projeto de pesquisa apresentamos um problema de programação da produção em linhas
paralelas, motivado por uma indústria alimentícia caracterizada pela perecibilidade dos produtos,
sequenciamento da produção dos lotes e pela necessidade de sincronização de recursos escassos
para operação das linhas de produção. Em indústrias desse ramo, existem altos custos relacioandos
a estocagem dos produtos, afim de evitar sua perda, de modo que é essencial a boa gestão
dos processos industriais e do estoque. Inicialmente foram propostos modelos matemáticos
para o problema de dimensionamento e sequenciamento de lotes e feito um estudo de restrições
propostas na literatura para o tratamento da perecibilidade. Testes computacionais foram
realizados para a validação do modelo proposto, entretanto, devido a elevada complexividade
do problema, foram propostos métodos heurísticos baseados na formulação matemática para a
resolução eficiente do problema. Com o objetivo de demonstrar a competitivade dos métodos de
resolução propostos, comparamos seus resultados com instâncias da literatura e com exemplares
baseados no cenário produtivo da indústria.",DISSERTAÇÃO,Problema de dimensionamento e sequenciamento de lotes em linhas paralelas: uma aplicação em uma indústria de alimentos,5010752,1
"Among the several Advanced Driver Assistance (ADAS) technologies that have been added to
modern vehicles are pedestrian detection systems. Those systems use sensors, such as radars,
lasers, and video cameras to capture information from the environment and avoid collision
with people in the context of traffic. Video cameras have come out as a great option for such
systems because of the relatively low cost and all of information they are able to capture from
the environment. Many techniques for vison-based pedestrian detection have appeared in the
last years, having as characteristic the necessity of a great computational power so that image
can be processed in real time, in a robust and reliable way, and with low error rate. In addition,
systems that implement these techniques require low power consumption, so they can operate in
an embedded environment such as automobiles. One trend of these systems is the processing
of images from multiple cameras mounted in vehicles, so that the system can detect potential
collision hazards around the vehicle. In this context, this work addresses the hardware and
software codesign of an architecture for pedestrian detection, considering the presence of four
cameras in a vehicle (one in the front, one in the rear and two in the sides). For this purpose,
the flexibility of FPGA devices is used for design space exploration the construction of an
architecture that provides the necessary performance, energy consumption at appropriate levels
and also allows adaptation to new scenarios and evolution of pedestrian detection techniques
through programmability. The development of the architecture was based on two algorithms
widely used for pedestrian detection, which are Histogram of Oriented Gradients (HOG) and
Integral Channel Features (ICF). Both introduce techniques that serve as the basis for modern
detection algorithms. The implemented architecture allowed the exploration of different types of
parallelism through the use of multiple softcore processors, as well as the acceleration of critical
functions through implementations in hardware. It has also been demonstrated its feasibility in
attending to a system containing four video cameras.",Versão de defesa - José Arnaldo Mascagni de Holanda.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,JOSE ARNALDO MASCAGNI DE HOLANDA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),17/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Pedestrian Detection;Multi-core architecture;Reconfigurable Computing;FPGA',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",EDUARDO MARQUES,146,Detecção de Pedestres;Arquiteturas Multi-core;Computação Reconfigurável;FPGA,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),-,"Dentre as diversas tecnologias de Assistência Avançada ao Condutor (ADAS) que têm sido
adiconadas aos automóveis modernos estão os sistemas de detecção de pedestres. Tais sistemas
utilizam sensores, como radares, lasers e câmeras de vídeo para captar informações do ambiente e
evitar a colisão com pessoas no contexto do trânsito. Câmeras de vídeo têm se apresentado como
um ótima opção para esses sitemas, devido ao relativo baixo custo e à riqueza de informações que
capturam do ambiente. Muitas técnicas para detecção de pedetres baseadas em visão têm surgido
nos últimos anos, tendo como característica a necessidade de um grande poder computacional
para que se possa realizar o processamento das imagens em tempo real, de forma robusta,
confiável e com baixa taxa de erros. Além disso, é necessário que sistemas que implementem
essas técnicas tenham baixo consumo de energia, para que possam funcionar em um ambiente
embarcado, como os automóveis. Uma tendência desses sistemas é o processamento de imagens
de múltiplas câmeras presentes no veículo, de forma que o sistema consiga perceber potenciais
perigos de colisão ao redor do veículo. Neste contexto, este trabalho aborda o coprojeto de
hardware e software de uma arquitetura para detecção de pedestres, considerando a presença de
quatro câmeras em um veículo (uma frontal, uma traseira e duas laterais). Com este propósito,
utiliza-se a flexibilidade dos dispositivos FPGA para a exploração do espaço de projeto e a
construção de uma arquitetura que forneça o desempenho necessário, o consumo de energia em
níveis adequados e que também permita a adaptação a novos cenários e a evolução das técnicas de
detecção de pedestres por meio da programabilidade. O desenvolvimento da arquitetura baseouse
em dois algoritmos amplamente utilizados para detecção de pedestres, que são o Histogram of
Oriented Gradients (HOG) e o Integral Channel Features (ICF). Ambos introduzem técnicas
que servem como base para os algoritmos de detecção modernos. A arquitetura implementada
permitiu a exploração de diferentes tipos de paralelismo das aplicações por meio do uso de
múltiplos processadores softcore, bem como a aceleração de funções críticas por meio de
implementações em hardware. Também foi demonstrada sua viabilidade no atendimento a um
sistema contendo quatro câmeras de vídeo.",TESE,Arquitetura multi-core reconfigurável para detecção de pedestres baseada em visão,5014990,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5015006,
"On-line Social networks become a new and important medium of exchange of information, ideas and
communication that approximate relatives and friends no matter the distances. Given the open nature
of the Internet, the information can flow very easy and fast in the population. The network can be
represented as a graph, where individuals or organizations are the set of vertices and the relationship
or connection among the vertices are the set of edge. Moreover, the social networks are also
intrinsically representing the structure of a more complex system that is the society. These structures
are related with characteristics of the individuals, like the most popular individuals have many
connections, and the society, the correlation in the connectivity of vertices that is a trace of homophily
phenomenon, among many other. In particular, is well accepted that the structure of the network can
affect the way the information propagates on the social networks. However, how the structure impacts
in the propagation, how to measure that impact and what are the strategies for controlling the
propagation of some information, it is still unclear. In this thesis, we seek to contribute in the analysis of
the interplay between the dynamics of information and rumor spreading and the structure of the
networks. We propose a more realistic propagation model considering the heterogeneity of the
individuals in the transmission of ideas or information. We confirm the presence of influential spreaders
in the rumor propagation process and found that selecting a very small fraction of influential spreaders,
it is possible to expressively improve or reduce de diffusion of some information on the network. In the
case we want to select a set of initial spreaders that maximize the information diffusion on the network,
the simple and best alternative is to select the most central or important individuals from the network's
communities. But, if the pattern of connection of the networks is negatively correlated, the best
alternative is to choose from the most central individuals in the whole network. On the other hand, we
identify, by topological approach and machine learning techniques, the least influential spreaders and
show that they act as a firewall in the propagation process. We propose an adaptative method that
rewires one edge for a given vertex to a central individual, without affecting the overall distribution of
connection. Applying our proposed method in a little fraction of least influential spreaders, we observed
an important increasing in the capacity of propagation of these vertices and in the overall network. Our
results are from a wide range of simulations in artificial and real-world data sets and the comparison
with the classical rumor propagation model. The propagation of information is of greatest relevance for
publicity and marketing area, education, political or health campaigns, among others. The results of this
these might be applicable extended in different research fields like biological networks and animal
social behavior models.",Versão de Defesa - Didier Augusto Vega Oliveros.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,DIDIER AUGUSTO VEGA OLIVEROS,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),12/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Social Networks;Information spreading;Rumor propagation;Influence maximization problem;Complex networks',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,FRANCISCO APARECIDO RODRIGUES,161,Redes sociais;Propagação de informação;Propagação de rumores;Problema de maximização da influência;Redes complexas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Redes Complexas,"As redes sociais se tornaram um novo e importante meio de intercâmbio de informações, ideias
e comunicação que aproximam parentes e amigos sem importar as distâncias. Dada a natureza
aberta da Internet, as informações podem fluir muito fácil e rápido na população. A rede pode
ser representada como um grafo, onde os indivíduos ou organizações são o conjunto de
vértices e os relacionamentos ou conexões entre os vértices são o conjunto de arestas. Além
disso, as redes sociais representam intrinsecamente a estrutura de um sistema mais complexo
que é a sociedade. Estas estruturas estão relacionadas com as características dos indivíduos.
Por exemplo, os indivíduos mais populares são aqueles com maior número de conexões, e da
sociedade. Em particular, é aceito que a estrutura da rede pode afetar a forma como a
informação se propaga nas redes sociais. No entanto, ainda não está claro como a estrutura
influencia na propagação, como medir esse impacto e quais são as possíveis estratégias para
controlar a propagação da informação. Nesta tese buscamos contribuir nas análises da
interação entre a dinâmica de propagação de informação e rumores e a estrutura das redes.
Propomos um modelo de propagação mais realista considerando a heterogeneidade dos
indivíduos na transmissão de ideias ou informações. Nós confirmamos a presença de
propagadores mais influentes na dinâmica de rumor e observamos que é possível melhorar ou
reduzir expressivamente a difusão de uma informação na rede ao selecionar uma fração muito
pequena de propagadores influentes. No caso em que desejamos selecionar um conjunto de
propagadores iniciais que maximize a difusão de informação, a melhor opção é selecionar os
indivíduos mais centrais ou importantes nas comunidades. Porém, se o padrão de conexão dos
vértices está negativamente correlacionado, a melhor alternativa é escolher entre os indivíduos
mais centrais de toda a rede. Por outro lado, através de abordagens topológicas e de técnicas de
aprendizagem máquina, identificamos os propagadores menos influentes e mostramos que eles
atuam como um firewall no processo de propagação. Nós propomos um método adaptativo que
reconexão entre os vértices menos influentes para um indivíduo central da rede, sem afetar a
distribuição global de conexão. Aplicando o nosso método em uma pequena fração de
propagadores menos influentes, observamos um aumento importante na capacidade de
propagação desses vértices e da rede toda. Nossos resultados vêm de uma ampla gama de
simulações em conjuntos de dados artificiais e do mundo real e a comparação com modelos
clássicos de propagação da literatura. A propagação da informação em redes é de grande
relevância para a área de publicidade e marketing, educação, campanhas políticas ou de saúde,
entre outras. Os resultados desta tese podem ser aplicados e estendidos em diferentes campos
de pesquisa como redes biológicas e modelos de comportamento social animal, modelos de
propagação de epidemias e na saúde pública, entre outros.",TESE,Dinâmicas de propagação de informações e rumores em redes sociais,5015007,1
"Mutation Testing is a powerful test criterion to detect faults and measure the effectiveness of a
test data set. However, it is a computationally expensive testing technique. The high cost comes
mainly from the effort to generate adequate test data to kill the mutants and by the existence
of equivalent mutants. In this thesis, an approach called Reach, Infect and Propagation to
Mutation Testing (RIP-MuT) is presented to generate test data and to suggest equivalent mutants.
The approach is composed of two modules: (i) an automated test data generation using hill
climbing and a fitness scheme according to Reach, Infect, and Propagate (RIP) conditions; and
(ii) a method to suggest equivalent mutants based on the analyses of RIP conditions during the
process of test data generation. The experiments were conducted to evaluate the effectiveness of
the RIP-MuT approach and a comparative study with a genetic algorithm and random testing.
The RIP-MuT approach achieved a mean mutation score of 18.25% higher than the GA and
35.93% higher than random testing. The proposed method for detection of equivalent mutants
demonstrate to be feasible for cost reduction in this activity since it obtained a precision of
75.05% on suggesting equivalent mutants. Therefore, the results indicate that the approach
produces effective test data able to strongly kill the majority of mutants on C programs, and also
it can assist in suggesting equivalent mutants correctly.",Versão de defesa - Francisco Carlos Monteiro Souza .pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,FRANCISCO CARLOS MONTEIRO SOUZA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),24/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Software Testing;Search-based Software Testing;Mutation Testing;Test Data Generation;Equivalent Mutants',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,MARCIO EDUARDO DELAMARO,149,Teste de Software;Teste de Software baseado em Busca;Teste de Mutação;Geração de Dados de Teste;Mutantes Equivalentes,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Apoio à sistematização e automatização da atividade de teste de software,"O teste de mutação é um critério de teste poderoso para detectar falhas e medir a eficácia de um
conjunto de dados de teste. No entanto, é uma técnica de teste computacionalmente cara. O
alto custo provém principalmente do esforço para gerar dados de teste adequados para matar
os mutantes e pela existência de mutantes equivalentes. Nesse contexto, o objetivo desta tese é
apresentar uma abordagem chamada de Reach, Infect and Propagation to Mutation Testing (RIPMuT)
que visa gerar dados de teste e sugerir mutantes equivalentes. A abordagem é composta
por dois módulos: (i) uma geração automatizada de dados de teste usando subida da encosta e
um esquema de fitness de acordo com as condições de alcançabilidade, infeção e propagação
(RIP); e (ii) um método para sugerir mutantes equivalentes com base na análise das condições
RIP durante o processo de geração de dados de teste. Os experimentos foram conduzidos para
avaliar a eficácia da abordagem RIP-MuT e um estudo comparativo com o algoritmo genético
e testes aleatórios foi realizado. A abordagem RIP-MuT obteve um escore médio de mutação
de 18,25 % maior que o AG e 35,93 % maior que o teste aleatório. O método proposto para
detecção de mutantes equivalentes se mostrou viável para redução de custos relacionado a essa
atividade, uma vez que obteve uma precisão de 75,05% na sugestão dos mutantes equivalentes.
Portanto, os resultados indicam que a abordagem gera dados de teste adequados capazes de matar
a maioria dos mutantes em programas C e, também auxilia à identificar mutantes equivalentes
corretamente.",TESE,Uma abordagem para geração de dados de teste para o teste de mutação utilizando técnicas baseadas em busca,5015008,1
"In order to support users during the consumption of products,Web systems have incorporated recommendation
techniques. The most popular approaches are content-based, which recommends
items based on interesting features to the user, and collaborative filtering, which recommends
items that were well evaluated by users with similar preferences to the target user, or that have
similar features to items which were positively evaluated. While the first approach has limitations
such as overspecialization and limited content analysis, the second technique has problems such
as the new user and the new item, limitation also known as cold start. In spite of the variety of
techniques available, a common problem is the lack of semantic information to represent item’s
features. Recent works in the field of recommender systems have been studying the possibility to
use knowledge databases from the Web as a source of semantic information. However, it is still
necessary to investigate how to use and integrate such semantic information in recommender
systems. In this way, this work has the proposal to investigate how semantic information gathered
from knowledge databases can help recommender systems by semantically describing items, and
how semantic similarity can overcome the challenge confronted in the cold-start scenario. As
a result, we obtained a technique that can produce recommendations suited to users’ profiles,
including relevant new items available in the database",Versão de defesa - Salmo Marques da Silva Júnior.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,SALMO MARQUES DA SILVA JUNIOR,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),10/05/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Content-based filtering;Neighborhood models;Cold start scenario;Semantic distance;Semantic similarity',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,MARCELO GARCIA MANZATO,99,Filtragem colaborativa;Modelos de vizinhança;Cenário de partida fria;Distância semântica;Similaridade semântica,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Métodos Avançados de Seleção de Conteúdo Multimídia,"A fim de auxiliar usuários durante o consumo de produtos, sistemas Web passaram a incorporar
módulos de recomendação de itens. As abordagens mais populares são a baseada em conteúdo,
que recomenda itens a partir de características que são do seu interesse, e a filtragem colaborativa,
que recomenda itens bem avaliados por usuários com perfis semelhantes ao do usuário alvo, ou
que são semelhantes aos que foram bem avaliados pelo usuário alvo. Enquanto que a primeira
abordagem apresenta limitações como a sobre-especialização e a análise limitada de conteúdo,
a segunda enfrenta problemas como o novo usuário e/ou novo item, também conhecido como
partida fria. Apesar da variedade de técnicas disponíveis, um problema comum existente na
maioria das abordagens é a falta de informações semânticas para representar os itens do acervo.
Trabalhos recentes na área de Sistemas de Recomendação têm estudado a possibilidade de
usar bases de conhecimento da Web como fonte de informações semânticas. Contudo, ainda
é necessário investigar como usufruir de tais informações e integrá-las de modo eficiente em
sistemas de recomendação. Dessa maneira, este trabalho tem o objetivo de investigar como
informações semânticas provenientes de bases de conhecimento podem beneficiar sistemas de
recomendação por meio da descrição semântica de itens, e como o cálculo da similaridade
semântica pode amenizar o desafio enfrentado no cenário de partida fria. Como resultado,
obtém-se uma técnica que pode gerar recomendações adequadas ao perfil dos usuários, incluindo
itens novos do acervo que sejam relevantes.",DISSERTAÇÃO,Recomendação de conteúdo baseada em informações semânticas extraídas de bases de conhecimento,5015011,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5022023,
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5022356,
"Projection and backprojection operators are the numerically intensive part of iterative
methods in tomographic reconstruction. Some alternatives based on interpolation over
a regular grid on the Fourier space or on nonequispaced fast transforms, among other
ideas, were developed in order to alleviate the computational cost. Several approaches
substantially speed up the computation of the iterations of classical algorithms, but
the incremental methods, such as osem, ramla, saem, among others, had not been
benefited from these techniques. In this work, it is presented an efficient approach
where the technique of nonequispaced fast Fourier transform (nfft) is used in each
sub-iteration of incremental methods in order to perform the most expensive calculations:
the projection and backprojection. The proposed methods are applied to synchrotron
radiation tomography and the results show a good performance.",Versão de defesa - Camila de Lima.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,CAMILA DE LIMA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),09/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Tomographic Reconstruction;Radon Transform;Iterative Methods;Incremental Methods;Fast Computation of the Projection and Backprojection',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,ELIAS SALOMAO HELOU NETO,106,Reconstrução Tomográfica;Transformada de Radon;Métodos Iterativos;Métodos Incrementais;Cálculo Rápido da Projeção e Retroprojeção,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Os operadores de projeção e retroprojeção são a parte numericamente mais intensa de
métodos iterativos em reconstrução tomográfica. Algumas alternativas baseadas na
interpolação em uma grade regular no espaço de Fourier ou em transformadas rápidas nãouniformes,
dentre outras ideias, foram desenvolvidas a fim de aliviar o custo computacional.
Diversas abordagens foram bem sucedidas em acelerar o cálculo das iterações de algoritmos
clássicos, mas nenhuma havia sido utilizada em conjunto com os métodos incrementais,
como osem, ramla, saem. Neste trabalho são apresentados abordagens eficientes em que
a técnica de transformada rápida de Fourier não uniforme é utilizada nas sub-iterações
de métodos incrementais com o objetivo de efetuar os cálculos mais caros: a projeção e a
retroprojeção. Os métodos propostos são aplicados a tomografia por radiação síncrotron e
os resultados da pesquisa mostram um bom desempenho.",TESE,Cálculo rápido do operador de retroprojeção com aplicações em reconstrução tomográfica de imagens,5022358,1
"Context: Epilepsy is not a single disease, but a family of syndromes that share recurrent seizures.
It is estimated that 3% of the population will have epilepsy at some moment of their life. Seizure
detection is frequently done through EEG analysis. There are several difficulties in seizure
detection, people variability, the location of the spectral content, interferences, among other
things. Motivation: There is a growing usage with good results of the complex networks to
analyze time series, but few studies focusing on epilepsy. The works that have analyzed epilepsy,
in general, have neglected a strict statistical analysis. There is still doubts regarding the usage of
prospective algorithms to predict seizures. Methods: The time series were analyzed on 7 different
window sizes, 256, 303, 512, 910, 1024, 2048, and 2730 points. We used 6 different algorithms
to convert the time series into complex networks, k nearest neighbors network, adaptive k nearest
neighbors network, epsilon neighborhood network, cycle network, transition network, visibility
graph. Each algorithm has its parameters, and in total, we performed 75 conversions. For each
conversion, the network extracted 21 measures. A new dataset is formed with these measures,
and it was used to train 37 classifiers, divided into 4 classes, linear discriminant analysis, decision
tree, k nearest neighbors, support vector machine. We used 10-fold cross-validation in a training
set, separated from the whole dataset, and only the best classifier between the 37 was selected
for each conversion. In the test set, we estimated the performance of the best classifiers, and
then they were compared with a random predictor and with the state-of-the-art. Results: The
epsilon neighborhood network presented the best result with 100% accuracy over almost all
scenarios in the test set, with small window sizes and the linear discriminant analysis. The other
networks also had good results, comparable to the state-of-the-art, except the transition network
which had poor performance. Conclusion: We were able to develop a prospective algorithm with
a linear classifier using the epsilon neighborhood network, with a performance comparable to
the state-of-the-art and with rigorous statistical analysis, and not only using the accuracy as our
performance measure.",Versão de defesa - Daniel Moreira Cestari.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,DANIEL MOREIRA CESTARI,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),09/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),"b'Classification;Complex Networks;EEG, Epilepsy'",INTELIGÊNCIA COMPUTACIONAL,JOAO LUIS GARCIA ROSA,142,Classificação;Redes Complexas;EEG;Epilepsia,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Desenvolvimento de Algoritmos e Técnicas Computacionais para Aplicação em  Interfaces Cérebro-Computador,"Contexto: Epilepsia não é uma única doença, mas uma família de síndromes que compartilham a
recorrência de crises. Estima-se que 3% da população em geral terá epilepsia em algum momento
em suas vidas. A detecção de crises epiléticas é frequentemente feita através da análise de exames
de eletroencefalografia. Há várias dificuldades na detecção de crises, variabilidade entre pessoas,
localização do conteúdo espectral, interferências, dentre outras. Motivação: Há um crescente
uso com bons resultados de redes complexas para análise de séries temporais, mas poucos destes
são voltados à análise de sinais de epilepsia. Os trabalhos que analisam epilepsia, em geral,
negligenciam uma análise estatística rigorosa. Ainda há dúvida quanto à utilização de algoritmos
prospectivos para predição de crises. Métodos: As séries temporais são analisadas utilizando 7
tamanhos diferentes de janelas, 256, 303, 512, 910, 1.024, 2.048, e 2.730 pontos. São utilizados 6
algoritmos de conversão de série temporal em rede complexa, redes de k vizinhos mais próximos,
redes de k vizinhos mais próximos adaptativo, redes de epsilon vizinhança, redes cíclicas, redes
de transição, e grafos de visibilidade. Cada um desses algoritmos têm seus parâmetros, e no total
são realizadas 75 conversões. Para cada rede complexa gerada, são extraídas 21 medidas que as
caracterizam. Com a extração dessas medidas, um novo conjunto de dados é formado e utilizado
para treinar 37 classificadores diferentes, divididos em 4 classes, análise de discriminante linear,
árvore de decisão, k vizinhos mais próximos, e máquina de vetores de suporte. É utilizada uma
validação cruzada com 10-folds numa parte do conjunto de dados separada para o treino dos
classificadores, e apenas o melhor classificador dentre os 37 foi selecionado em cada conversão
realizada. No conjunto de teste, é feita a estimativa de desempenho do melhor classificador, que
é então comparado à um preditor aleatório e ao estado da arte. Resultados: A rede de epsilon
vizinhança obteve o melhor resultado, com 100% de acurácia no conjunto de teste em quase
todos os cenários, com janelas de tamanho pequeno e com a análise de discriminante linear.
As outras redes também tiveram bons resultados, comparáveis ao estado da arte, exceto a rede
de transição cujo desempenho foi ruim. Conclusão: Foi possível desenvolver um algoritmo
prospectivo com classificador linear utilizando a rede de epsilon vizinhança, com desempenho
comparável ao estado da arte e com rigorosa avaliação estatística, e não apenas utilizando a
acurácia como medida de desempenho.",DISSERTAÇÃO,Classificação de sinais de epilepsia utilizando redes complexas,5022377,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5022381,
"Several critical human activities depend on the weather forecasting. Some of them are transportation,
health, work, safety, and agriculture. Such activities require computational solutions for
weather forecasting through numerical models. These numerical models must be accurate and
allow the computers to process them quickly. In this project, we aim at migrating a small part of
the software of the weather forecasting model of Brazil, BRAMS — Brazilian developments
on the Regional Atmospheric Modelling System — to a heterogeneous system composed of
Xeon (Intel) processors coupled to a reprogrammable circuit (FPGA) via PCIe bus. According
to the studies in the literature, the chemical equation from the mass continuity equation is
the most computationally demanding part. This term calculates several linear systems Ax = b.
Thus, we implemented such equations in hardware and provided a portable and highly parallel
design in OpenCL language. The OpenCL framework also allowed us to couple our circuit to
BRAMS legacy code in Fortran90. Although the development tools present several problems, the
designed solution has shown to be viable with the exploration of parallel techniques. However,
the performance was below of what we expected.",Versão de defesa - Carlos Alberto Oliveira de Souza Junior.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,CARLOS ALBERTO OLIVEIRA DE SOUZA JUNIOR,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),05/06/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Hardware;FPGA;OpenCL;codesign;heterogeneous-computing',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",EDUARDO MARQUES,107,Hardware;FPGA;OpenCL;coprojeto;computação-heterogênea,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),-,"Several critical human activities depend on the weather forecasting. Some of them are transportation,
health, work, safety, and agriculture. Such activities require computational solutions for
weather forecasting through numerical models. These numerical models must be accurate and
allow the computers to process them quickly. In this project, we aim at migrating a small part of
the software of the weather forecasting model of Brazil, BRAMS — Brazilian developments
on the Regional Atmospheric Modelling System — to a heterogeneous system composed of
Xeon (Intel) processors coupled to a reprogrammable circuit (FPGA) via PCIe bus. According
to the studies in the literature, the chemical equation from the mass continuity equation is
the most computationally demanding part. This term calculates several linear systems Ax = b.
Thus, we implemented such equations in hardware and provided a portable and highly parallel
design in OpenCL language. The OpenCL framework also allowed us to couple our circuit to
BRAMS legacy code in Fortran90. Although the development tools present several problems, the
designed solution has shown to be viable with the exploration of parallel techniques. However,
the performance was below of what we expected.",DISSERTAÇÃO,Um coprojeto de hardware/software para a reatividade química do BRAMS,5022393,01
"The assessment of programming assignments is a costly task. Several tools have been proposed
and developed in order to automate the repetitive tasks performed by instructors in the assessment
of programming assignments and provide a faster and more adequate feedback to students.
However, adding a new tool increases the overload of new information and environments that
students have to deal with. Similarly, the assessment tool is another resource that the instructor
has to configure, maintain and teach students to use, wasting time and effort that could be used
in other pedagogical activities. For this reason, several works have been conducted to integrate
assessment tools for programming assignments in learning management systems (LMSs). The
integration of assessment tools into LMSs promotes their adoption in computing courses, since
they will be in agreement with the LMS already familiar to students and instructors, dispensing
with the need to adopt, learn and manage the submission and correction of assignments in another
environment. In this perspective, the main goal of this doctoral work is the proposition of the
IMPACTLE architecture, a solution that allows the integration of different assessment tools
for programming assignments in LMSs. The idea is that students and teachers can access the
features of the tools through the LMSs they are already habituated, without the need to learn how
to use and adopt a new tool. Architecture prototypes were instantiated and experiments were
carried out involving the use of these tools through LMSs. In general, it has been observed that
the use of IMPACTLE enables instructors and students perform tasks related to programming
activities in a more efficient, efficacious and effective way through the LMSs.",Versão de defesa - Draylson Micael de Souza.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,DRAYLSON MICAEL DE SOUZA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),20/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Assessment tools for programming assignments;Learning management systems;Teaching and learning of programming;Architecture;Middleware',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ELLEN FRANCINE BARBOSA,191,Ferramentas de avaliação automática de trabalhos de programação;Sistemas de gerenciamento de aprendizagem;Ensino e aprendizagem de programação;Arquitetura;Middleware,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Computação Aplicada à Educação,"A avaliação de trabalhos práticos de programação é uma tarefa dispendiosa. Diversas ferramentas
têm sido propostas e desenvolvidas a fim de automatizar as tarefas repetitivas realizadas pelos
professores na avaliação de trabalhos práticos de programação e fornecer um feedback mais
rápido e adequado aos alunos. No entanto, a adição de uma nova ferramenta aumenta a sobrecarga
de novas informações e ambientes que os alunos têm que lidar. De forma análoga, a ferramenta
de avaliação é mais um recurso que o professor tem de configurar, manter e ensinar os alunos a
utilizar. Por este motivo, vários trabalhos vêm sendo conduzidos a fim de integrar ferramentas
de avaliação para trabalhos de programação em sistemas de gestão da aprendizagem (Learning
Management Systems – LMSs). A integração de ferramentas em LMSs promove sua adoção em
disciplinas de computação, uma vez que estarão em concordância com o LMS já familiar aos
alunos e professores, dispensando a necessidade de adotar, aprender e gerenciar a submissão
e correção de trabalhos em um outro ambiente. Nessa perspectiva, o principal objetivo deste
trabalho de doutorado é a proposição da arquitetura IMAPCTLE, uma solução que permite
a integração de diferentes ferramentas de avaliação para trabalhos de programação em vários
LMSs. A ideia é que os alunos e professores possam acessar as funcionalidades das ferramentas
por meio dos LMSs em que já estão habituados, sem a necessidade de aprender a utilizar e adotar
uma nova ferramenta. Protótipos da arquitetura IMPACTLE foram instanciados e experimentos
foram realizados envolvendo a utilização destas ferramentas por meio de LMSs. De modo geral,
observou-se que a utilização da IMPACTLE possibilita que professores e alunos realizem as
tarefas relacionadas as atividades de programação de forma mais eficiente, eficaz e efetiva por
meio dos LMSs.",TESE,Subsídios à integração de ferramentas de avaliação automática e sistemas de gerenciamento de aprendizagem,5022395,1
"Studies indicate that the use of mobile learning applications has grown continuously,
allowing students and teachers greater flexibility and convenience in the execution of
educational activities and practices. Although several institutions have already adhered
to the mobile learning (m-learning) modality, their adoption still brings organizational,
cultural and technological problems and challenges. One of these problems is how to
adequately evaluated the quality of the mobile learning applications developed. In fact,
existing methods for evaluating software quality are still very generic, not considering
aspects specific to the pedagogical and mobile contexts. In this scenario, the present
work presents the MoLEva method, developed to evaluate the quality of mobile learning
applications. The model is based on the McCall model, Boehm model, an ISO / IEC
9126 standard and an ISO / IEC 25010 standard, being composed of: (i) quality model;
(ii) metrics; and (iii) criteria of judgment. To validate the method, two case studies were
performed; the first consisted of applying MoLEva to evaluate the ENEM application; the
second consisted of applying the method for evaluating applications for language teaching.
From the obtained results, it was possible to identify problems and improvement points
in the evaluated applications. In addition, the case studies conducted provided good
indications regarding the feasibility of using the MoLEva method in evaluating mobile
learning applications.",Versão de defesa - Gustavo Willians Soad.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,GUSTAVO WILLIANS SOAD,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),21/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Mobile Learning;Mobile Learning Applications;Quality Evaluation;Software Quality',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ELLEN FRANCINE BARBOSA,147,Aprendizagem Móvel;Aplicativos Educacionais Móveis;Avaliação de Qualidade;Qualidade de Software,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Computação Aplicada à Educação,"Estudos indicam que a utilização de aplicativos educacionais móveis vêm crescendo continuamente,
possibilitando a alunos e professores maior flexibilidade e comodidade na
execução de atividades e práticas educacionais. Embora várias instituições já tenham
aderido à modalidade de aprendizagem móvel (m-learning), sua adoção ainda traz problemas
e desafios organizacionais, culturais e tecnológicos. Um destes problemas consiste em
como avaliar adequadamente a qualidade dos aplicativos educacionais desenvolvidos. De
fato, os métodos existentes para avaliação da qualidade de software ainda são muito genéricos,
não contemplando aspectos específicos aos contextos pedagógico e móvel. Nesse
cenário, o presente trabalho apresenta o método MoLEva, desenvolvido para avaliar a qualidade
de aplicativos educacionais móveis. O método tem como base o modelo de McCall,
modelo de Boehm, a norma ISO/IEC 9126 e a norma ISO/IEC 25010, sendo composto
por: (i) modelo de qualidade; (ii) métricas; e (iii) critérios de julgamento. Para validar
o método, foram realizados dois estudos de caso; o primeiro consistiu na aplicação do
MoLEva para avaliar o aplicativo do ENEM; o segundo consistiu na aplicação do método
para avaliação de aplicativos para o ensino de idiomas. A partir dos resultados obtidos,
foi possível identificar problemas e pontos de melhoria nos aplicativos avaliados. Além
disso, os estudos de caso conduzidos forneceram bons indicativos a respeito da viabilidade
de uso do método MoLEva na avaliação de aplicativo educacionais móveis.",DISSERTAÇÃO,Avaliação de qualidade em aplicativos educacionais móveis,5022396,1
"This work applies complex network theory to the problem of semi-supervised and unsupervised
learning in networks that are representations of multivariate datasets. Complex
networks allow the use of nonlinear dynamical systems to represent behaviors according
to the connectivity patterns of networks. Inspired by behavior observed in nature, such as
competition for limited resources, dynamical system models can be employed to uncover
the organizational structure of a network. In this dissertation, we develop a technique for
classifying data represented as interaction networks. As part of the technique, we model
a dynamical system inspired by the biological dynamics of resource competition. So far,
similar methods have focused on vertices as the resource of competition. We introduce
edges as the resource of competition. In doing so, the connectivity pattern of a network
might be used not only in the dynamical system simulation but in the learning task as well.",Versão de defesa - Paulo Roberto Urio.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,PAULO ROBERTO URIO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),12/06/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Semi-supervised learning;Complex networks;Machine learning;Data clustering;Community detection',INTELIGÊNCIA COMPUTACIONAL,ZHAO LIANG,75,Aprendizado semissupervisionado;Redes complexas;Aprendizado de máquina;Agrupamento de dados;Detecção de comunidades,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Modelagem de Sistemas Complexos para Análise e Reconhecimento de Padrões,"Este trabalho aplica a teoria de redes complexas para o estudo de uma técnica aplicada
ao problema de aprendizado semissupervisionado e não-supervisionado em redes, especificamente,
aquelas que representam conjuntos de dados multivariados. Redes complexas
permitem o emprego de sistemas dinâmicos não-lineares que podem apresentar comportamentos
de acordo com os padrões de conectividade de redes. Inspirado pelos comportamentos
observados na natureza, tais como a competição por recursos limitados, sistema
dinâmicos podem ser utilizados para revelar a estrutura da organização de uma rede.
Nesta dissertação, desenvolve-se uma técnica aplicada ao problema de classificação de
dados representados por redes de interação. Como parte da técnica, um sistema dinâmico
inspirado na competição por recursos foi modelado. Métodos similares concentraram-se
em vértices como o recurso da concorrência. Neste trabalho, introduziu-se arestas como
o recurso-alvo da competição. Ao fazê-lo, utilizar-se-á o padrão de conectividade de uma
rede tanto na simulação do sistema dinâmico, quanto na tarefa de aprendizado.",DISSERTAÇÃO,Desdobramento de componentes de redes complexas usando técnica de competição de partículas,5022402,1
"Distributed Systems (DSs) have an increasing complexity and do not have their management,
besides having a quality of service (QoS) to its users. Autonomic Computing (AC) emerges as a
way of transforming the SDs into Autonomous Distributed Systems (ADSs), with a capacity for
self-management. However, your software development process is focused on creating SDAs.
In the vast majority of related works, simply an SD model, along with what aspect of the AC
implement, a technique used and the results obtained. This is only a part of the development of an
ADS, not approaching from an definition of requirements for a maintenance of software. More
importantly, it does not show how such requirements can be formalized and subsequently solved
through the self-management provided by AC. This proposal aims at a software development
process for the DASs. To this end, different areas of knowledge were integrated, including:
Unified Software Development Process (PU), SDs, CA, Operations Research (OR) and Computer
Systems Performance Evaluation (CSPE). The proof of concept was made through three case
studies, all focusing on NP-Hard problems, namely: (i) off-line optimization (problem of the
backpack with multiple choices), (ii) (Problem of the backpack with multiple choices) and (iii)
creation of the scheduling module of an autonomic manager, aiming to carry out the scheduling
of requests (problem of generalized assignment). The results of the first case study show that it
is possible to use OR and CSPE to define a base architecture for the DAS in question, as well as
reduce the size of the search space when SDA is running. The second, proves that it is possible
to guarantee the QoS of the DAS during its execution, using the formalization provided by the
OR and its respective solution. The third, proves that it is possible to use the PO to formalize the
self-management problem, as well as the ADSC to evaluate different algorithms or architecture
models for the ADS.",Versão de defesa - Pedro Felipe do Prado.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,PEDRO FELIPE DO PRADO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),20/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Unified Process;Autonomic Computing;Performance Evaluation;Distributed Systems;Operations Research',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",MARCOS JOSE SANTANA,138,Processo de Desenvolvimento de Software;Computação Autonômica;Avaliação de Desempenho;Sistemas Distribuídos;Pesquisa Operacional,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Computação Paralela e Distribuída,"Os Sistemas Distribuídos (SDs) tem apresentado uma crescente complexidade no seu gerenciamento,
além de possuir a necessidade de garantir Qualidade de Serviço (QoS) aos seus usuários.
A Computação Autonômica (CA) surge como uma forma de transformar os SDs em Sistemas
Distribuídos Autonômicos (SDAs), com capacidade de auto-gerenciamento. Entretanto, não
foi encontrado um processo de desenvolvimento de software, focado na criação de SDAs. Na
grande maioria dos trabalhos relacionados, simplesmente é apresentado um SD, juntamente com
qual aspecto da CA deseja-se implementar, a técnica usada e os resultados obtidos. Isso é apenas
uma parte do desenvolvimento de um SDA, não abordando desde a definição dos requisitos
até a manutenção do software. Mais importante, não mostra como tais requisitos podem ser
formalizados e posteriormente solucionados por meio do auto-gerenciamento fornecido pela
CA. Esta tese foca na proposta de um processo de desenvolvimento de software voltado para
SDAs. Com esse objetivo, foram integradas diferentes áreas de conhecimento, compreendendo:
Processo Unificado de Desenvolvimento de Software (PU), SDs, CA, Pesquisa Operacional
(PO) e Avaliação de Desempenho de Sistemas Computacionais (ADSC). A prova de conceito
foi feita por meio de três estudos de caso, todos focando-se em problemas NP-Difícil, são
eles: (i) otimização off-line (problema da mochila com múltiplas escolhas), (ii) otimização
online (problema da mochila com múltiplas escolhas) e (iii) criação do módulo planejador de
um gerenciador autonômico, visando realizar o escalonamento de requisições (problema de
atribuição generalizado). Os resultados do primeiro estudo de caso, mostram que é possível usar
PO e ADSC para definir uma arquitetura de base para o SDA em questão, bem como reduzir
o tamanho do espaço de busca quando o SDA estiver em execução. O segundo, prova que é
possível garantir a QoS do SDA durante sua execução, usando a formalização fornecida pela PO
e sua respectiva solução. O terceiro, prova que é possível usar a PO para formalizar o problema
de auto-gerenciamento, bem como a ADSC para avaliar diferentes algoritmos ou modelos de
arquitetura para o SDA.",TESE,Um processo de desenvolvimento de software focado em sistemas distribuídos autonômicos,5022724,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5022739,
"UML is a graphical notation used for modeling object-oriented software systems in different
domains in computer science. Being simple to use, compared to other modeling techniques,
UML is widespread among software developers, both in academia and industry. Among its
advantages are: (i) the visual representation of the relationships between classes and entities, as
when using diagrams, UML facilitates understanding and visualization of relationships within
the modeled system; (ii) readability and usability without having to read the system code,
since a developer can understand which parts of the code are redundant or reusable; and (iii)
a planning tool, helping to define what needs to be done before the implementation actually
begins, as well as being able to produce code and reduce development time. However, the
UML also has disadvantages, such as: (i) ambiguity between different UML elements due to
overlapping diagrams; and (ii) lack of clear semantics, which generally causes the semantics
of the programming language to be adopted. To mitigate these disadvantages, researchers seek
to assign a formal semantics to the UML. This type of semantics is found in formal models,
where the modeled system is free of ambiguity and has a clear and precise semantics. On
the other hand, formal models are not simple to create and understand by developers. The
degree of formalism knowledge required to use such a model is high, which makes their use less
widespread, compared to UML non-formal graphical notation. Despite the researchers efforts,
in general the techniques that formalize the UML semantics has a problem that is forgotten:
although using the UML to model the system, the final artifact of these techniques is a formal
trace. Considering the common knowledge of a software developer, this trace makes it difficult
to analyze the problems encountered by model checkers and to correct them in the UML model.
In order to assist the developer in understanding the formal results (the trace above), this thesis
presents an approach based on Model-driven Architecture (MDA) capable of representing the
information of the formal results in the UML model. Through UML model transformations,
these representations, set using the approach, help the developer to visualize the execution flow
of the model checker within the UML model. Thus, we believe that the advantages obtained by
formalizing the UML semantics may be more widespread and used by developers, especially in
industry.",Versão de defesa - Vinicius Pereira.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,VINICIUS PEREIRA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),05/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),"b'MDA Approach;UML;Formal Semantics;Model Checker, Traceability'",ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,MARCIO EDUARDO DELAMARO,197,Abordagem MDA;UML;Semântica Formal;Model Checker;Rastreabilidade,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Apoio à sistematização e automatização da atividade de teste de software,"UML é uma notação gráfica utilizada na modelagem de sistemas orientados a objetos, em
diferentes domínios da computação. Por ser simples de utilizar, em relação a outras formas de
modelagem, a UML é amplamente difundida entre os desenvolvedores de software, tanto na
academia quanto na indústria. Entre as suas vantagens, encontram-se: (i) a representação visual
das relações entre classes e entidades, pois ao se utilizar de diagramas, a UML facilita o entendimento
e a visualização das relações dentro do sistema modelado; (ii) a legibilidade e usabilidade,
sem que seja necessário a leitura do código do sistema, uma vez que um desenvolvedor pode
compreender quais partes do código são redundantes ou reutilizadas; e (iii) uma ferramenta de
planejamento, ao auxiliar na definição do que deve ser feito, antes que a implementação comece
de fato, além de poder produzir código e reduzir o tempo de desenvolvimento. Todavia, a UML
possui desvantagens, tais como: (i) ambiguidade entre elementos UML diferentes, devido a
sobreposição dos diagramas; e (ii) falta de uma semântica clara, o qual geralmente faz com que a
semântica da linguagem de programação seja adotada. Para mitigar essas desvantagens, pesquisadores
buscam atribuir uma semântica formal à UML. Esse tipo de semântica é encontrado em
modelos formais, onde o sistema modelado é livre de ambiguidades e possui uma semântica clara
e precisa. Por sua vez, os modelos formais não são simples de serem criados e compreendidos
por desenvolvedores. O grau de conhecimento em formalismo necessário para utilizar tal modelo
é alto, o que faz com que seu uso seja menos difundido, comparado com a notação gráfica não
formal da UML. Apesar dos esforços dos pesquisadores, as técnicas de formalização semântica
da UML apresentam, no geral, um problema pouco abordado: apesar de utilizar a UML para
modelar o sistema, o artefato final dessas técnicas é um trace formal. Considerando o conhecimento
comum de um desenvolvedor de software, esse trace dificulta a análise dos problemas,
encontrados pelos model checkers, e a correção dos mesmos no modelo UML. Com o objetivo
de auxiliar o desenvolvedor na compreensão dos resultados formais (o trace citado), esta tese
de doutorado apresenta uma abordagem baseada em Model-driven Architecture (MDA) capaz
de representar as informações dos resultados formais dentro de um modelo UML. Por meio
de transformações do modelo UML, essas representações, definidas utilizando a abordagem,
auxiliam o desenvolvedor a visualizar o fluxo de execução do model checker dentro do modelo
UML. Assim, acredita-se que as vantagens obtidas pela formalização semântica da UML podem
ser mais difundidas e utilizadas pelos desenvolvedores, principalmente na indústria.",TESE,Uma abordagem para representação de resultados formais na UML,5022763,1
"The validation, verification and test activities contribute to improve the quality of the programs, independently of the used programing paradigm. Erlang is an example of functional language and aspects such as: immutable data, higher-order functions, lazy evaluation e pattern matching impose restrictions to the software structural testing activity, which require a special attention by the tester. The Erlang language was created for the development of concurrent applications, in real time and with fault tolerance. The application of the software testing activity becomes necessary for applications developed in Erlang. An executed systematic mapping identified the related works and the identification of typical failures found in Erlang programs. The result of the mapping highlighted the lack of proposals that consider the main features of Erlang, including a testing tool that supports the practical application of the testing activity. This gap was considered significant. A set of structural testing criteria was proposed to verify the coverage of codes in sequential and distributed applications in Erlang programs. The test criteria explore the possible failure related to the communication between functions, communication between processes, synchronization, concurrence, recursion and fault tolerance. The definition of the criteria had the support of a test model to obtain information about control flux, data flux, and communication flux of Erlang programs. The model established 15 types of nodes, 9 types of edges and 5 types of variable uses. To support the application of criteria, a testing tool called ValiErlang was also implemented. ValiErlang is composed of 5 modules that execute the following stages: static analysis, source code instrumentation, CFG generation, definition of the required elements, instrumented code execution, execution trace generation and evaluation of the covered and non-executable elements. An experimental study was executed to verify the applicability of the testing criteria by ValiErlang. In this study six programs with different characteristics were used. All characteristics were essential to the Erlang programs. Based on the obtained results, it was possible to prove that the approach contributes with the tester because of the success in the criteria application and the efficiency of the criteria in revealing defects. The stages for the testes to execute the structural test application have the support of the Erlang tool.",Versão de defesa - Alexandre Ponce de Oliveira.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,ALEXANDRE PONCE DE OLIVEIRA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),27/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Functional language;Erlang language;structural testing;test model;testing criteria;ValiErlang',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",PAULO SERGIO LOPES DE SOUZA,143,Linguagem funcional;linguagem Erlang;teste estrutural;modelo de teste;critérios de teste;ValiErlang,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Teste Paralelo de Programas Concorrentes,"As atividades de validação, verificação e teste contribuem para melhorar a qualidade dos programas, independentemente do paradigma de programação utilizado. Erlang é um exemplo de linguagem funcional, e aspectos como: dados imutáveis, higher-order functions, lazy evaluation e pattern matching impõem restrições à atividade de teste estrutural de software, as quais requerem uma atenção especial do testador. A linguagem Erlang foi criada para o desenvolvimento de aplicações concorrentes, em tempo real e com tolerância a falhas. A aplicação da atividade de teste de software torna-se necessária para aplicações desenvolvidas em Erlang. Um mapeamento sistemático realizado identificou os trabalhos relacionados e também a identificação de falhas típicas encontradas em programas Erlang. O resultado do mapeamento evidenciou a falta de propostas que considerem as principais características de Erlang, incluindo uma ferramenta de teste que dê suporte à aplicação prática da atividade de teste. Esta lacuna foi considerada significativa. Foi proposto um conjunto de critérios de teste estruturais para verificar a cobertura de códigos em aplicações sequenciais e distribuídas de programas Erlang. Os critérios de teste exploram as possíveis falhas relacionadas à comunicação entre funções, comunicação entre processos, sincronização, concorrência, recursividade e tolerância a falhas. A definição dos critérios contou com o suporte de um modelo de teste para a obtenção das informações sobre o fluxo de controle, fluxo de dados e fluxo de comunicação de programas Erlang. O modelo estabeleceu 15 tipos de nós, 9 tipos de arestas e 5 tipos de usos de variáveis. Para apoiar a aplicação dos critérios, uma ferramenta de teste, chamada Valierlang, também foi implementada. A ValiErlang é composta por 5 módulos que realizam as seguintes etapas: análise estática, instrumentação do código fonte, geração do GFC, definição dos elementos requeridos, execução do código instrumentado, geração do rastro de execução e avaliação dos elementos cobertos e elementos não executáveis. Foi realizado um estudo experimental para verificar a aplicabilidade dos critérios de teste por meio da ValiErlang. Neste estudo foram utilizados seis programas com diferentes características, todas essenciais aos programas em Erlang. Com base nos resultados obtidos, foi possível comprovar que a abordagem contribui com o testador devido ao sucesso na aplicação dos critérios e também a eficácia dos critérios em revelar defeitos. E as etapas para o testador de software realizar a aplicação do teste estrutural, tem o apoio ferramental da ValiErlang.",TESE,Teste estrutural para aplicações concorrentes em Erlang,5033873,1
"Within the class of cutting and packing problems, there are some problems known as nesting
problems, which aim to determine an optimal arrangement of smaller irregular objects (items),
without overlap, inside larger objects (bins) in order to attend a demand. They are of great
practical importance, since they arise in many types of industries, such as textiles, furniture and
footwear, for example. Among these problems, we still have the so-called irregular bin packing
problem in which the bins are closed, that is, they have fixed dimensions, and may be rectangular
or irregular. In this case, the goal is to arrange all items in order to use the least amount of
bins. To these problems, another constraint can still be added: the bins may have defects, that is,
areas where no item can be placed, and different levels of quality, called quality zones, where
only specific items can be allocated. In this work, therefore, we introduce a set of constructive
heuristics to solve the irregular bin packing problem in which the bins have defects and quality
zones. The computational experiments, carried out using a set of 15 instances, showed that all
methods can solve the problem in a low computational time, and also that some of them perform
better than others.",Versão de defesa- Felipe Augusto Aureliano.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,FELIPE AUGUSTO AURELIANO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),30/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'cutting and packing problems;nesting problems;heuristics',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,MARINA ANDRETTA,87,problemas de corte e empacotamento;problemas de corte de itens irregulares;heurísticas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Dentro da classe de problemas de corte e empacotamento, existem os problemas de corte de
itens irregulares (não-circulares e não-retangulares), os quais visam determinar um arranjo ótimo
de objetos irregulares menores (itens), sem sobreposição, dentro de objetos maiores (recipientes)
a fim de atender a uma demanda. Possuem grande importância prática, uma vez que surgem em
vários tipos de indústrias, como a têxtil, a de móveis e a de calçados, por exemplo. Entre estes
problemas, ainda temos o chamado problema de corte de itens irregulares em recipientes, no
qual estes últimos são fechados, isto é, possuem dimensões fixas, podendo ser retangulares ou
irregulares. Neste caso, o objetivo é arranjar todos os itens de modo a utilizar o menor número
possível de recipientes. A estes problemas, uma outra restrição ainda pode ser adicionada: os
recipientes podem ter defeitos, isto é, áreas onde não pode ser posicionado qualquer item, e
regiões com diferentes níveis de qualidade, chamadas de zonas de qualidades, em que apenas
determinados itens podem ser alocados. Neste trabalho, portanto, introduzimos um conjunto de
heurísticas construtivas para a resolução do problema de corte de itens irregulares em recipientes
irregulares com defeitos e zonas de qualidades. Os experimentos computacionais, realizados
utilizando um conjunto com 15 instâncias, mostraram que todos os métodos são capazes de
resolver o problema em um tempo computacional considerado baixo, sendo que alguns deles
apresentam melhor desempenho que outros.",DISSERTAÇÃO,Estudo de métodos de solução para problemas de corte de itens irregulares em recipientes irregulares,5034074,1
"In this doctoral thesis, we propose new iterative methods for solving a class of convex
optimization problems. In general, we consider problems in which the objective
function is composed of a finite sum of convex functions and the set of constraints is,
at least, convex and closed. The iterative methods we propose are basically designed
through the combination of incremental subgradient methods and string-averaging
algorithms. Furthermore, in order to obtain methods able to solve optimization
problems with many constraints (and possibly in high dimensions), generally given
by convex functions, our analysis includes an operator that calculates approximate
projections onto the feasible set, instead of the Euclidean projection. This feature is
employed in the two methods we propose; one deterministic and the other stochastic.
A convergence analysis is proposed for both methods and numerical experiments are
performed in order to verify their applicability, especially in large scale problems.",Versão de defesa - Rafael Massambone de Oliveira.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,RAFAEL MASSAMBONE DE OLIVEIRA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),12/07/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Incremental subgradient methods;string-averaging algorithms;convex optimization;stochastic optimization',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,ELIAS SALOMAO HELOU NETO,115,Métodos de subgradientes incrementais;algoritmo de média das sequências;otimização convexa;otimização estocástica,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Nesta tese de doutorado, propomos novos métodos iterativos para a solução de uma
classe de problemas de otimização convexa. Em geral, consideramos problemas nos
quais a função objetivo é composta por uma soma finita de funções convexas e o
conjunto de restrições é, pelo menos, convexo e fechado. Os métodos iterativos que
propomos são criados, basicamente, através da junção de métodos de subgradientes
incrementais e do algoritmo de média das sequências. Além disso, visando obter
métodos flexíveis para soluções de problemas de otimização com muitas restrições
(e possivelmente em altas dimensões), dadas em geral por funções convexas, a nossa
análise inclui um operador que calcula projeções aproximadas sobre o conjunto viável,
no lugar da projeção Euclideana. Essa característica é empregada nos dois métodos
que propomos; um determinístico e o outro estocástico. Uma análise de convergência
é proposta para ambos os métodos e experimentos numéricos são realizados a fim de
verificar a sua aplicabilidade, principalmente em problemas de grande escala.",TESE,Média das sequências e algoritmos de subgradientes incrementais para problemas de otimização convexa com restrições,5034075,1
"The demand for new technologies, enhanced security and comfort for urban cars has grown
considerably in recent years prompting the industry to create systems designed to support drivers
(ADAS - Advanced Driver Assistance Systems). This fact contributed to the development of
many embedded systems in the automotive area among them, the pedestrians collision avoidance.
Through the advancement in various research, began circulating through the streets vehicles with
anti-collision systems and autonomous navigation. However, to achieve ever more challenging
goals, designers need tools to unite technology and expertise from different areas efficiently. In
this context, there is a demand for building systems that increase the level of abstraction of models
of image processing for use in embedded systems enabling better design space exploration.
To help minimize this problem, this research demonstrates a develop a specific framework for
hardware/software codesign to build ADAS systems using computer vision. The framework
aims to facilitate the development of applications, allowing better explore the design space, and
thus contribute to a performance gain in the development of embedded systems in relation to
building entirely in hardware. One of the requirements of the project is the possibility of the
simulation of an application before synthesis on a reconfigurable system. The main challenges
of this system were related to the construction of the intercommunication system between the
various Intellectual Property (IP) blocks and the software components, abstracting from the
end user numerous hardware details, such as memory management, interruptions, cache, types
(Floating point, fixed point, integers) and so on, enabling a more user-friendly system for the
designer.",Versão de defesa- Leandro Andrade Martinez.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,LEANDRO ANDRADE MARTINEZ,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),30/06/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Codesign;ADAS;Embedded Systems;Hardware',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",EDUARDO MARQUES,117,CoProjeto;ADAS;Sistemas Embarcados;Hardware,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),-,"A demanda por novas tecnologias, melhoria de segurança e conforto para veículos urbanos
cresceu consideravelmente nos últimos anos, motivando a indústria na criação de sistemas
destinados ao apoio de motoristas (ADAS - Advanced Driver Assistance Systems). Este fato
contribuiu para o desenvolvimento de diversos sistemas embarcados na área automobilística
dentre elas, à prevenção de colisão a pedestres com outros veículos. Através do avanço em
diversas pesquisas, começaram a circular pelas ruas veículos com sistemas anticolisão e com
navegação autônoma. Contudo, para alcançar objetivos cada vez mais desafiadores, os projetistas
precisam de ferramentas que permitam unir tecnologias e conhecimentos de áreas distintas de
forma eficiente. Nesse contexto, há uma demanda para a construção de sistemas que aumentem
o nível de abstração da modelagem de projetos para o processamento de imagens em sistemas
embarcados e assim, possibilitando uma melhor exploração do espaço de projetos.
A fim de contribuir para minimizar este problema, este trabalho de pesquisa demonstra o
desenvolvimento de um framework para coprojeto de hardware e software específico para a
construção de sistemas ADAS que utilizam visão computacional. O Framework visa facilitar o
desenvolvimento dessas aplicações permitindo a exploração o espaço de projeto (DSE - Design
Space Exploration), e assim contribuindo para um ganho de desempenho no desenvolvimento de
sistemas embarcados quando comparados à construção totalmente de um modo manual. Um das
características deste projeto é a possibilidade da simulação da aplicação antes da síntese em um
sistema reconfigurável. Os principais desafios deste sistema foram relacionados à construção
do sistema de intercomunicação entre os diversos blocos de Propriedade Intelectual (IP) e os
componentes de software, abstraindo do usuário final inúmeros detalhes de hardware, tais como
gerenciamento de memória, interrupções, cache, tipos de dados (ponto flutuante, ponto fixo,
inteiros) e etc, possibilitando um sistema mais amigável ao projetista.",TESE,Um framework para coprojeto de hardware e software de sistemas avançados de assistência ao motorista baseados em câmeras,5034095,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5034096,
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_1.html,,,,,,,,,,,5040855,
"Graph-based semi-supervised learning (SSL) algorithms have been widely studied in the last
few years. Most of these algorithms were designed from unconstrained optimization problems
using a Laplacian regularizer term as smoothness functional in an attempt to reflect the intrinsic
geometric structure of the data’s marginal distribution. Although a number of recent research
papers are still focusing on unconstrained methods for graph-based SSL, a recent statistical
analysis showed that many of these algorithms may be unstable on transductive regression.
Therefore, we focus on providing new constrained methods for graph-based SSL. We begin by
analyzing the regularization framework of existing unconstrained methods. Then, we incorporate
two normalization constraints into the optimization problem of three of these methods. We
show that the proposed optimization problems have closed-form solution. By generalizing
one of these constraints to any distribution, we provide generalized methods for constrained
graph-based SSL. The proposed methods have a more flexible regularization framework than the
corresponding unconstrained methods. More precisely, our methods can deal with any graph
Laplacian and use higher order regularization, which is effective on general SSL taks. In order
to show the effectiveness of the proposed methods, we provide comprehensive experimental
analyses. Specifically, our experiments are subdivided into two parts. In the first part, we evaluate
existing graph-based SSL algorithms on time series data to find their weaknesses. In the second
part, we evaluate the proposed constrained methods against six state-of-the-art graph-based SSL
algorithms on benchmark data sets. Since the widely used best case analysis may hide useful
information concerning the SSL algorithms’ performance with respect to parameter selection,
we used recently proposed empirical evaluation models to evaluate our results. Our results show
that our methods outperforms the competing methods on most parameter settings and graph
construction methods. However, we found a few experimental settings in which our methods
showed poor performance. In order to facilitate the reproduction of our results, the source codes,
data sets, and experimental results are freely available.",Versão Revisada - Celso Andre Rodrigues de Sousa.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,CELSO ANDRE RODRIGUES DE SOUSA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),10/08/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'semi-supervised learning;graph-based methods;constrained optimization;higher order regularization',INTELIGÊNCIA COMPUTACIONAL,GUSTAVO ENRIQUE DE ALMEIDA PRADO ALVES BATISTA,158,aprendizado semissupervisionado;métodos baseados em grafos;otimização restrita;regularização de ordem elevada,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Aprendizado semissupervisionado em grafos em grandes bases de dados,"Algoritmos de aprendizado semissupervisionado baseado em grafos foram amplamente estudados
nos últimos anos. A maioria desses algoritmos foi projetada a partir de problemas de otimização
sem restrições usando um termo regularizador Laplaciano como funcional de suavidade numa
tentativa de refletir a estrutura geométrica intrínsica da distribuição marginal dos dados. Apesar
de vários artigos científicos recentes continuarem focando em métodos sem restrição para
aprendizado semissupervisionado em grafos, uma análise estatística recente mostrou que muitos
desses algoritmos podem ser instáveis em regressão transdutiva. Logo, nós focamos em propor
novos métodos com restrições para aprendizado semissupervisionado em grafos. Nós começamos
analisando o framework de regularização de métodos sem restrições existentes. Então,
nós incorporamos duas restrições de normalização no problema de otimização de três desses
métodos. Mostramos que os problemas de otimização propostos possuem solução de forma
fechada. Ao generalizar uma dessas restrições para qualquer distribuição, provemos métodos
generalizados para aprendizado semissupervisionado restrito baseado em grafos. Os métodos
propostos possuem um framework de regularização mais flexível que os métodos sem restrições
correspondentes. Mais precisamente, nossos métodos podem lidar com qualquer Laplaciano
em grafos e usar regularização de ordem elevada, a qual é efetiva em tarefas de aprendizado
semissupervisionado em geral. Para mostrar a efetividade dos métodos propostos, nós provemos
análises experimentais robustas. Especificamente, nossos experimentos são subdivididos em
duas partes. Na primeira parte, avaliamos algoritmos de aprendizado semissupervisionado
em grafos existentes em dados de séries temporais para encontrar possíveis fraquezas desses
métodos. Na segunda parte, avaliamos os métodos restritos propostos contra seis algoritmos de
aprendizado semissupervisionado baseado em grafos do estado da arte em conjuntos de dados
benchmark. Como a amplamente usada análise de melhor caso pode esconder informações
relevantes sobre o desempenho dos algoritmos de aprendizado semissupervisionado com respeito
à seleção de parâmetros, nós usamos modelos de avaliação empírica recentemente propostos
para avaliar os nossos resultados. Nossos resultados mostram que os nossos métodos superam
os demais métodos na maioria das configurações de parâmetro e métodos de construção de
grafos. Entretanto, encontramos algumas configurações experimentais nas quais nossos métodos
mostraram baixo desempenho. Para facilitar a reprodução dos nossos resultados, os códigos
fonte, conjuntos de dados e resultados experimentais estão disponíveis gratuitamente.",TESE,Aprendizado semissupervisionado restrito baseado em grafos com regularização de ordem elevada,5046988,1
"Several industrial, scientific and commercial processes produce open-ended sequences of observations which are
referred to as data streams. We can understand the phenomena responsible for such streams by analyzing data
in terms of their inherent recurrences and behavior changes. Recurrences support the inference of more stable
models, which are deprecated by behavior changes though. External influences are regarded as the main agent
actuacting on the underlying phenomena to produce such modifications along time, such as new investments and
market polices impacting on stocks, the human intervention on climate, etc. In the context of Machine Learning,
there is a vast research branch interested in investigating the detection of such behavior changes which are also
referred to as concept drifts. By detecting drifts, one can indicate the best moments to update modeling, therefore
improving prediction results, the understanding and eventually the controlling of other influences governing
the data stream. There are two main concept drift detection paradigms: the first based on supervised, and the
second on unsupervised learning algorithms. The former faces great issues due to the labeling infeasibility when
streams are produced at high frequencies and large volumes. The latter lacks in terms of theoretical foundations
to provide detection guarantees. In addition, both paradigms do not adequately represent temporal dependencies
among data observations. In this context, we introduce a novel approach to detect concept drifts by tackling two
deficiencies of both paradigms: i) the instability involved in data modeling, and ii) the lack of time dependency
representation. Our unsupervised approach is motivated by Carlsson and Memoli’s theoretical framework which
ensures a stability property for hierarchical clustering algorithms regarding to data permutation. To take full
advantage of such framework, we employed Takens’ embedding theorem to make data statistically independent
after being mapped to phase spaces. Independent data were then grouped using the Permutation-Invariant
Single-Linkage Clustering Algorithm (PISL), an adapted version of the agglomerative algorithm Single-Linkage,
respecting the stability property proposed by Carlsson and Memoli. Our algorithm outputs dendrograms (seen as
data models), which are proven to be equivalent to ultrametric spaces, therefore the detection of concept drifts is
possible by comparing consecutive ultrametric spaces using the Gromov-Hausdorff (GH) distance. As result,
model divergences are indeed associated to data changes. We performed two main experiments to compare our
approach to others from the literature, one considering abrupt and another with gradual and, therefore, more
“complex” changes. Results confirm our approach is capable of detecting concept drifts, both abrupt and gradual
ones, however it is more adequate to operate on “complex” scenarios. The main contributions of this thesis are:
i) the usage of Takens’ embedding theorem as tool to provide statistical independence to data streams; ii) the
implementation of PISL in conjunction with GH (called PISL–GH); iii) a comparison of detection algorithms
in different scenarios; and, finally, iv) an R package (called streamChaos) that provides tools for processing
nonlinear data streams as well as other algorithms to detect concept drifts.",Versão de defesa - Fausto Guzzo da Costa.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,FAUSTO GUZZO DA COSTA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),17/08/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),"b'Machine Learning;Data Streams;Concept Drift;Clustering, Nonlinear Time Series'",INTELIGÊNCIA COMPUTACIONAL,RODRIGO FERNANDES DE MELLO,102,Aprendizado de Máquina;Fluxos de Dados;Mudanças de Conceito;Agrupamento;Séries Temporais Não Lineares,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Employing nonlinear time series analysis tools with stable clustering algorithms for detecting concept drift on data streams,"Diversos processos industriais, científicos e comerciais produzem sequências de observações continuamente,
teoricamente infinitas, denominadas fluxos de dados. Pela análise das recorrências e das mudanças de comportamento
desses fluxos, é possível obter informações sobre o fenômeno que os produziu. A inferência de
modelos estáveis para tais fluxos é suportada pelo estudo das recorrências dos dados, enquanto é prejudicada
pelas mudanças de comportamento. Essas mudanças são produzidas principalmente por influências externas
ainda desconhecidas pelos modelos vigentes, tal como ocorre quando novas estratégias de investimento surgem
na bolsa de valores, ou quando há intervenções humanas no clima, etc. No contexto de Aprendizado de Máquina
(AM), várias pesquisas têm sido realizadas para investigar essas variações nos fluxos de dados, referidas como
mudanças de conceito. Sua detecção permite que os modelos possam ser atualizados a fim de apurar a predição,
a compreensão e, eventualmente, controlar as influências que governam o fluxo de dados em estudo. Nesse
cenário, algoritmos supervisionados sofrem com a limitação para rotular os dados quando esses são gerados
em alta frequência e grandes volumes, e algoritmos não supervisionados carecem de fundamentação teórica
para prover garantias na detecção de mudanças. Além disso, algoritmos de ambos paradigmas não representam
adequadamente as dependências temporais entre observações dos fluxos. Nesse contexto, esta tese de doutorado
introduz uma nova metodologia para detectar mudanças de conceito, na qual duas deficiências de ambos paradigmas
de AM são confrontados: i) a instabilidade envolvida na modelagem dos dados, e ii) a representação das
dependências temporais. Essa metodologia é motivada pelo arcabouço teórico de Carlsson e Memoli, que
provê uma propriedade de estabilidade para algoritmos de agrupamento hierárquico com relação à permutação
dos dados. Para usufruir desse arcabouço, as observações são embutidas pelo teorema de imersão de Takens,
transformando-as em independentes. Esses dados são então agrupados pelo algoritmo Single-Linkage Invariante
à Permutação (PISL), o qual respeita a propriedade de estabilidade de Carlsson e Memoli. A partir dos dados de
entrada, esse algoritmo gera dendrogramas (ou modelos), que são equivalentes a espaços ultramétricos. Modelos
sucessivos são comparados pela distância de Gromov-Hausdorff a fim de detectar mudanças de conceito no fluxo.
Como resultado, as divergências dos modelos são de fato associadas a mudanças nos dados. Experimentos foram
realizados, um considerando mudanças abruptas e o outro mudanças graduais e, portanto, mais “complexas”. Os
resultados confirmam que a metodologia proposta é capaz de detectar mudanças de conceito, tanto abruptas
quanto graduais, no entanto ela é mais adequada para cenários mais “complexos”. As contribuições principais
desta tese são: i) o uso do teorema de imersão de Takens para transformar os dados de entrada em independentes;
ii) a implementação do algoritmo PISL em combinação com a distância de Gromov-Hausdorff (chamado PISL–
GH); iii) a comparação da metodologia proposta com outras da literatura em diferentes cenários; e, finalmente,
iv) a disponibilização de um pacote em R (chamado streamChaos) que provê tanto ferramentas para processar
fluxos de dados não lineares quanto diversos algoritmos para detectar mudanças de conceito.",TESE,Aplicando ferramentas de análise de séries temporais não lineares e algoritmos de agrupamento estáveis para a detecção de mudanças de conceito em fluxos de dados,5047150,1
"Natural disasters have been increasing intensely all around the globe. The consequences
of these disasters are significantly amplified when they occur in urban areas or places
where there are human activities due to loss of lives and assets. The usage of Wireless
Sensor Networks (WSN) for data collection and Machine Learning (ML) to create natural
disasters forecast models are viable options. However, new technology trends have been
showing promising results, which can aggregate to the tasks of environmental monitoring
and natural disasters forecast. One of these new trends is to adopt IP based sensor networks
and to use emergent Internet of Things (IoT) standards. In this context, this Thesis
presents and analyzes an approach called SENDI (System for dEtecting and forecasting
Natural Disasters based on IoT), a fault-tolerant system based on IoT, ML and WSN to
detect and forecast natural disasters. SENDI was modelled using ns-3 and validated by
means of real data collected by a WSN installed in São Carlos - Brazil, which collects the
data of rivers around the region. This system also foresees the possibility of communication
failures and loss of nodes during disasters, also adding intelligence to the nodes in
order to perform the distribution of data and forecasts, even in such cases. This Thesis
presents a case study about flash flooding forecast as well, which uses the system model
and the data collected by the WSN. The results of the experiments show that SENDI
allows generate warnings in time to make decisions as such predictions can be foreseen
even if partial failure of the system occurs. However, there is a variable accuracy, which
depends on the system degradation.",Versão de defesa - Gustavo Antonio Furquim.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,GUSTAVO ANTONIO FURQUIM,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),11/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Natural Disasters Forecast;Wireless Sensor Network;Internet of Things;Machine Learning;Fault-tolerance',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",JO UEYAMA,119,Previsão de Desastres Naturais;Rede de Sensores sem Fio;Internet das Coisas;Aprendizado de Máquina;Tolerância a Falha,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Explorando a Abordagem Sensor Web e o Sensoriamento Participatório no Monitoramento de Rios Urbanos,"O aumento na quantidade e na intensidade de desastres naturais é um problema que está
se agravando em todo o mundo. As consequências desses desastres são significantemente
ampliadas quando ocorrem em regiões urbanas ou com atuação humana devido à perda
de vidas e à quantidade de bens materiais afetados. O uso de redes de sensores sem fio
para a coleta de dados e o uso de técnicas de aprendizado de máquina para a previsão
de desastres naturais são opções viáveis, porém novas tendências tecnológicas têm se
mostrado promissoras e podem agregar na tarefa de monitoramento de ambientes e na
previsão de desastres naturais. Uma dessas tendências é adotar redes de sensores baseadas
em IP e utilizar padrões emergentes para IoT. Nesse contexto, esta Tese propõe e analisa
uma abordagem chamada SENDI (System for dEtecting and forecasting Natural Disasters
based on IoT), um sistema tolerante a falhas baseado em IoT, WSN e AM para a detecção
e a previsão de desastres naturais. O SENDI foi modelado empregando o ns-3 e validado
utilizando dados coletados por uma WSN real instalada na cidade de São Carlos - Brasil, a
qual realiza a coleta de dados de rios da região. Esse sistema também prevê a possibilidade
de falhas na comunicação e a perda de nós durante a ocorrência de desastres, além de
agregar inteligência aos nós para realizar a distribuição de dados e de previsões, mesmo
nesses casos. Esta Tese também apresenta um estudo de caso sobre previsão de enchentes
que utiliza a modelagem do sistema e os dados colhidos pela WSN. Os resultados dos
experimentos mostram que o SENDI permite gerar alertas para a tomada de decisões em
tempo hábil, realizando as previsões mesmo com falhas parciais no sistema, porém com
acurácia variável dependendo do nível de degradação do mesmo.",TESE,Uma abordagem tolerante a falhas para a previsão de desastres naturais baseada em IoT e aprendizado de máquina,5047599,1
"Online social networks (OSNs) are Web platforms providing different services to facilitate social
interaction among their users. A particular kind of OSNs are the location-based social networks
(LBSNs), which add services based on location. One of the most important challenges in LBSNs
is the link prediction problem. Link prediction problem aims to estimate the likelihood of
the existence of future friendships among user pairs. Most of the existing researches in link
prediction focus on the use of a single information source to perform predictions, i.e. only
social information (e.g. social neighborhood) or only location information (e.g. common visited
places). However, some researches have shown that the combination of different information
sources can lead to more accurate predictions. In this sense, in this thesis we propose different
link prediction methods based on the use of different information sources naturally existing in
these networks. Thus, we propose seven new link prediction methods using the information
related to user membership in social overlapping groups: common neighbors within and outside
of common groups (WOCG), common neighbors of groups (CNG), common neighbors with total
and partial overlapping of groups (TPOG), group naïve Bayes (GNB), group naïve Bayes of
common neighbors (GNB-CN), group naïve Bayes of Adamic-Adar (GNB-AA), e group naïve
Bayes of Resource Allocation (GNB-RA). Due to that social groups exist not only in LBSNs, our
proposals can be used in any type of OSN. We also propose new eight link prediction methods
combining location and social information: Check-in Observation (ChO), Check-in Allocation
(ChA), Within and Outside of Common Places (WOCP), Common Neighbors of Places (CNP),
Total and Partial Overlapping of Places (TPOP), Friend Allocation Within Common Places
(FAW), Common Neighbors of Nearby Places (CNNP), e Nearby Distance Allocation (NDA).
These eight methods are exclusively for use in LBSNs. Obtained results indicate that our
proposals are as competitive as state-of-the-art methods, or better than they in certain scenarios.
Moreover, since our proposals are computationally more efficient, they are more suitable for
real-world applications.",Versão de defesa - Jorge Carlos Valverde Rebaza.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,JORGE CARLOS VALVERDE REBAZA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),18/08/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Social Networks;Location-based Social Networks;Link Prediction;Friendship Recommendation;User Behavior Analysis',INTELIGÊNCIA COMPUTACIONAL,ALNEU DE ANDRADE LOPES,221,Redes Sociais;Redes Sociais baseadas em Localização;Predição de Links;Recomendação de Amizade;Análise do Comportamento de Usuários,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Mineração de Dados Proposicionais e de Dados Relacionais Baseados em Grafos,"Redes sociais online (OSNs) são plataformas Web que oferecem serviços para promoção da
interação social entre usuários. OSNs que adicionam serviços relacionados à geolocalização
são chamadas redes sociais baseadas em localização (LBSNs). Um dos maiores desafios na
análise de LBSNs é a predição de links. A predição de links refere-se ao problema de estimar
a probabilidade de conexão entre pares de usuários que não se conhecem. Grande parte das
pesquisas que focam nesse problema exploram o uso, de maneira isolada, de informações
sociais (e.g. amigos em comum) ou de localização (e.g. locais comuns visitados). Porém,
algumas pesquisas mostraram que a combinação de diferentes fontes de informação pode
significar o incremento da acurácia da predição. Motivado por essa lacuna, neste trabalho foram
desenvolvidos diferentes métodos para predição de links combinando eficientemente diferentes
fontes de informação. Assim, propomos sete métodos que usam a informação relacionada à
participação simultânea de usuários en múltiples grupos sociais: common neighbors within and
outside of common groups (WOCG), common neighbors of groups (CNG), common neighbors
with total and partial overlapping of groups (TPOG), group naïve Bayes (GNB), group naïve
Bayes of common neighbors (GNB-CN), group naïve Bayes of Adamic-Adar (GNB-AA), e
group naïve Bayes of Resource Allocation (GNB-RA). Devido ao fato que a presença de grupos
sociais não está limitada apenas às LBSNs, essas propostas podem ser usadas nas diversas OSNs
existentes. Também, propomos oito métodos que combinam o uso de informações sociais e de
localização: Check-in Observation (ChO), Check-in Allocation (ChA), Within and Outside of
Common Places (WOCP), Common Neighbors of Places (CNP), Total and Partial Overlapping
of Places (TPOP), Friend Allocation Within Common Places (FAW), Common Neighbors of
Nearby Places (CNNP), e Nearby Distance Allocation (NDA). Tais propostas são para uso
exclusivo em LBSNs. Os resultados obtidos indicam que nossas propostas são tão competitivas
quanto métodos do estado da arte, podendo até superá-los em determinados cenários. Ainda
mais, devido a que nossas propostas são computacionalmente mais eficientes, seu uso resulta
mais adequado em aplicações do mundo real.",TESE,Mineração do comportamento de usuários em redes sociais baseadas em localização,5047605,1
"Systems-of-Systems (SoS) represent an emerging research field in the Software Engineering area. In
particular, SoS refer to systems that make possible the interoperability of distributed, complex systems,
cooperating among them to reach a common mission. Several SoS have already been developed and
used, but there is no consensus about diverse terms and concepts in this field, what can difficult
communication among different stakeholders involved in the development and evolution of SoS, besides
lacking of a standardization and common understanding among researchers and practitioners. This
Master’s project established OntoSoS, an ontology to formalize terms and concepts in the SoS field,
expliciting and allowing sharing and reuse of knowledge contained in such ontology. As a result, this
project intends to contribute to the field of SoS, also supporting activities related to SoS Engineering. It
is also expected that this ontology can serve as a learning material in courses related to SoS..",,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,GABRIEL ABDALLA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),11/08/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Systems-of-Systems;Ontology;OntoSoS;Terminology',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ELISA YUMI NAKAGAWA,77,Sistemas-de-Sistemas;Ontologia;OntoSoS;Terminologia,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),-,"Sistemas-de-Sistemas (do inglês, Systems-of-Systems ou simplesmente SoS) representam um
emergente campo de pesquisa na Engenharia de Software. Em particular, SoS referem-se a sistemas
que possibilitam a interoperabilidade de sistemas complexos, distribuídos, cooperando entre si para
atingir uma missão comum. Diversos SoS têm sido desenvolvidos e utilizados, mas não há um consenso
sobre os diversos termos e conceitos nesse campo, o que pode dificultar a comunicação entre os
diferentes interessados envolvidos no desenvolvimento e evolução dos SoS, além da falta de
padronização e entendimento comum entre pesquisadores e profissionais. Este projeto de Mestrado
estabeleceu a OntoSoS, uma ontologia para formalizar termos e conceitos no campo de SoS,
explicitando e permitindo o compartilhamento e reúso do conhecimento contido na ontologia. Como
resultado, este projeto pretende contribuir para o campo de SoS, auxiliando também nas atividades
relacionadas à Engenharia de SoS. É também esperado que essa ontologia possa servir como um
material de ensino em cursos relacionados à Engenharia de SoS.",DISSERTAÇÃO,Estabelecimento de uma ontologia para Sistemas-de-Sistemas,5047613,1
"Cities are complex systems of transportation and social activity; their structure can be
used to model urban street networks — i.e. complex network that represents the geometry
of a city — allowing analytical activities for data-driven decision-making. The geometry
of a city holds intrinsic information that can support activities related to the analysis
of the urban scenario; of higher importance is the use of such information to enhance
the quality of life of its inhabitants and/or to understand the dynamics of an urban
center. Several of these analytical processes lacks in-depth methodologies to analyze
crime patterns and ill-designed urban structures, which can provide for public safety
and urban design. Consequently, it is our goal to provide means for the structural and
topological analysis of highly criminal regions of cities represented as complex networks,
and for the identification of urban planning inconsistencies that point to regions that lack
access from/to points of interest in a city. In this regard, we devised a set of algebraic
and algorithmic procedures that are capable of revealing patterns and provide for data
comprehension. More specifically, we introduced pre-processing techniques to transform
georeferenced electronic maps into graph representations of cities; we used metric-based
and epidemic processes to understand the dynamics of cities in what refers to criminality;
finally, we introduced a novel set of formalisms and operations based on set theory to
identify design flaws concerning access in urban centers. Our results refer to approaches
to preprocess and prepare maps in the form of urban street networks; to the analyses
of crimes based on their spatial disposition; to the development of a model to describe
criminal activities; and, to the advance of a concept based on critical problems in the
urban design.",,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,GABRIEL SPADON DE SOUZA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),31/07/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Complex Networks;Urban Systems;Collective Behavior;Pattern Recognition',BANCO DE DADOS/COMPUTAÇÃO GRÁFICA E PROCESSAMENTO DE IMAGENS,JOSE FERNANDO RODRIGUES JUNIOR,72,Redes Complexas;Sistemas Urbanos;Comportamento Coletivo;Reconhecimento de Padrões,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Processamento analítico de grandes grafos: identificação de padrões para o suporte à decisão na Web 2.0,"As cidades são sistemas complexos de interação social e de transporte. Suas estruturas
podem ser usadas para modelar redes de mobilidade urbana — i.e. redes complexas
que representam a geometria de uma cidade — permitindo a consecução de atividades
analíticas para descoberta de padrões e para a tomada de decisão baseada em dados. A
geometria da cidade carrega informações intrínsecas que auxiliam atividades relacionadas
à análise de dados provenientes do cenário urbano. As informações inerentes a tais análises
podem ser usadas para melhorar a qualidade de vida dos habitantes de uma região, ou
para entender a dinâmica de centros urbanos. Diversos processos analíticos aplicados a
tais cenários carecem de metodologias para analisar o padrão criminal e para identificar
estruturas urbanas mal planejadas. Deste modo, este trabalho tem por objetivo prover
meios para análise topológica de regiões criminais e para a identificação de inconsistências
urbanas, as quais apontam para regiões que carecem de mobilidade e acesso para outras
regiões de uma cidade. Neste sentido, foi desenvolvido um conjunto de procedimentos
algébricos e algorítmicos capazes de revelar padrões e meios para compreensão e análise
dos dados. Mais especificamente, foram desenvolvidos métodos de pré-processamento para
transformar mapas eletrônicos georreferenciados em grafos que representam cidades, foi
utilizado um conjunto métrico analítico e outro com base em processos epidêmicos para
entender a dinâmica intrínseca à criminalidade de uma cidade, e por fim, foi desenvolvido
um conjunto de formalismos e operações baseados em teoria dos conjuntos para identificar
falhas no desenho das estruturas urbanas que impactam no acesso viário em centros
urbanos. Os resultados deste trabalho versam sobre o desenvolvimento de novos métodos
para preparar mapas na forma de redes de mobilidade urbana; na análise de crimes
baseada em sua disposição espacial; no desenvolvimento de um modelo capaz de descrever
a atividade criminal de uma cidade; e, em um conceito baseado na análise de regiões
críticas identificadas a partir do desenho urbano.",DISSERTAÇÃO,Caracterização de padrões de mobilidade e comportamento coletivo por meio de processamento analítico de redes complexas do mundo real,5047616,1
"Irregular cutting and packing problems, with convex and non-convex polygons, are found in
various industries such as metal mechanics, textiles, of shoe making, the furniture making and
many others. In this thesis we study the two-dimensional version of these problems, where
we want to allocate a set of items, without overlap, inside one or more containers, limited or
unlimited, so as to optimize an objective function. This document presents a review of the
problems studied, the value proposition for this thesis with the main contributions and ideas. In
addition, we present five complete papers that were submitted to journals of excellence. The
papers present a brief literature review, a detailed description of the proposed method, a set of
computational experiments and descriptions of future directions. Finally, we present a concise
direction for future work.",Versão revisada - Leandro Resende Mundim.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,LEANDRO RESENDE MUNDIM,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),18/08/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Irregular Cutting and Packing Problems;Mathematical Programming;Heuristics',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,MARINA ANDRETTA,164,Problemas de Corte e Empacotamento de Itens Irregulares;Programação Matemática;Métodos Heurísticos,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Os problemas de corte e empacotamento de itens irregulares, polígonos convexos e não convexos,
são encontrado em diversas indústrias, tais como a metal-mecânica, a têxtil, a de calçados, a
moveleira e muitas outras. Nesta tese estudamos a versão bidimensional destes problemas,
na qual desejamos alocar um conjunto de itens, sem sobreposição, no interior de um ou mais
recipientes, limitados ou ilimitados, de modo a otimizar uma função objetivo. Este trabalho
apresenta uma revisão dos problemas estudados, a proposta de valor deste doutorado com as
principais contribuições e ideias. Além disso, apresentamos cinco artigos completos que foram
submetidos a revistas de excelência. Os artigos apresentam uma breve revisão bibliográfica,
uma descrição detalhada do método proposto, uma série de experimentos computacionais e uma
descrição das direções futuras. Finalmente, apresentamos uma sucinta direção para os trabalhos
futuros.",TESE,Modelos matemáticos e métodos heurísticos para os problemas de corte de itens irregulares,5047654,1
"This paper aims to present the thesis developed in the Doctoral Program in Computer
Science and Computational Mathematics of the ICMC / USP. The thesis theme seeks
to advance the state of the art by solving the problems of scalability and representation
present in mission planning algorithms for Unmanned Aerial Vehicle (UAV). Techniques
based on mathematical programming and evolutionary computation are proposed. Articles
have been published, submitted or they are in final stages of preparation. These studies
report the most significant advances in the representation and scalability of this problem.
Mission planners worked on the thesis deal with stochastic problems in non-convex
environments, where collision risks or failures in mission planning are treated and limited to
a tolerated value. The advances in the representation allowed to solve violations in the risks
present in the original literature modeling, besides making the models more realistic when
incorporating aspects such as effects of the air resistance. Efficient mathematical modeling
techniques allowed to advance from a Mixed Integer Nonlinear Programming (MINLP)
model, originally proposed in the literature, to a Mixed Integer Linear Programming
(MILP) problem. Modeling as a MILP led to problem solving more efficiently through the
branch-and-cut algorithm. The proposed new representations resulted in improvements
from scalability, solving more complex problems within a shorter computational time.
In addition, advances in scalability are even more effective when techniques combining
mathematical programming and metaheuristics have been applied to the problem.",Versão revisada - Márcio da Silva Arantes.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,MARCIO DA SILVA ARANTES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),11/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Mission Planning;Unmanned Aerial Vehicle;Integer-Mixed Linear Programming;Evolutionary Computation',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",CLAUDIO FABIANO MOTTA TOLEDO,157,Planejamento de Missão;Veículos Aéreos Não Tripulados;Programação Linear Inteira-Mista;Computação Evolutiva,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Eficiência em Soluções no Mundo Real,"O presente documento tem por objetivo apresentar a tese desenvolvida no Programa de
Doutorado em Ciência da Computação e Matemática Computacional do ICMC/USP. O
tema da tese busca avançar o estado da arte ao resolver os problemas de escalabilidade e
representação presentes em algoritmos de planejamento para missões com Veículos Aéreos
Não Tripulados (VANTs). Técnicas baseadas em programação matemática e computação
evolutiva são propostas. Artigos foram publicados, submetidos ou se encontram em fase
final de elaboração. Esses trabalhos reportam os avanços mais significativos obtidos na
representação e escalabilidade deste problema. Os planejadores de missão trabalhados
na tese lidam com problemas estocásticos em ambientes não convexos, onde os riscos de
colisão ou falhas no planejamento da missão são tratados e limitados a um valor tolerado.
Os avanços na representação permitiram solucionar violações nos riscos presentes na
modelagem original, além de tornar os modelos mais realistas ao incorporar aspectos
como efeitos da resistência do ar. Para isso, técnicas eficientes de modelagem matemática
permitiram avançar de um modelo de Programação Não-Linear Inteira Mista (PNLIM),
originalmente proposto na literatura, para um problema de Programação Linear Inteira
Mista (PLIM). A modelagem como um PLIM levou à resolução do problema de forma
mais eficiente através do algoritmo branch-and-cut. As novas representações propostas
resultaram em melhorias na escalabilidade, solucionando problemas mais complexos em
um tempo computacional menor. Além disso, os avanços em escalabilidade mostraram-se
mais efetivos quando técnicas combinando programação matemática e metaheurísticas
foram aplicadas ao problema.",TESE,Hybrid qualitative state plan problem e o planejamento de missão com VANTs,5047911,1
"Vehicles will use pseudonyms instead of relying on long-term certificates to provide security and
privacy. Pseudonyms are short-term public key certificates that do not contain identity-linking
information about the vehicle. However, there is a constant risk that authorised vehicles may
send fake messages or behave selfishly, and this can affect the performance of the Vehicular Ad
hoc NETwork (VANET). In this context, trust management is another important component of
security services in VANETs, which provides a unified system for establishing a relationship
between the nodes and helps by keeping record of the behaviour of the vehicles. Nevertheless,
it is a challenging task to monitor the evolving pattern of the vehicular behaviour, since communication
between the vehicles is anonymous. It is not easy to find a balanced solution that
meets the requirements of security, privacy, and trust management in VANET. In view of this,
we put forward a Preserving-Privacy Reputation Scheme (PPRS) applied to VANETs, in which a
reputation server through a Roadside Unit receives feedback about the behaviour of the vehicles.
The server updates and certifies the reputation of the vehicles by matching their anonymous
identities with their real ones. Our scheme introduces geographical areas of security, in which
the security of an area can be adapted to higher or lower levels depending on the reputation of
the vehicles. In addition, complex reputation is examined, in which the reputation of a vehicle
is linked to several behavioural factors. A further key area that is explored is the performance
evaluation of PPRS which is conducted through a set of simulations in a grid scenario, based on
an opportunistic message forwarding application. The results showed the effectiveness of PPRS
in terms of assessing the behaviour of the vehicles and taking measures against the misbehaving
vehicles. We used SUMO to simulate the mobility model; OMNET++ and Veins supported the
simulation of the network model. In addition, Crypto++ was used to implement the elliptical
curve cryptographic functions of signature and verification of messages, as recommended by the
security standards. Finally, we employ a pseudonym changing strategy in which the reputation
is discretised at two levels of reputation. The strategy was implemented in a realistic traffic
simulation scenario, and was compared with the so called status and synchronous strategies
through a series of simulations. The results showed that the number of pseudonyms used in
our strategy is lower than the strategies mentioned above, and maintains the rate of success of
changing pseudonym achieved by the synchronous strategy.",Versão de defesa - Luz Marina Santos Jaimes.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,LUZ MARINA SANTOS JAIMES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),10/08/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Mobile network;Vehicular network;Security;Reputation System;Trust management',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,EDSON DOS SANTOS MOREIRA,130,Rede móvel;Rede veicular;Segurança;Sistema de Reputação;Gerenciamento de Confiança,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),SOHAND: Gerenciamento de Handovers em Ambientes NGN,"Os veículos usarão pseudônimos em vez de depender de certificados de longo prazo para fornecer
segurança e privacidade. Os pseudônimos são certificados de chaves públicas de curto prazo que
não contêm informação da identidade do veículo. No entanto, existe um constante risco que
veículos autorizados possam enviar mensagens falsas ou se comportar de maneira egoísta, e isso
pode afetar o desempenho das redes veiculares (VANETs). Nesse contexto, o gerenciamento de
confiança é outro componente importante dos serviços de segurança nas VANETs, o qual fornece
um sistema unificadao para estabelecer uma relação entre os nós e ajuda a manter um registro do
comportamento dos veículos. No entanto, é uma tarefa desafiante monitorar o padrão evolutivo
do comportamiento veícular, já que a comunicação entre os veículos é anônima. Não é uma tarefa
fácil encontrar uma solução equilibrada que atenda aos requisitos de segurança, privacidade
e gerenciamento de confiança em VANET. Em vista disso, apresentamos um Esquema de
Reputação Preservando a Privacidade (ERPP) aplicado a VANETs, no qual um servidor de
reputação através de uma unidade de acostamento recebe avaliações sobre o comportamento dos
veículos. O servidor atualiza e certifica a reputação dos veículos relacionando seus identidades
anônimas com as reais. ERPP introduz áreas geográficas de segurança, na qual a segurançã de
uma área pode ser adaptada a níveis mais elevados ou mais baixos dependendo da reputação
dos veículos. Além, uma reputação complexa é examinada, na qual a reputação de um veículo
está vinculada a varios fatores do comportamento. Uma outra área-chave que é explorada é a
avaliação de desempenho do ERPP o qual é conduzida através de um conjunto de simulações em
um cenário urbano, com base na aplicação de encaminamento oportunista de mensagens. Os
resultados mostraram a eficácia do ERPP em termos de avaliar o comportamento dos veículos e
tomar medidas contra os veículos mal comportados. Utilizamos SUMO para simular o modelo
de mobilidade; OMNET++ e Veins suportaram a simulação do modelo de red. Além disso,
Crypto++ foi usado para implementar as funções criptográficas de curvas elípticas de assinatura e
verificação de mensagens como recomendam os padrões de segurança. Finalmente, empregamos
uma estratégia de mudança de pseudônimo na qual a reputação é discretizada em dois níveis
de reputação. A estratégia foi implementada em um cenário de simulação de tráfego realista
e foi comparada com as estratégias nomeadas de estado e síncrona mediante um conjunto
de simulações. Os resultados mostraram que o número de pseudônimos utilizados em nossa
estratégia é menor que os esquemas antes mencionados, e mantém a taxa de sucesso de mudança
de pseudônimo alcançada pela estratégia síncrona.",TESE,Um esquema de reputacão preservando a privacidade para o gerenciamento de confiança em aplicações VANETs,5047919,1
"User Generated Content (UGC) is the name given to content created spontaneously by
ordinary individuals, without connections to the media. This type of content carries valuable
information and can be exploited by several areas of knowledge. Much of the UGC
is provided in the form of texts – product reviews, comments on forums about movies,
and discussions on social networks are examples. However, the language used in UGC
texts differs, in many ways, from the cultured norm of the language, making it difficult
for NLP techniques to handle them. UGC language is strongly linked to the language
used in daily life, containing a large amount of noise. Spelling mistakes, abbreviations,
slang, absence or misuse of punctuation and capitalization are some noises that make
it difficult to process these texts. Several works report considerable loss of performance
when testing NLP state-of-the-art tools in UGC texts. Textual Normalization is the process
of turning noisy words into words considered correct and can be used to improve the
quality of UGC texts. This work reports the development of methods and systems that
aim to (a) identify noisy words in UGC, (b) find candidate words for substitution, and
(c) rank candidates for normalization. For the identification of noisy words, lexical-based
methods and machine learning ones using deep neural networks were proposed. The automatic
identification presented results comparable to the use of lexicons, proving that this
process can be done with low dependence of resources. For the generation and ranking
of candidates, techniques based on lexical similarity and word embeddings were investigated.
It was concluded that the use of embeddings is highly suitable for normalization,
having achieved the best results. All proposed methods were evaluated based on a UGC
corpus annotated throughout the project, containing texts from different sources: discussion
forums, product reviews and tweets. A system, Enelvo, combining all methods was
implemented and compared to another existing normalizing system, UGCNormal. The
results obtained by the Enelvo system were considerably higher, with a correction rate
between 67 % and 97 % for different types of noise, with less dependence on resources
and greater flexibility in normalization.",Versão de defesa - Thales Felipe Costa Bertaglia.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,THALES FELIPE COSTA BERTAGLIA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),18/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'text normalization;user-generated content;noisy text analysis',INTELIGÊNCIA COMPUTACIONAL,MARIA DAS GRACAS VOLPE NUNES,137,normalização textual;conteúdo gerado por usuário;análise de textos ruidosos,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Recursos, Métodos e Sistemas Baseados em Córpus para a Área de Processamento de Línguas Naturais (Pln) e Inteligência Artificial na Educação","Conteúdo Gerado por Usuário (CGU) é a denominação dada ao conteúdo criado de forma
espontânea por indivíduos comuns, sem vínculos com meios de comunicação. Esse tipo de
conteúdo carrega informações valiosas e pode ser explorado por diversas áreas do conhecimento.
Muito do CGU é disponibilizado em forma de textos – avaliações de produtos,
comentários em fóruns sobre filmes e discussões em redes sociais são exemplos. No entanto,
a linguagem utilizada em textos de CGU diverge, de várias maneiras, da norma culta da
língua, dificultando seu processamento por técnicas de PLN. A linguagem de CGU é fortemente
ligada à língua utilizada no cotidiano, contendo, assim, uma grande quantidade
de ruídos. Erros ortográficos, abreviações, gírias, ausência ou mau uso de pontuação e de
capitalização são alguns ruídos que dificultam o processamento desses textos. Diversos
trabalhos relatam perda considerável de desempenho ao testar ferramentas do estado-daarte
de PLN em textos de CGU. A Normalização Textual é o processo de transformar
palavras ruidosas em palavras consideradas corretas e pode ser utilizada para melhorar
a qualidade de textos de CGU. Este trabalho relata o desenvolvimento de métodos e
sistemas que visam a (a) identificar palavras ruidosas em textos de CGU, (b) encontrar
palavras candidatas a sua substituição, e (c) ranquear os candidatos para realizar a normalização.
Para a identificação de ruídos, foram propostos métodos baseados em léxicos
e em aprendizado de máquina, com redes neurais profundas. A identificação automática
apresentou resultados comparáveis ao uso de léxicos, comprovando que este processo
pode ser feito com baixa dependência de recursos. Para a geração e ranqueamento de candidatos,
foram investigadas técnicas baseadas em similaridade lexical e word embeddings.
Concluiu-se que o uso de word embeddings é altamente adequado para normalização, tendo
atingido os melhores resultados. Todos os métodos propostos foram avaliados com base
em um córpus de CGU anotado no decorrer do projeto, contendo textos de diferentes
origens: fóruns de discussão, reviews de produtos e publicações no Twitter. Um sistema,
Enelvo, combinando todos os métodos foi implementado e comparado a um outro sistema
normalizador existente, o UGCNormal. Os resultados obtidos pelo sistema Enelvo foram
consideravelmente superiores, com taxa de correção entre 67% e 97% para diferentes tipos
de ruído, com menos dependência de recursos e maior flexibilidade na normalização.",DISSERTAÇÃO,Normalização textual de conteúdo gerado por usuário,5048557,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5048575,
"Machine learning consists of concepts and techniques that enable computers to improve their
performance with experience, i.e., “learn” from data. Unsupervised and semi-supervised learning
are important categories of machine learning, which respectively consists of inferring patterns
in datasets whose data have no label (class) and classifying data in partially-labeled datasets.
Although intensively studied, machine learning is still a field full of challenges and with many
open topics. Collective dynamical systems, in turn, are systems made of a large group of
individuals, each one a dynamical system by itself, such that all of them behave collectively,
i.e., the action of each individual is influenced by the action of its neighbors. A remarkable
feature of those systems is that global patterns may spontaneously emerge from the local
interactions among individuals, a phenomenon known as “emergence”. Their relevance and
intrinsic challenges motivate research in various branches of science and engineering. In
this doctorate research, we develop and analyze collective dynamical models for their usage in
machine-learning tasks, specifically unsupervised and semi-supervised ones. Image segmentation
and network community detection are also addressed, as they are related to machine learning as
well. In particular, we propose to work on models in which the objects’ motion is determined
by the location and velocity of their neighbors. By doing so, the dynamical system reaches a
configuration in which the patterns developed by the set of individuals highlight underlying
patterns of the dataset. Due to their self-organizing nature, it is also expected that the models
can be robust and the information generated during the process (values of the system variables)
can be rich and reveal, for example, features to perform soft labeling and determine overlapping
classes.",Versão revisada - Roberto Alves Gueleri.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,ROBERTO ALVES GUELERI,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),04/07/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Machine learning;collective behavior;collective motion;flocking;self-organizing systems',INTELIGÊNCIA COMPUTACIONAL,ZHAO LIANG,146,Aprendizado de máquina;comportamento coletivo;movimento coletivo;flocking;sistemas auto-organizáveis,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Modelagem de Sistemas Complexos para Análise e Reconhecimento de Padrões,"O aprendizado de máquina consiste em conceitos e técnicas que permitem aos computadores melhorar
seu desempenho com a experiência, ou em outras palavras, “aprender” com dados. Duas
de suas principais categorias são o aprendizado não-supervisionado e o semissupervisionado,
que respectivamente consistem em inferir padrões em bases cujos dados não têm rótulo (classe) e
classificar dados em bases parcialmente rotuladas. Embora muito estudado, trata-se de um campo
repleto de desafios e com muitos tópicos abertos. Sistemas dinâmicos coletivos, por sua vez, são
sistemas constituídos por muitos indivíduos, cada qual um sistema dinâmico por si só, de modo
que todos eles agem coletivamente, ou seja, a ação de cada indivíduo é influenciada pela ação
dos vizinhos. Uma característica notável desses sistemas é que padrões globais podem surgir
espontaneamente das interações locais entre os indivíduos, fenômeno conhecido como “emergência”.
Os desafios intrínsecos e a relevância do tema vêm motivando sua pesquisa em diversos
ramos da ciência e da engenharia. Este trabalho de doutorado consiste no desenvolvimento e
análise de modelos dinâmicos coletivos para o aprendizado de máquina, especificamente suas
categorias não-supervisionada e semissupervisionada. As tarefas de segmentação de imagens e
de detecção de comunidades em redes, que de certo modo podem ser entendidas como tarefas
do aprendizado de máquina, são também abordadas. Em especial, desenvolvem-se modelos
nos quais a movimentação dos objetos é determinada pela localização e velocidade de seus
vizinhos. O sistema dinâmico assim modelado é então conduzido a um estado cujo padrão
formado por seus indivíduos realça padrões subjacentes do conjunto de dados. Devido ao seu
caráter auto-organizável, os modelos aqui desenvolvidos são robustos e as informações geradas
durante o processo (valores das variáveis do sistema) são ricas e podem, por exemplo, revelar
características para realizar soft labeling e determinar classes sobrepostas.",TESE,Desenvolvimento de técnicas de aprendizado de máquina via sistemas dinâmicos coletivos,5048596,1
"The cytoskeleton is the most important cellular structure in eukaryotic cells and is responsible
for maintaining the shape of the cell and cellular junctions, aiding in cell movements. This is
composed of filaments of Actin, Microtubules and intermediate filaments. Recently, the analysis
of two of these structures has become important because it is possible to obtain micrographs
using microscopes of high resolution and fluorescence technology, in combination with complex
methods of application of substances of contrast for labeling and later visual analysis. Of these
techniques, however, is limited to being descriptive and subjective. In this work, we evaluate
some most popular image analysis techniques like a Bag of Visual Words (BoVW), Local
Binary Pattern (LBP), Textons based in Discrete Fourier Transform(TDFT) , Gabor Filter banks
(TGFB), and based in Complex Networks theory (TCN) over famous dataset 2D Hela and FDIG
Olympus. Extensive experiments are conducted on both datasets which their results can serve as
a baseline for future research with cytoskeleton classification in microscopy fluorescence images.
In this work, we present the quantitative and qualitative comparison of above mentioned methods
for understand the behavior of these methods and properties of Actin microfilaments (MA)
and Microtubules (MT) on both datasets. The results show that it is possible to classify the
FDIG Olympus data set with accuracy of up to 90.07% and 98.94% for 2D Hela, in addition to
obtaining 86.05% and 96.84% respectively, using complex network theory.",Versão de defesa - Filomen Incahuanaco Quispe .pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,FILOMEN INCAHUANACO QUISPE,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),14/09/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Microscopy fluorescence image;BoVW;LBP;Textons;Complex Networks',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,AFONSO PAIVA NETO,71,Imagens microscópicas fluorescentes;BoVW;LBP;Textons;Redes Complexas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Modelagem computacional,"O citoesqueleto é a estrutura celular mais importante em células eucariotas e é responsável por
manter a forma da célula e as junções celulares, auxiliando nos movimentos celulares. Esta é
composta de filamentos de Actina, Microtúbulos e filamentos intermediários. Recentemente, a
análise de duas dessas estruturas tornaram-se importantes, pois é possível obter micrografias
usando microscópios de alta resolução, que contém microscopia de fluorescência, em combinação
com métodos complexos de aplicação de substâncias de contraste para rotulagem e
posterior análises visuais.A combinação dessas técnicas, entretanto, limita-se a ser descritiva
e subjetiva. Neste trabalho, são avaliadas algumas técnicas de análise de imagens como: Bag
of Visual Words (BoVW), Local Binary Local (LBP), Textons Baseados em Discrete Fourier
Transform (TDFT), Textons baseados em Gabor Filter Banks (TGFB) e textons baseados em
Complex Networks (TCN) sobre o conjunto de dados 2D Hela e FDIG Olympus. Experimentos
extensivos foram conduzidos em ambos os conjuntos de dados, e seus resultados podem servir
de base para futuras pesquisas como análises do citoesqueleto em imagens de microscopia
fluorescente. Neste trabalho, é apresentada uma comparação quantitativa e qualitativa dos métodos
acima mencionados para entender o comportamento desses métodos e propriedades dos
microfilamentos de actina (MA) e Microtúbulos (MT) em ambos os conjuntos de dados. Os
resultados obtidos evidenciam que é possível classificar o conjunto de dados da FDIG Olympus
com uma precisão de até 90.07% e 98.94% para 2D Hela, além de obter 86.05% e 96.84%,
respectivamente, de precisão, usando teoria de redes complexas.",DISSERTAÇÃO,Classificação de imagens de fluorescência do citoesqueleto através de técnicas em processamento de imagens,5065038,01
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5067098,
"Given a very large dataset of moderate-to-high dimensionality, how to mine useful patterns
from it? In such cases, dimensionality reduction is essential to overcome the well-known “curse
of dimensionality”. Although there exist algorithms to reduce the dimensionality of Big Data,
unfortunately, they all fail to identify/eliminate non-linear correlations that may occur between
the attributes. This MSc work tackles the problem by exploring concepts of the Fractal Theory
and massive parallel processing to present Curl-Remover, a novel dimensionality reduction
technique for very large datasets. Our contributions are: (a) Curl-Remover eliminates linear
and non-linear attribute correlations as well as irrelevant attributes; (b) it is unsupervised and
suits for analytical tasks in general – not only classification; (c) it presents linear scale-up on
both the data size and the number of machines used; (d) it does not require the user to guess the
number of attributes to be removed, and; (e) it preserves the attributes’ semantics by performing
feature selection, not feature extraction. We executed experiments on synthetic and real data
spanning up to 1.1 billion points, and report that our proposed Curl-Remover outperformed two
PCA-based algorithms from the state-of-the-art, being in average up to 8% more accurate.",Versão de defesa - Antonio Canabrava Fraideinberze.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,ANTONIO CANABRAVA FRAIDEINBERZE,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),04/09/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Massive parallel processing;Feature selection;Non-linear attribute correlations;Big Data;Fractal Theory',BANCO DE DADOS/COMPUTAÇÃO GRÁFICA E PROCESSAMENTO DE IMAGENS,ROBSON LEONARDO FERREIRA CORDEIRO,90,Processamento paralelo em massa;Seleção de atributos;Correlações não lineares entre atributos;Big Data;Teoria de Fractais,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Divisão Relacional por Similaridade em Banco de Dados,"Dada uma grande base de dados de dimensionalidade moderada a alta, como identificar padrões
úteis nos objetos de dados? Nesses casos, a redução de dimensionalidade é essencial para superar
um fenômeno conhecido na literatura como a “maldição da alta dimensionalidade”. Embora
existam algoritmos capazes de reduzir a dimensionalidade de conjuntos de dados na escala
de Terabytes, infelizmente, todos falham em relação à identificação/eliminação de correlações
não lineares entre os atributos. Este trabalho de Mestrado trata o problema explorando conceitos
da Teoria de Fractais e processamento paralelo em massa para apresentar Curl-Remover, uma
nova técnica de redução de dimensionalidade bem adequada ao pré-processamento de Big Data.
Suas principais contribuições são: (a) Curl-Remover elimina correlações lineares e não lineares
entre atributos, bem como atributos irrelevantes; (b) não depende de supervisão do usuário e é
útil para tarefas analíticas em geral – não apenas para a classificação; (c) apresenta escalabilidade
linear tanto em relação ao número de objetos de dados quanto ao número de máquinas utilizadas;
(d) não requer que o usuário sugira um número de atributos para serem removidos, e; (e) mantêm
a semântica dos atributos por ser uma técnica de seleção de atributos, não de extração de atributos.
Experimentos foram executados em conjuntos de dados sintéticos e reais contendo até 1,1 bilhões
de pontos, e a nova técnica Curl-Remover apresentou desempenho superior comparada a dois
algorítmos do estado da arte baseados em PCA, obtendo em média até 8% a mais em acurácia de
resultados.",DISSERTAÇÃO,Seleção de atributos efetiva e não-supervisionada em grandes bases de dados: aplicando a Teoria de Fractais para remover correlações lineares e não-lineares,5067132,1
"The Semantic Web is an extension of the World Wide Web in which the information
has explicit meaning, allowing computers and people to work in cooperation. In order to
explicitly define meaning, ontologies are used to structure information. As more scientific
fields adopt Semantic Web technologies, more complex ontologies are needed. Moreover,
the quality assurance of the ontologies and their management are undermined as these
ontologies increase in size and complexity. One of the causes for these difficulties is the
existence of problems, also called anomalies, in the ontologies’ structure. These anomalies
range from subtle problems, such as poorly projected concepts, to more serious ones, such
as inconsistencies. The identification and elimination of anomalies can diminish the ontologies’
size and provide a better understanding of the ontologies. However, methods to
identify anomalies found in the literature do not provide anomaly visualizations, many do
not work on OWL ontologies or are not user extensible. For these reasons, a new method
for anomaly identification and visualization, the ONTO-Analyst, was created. It allows
ontology developers to automatically identify anomalies, using SPARQL queries, and visualize
them as graph images. The method uses a proposed ontology, the METAdata
description For Ontologies/Rules (MetaFOR), to describe the structure of other ontologies,
and SPARQL queries to identify anomalies in this description. Once identified, the
anomalies can be presented as graph images. A system prototype, the ONTO-Analyst,
was created in order to validate this method and it was tested in a representative set of
ontologies, trough the verification of representative anomalies. The prototype tested 18
types of anomalies, taken from the scientific literature, in a set of 608 OWL ontologies
from major public repositories and two articles. The system detected 4.4 million anomaly
occurrences in the 608 ontologies: 3.5 million occurrences from the same type and 900
thousand distributed in 11 other types. These anomalies occurred in various parts of the
ontologies, such as classes, object and data properties, etc. In a second test, a case study
was performed in the visualizations generated by the ONTO-Analyst prototype, from the
anomalies found in the first test. It was shown that each visualization presented the elements
involved in the anomaly and that at least one possible solution could be deduced
from the visualization. These results demonstrate that the method can efficiently find
anomaly occurrences in a representative set of OWL ontologies and that the visualization
aids in the understanding and correcting of said anomalies. In order to extend the types
of detectable anomalies, users can write new SPARQL queries.",Versão revisada - João Paulo Orlando.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,JOAO PAULO ORLANDO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),21/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Semantic Web;OWL Ontologies;Anomaly Identification;Anomaly Visualization;Ontology Problems',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,DILVAN DE ABREU MOREIRA,150,Web Semântica;Ontologias OWL;Identificação de Anomalias;Visualização de Anomalias;Problemas de Ontologias,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Web Semântica na Criação de Anotações Inteligentes em Bioinformática,"A Web Semântica é uma extensão da Web em que as informações tem um significado
explícito, permitindo que computadores e pessoas trabalhem em cooperação. Para definir
os significados explicitamente, são usadas ontologias na estruturação das informações. À
medida que mais campos científicos adotam tecnologias da Web Semântica, mais ontologias
complexas são necessárias. Além disso, a garantia de qualidade das ontologias e seu
gerenciamento ficam prejudicados quanto mais essas ontologias aumentam em tamanho
e complexidade. Uma das causas para essas dificuldades é a existência de problemas,
também chamados de anomalias, na estrutura das ontologias. Essas anomalias englobam
desde problemas sutis, como conceitos mal projetados, até erros mais graves, como inconsistências.
A identificação e a eliminação de anomalias podem diminuir o tamanho da
ontologia e tornar sua compreensão mais fácil. Contudo, métodos para identificar anomalias
encontrados na literatura não visualizam anomalias, muitos não trabalham com
OWL e não são extensíveis por usuários. Por essas razões, um novo método para identificar
e visualizar anomalias em ontologias, o ONTO-Analyst, foi criado. Ele permite
aos desenvolvedores identificar automaticamente anomalias, usando consultas SPARQL,
e visualizá-las em forma de grafos. Esse método usa uma ontologia proposta, a METAdata
description For Ontologies/Rules (MetaFOR), para descrever a estrutura de outras
ontologias, e consultas SPARQL para identificar anomalias nessa descrição. Uma vez
identificadas, as anomalias podem ser apresentadas na forma de grafos. Um protótipo
de sistema, chamado ONTO-Analyst, foi criado para a validação desse método e testado
em um conjunto representativo de ontologias, por meio da verificação de anomalias representativas.
O protótipo testou 18 tipos de anomalias retirados da literatura científica,
em um conjunto de 608 ontologias OWL de 4 repositórios públicos importantes e dois
artigos. O sistema detectou 4,4 milhões de ocorrências de anomalias nas 608 ontologias:
3,5 milhões de ocorrências de um mesmo tipo e 900 mil distribuídas em 11 outros tipos.
Essas anomalias ocorreram em várias partes das ontologias, como classes, propriedades de
objetos e de dados, etc. Num segundo teste foi realizado um estudo de caso das visualizações
geradas pelo protótipo ONTO-Analyst das anomalias encontradas no primeiro teste.
Visualizações de 11 tipos diferentes de anomalias foram automaticamente geradas. O protótipo
mostrou que cada visualização apresentava os elementos envolvidos na anomalia e
que pelo menos uma solução podia ser deduzida a partir da visualização. Esses resultados
demonstram que o método pode eficientemente encontrar ocorrências de anomalias em um conjunto representativo de ontologias OWL, e que as visualizações facilitam o
entendimento e correção da anomalia encontrada. Para estender os tipos de anomalias
detectáveis, usuários podem escrever novas consultas SPARQL.",TESE,ONTO-Analyst: um método extensível para a identificação e visualização de anomalias em ontologias,5075173,1
"The Web education systems (WES) global community is in continuous change, growth and evolve all
around the world, driven by several factors, including new trends in emerging technologies and tools
that support WES, and even the growing role of social learning as a priority for the development process
of such systems. Semantic Web-based Educational Systems (SWBES) are Web-based educational
platforms designed to address a variety of problems faced by students and other users, such as
difficulties in finding, sharing, and reusing educational resources. SWBES have often been used as
search engines for MOOC platforms and Intelligent Tutoring Systems (STI). Therefore, ensuring the
quality of these systems is necessary to enable better teaching and learning experiences. However,
SWBES quality assessment is a complex task, since it requires extensive knowledge about Semantic
Web technologies, Web Educational Systems, Software Engineering, and software quality assessment
standards. The problem identified was the absence of a quality evaluation approach of these systems
from the elements of the system architecture, ie, the actuation and structuring of intelligent agents,
ontologies, learning objects, repositories, metadata etc. In this context, the purpose of this thesis was
to develop an approach for the quality evaluation of SWBES. In order to achieve the proposed objective,
it was necessary to identify the quality criteria used in software quality assessment, WES (including
educational resources, such as learning objects), Semantic Web technologies (such as ontologies).
These criteria were classified, analyzed and adjusted, based on the quality requirements of SWBES in
the literature and named as factors. Next, a set of quality factors was established, with subfactors and
criteria that will be used as a general guideline to evaluate and compare the quality of SWBES. The
evaluators designated to evaluate the SWBES were defined according to the role they represent in the
systems. It was also established which SWBES artifacts should be evaluated by the approach, in order
of making the evaluation process simplified. It was developed a Web tool to automate the approach
developed which were validated through case studies by specialists in the related research fields. It is
expected that the approach will contribute with professionals, developers and other users (teachers,
educators, students, virtual tutors and managers) in acquiring and using SWBES, so that they can
choose the appropriate system to fill out their needs and goals. In addition, it is expected to collaborate
in the SWBES development process, based on the quality factors established, and promoting quality
assurance since the beginning of the development process. As future work, it is intended to broaden the
quality factors and extend the approach allowing the evaluation of other Semantic Web-based Systems
in other knowledge domain.",Versão de defesa - Aparecida Maria Zem Lopes.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,APARECIDA MARIA ZEM LOPES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),05/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Quality evaluation;Semantic Web;Web education systems;SWBES;Approach',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,SEIJI ISOTANI,263,Avaliação de qualidade;Web Semântica;Sistemas educacionais Web;SWBES;Abordagem,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Ambientes Inteligentes de Aprendizagem Colaborativa,"O mercado global dos sistemas educacionais Web (WES, do inglês, Web Educational Systems)
continua a mudar, crescer e evoluir em todo o mundo, impulsionado por diversos fatores, entre eles,
pelas novas tendências em tecnologias emergentes e ferramentas que dão suporte aos WES e, ainda,
ao papel crescente da aprendizagem social como prioridade para o processo de desenvolvimento de
tais sistemas. Os Sistemas Educacionais baseados em Web Semântica (SWBES, do inglês Semantic
Web-based Educational Systems) são plataformas educacionais Web desenvolvidas para resolver
diversos problemas enfrentados pelos alunos e outros usuários, como dificuldades relacionadas à
busca, compartilhamento e reutilização de recursos educacionais. SWBES têm sido frequentemente
usados como motores de busca para plataformas MOOC e Sistemas Tutores Inteligentes (STI).
Portanto, é necessário garantir a qualidade desses sistemas para possibilitar melhores experiências de
ensino e aprendizagem. No entanto, a avaliação da qualidade dos SWBES é uma tarefa complexa,
uma vez que requer um amplo conhecimento sobre as tecnologias da Web Semântica, Educação a
Distância, Engenharia de Software, além das normas e padrões utilizados para a Avaliação da
Qualidade de Software. O problema identificado foi a ausência de uma abordagem de avaliação de
qualidade desses sistemas a partir dos elementos da arquitetura do sistema, ou seja, a atuação e
estruturação dos agentes inteligentes, ontologias, objetos de aprendizagem, repositórios, metadados
etc. Nesse contexto, o objetivo dessa tese foi desenvolver uma abordagem para a avaliação de
qualidade de SWBES. Para atingir o objetivo proposto, foi necessário identificar os critérios de
qualidade utilizados na avaliação de qualidade de software, de WES (incluindo recursos educacionais,
tais como os objetos de aprendizagem), das tecnologias da Web Semântica (tais como ontologias).
Esses critérios foram classificados, analisados e ajustados, com base nos requisitos de qualidade dos
SWBES apontados na literatura. Em seguida, um conjunto de fatores de qualidade foi estabelecido,
com subfatores e critérios que serão utilizados como diretriz geral para avaliar e comparar a qualidade
de SWBES. Foram definidos, também, os avaliadores responsáveis de acordo com os papéis que
representam no sistema. Estabeleceu-se também quais os artefatos do SWBES que devem ser
avaliados pela abordagem, de modo que o processo de avaliação seja simplificado. A abordagem foi
automatizada em uma ferramenta Web e validada por meio de estudos de caso, por meio de
especialistas no domínio da Web Semântica, Informática e Educação. Espera-se que a abordagem
desenvolvida possa contribuir com profissionais, desenvolvedores e outros usuários (professores,
educadores, alunos, mediadores, tutores e gestores) que desejam ou necessitam adquirir e utilizar
SWBES, de forma que possam efetuar a aquisição adequada às suas necessidades e objetivos. Além
disso, espera-se poder colaborar no processo de desenvolvimento de SWBES, a partir dos fatores de
qualidade estabelecidos para a abordagem, e promover a garantia de qualidade desde o início do
processo do desenvolvimento. Como trabalhos futuros, pretende-se ampliar os fatores de qualidade e
estender a abordagem para permitir a avaliação de outros Sistemas baseados em Web Semântica, em
outros domínios do conhecimento.",TESE,Uma abordagem de suporte à avaliação de qualidade de sistemas educacionais baseados em Web Semântica,5075203,1
"Objects detection and recognition is a critical task in applications for mobile robots and intelligent
vehicles autonomous navigation. With the advent of many 3D sensors, environment elements
can be detected and represented in three-dimensional mode, in structures known as point clouds.
3D sensors usually capture a large amount of points at high rates, requiring robust techniques
to process this information and also deal with noise on input data. A common approach in
the Computer Vision field for dimensionality reduction is the use of robust features extraction
techniques. This way, only a subset with representative and simplified information from the
dataset is processed. This thesis presents a methodology for objects recognition in 3D point
clouds using global 3D features extraction. A novel 3D descriptor invariant to scale, translation
and rotation named 3D-CSD (3D-Contour Sample Distances) was developed to represent the
objects surface, and a supervised learning method used for pattern recognition. The experiments
were performed using Artificial Neural Networks for the recognition of different classes of
objects, evaluating and validating the proposed methodology. Obtained results demonstrated the
feasibility of this approach application for object recognition in 3D perception systems.",Versão de defesa - Daniel Oliva Sales.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,DANIEL OLIVA SALES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),16/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Mobile Robotics;Computer Vision;Feature Extraction;Pattern Recognition',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",FERNANDO SANTOS OSORIO,86,Robótica Móvel;Visão Computacional;Extração de features 3D;Reconhecimento de Padrões,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Desenvolvimento de Robôs Táticos para Ambientes Internos e Desenvolvimento de Veículos Terrestres Autônomos,"A detecção e reconhecimento de objetos é uma tarefa fundamental em aplicações relacionadas
à navegação autônoma de robôs móveis e veículos inteligentes. Com a evolução tecnológica
nos sistemas sensoriais, surgiram equipamentos capazes de detectar e representar os elementos
presentes no ambiente de forma tridimensional, em estruturas chamadas nuvem de pontos. Os
sensores 3D geralmente capturam um grande volume de pontos em curtos intervalos de tempo,
o que demanda técnicas robustas para processamento dessa informação além de tolerância a
eventuais ruídos nos dados. Uma abordagem frequentemente utilizada na área de Visão Computacional
para redução de dimensionalidade é a extração de features robustas, armazenando
um subconjunto de informações representativas e simplificadas do conjunto de dados. Esta
tese apresenta uma metodologia de classificação de objetos em nuvens de pontos 3D através
da extração de features 3D globais. Foi desenvolvido um novo descritor 3D invariante à escala,
translação e rotação denominado 3D-CSD (3D-Contour Sample Distances) para representação
da superfície dos objetos presentes no ambiente, e utilizado um método de aprendizado supervisionado
para reconhecimento de padrões. Os experimentos realizados envolveram o uso de
Redes Neurais Artificiais para o reconhecimento de diferentes classes de objetos, avaliando e
validando a metodologia proposta. Os resultados obtidos demostraram a viabilidade da aplicação
desta abordagem para o reconhecimento de objetos em sistemas de percepção 3D.",TESE,Extração de features 3D para o reconhecimento de objetos em nuvem de pontos,5075230,1
"In this work, we studied Riccati equations for filtering Markovian jump linear systems in discrete
time. We found a general condition for the stability of the algebraic filtering equation, and it
is also valid for no multiplicity of solutions. We also review the fact of existence, arriving at a
condition in terms of the sequence of gains of a Luenberger observer. These results used Markov
chains in reverse time scale, inspiring us to explore the duality between filtering and control in
systems with chain reversion, arriving at a simple relation of duality.",Versão de defesa - Daniel Alexis Gutierrez Pachas.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,DANIEL ALEXIS GUTIERREZ PACHAS,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),28/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Riccati equations;Stabilility;Duality;Jump linear systems',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,EDUARDO FONTOURA COSTA,90,Equações de Riccati;Estabilidade;Dualidade;Sistemas lineares com saltos,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Neste trabalho estudamos as equações de Riccati para a filtragem de sistemas lineares com
saltos Markovianos a tempo discreto. Obtemos uma condição geral para estabilidade da equação
algébrica de filtragem, e que também é válida para que não haja multiplicidade de soluções.
Revisitamos também a questão da existência, chegando a uma condição em termos da sequência
de ganhos de um observador de Luenberger. Estes resultados usaram cadeias de Markov em
escala reversa de tempo, inspirando a explorar a dualidade entre filtragem e controle em sistemas
com reversão na cadeia, chegando a uma relação simples de dualidade.",TESE,Um estudo sobre as equações de Riccati de filtragem para sistemas com saltos Markovianos: estabilidade e dualidade com controle,5075260,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5075283,
"Software architectures and software testing play an essential role in the development process of high quality software products. Based on a systematic mapping, it could be observed that the activities of establishing software architectures do not consider to represent software test information associated with the architecture views, in particular of reference architectures. The objective of this work is to propose a process that induces the representation of software test information in the establishment of reference architectures. Thus, in addition to abstracting the essence of a set of architectures of an application domain, facilitating the design of new architectures through the reuse and standardization of architectural elements, reuse and standardization of test information are also promoted. This work is carried out using the RAModel and the ProSA-RA. RAModel presents a set of essential elements for the design of reference architectures. ProSA-RA, on the other hand, presents a process that systematizes the establishment of reference architectures, considering the elements of the RAModel. In particular, a process named ProSA-RAT is proposed. This process facilitates the definition of test processes adequate to specific application domains by promoting the reuse and standardization of test information in the early stages of architecture based development processes. A feasibility study of ProSA-RAT has been carried out and an example in the field of robotics is presented.",Versão de defesa - Diógenes Dias Simão.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,DIOGENES DIAS SIMAO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),12/09/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Software Test;Software Architecture',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,JOSE CARLOS MALDONADO,119,Teste de Software;Arquiteturas de Software,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Processos, Métodos, Tecnologias e Ferramentas para o Reuso de Software","Arquiteturas de software e teste de software desempenham um papel essencial no processo de desenvolvimento de produtos de software de alta qualidade. Com base em um mapeamento sistemático, pode-se observar que as atividades de estabelecimento de arquiteturas de software não consideram representar informações de teste de software associadas às visões da arquitetura, em particular de arquiteturas de referência. Este trabalho tem por objetivo propor um processo que induza a representação de informações de teste de software no estabelecimento de arquiteturas de referência. Dessa forma, além de se abstrair a essência de um conjunto de arquiteturas de software de um domínio de aplicação, facilitando o projeto de novas arquiteturas por meio do reúso e padronização de elementos arquiteturais, promove-se também o reúso e a padronização de informações de teste. Este trabalho é realizado a partir do modelo RAModel e do processo ProSA-RA. O RAModel apresenta um conjunto de elementos essenciais para o projeto de arquiteturas de referência. O ProSA-RA por sua vez apresenta um processo que sistematiza o estabelecimento de arquiteturas de referência, considerando os elementos do RAModel. Em particular, propõe-se um processo denominado ProSA-RAT. Esse processo viabiliza a definição de processos de teste adequados a domínios de aplicação específicos promovendo o reúso e padronização de informações de teste nas etapas iniciais de processos de desenvolvimento baseados em arquitetura. Um estudo de viabilidade do ProSA-RAT foi conduzido e um exemplo no domínio de robótica é apresentado.",DISSERTAÇÃO,ProSA-RAT: Um processo para estabelecimento de arquiteturas de referência com informações de teste de software,5077024,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5077054,
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5078984,
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5079037,
"Software reuse, specially when supported by computational tools, is a way to face the constant
challenges of Software Engineering in increasing productivity and quality in software
development. Several enviroments to support software reuse have been proposed, mostly for
specific purposes, for example, component repositories, application generation tools, and tools
for configuring product lines. However, the integration among these environments is often
deficient and left to the developers’s responsibility. On the other hand, Systems-of-Systems (SoS)
is a way of integrating independent systems and their relationships, forming a whole greater than
the sum of the parts. In this way, SoS allows us to achieve complex goals that could not be easily
achieved individually by their constituent systems. In a SoS, emergent behaviors can arise at any
time or have been previously implemented. Because they are recent, SoS concepts still feature a
variety of open research topics, including their application to software reuse environments. Thus,
in this work, we investigated how to integrate reuse environments based on SoS concepts. The
resulting SoS, called SoS-Reuse, aims to facilitate the implementation of emergent behaviors
related to software reuse. This allows to enhance the reuse activity, facilitating the search for
reusable assets in different constituent systems. To implement this idea, it was developed a
prototype of the SoS-Reuse, called P-SoS-Reuse. An evalutation of the usability and functional
adequacy of the P-SoS-Reuse was performed in order to obtain feedback from users regarding
its use. The approach used to develop SoS-Reuse can be adapted in the future to integrate other
types of systems, which could benefit from the SoS-based approach.",Versão de defesa - Iohan Gonçalves Vargas.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,IOHAN GONCALVES VARGAS,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),28/09/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Software Reuse;Reusable Asset;Systems-of-Systems;Emergent Behavior;RAS;Integration;Interoperability',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ROSANA TERESINHA VACCARE BRAGA,186,Reúso de Software;Ativos Reusáveis;Comportamento Emergente;RAS;Integração;Interoperabilidade,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Processos, Métodos, Tecnologias e Ferramentas para o Reuso de Software","Reúso de software, principalmente quando apoiado por ferramentas computacionais, é uma forma
de enfrentar os constantes desafios da Engenharia de Software em aumentar a produtividade e
qualidade no desenvolvimento de software. Diversos ambientes de apoio ao reúso de software
têm sido propostos, em sua maioria com objetivos específicos, por exemplo, repositórios de
componentes, ferramentas para geração de aplicações e ferramentas para configuração de linhas
de produtos. Entretanto, a integração desses ambientes é, muitas vezes, deficiente e deixada
por conta do próprio desenvolvedor. Por outro lado, Sistemas-de-Sistemas (SoS) constituem
uma forma de integrar sistemas independentes e seus relacionamentos, formando um todo maior
que a soma das partes. Dessa forma, o SoS permite alcançar objetivos complexos que não
poderiam ser facilmente alcançados individualmente pelos seus sistemas constituintes. Em um
SoS, comportamentos emergentes podem surgir a qualquer momento ou terem sido previamente
implementados. Por serem recentes, os conceitos de SoS ainda apresentam uma vasta gama
de tópicos em aberto, entre eles sua aplicação a ambientes de reúso de software. Assim, neste
trabalho, investigou-se como integrar ambientes de reúso com base nos conceitos de SoS. O
SoS resultante, denominado SoS-Reúso, visa facilitar a implementação de comportamentos
emergentes relacionados ao reúso de software. Com isso, pode-se potencializar a atividade de
reúso, facilitando a busca por ativos reusáveis em diferentes sistemas constituintes. Para colocar
em prática essa ideia, desenvolveu-se um protótipo do SoS-Reúso, denominado P-SoS-Reúso.
Realizou-se uma avaliação da usabilidade e adequação funcional do P-SoS-Reúso, a fim de obter
do usuário feedbacks em relação ao seu uso. A abordagem utilizada no desenvolvimento do
SoS-Reúso poderá ser adaptada futuramente para integrar outros tipos de sistemas, os quais
poderiam se beneficiar da abordagem baseada em SoS.",DISSERTAÇÃO,SoS-Reúso: um SoS do tipo direcionado para facilitar o reúso de software,5079365,1
"The formalization and knowledge sharing has increasingly encouraged the use of ontologies
in several areas of computing. In Software Engineering, for example, they have been used in
different phases of software life cycle. Specifically, in software development the ontology can be
considered as a software artifact, which acts in the formalization of knowledge and requirements,
automatic generation of code, continuous integration and data transformation into knowledge.
However, few studies deal with these factors in a systematized way in the development of
ontology based software, regarding to associating of the Software Engineering and Ontology
Engineering concepts. In addition, current approaches do not address agile principles in their
definitions. In this sense, this work aims to define a development process concerning the
principles and agile values for ontology based software development. In the process, called
OntoSoft, phases, activities, tasks, roles, and artifact models were defined in detail to guide
development teams. In addition, three development scenarios were specified considering the
complexity of the software to be developed, in order to demonstrate distinct possibilities of
development flow of the ontology based software. Based on case studies conducted in different
development environments, the results suggest that the OntoSoft process contributes positively
to the development of ontology-based software artifacts, contributing to the effectiveness and
productivity of the team.",Versão de defesa - Joice Basílio Machado Marques.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,JOICE BASILIO MACHADO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),02/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Ontology;Software Engineering;Ontology Engineering;Scrum;Development process',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ELLEN FRANCINE BARBOSA,196,Ontologia;Engenharia de Software;Engenharia de Ontologias;Scrum;Processo de desenvolvimento,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Computação Aplicada à Educação,"A formalização e o compartilhamento do conhecimento tem incentivado cada vez mais o uso de
ontologias em diversas áreas da computação. Na Engenharia de Software, por exemplo, elas são
usadas em diferentes fases do ciclo de vida do software. Especificamente, no desenvolvimento de
software a ontologia pode ser considerada como um artefato de software, que atua na formalização
do conhecimento e requisitos, na geração automática de código, na integração contínua e na
transformação de dados em conhecimento. No entanto, poucos estudos abordam esses fatores de
maneira sistematizada na construção do software baseado em ontologia, ao associar os conceitos
da Engenharia de Software à Engenharia de Ontologias. Além disso, as abordagens atuais não
inserem princípios ágeis em suas definições. Portanto, este trabalho tem por objetivo definir um
processo de desenvolvimento considerando os princípios e valores ágeis para o desenvolvimento
de software baseado em ontologia. No processo, denominado OntoSoft, fases, atividades, tarefas,
papeis e modelos de artefatos foram definidos de maneira detalhada para guiar as equipes de
desenvolvimento. Ademais, foram especificados três cenários de desenvolvimento considerando
a complexidade do software a ser desenvolvido, a fim de evidenciar possibilidades distintas na
sequência das atividades durante o fluxo de desenvolvimento do software baseado em ontologia.
Os resultados obtidos sugerem que o processo OntoSoft contribui positivamente na produção
dos artefatos do software baseado em ontologia, colaborando para a efetividade e produtividade
da equipe.",TESE,OntoSoft: um processo de desenvolvimento ágil para software baseado em ontologia,5079434,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5079439,
"One of the most important patterns that occur in ecosystems is the species-area relationship, which
says that the number of species increases with the sampled area. There is a great interest among
ecologists about this pattern, since it is possible to verify the human impact on the environment and the
area of reserves necessary to maintain species. Thus, motivated by the explanation of such behavior,
some mathematical and computational strategies have been developed over the years. However, most
approaches are simulated in homogeneous and regular scenarios, however, in the ecosystem, there
are regions with landforms, different climates and vegetation. Thus, in this work, we are interested in
studying the influence of different environments in the evolution process of the species. We consider
ecological models that use geographical characteristics for colonization and individual behaviors such
as dispersion, mutation, and mating. Thereby, it was possible to simulate the propagation of the species
in different topologies and to analyze how the dynamics occurred in each case. Therefore, we verified
that the regular topology and the homogeneous dispersion of the individuals are two characteristics that
maximize the diversity of species. On the other hand, denser regions and heterogeneous interactions,
contribute to the decrease the number of species, even when in some cases, they help in the speed of
propagation and colonization.",Versão de defesa - Livia Akemi Hotta.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,LIVIA AKEMI HOTTA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),30/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Complex network;species-area relationship;patterns of biodiversity',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,FRANCISCO APARECIDO RODRIGUES,58,Redes complexas;relação espécie-área;padrões de biodiversidade,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Redes Complexas,"Um dos padrões mais importantes que ocorrem em ecossistemas é a relação espécie-área, que
relaciona o número de espécies em um ecossistema com a sua área disponível. O estudo dessa
relação é fundamental para entender-se a biodiversidade e o impacto de políticas ambientais de
preservação de espécies, de modo que é possível analisar desde os tamanhos das reservas
necessários para a conservação das espécies e até verificar o impacto da intervenção humana em
habitats naturais. Assim sendo, várias estratégias matemáticas e computacionais foram desenvolvidas
para prever e entender esse padrão ecológico em modelos ecológicos. Todavia, muitas abordagens
são simuladas em ambientes homogêneos e regulares, porém, sabe-se que, em cada ecossistema, há
regiões com acidentes geográficos, variações de altitudes, vegetação e clima. Dessa forma, nesse
trabalho, estamos interessados em estudar a influência de diferentes ambientes no processo de
evolução das espécies. Para isso, consideramos modelos ecológicos que utilizam caracteríticas
geográficas para colonização e, comportamentos individuais como dispersão, mutação, acasalamento.
Com isso, foi possível simular a propagação das espécies em diferentes topológias e analisar como
ocorreu a dinâmica em cada uma delas. Assim, verificamos que a topologia regular e a dispersão
homogênea dos indivíduos são duas características que maximizam a diversidade de espécies. E por
outro lado, a formação de regiões mais densas e interações heterogêneas, contribuem para a
diminuição da quantidade de espécies, apesar de em alguns casos, ajudarem na velocidade de
propagação e colonização.",DISSERTAÇÃO,Modelos ecológicos em redes complexas,5079598,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5079753,
"Educational systems support dynamic, synchronous and asynchronous interaction between
students and educators. Researches in Learning Analytics, Academic Analytics and
Educational Data Mining explore data from educational systems for knowledge discovery
through analytical processing, statistical analysis and data mining. However, there are some
factors that hinder an ecient management of the educational process. The transformation
of data from dierent kinds of educational system, as Learning Management Systems and
Student Information Systems, can be even more dicult due to data heterogeneity. Data
from these systems can be analyzed considering dierent stakeholders, under dierent
perspectives and under dierent granularities. Motivated by this scenario, in this work we
propose Modelo de Refer^encia de Dados Educacionais (EDRM), a reference data model
for knowledge discovery in data from educational systems. EDRM is an analytical model
structured under a Data Warehouse architecture following a multidimensional data model.
EDRM is projected for being an resource of integrated and correlated data focused in
decision taking in the educational process. EDRM was developed considering a deep
analysis of data and functionalities from dierent educational systems. In this sense, data
from dierent kinds of systems and sources can be used unied, integrated and consistently.
This allows institutions to better comprehend their data, as well as discover patterns, gaps
and ineciencies about their educational process. In this work, EDRM was validated in
a case study using real-world databases from dierent educational systems. The results
indicate that EDRM is ecient in tasks with dierent objectives, using Learning A",Versão de defesa - Vanessa Araujo Borges.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,VANESSA ARAUJO BORGES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),04/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Learning Analytics;Academic Analytics;Educational Data Mining;Business Intelligence;Educational Systems',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ELLEN FRANCINE BARBOSA,184,Learning Analytics;Academic Analytics;Educational Data Mining;Business Intelligence;Sistemas Educacionais,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Computação Aplicada à Educação,"Sistemas educacionais possuem diversas funcionalidades capazes de apoiar a interac~ao
entre alunos e professores de maneira din^amica, sncrona e assncrona. Uma das formas de
monitorar a ecacia do processo educacional e por meio da utilizac~ao dos dados armazenados
nesses sistemas como fonte de informac~ao. Pesquisas em Learning Analytics, Academic
Analytics e Minerac~ao de Dados Educacionais, buscam explorar os dados de sistemas
educacionais utilizando processamento analtico e tecnicas de minerac~ao de dados. No
entanto, ha uma serie de fatores que dicultam a gest~ao eciente do processo educacional
a partir dos dados de sistemas educacionais. A transformac~ao de dados provenientes de
diferentes tipos de sistemas educacionais, como Sistemas de Gest~ao de Aprendizagem e
Sistemas Acad^emicos, e uma tarefa complexa devido a natureza heterog^enea dos dados.
Dados provenientes desses sistemas podem ser analisados considerando diferentes stakeholders,
sob varias perspectivas e nveis de granularidade. Neste cenario, um modelo de
refer^encia para a descoberta de conhecimento a partir de dados de sistemas educacionais,
denominado Modelo de Refer^encia de Dados Educacionais (EDRM), foi desenvolvido neste
trabalho. O EDRM e um modelo dimensional no formato star schema, estruturado em
um Data Warehouse, projetado para ser uma fonte unica de dados integrados e correlacionados
voltada a tomada de decis~ao. Assim, e possvel armazenar dados de diversas
fontes, combina-los e, por m, realizar analises que levem as instituic~oes a desenvolver
uma melhor compreens~ao, rastrear tend^encias e descobrir lacunas e ineci^encias acerca
do processo educacional. Neste trabalho, o EDRM foi validado por meio de um estudo
de caso, utilizando bases de dados reais coletadas de diferentes sistemas educacionais. Os
resultados mostram que o EDRM e eciente em tarefas com diferentes objetivos, utilizando
processamento analtico e minerac~ao de dados.",TESE,Definição de um modelo de referência de dados educacionais para a descoberta de conhecimento,5080115,1
"The task of planning path for autonomous mobile robots consists in determine intermediary
goals in order to allow a robot be able to leave its initial location and reach its final goal. Besides
the planning, it is important to define a method of navigation control (the trajectory following)
of the robot for it be able to do its path safely. This project proposes a hybrid approach to path
planning and execution of an autonomous mobile robot in indoor environments. For the path
planning, search algorithms in state space have been investigated, with emphasis in evolutionary
algorithms and ant colony optimization algorithms for the trajectory search and optimization.
The navigation control is done by local reactive behaviors, based on topological maps, which
allow more flexibility concerning localization definition of position of the mobile robot and
about the details of the environment map (maps with approximate information and not metric).
Thus, a robust method able to plan an optimum or almost optimum path for the robot to reach its
goal safely has been proposed, with little previous information of the environment. Furthermore,
the robot can react to dynamic elements in the environment structure, concerning, for example,
dynamic elements such as doors that can be opened or closed and ways that are blocked. Finally,
several tests and simulations has been carried out to validate the proposed method, with evaluation
of the solutions quality and comparison with others traditional approaches for the path planning
task (A* and D* algorithms).",Versão de defesa - Valéria de Carvalho Santos.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,VALERIA DE CARVALHO SANTOS,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),17/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Exploratory Path Planning;Navigation;Genetic Algorithms;Ant Colony Optimization Algorithms',INTELIGÊNCIA COMPUTACIONAL,FERNANDO SANTOS OSORIO,109,Planejamento Exploratório de Trajetórias;Navegação;Algoritmos Genéticos;Algoritmos de Otimização por Colônia de Formigas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Aperfeiçoamento de uma Plataforma de Aprendizado de Robôs Móveis,"A tarefa de planejamento de trajetórias de robôs móveis autônomos consiste em determinar
objetivos intermediários para que um robô seja capaz de partir de sua localização inicial e
alcançar seu objetivo final. Além do planejamento, é importante definir um método de controle
da navegação (seguimento da trajetória) do robô para que ele seja capaz de realizar seu trajeto
de forma segura. Este projeto propõe uma abordagem híbrida para planejamento exploratório e
execução de trajetórias de robôs móveis autônomos em ambientes indoor. Para o planejamento
de trajetória, foram investigados algoritmos de busca em espaço de estados, dando ênfase ao uso
de algoritmos evolutivos e algoritmos de otimização por colônia de formigas para a descoberta
e otimização da trajetória. O controle da navegação é realizado por meio de comportamentos
locais reativos, baseado na exploração e uso de mapas topológicos, os quais permitem uma
maior flexibilidade em termos de definição da localização da posição do robô móvel e sobre os
detalhes do mapa do ambiente (mapas com informações aproximadas e não métricos). Assim, foi
proposto e desenvolvido um método robusto capaz de planejar, mapear e explorar um caminho
ótimo ou quase ótimo para que o robô possa navegar e alcançar seu objetivo de forma segura,
com pouca informação prévia do ambiente ou mesmo sobre sua localização. Além disso, o
robô pode reagir a ambientes com alterações dinâmicas em sua estrutura, considerando por
exemplo, elementos dinâmicos como portas que possam ser abertas ou fechadas e passagens
que são obstruídas. Por fim, foram realizados diversos testes e simulações a fim de validar o
método proposto, com a avaliação da qualidade das soluções encontradas e comparação com
outras abordagens tradicionais de planejamento de trajetórias (algoritmos A* e D*).",TESE,Uma abordagem híbrida para planejamento exploratório de trajetórias e controle de navegação de robôs móveis autônomos,5080120,1
"Gamification is a term that refers to the use of game design elements in contexts other than video
games. In these contexts, the primary goal of gamification is not playful, but rather to motivate
users to perform tasks or change behaviors. It is also the goal of gamification, captivate users and
influence them to persist in the use of the gamified system. In recent years, we have witnessed a
growing interest in gamification and its application in learning environments, especially online.
In learning contexts, motivating students to follow up on teaching tasks is an important role for
teachers and intelligent educational systems. However, ill-designed gamification interventions
can become a distraction capable of interfering on the teaching-learning process. Despite this,
most studies in the area remain focused on the potential benefits of gamification and less on
investigating systematized solutions to achieve these benefits. Our contribution to the solution of
the problem is based on the use of persuasion profiles that take into account the student’s player
roles. We conduct systematic mappings of the literature to gather information about gamification
in education, and how group formation in collaborative learning environments. As a result,
we created two conceptual frameworks. One framework to help understand and classify group
formation in the context of computer-supported collaborative learning, and other to support the
definition of player roles in collaborative learning environments. Also, in a preliminary study (N
= 481), we adapted and validated for Brazilian Portuguese speakers a scale to measure users’
susceptibility to persuasion. In another study (N = 149) we developed a theoretical model to
map persuasive strategies and different roles of players to support the elaboration of persuasion
profiles. Finally, to verify the feasibility of our model, in another study (N = 18) we elaborated
prototypes of user interfaces and analyzed the perceived persuasiveness of the interfaces for
different players roles and their susceptibility to persuasion. Results show that less motivated
students were more likely to accept the suggestions of the prototypes, whereas users with
above-average motivation (among observed students) reacted negatively to influence attempts by
showing low agreement rates for the requirements of the prototypes. We also observed in the
three studies (N = 648) that the number of individuals susceptible to the principle of authority
were the lowest, compared to the other influence principles. Few research initiatives have been
investigating the development of tailored gamified. One of the reasons for such deficiency is
the difficulty of creating computational models based on learner’s psychological traits (e.g.,
psychological needs, susceptibility to persuasion, and learner and player roles). However, more
worrisome than the ineffectiveness of gamification models based on one-size-fits-all is the risk
of designing counterproductive models that could backfire, since the appropriate strategy to motivate an individual may end up discouraging others. Thus, evidence suggest that gamification
design could benefit of influence principles, although tailored solutions should be designed to
minimize the risks of selecting counter-tailored and ill-defined persuasive strategies.",,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,SIMONE DE SOUSA BORGES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),05/10/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Gamification;Persuasion Profiling;Influence Principles;Persuasive Technology;Group Formation',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,SEIJI ISOTANI,190,Gamificação;Perfil Persuasivo;Princípios de Influência;Techonologia Persuasiva;Formação de Grupos,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Ambientes Inteligentes de Aprendizagem Colaborativa,"Gamificação é um termo que se refere ao uso de elementos do projeto de jogos em contextos
que não são jogos. Nestes contextos, o objetivo primário da gamificação não é lúdico, mas
sim o desmotivar os usuários a realizarem tarefas ou alterarem comportamentos. Também é
objetivo da gamificação, cativar usuários e influenciá-los a persistirem na utilização do sistema
gamificado. Nos últimos anos, testemunhamos um crescente interesse em gamificação e sua
aplicação em ambientes de aprendizagem, especialmente online. No contexto da aprendizagem,
motivar os estudantes a dar seguimento as tarefas pedagógicas é um papel importante dos
professores e dos ambientes educacionais inteligentes. Por essa razão, as tecnologias persuasivas
como a gamificação têm sido usadas também em ambientes de aprendizagem colaborativa
para aumentar o engajamento dos estudantes e para reduzir o sentimento de obrigação na
execução de tarefas pedagógicas. Contudo, quando mal utilizada, a gamificação pode se tornar
uma distração capaz de interferir no processo de ensino-aprendizagem. Entretanto, a maioria
dos estudos na área continuam focados nos potenciais benefícios da gamificação e menos em
investigar soluções sistematizadas para se atingir os benefícios. Nossa contribuição para a
solução do problema é baseada no uso de perfis de persuasão que levam em consideração o
papel de jogador do estudante. Nós conduzimos mapeamentos sistemáticos da literatura para
obter informação sobre gamificação em educação e como são formados grupos de estudantes
em ambientes de aprendizagem colaborativa. Como resultado nós criamos dois arcabouços
conceituais. Um arcabouço para ajudar a compreender e classificar a formação de grupos
no contexto da aprendizagem colaborativa com suporte computacional, e outro para apoiar a
definição de papéis de jogadores em ambientes colaborativos. Em um estudo preliminar (N=481),
adaptamos e validamos para o português brasileiro uma escala para medir a susceptibilidade
à persuasão dos usuários. Em outro estudo (N=149) desenvolvemos um modelo teórico para
mapear estratégias persuasivas e diferentes papéis de jogadores para apoiar a elaboração de perfis
de persuasão. Para verificar a viabilidade de nosso modelo, em outro estudo (N=18) elaboramos
protótipos de interfaces do usuário. Analisamos a capacidade de influenciar das interfaces
comparando papéis de jogadores e susceptibilidade a princípios de influência. Os resultados
mostram que os estudantes menos motivados eram mais susceptíveis a aceitar as sugestões
do protótipo, enquanto usuários com índices de motivação acima da média (dentre estudantes
observados), tendiam a reagir negativamente às tentativas de influenciá-los, apresentando índices
menores de concordância para com as solicitações do protótipo gamificado. Observamos ainda
nos três estudos conduzidos (N=648), comparado aos outros princípios de influência, o número de indivíduos suscetíveis ao princípio de autoridade eram os menores. Poucas iniciativas de
pesquisa vêm investigando como desenvolver sistemas de gamificados que se adaptam aos papéis
de jogadores. Parte desta deficiência pode ser explicada devido à complexidade no projeto e
desenvolvimento destes sistemas. Entretanto como evidenciado, além da ineficácia dos modelos
de gamificação baseados em uma solução para todos, o maior risco observado está no uso de
modelos contraproducentes, uma vez que a estratégia apropriada para motivar um indivíduo,
pode acabar desmotivando outros (backfire effect).",TESE,Design de gamificação em aprendizagem colaborativa com suporte computacional: utilizando uma abordagem para a adaptação de princípios de influência a papéis de jogadores,5080176,1
"Groundwater studies face computational limitations when providing local detail within regional
models. The researchers are concentrated on applying the numerical models to minimize the
difference between the physical reality and the implemented numerical model by considering
the minimum computational cost. This work consists of the study of line-elements (such as
line-doublets, circles, polygons, fractures) using the Analytic Element Method (AEM) for
groundwater flow. In this work, we consider the study of two-dimensional groundwater flow in
fractured porous media by the Analytic Element Method. We develop a numerical solution based
on a series expansion for a problem with more than one fracture. Each fracture has an influence
that can be expanded in a series that satisfies Laplace’s equation exactly. In the series expansion,
the unknown coefficients are obtained from the discharge potentials of all other elements that are
related to the expansion coefficients. Sizes, locations and conductivities for all inhomogeneities
are selected arbitrarily. This work also discusses a matrix method obtained by imposing the
intern boundary conditions for the Analytic Element Method. The convergence analysis of a
Gauss-Seidel type iterative method is also discussed.",Versão revisada - Sardar Muhammad Hussain.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,SARDAR MUHAMMAD HUSSAIN,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),28/09/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Analytic Element Method;Fractures;Groundwater Flow;Matrix Method;Numerical Simulations',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,JOSE ALBERTO CUMINATO,135,Método do Elemento Analítico;Fracturas;Escoamento de Águas Subterrâneas;Método da Matrix;Simulações Numéricas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Solução Numérica das Equações de Navier-Stokes - Escoamentos Tridimensionais,"Estudos de águas subterrâneas enfrentam limitações computacionais ao fornecer detalhes locais
em modelos regionais. Os pesquisadores estão concentrados na aplicação dos modelos numéricos
para minimizar a diferença entre a realidade física e o modelo numérico implementado
considerando o custo computacional mínimo. Este trabalho consiste no estudo de elementos de
linha (como line-doublets, círculos, polígonos, fraturas) usando o Método de Elemento Analítico
(AEM) para o fluxo de águas subterrâneas. Neste trabalho, consideramos o estudo do fluxo
bidimensional de águas subterrâneas em meios porosos fraturados pelo Método dos Elementos
Analíticos. Desenvolvemos uma solução numérica baseada em uma expansão em série para um
problema com mais de uma fratura. Cada fratura tem uma influência que pode ser expandida em
uma série que satisfaça exatamente a equação de Laplace. Na expansão da série, os coeficientes
desconhecidos são obtidos a partir dos potenciais de descarga de todos os outros elementos
que estão relacionados aos coeficientes de expansão. Tamanhos, locais e condutividades para
todas as não-homogeneidades são arbitrariamente selecionados. Este trabalho também discute
o método da matriz obtido impondo as condições de contorno do interno para o Método do
Elemento Analítico. A análise de convergência de um método iterativo tipo Gauss-Seidel também
é discutida.",TESE,Simulação do escoamento de água subterrânea pelo o método de elemento analítico,5080260,1
"This work introduces a methodology for characterizing the runtime resource demands of a
computer program from the analysis of its binary executable file. Categorization of processes
according to the kind of resources required during execution — such as CPU and memory usage
— is a sought-after piece of knowledge for the aims of computer system design and management.
Conventional techniques available for this purpose include white-box static source code analysis
and profile matching based on historical execution data. The former tends to be challenging
in face of complex software architectures and requires access to the source code; the latter
is dependent on the availability of reliable past data and on the selection of features yielding
effective correlations with resource usage. The alternative data mining approach proposed
in this paper avoids those difficulties by manipulating binary executable files. The method
combines techniques from information theory, complex networks and phylogenetics to produce
a hierarchical clustering of a set of executable files, which can be used to predict potential
similarities in terms of runtime resource usage. The work’s research questions investigate if the
transformation performed by the compiler preserves similarity information across the source and
binary code representation such that it can be detected by standard compression algorithms; and
if the so identified similarities in the symbolic object encoding are correlated to runtime resource
usage to an extent which allows for inferring the program’s behavior from the analysis of the
binary file. The paper introduces the method’s rationales and presents results of its application to
characterize CPU and IO usages of benchmark applications executed on a standard PC platform.
Essays carried out over a set of 80 executable programs from varying sources yielded numerically
significant evidences that the prediction of resource usage similarity obtained by the approach is
consistent with experimentally measured runtime profile. The application of the method is also",,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,RENE DE SOUZA PINTO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),14/09/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Data mining;performance evaluation;computational modeling;runtime profiling',"SISTEMAS DISTRIBUÍDOS E PROGRAMAÇÃO CONCORRENTE/SISTEMAS EMBARCADOS, EVOLUTIVOS E ROBÓTICOS",ALEXANDRE CLAUDIO BOTAZZO DELBEM,199,Mineração de dados;avaliação de desempenho;modelagem computacional;perfil de execução,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Eficiência em Soluções no Mundo Real,"Este trabalho apresenta uma metodologia para caracterização do perfil de consumo de recursos
demandados por um programa de computador a partir da análise do código binário do arquivo
executável. A categorização de processos de acordo com seus perfis de consumo de recursos
durante a execução—tais como uso de CPU e memória—é uma informação muito desejada para
objetivos de projeto e gerenciamento de sistemas. Técnicas convencionais para este propósito
são baseadas em testes de caixa branca (que avaliam o código fonte da aplicação), que tendem a
ser de difícil aplicação dado a complexidade das arquiteturas de software além da necessidade
de acesso ao código fonte; ou detecção de perfis baseada em dados de execução, que depende da
disponibilidade de dados de execução confiáveis e da seleção de características que de fato vão
correlacionar o perfil de consumo. A abordagem baseada em mineração de dados proposta neste
trabalho evita estas dificuldades uma vez que manipula somente os arquivos binários executáveis.
O método combina técnicas provindas da teoria da informação, redes complexas e filogenia para
produzir um agrupamento hierárquico de um conjunto de arquivos de programas executáveis que
pode ser utilizado para prever potenciais similaridades em termos de consumo de recursos em
tempo de execução. As questões de pesquisa deste trabalho investigam se a transformação feita
pelo compilador preserva similaridades entre código fonte e binário que podem ser detectadas
através de algoritmos de compressão; em caso positivo, verificar se as similaridades encontradas
no código binário estão relacionadas com o perfil de execução das aplicações, permitindo inferir
o comportamento dos programas a partir da análise do código binário. Este trabalho apresenta a
sistematização do método assim como os resultados da aplicação para caracterizar aplicações
em termos de consumo de CPU e Entrada/Saída em uma plataforma PC padrão. Diversos
experimentos foram executados em um repositório de 80 programas de várias fontes obtendo-se
resultados significativos que evidenciam que a similaridade dos perfis de execução obtidas com
esta abordagem é consistente com as obtidas experimentalmente por aferição. A aplicação do
método também é exemplificado através de casos de estudo que caracterizam o perfil de execução
de programas executáveis.",TESE,Caracterização do perfil de utilização de recursos de programas a partir de arquivos executáveis utilizando mineração de dados,5080271,1
"In this work, a numerical method for simulating viscoelastic free surface flows governed by the
Giesekus constitutive equation is developed. The governing equation are solved by the finite difference
method on a staggered grid. The fluid free surface is approximated by marker particles
which enables the visualization and location of the free surface fluid. The Giesekus constitutive
equation is solved by the following techniques: second-order Runge-Kutta, conformation tensor
and logarithmic transformation of the conformation tensor. The numerical method is verified
by comparing the numerical solutions obtained on a series of embedding meshes of the flow in
a tube and by the flow produced by a jet flowing onto a planar surface. Additional verification
and convergence results are obtained by by solving tube flow employing several meshes. Results
obtained include the simulation of a jet flowing into a three dimensional container and the
simulation of extrudate swell using several values of the Reynolds and Weissenberg numbers
and different values of the mobility parameter a. Furthermore, we present results from the
simulation of the phenomenon know as delayed dieswell using highWeissenberg and Reynolds
numbers. Comparisons with experimental results are given.",Versão de defesa - Reginaldo Merejolli.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,REGINALDO MEREJOLLI,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),17/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Finite difference;Viscoelastic flows;Free surface;Giesekus model;Extrudate swell',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,MURILO FRANCISCO TOME,91,Diferenças finitas;Escoamentos viscoelásticos;Superfícies livres;Modelo Giesekus;Inchamento do extrudado,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Solução Numérica das Equações de Navier-Stokes - Escoamentos Tridimensionais,"Este trabalho tem como objetivo o desenvolvimento de um método numérico para simular escoamentos
viscoelásticos tridimensionais com superfícies livres governados pelo modelo constitutivo
Giesekus. As equações governantes são resolvidas pelo método de diferenças finitas numa
malha deslocada. A superfície livre do fluido é modelada por partículas marcadoras, possibilitando
assim a visualização e localização da superfície livre do fluido. A equação constitutiva de
Giesekus é resolvida utilizando as seguintes formulações: método de Runge-Kutta de segunda
ordem (também conhecido como método de Euler modificado) e transformação logarítmica do
tensor conformação. O método numérico apresentado é verificado comparando-se os resultados
obtidos por meio de refinamento de malha para os escoamentos em um tubo e de um jato incidindo
em uma placa plana. Resultados de convergência foram obtidos por meio de refinamento
de malha do escoamento totalmente desenvolvido em um tubo. Os resultados numéricos obtidos
incluem a simulação de um jato incidindo em uma caixa vazia e a simulação do inchamento
do extrudado (dieswell) para vários números de Weissenberg utilizando diferentes valores do
fator de mobilidade do fluido. Resultados adicionais incluem simulações do fenômeno delayed
dieswell para altos números de Weissenberg e altos valores do número de Reynolds. Uma comparação
qualitativa com resultados experimentais é apresentada.",TESE,Simulação numérica de escoamentos tridimensionais com superfícies livres governados pelo modelo Giesekus,5080323,1
"Temporal data are ubiquitous in nearly all areas of human knowledge. The research field known
as machine learning has contributed to temporal data mining with algorithms for classification,
clustering, anomaly or exception detection, and motif detection, among others. These algorithms
oftentimes are reliant on a distance function that must be capable of expressing a similarity
concept among the data. One of the most important classification models, the 1-NN, employs a
distance function when comparing a time series of interest against a reference set, and assigns to
the former the label of the most similar reference time series.
There are, however, several domains in which the temporal data are insufficient to characterize
neighbors according to the concepts associated to the classes. One possible approach to this
problem is to transform the time series into a representation domain in which the meaningful
attributes for the classifier are more clearly expressed. For instance, a time series may be
decomposed into periodic components of different frequency and amplitude values. For several
applications, those components are much more meaningful in discriminating the classes than the
temporal evolution of the original observations.
In this work, we employ diversity of representation and distance functions for the classification of
time series. By choosing a data representation that is more suitable to express the discriminating
characteristics of the domain, we are able to achieve classification that are more faithful to the
target-concept. With this goal in mind, we promote a study of time series representation domains,
and we evaluate how such domains can provide alternative decision spaces. Different models
of the 1-NN classifier are evaluated both isolated and associated in classification ensembles in
order to construct more robust classifiers.
We also use distance functions and alternative representation domains in order to extract nontemporal
attributes, known as distance features. Distance features reflect neighborhood concepts
of the instances to the training samples, and they may be used to induce classification models
which are typically not as efficient when trained with the original time series observations. We
show that distance features allow for classification results compatible with the state-of-the-art.",Versão Revisada - Rafael Giusti.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,RAFAEL GIUSTI,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),23/08/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Machine learning;Artificial Intelligence;Time series;Time series classification;Time series representation',INTELIGÊNCIA COMPUTACIONAL,GUSTAVO ENRIQUE DE ALMEIDA PRADO ALVES BATISTA,232,Aprendizado de máquina;Inteligência artificial;Séries temporais;Classificação de séries temporais;Representação de séries temporais,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Invariânca à Complexidade em Classificação, Agrupamento e Descoberta de Motifs em Séries Temporais","Dados temporais são ubíquos em quase todas as áreas do conhecimento humano. A área de
aprendizado de máquina tem contribuído para a mineração desse tipo de dados com algoritmos
para classificação, agrupamento, detecção de anomalias ou exceções e detecção de padrões
recorrentes, dentre outros. Tais algoritmos dependem, muitas vezes, de uma função capaz
de expressar um conceito de similaridade entre os dados. Um dos mais importantes modelos
de classificação, denominado 1-NN, utiliza uma função de distância para comparar uma série
temporal de interesse a um conjunto de referência, atribuindo à primeira o rótulo da série de
referência mais semelhante.
Entretanto, existem situações nas quais os dados temporais são insuficientes para identificar
vizinhos de acordo com o conceito associado às classes. Uma possível abordagem é transportar as
séries para um domínio de representação no qual atributos mais relevantes para a classificação são
mais claros. Por exemplo, uma série temporal pode ser decomposta em componentes periódicas
de diferentes frequências e amplitudes. Para muitas aplicações, essas componentes são muito
mais significativas na discriminação das classes do que a evolução da série ao longo do tempo.
Nesta Tese, emprega-se diversidade de representações e de distâncias para a classificação de
séries temporais. Com base na escolha de uma representação de dados adequada para expor as
características discriminativas do domínio, pode-se obter classificadores mais fiéis ao conceitoalvo.
Para esse fim, promove-se um estudo de domínios de representação de dados temporais,
visando identificar como esses domínios podem estabelecer espaços alternativos de decisão.
Diferentes modelos do classificador 1-NN são avaliados isoladamente e associados em ensembles
de classificadores a fim de se obter classificadores mais robustos.
Funções de distância e domínios alternativos de representação são também utilizados neste
trabalho para produzir atributos não temporais, denominados atributos de distâncias. Esses
atributos refletem conceitos de vizinhança aos exemplos do conjunto de treinamento e podem
ser utilizados para treinar modelos de classificação que tipicamente não são eficazes quando
treinados com as observações originais. Nesta Tese mostra-se que atributos de distância permitem
obter resultados compatíveis com o estado-da-arte.",TESE,Classiﬁcação de séries temporais utilizando diferentes representações de dados e ensembles,5080346,1
"Brazil is a world reference in the production and export of citrus, although this crop can suffer
several problems and losses of productivity for diverse reasons, as for example, pests. In order to
reduce risks and losses, it is interesting to use automated monitoring systems, justifying the need
to perform data collection to determine several factors, such as: fruit quantity estimation, pest
detection, rate of development and Degree of maturity of the plants, which is directly related to
the resistance, flavor and quality of the fruit, among others. Certain plantations, such as citrus
plantations, can not be monitored only via soil or only via aerial images, making it necessary to
merge both approaches according to the parameter to be monitored. With this we propose to a
system of monitoring of agricultural environments based on the cooperation of terrestrial robots
and aerial robots, that work together to carry out the data collection in the field.",Versão de defesa - Eduardo Sacogne Fraccaroli.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,EDUARDO SACOGNE FRACCAROLI,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),05/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Task Allocation;Heterogeneous Robots;Precision Agriculture;MD-MTSP;Quadrotor;Robot Operating System (ROS)',INTELIGÊNCIA COMPUTACIONAL,ROSELI APARECIDA FRANCELIN ROMERO,104,Alocação de Tarefas;Robôs Heterogêneos;Agricultura de Precisão;MDMTSP;Quadrirotor;Robot Operating System - ROS,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Aperfeiçoamento de uma Plataforma de Aprendizado de Robôs Móveis,"O Brasil é uma referência mundial na produção e exportação de citrus, entretanto esse cultivo
pode sofrer diversos problemas e perdas de produtividade por motivos diversos, como por
exemplo, pragas. Para reduzir os riscos e perdas, torna-se interessante o uso de sistemas
automatizados de monitoramento, justificando a necessidade de realizar a coleta de dados para
determinar diversos fatores, tais como: estimativa da quantidade de frutos, detecção de pragas,
taxa de desenvolvimento e grau de maturidade das plantas, que é diretamente relacionada com
a resistência, sabor e qualidade do fruto, entre outros. Determinadas plantações, como a de
citrus, não podem ser monitoradas somente via solo ou somente via imagens aéreas, tornando
necessário mesclar ambas as abordagens de acordo com o parâmetro a ser monitorado. Com
isso propomos a um sistema de monitoramento de ambientes agrícolas baseado na cooperação
de robôs terrestres e robôs aéreos, que trabalham em conjunto para realizar a coleta dados em
campo.",TESE,Alocação de tarefas para a coordenação de robôs heterogêneos aplicados a agricultura de precisão,5377821,1
"The huge amount of textual data in the web may be handled as the main scenery for many
application of Natural Language Procession. Here, there are many sources that share distinct
information about a given subject and, moreover, usually new related content is
quickly produced in digital media. This scenery may be handled for the Update Summarization
(US) task, which aims to produce a summary from a text collection under the
assumption the user has some previous knowledge. For instance, if the user has already
read some texts previously, we may produce a summary from new related documents
in order to show him the most salient, recent and updated content. The production of
summaries automatically requires many research challenges, mainly during the content selection
and synthesis of the summary stages. For the first one, it has been proposed many
and distinct approaches, which requires different theories and computational complexity.
However, most of them do not use some deep linguistic knowledge, which may assist the
identification of the most salient and updated information in the source-texts. Aiming
the content synthesis of summaries, the most of the systems use an extractive approach,
in which some sentences from the source-texts are picked and organized in the output
without rewriting operations. Even with satisfactory results, the extractive synthesis may
to reduce the informativeness of the summaries, once some irrelevant or redundant information
may occur in some segments of the selected sentences. Thus, recent investigations
have focused on the compressive approach, in which the systems may use short versions
of some picked sentences in the output summaries. Before this work, most of the investigations
related to Automatic Summarization for the Portuguese language were focused
on single and multi-document summarization by use the extractive approach. Given this
background, we have investigated US methods by use the extractive and compressive approaches
for Portuguese. Given the amount of approaches for content selection, our main
hypothesis is based on the idea to enrich them with some linguistic theories can to produce
more informative summaries. Thus, we have proposed distinct ways to use this kind
of information for the most relevant US methods of different approaches. The results
show the used linguistic knowledge assists just some US approaches. Furthermore, we
also have proposed some topic model simplifications based on the Subtopic theory. These
methods require less computational power and have shown good results. It is important
to say we have performed experiments for US over datasets on Portuguese and English, in
which there are more resources. After that, we have investigated Sentence Compression methods by use Machine Learning in order to use them into a compressive architecture for
US. The proposed method with the highest evaluation scores beats a state of art system,
which is based on Deep Learning techniques. Once we have focused on Portuguese, we
also have proposed three corpora for this language, the CSTNews-Update, which enables
the investigation of US, and the PCSC-Pairs and G1-Pairs datasets, which are used to
produce and evaluate the sentence compression methods.",,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,FERNANDO ANTONIO ASEVEDO NOBREGA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),12/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Update Summarization;Sentence Compression;Compressive Summarization',INTELIGÊNCIA COMPUTACIONAL,THIAGO ALEXANDRE SALGUEIRO PARDO,173,Sumarização Automática de Atualização;Compressão Sentencial;Sumarização Compressiva,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Sumarização Automática,"O enorme volume de dados textuais disponível na web se caracteriza como um cenário
ideal para inúmeras aplicações do Processamento de Língua Natural. Nesse ambiente,
diferentes informações sobre algum assunto são publicadas por inúmeras fontes e novo
conteúdo relacionado é rapidamente divulgado em meios digitais. Nesse contexto, tem-se
a tarefa da Sumarização Automática de Atualização (SAA), que tem por objetivo a geração
automática de resumos a partir de uma coleção textual relacionados admitindo-se
que o leitor possui algum conhecimento prévio sobre os textos-fonte. Por exemplo, tendo
em vista os textos já lidos por um usuário, pode-se produzir um sumário a partir de novos
conteúdos visando lhe apresentar somente as informações mais relevantes, recentes, novas
e atualizadas. O processo de geração de resumos implica em diversos desafios, sobretudo
na seleção e síntese de conteúdo para o sumário. No contexto da seleção de conteúdo,
têm-se inúmeras abordagens na literatura, com diferentes níveis de complexidade teórica e
computacional. Entretanto, pouco dessas investigações fazem uso de algum conhecimento
linguístico profundo, que pode auxiliar a identificação de conteúdo mais relevante. No âmbito
da síntese de conteúdo, a abordagem mais empregada é a extrativa, na qual algumas
sentenças dos textos-fonte são selecionadas e, sem alteração, organizadas no sumário. Tal
abordagem, embora apresente resultados satisfatórios, pode limitar a informatividade do
sumário, uma vez que alguns segmentos sentenciais podem conter informação redundante
ou irrelevante. Dessa forma, esforços recentes foram direcionados à síntese compressiva,
na qual as sentenças selecionadas para o sumário são eventualmente comprimidas previamente
à inserção no sumário. Anteriormente a este trabalho, a maioria das investigações
de Sumarização Automática para a língua Portuguesa foi direcionada à geração de sumários
a partir de um (monodocumento) ou vários textos relacionados (multidocumento) por
meio da síntese extrativa. Dado esse cenário, este trabalho de doutorado objetivou investigar
a SAA por meio da síntese extrativa e compressiva com ênfase na língua Portuguesa.
Dada as inúmeras abordagens para a etapa de seleção de conteúdo na SAA, a tese deste
trabalho foi que enriquecer-las com conhecimentos linguísticos auxilia a produção de sumários
mais informativos. Assim, investigaram-se maneiras para adicionar conhecimentos
linguísticos nos métodos mais representativos de cada abordagem de SAA. Por meio dos
resultados, observou-se que somente algumas abordagens foram auxiliadas pelo uso desse
conhecimento. Além disso, foram propostas algumas simplificações para um modelo de
distribuição de tópicos por meio da teoria linguística de Subtópicos. Tais métodos requerem menor complexidade computacional e apresentaram bons desempenhos. Ressalta-se
que no escopo da SAA, foram realizados experimentos para a língua Portuguesa e Inglesa,
que possui recursos amplamente difundidos. Posteriormente, desenvolveram-se inúmeros
métodos de Compressão Sentencial por meio de algoritmos de Aprendizado de Máquina
para incorporar uma arquitetura de síntese compressiva para a SAA. O melhor método
apresentou resultados superiores a um trabalho do estado da arte, que faz uso de algoritmos
de Deep Learning. Tendo a língua Portuguesa como principal objeto de estudo,
foram organizados três córpus, o CSTNews-Update, que viabiliza experimentos de SAA, e
o PCSC-Pares e G1-Pares, para o desenvolvimento/avaliação de métodos de Compressão
Sentencial.",TESE,Sumarização Automática de Atualização para a língua portuguesa,5377862,1
"In the last few years, the Web of data is being rapidly populated with biodiversity data. However,
when researchers need to retrieve, integrate, and visualize these data, they need to rely on
semi-manual approaches. That is due to the fact that biodiversity repositories, such as GBIF,
offer data as just strings in CSV format spreadsheets. There is no machine readable metadata
that could add meaning (semantics) to data. Without this metadata, automatic solutions are
impossible and labor intensive semi-manual approaches for data integration and visualization
are unavoidable. To reduce this problem, we present a novel architecture, called STBioData, to
automatically link spatiotemporal biodiversity data, from heterogeneous data sources, to enable
easier searching, visualization and downloading of relevant data. It supports the generation of
interactive maps and mapping between biodiversity data and ontologies describing them (such
as Darwin Core, DBpedia, GeoSPARQL, Time and PROV-O). A new biodiversity provenance
model (BioProv), extending the W3C PROV Data Model, was proposed. BioProv enables
applications that deal with biodiversity data to incorporate provenance data in their information.
A web based prototype, based on this architecture, was implemented. It supports biodiversity
domain experts in tasks, such as identifying a species conservation status, by automating most
of the necessary tasks. It uses collection data, from important Brazilian biodiversity research
institutions, and species geographic distributions and conservation status, from the IUCN Red
List of Threatened Species. These data are converted to linked data, enriched and saved as
RDF Triples. Users can access the system, using a web interface, and search for collection
and species distribution records based on species names, time ranges and geographic location.
After a data set is recovered, it can be displayed in an interactive map. The records contents are
also shown (including provenance data) together with links to the original records at GBIF and
IUCN. Users can export datasets, as a CSV or RDF file, or get a print out in PDF (including the
visualizations). Choosing different time ranges, users can, for instance, verify the evolution of a
species distribution. The STBioData prototype was tested using use cases. For the tests, 46,211
collection records, from SpeciesLink, and 38,589 conservation status records (including maps),
from IUCN, for marine mammal were converted to 2,233,782. RDF triples and linked using well
known ontologies. 90% of biodiversity experts, using the tool to determine conservation status,
were able to find information about dolphin species, with a satisfactory recovery time, and were
able to understand the interactive map. In an information retrieval experiment, when compared
with SpeciesLink keyword based search, the prototype’s semantic based search performed, on
average, 24% better in precision and 22% in recall tests. And that does not takes into accountcases where only the prototype returned search results. These results demonstrate the value of
having public available linked biodiversity data with semantics.",Versão revisada - Flor Karina Mamani Amanqui.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,FLOR KARINA MAMANI AMANQUI,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),20/11/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'SemanticWeb;Linked Open Data;Provenance;Biodiversity',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,DILVAN DE ABREU MOREIRA,126,"Web semântica;Dados abertos vinculados;Proveniência, Biodiversidade",CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Web Semântica na Criação de Anotações Inteligentes em Bioinformática,"Nos últimos anos, a Web de dados está sendo rapidamente preenchida com dados de biodiversidade.
No entanto, quando pesquisadores precisam recuperar, integrar e visualizar esses dados,
eles precisam confiar em abordagens semi-manuais. Isso ocorre devido ao fato de que repositórios
sobre biodiversidade, como GBIF, oferecem dados como cadeias de caracteres em planilhas
no formato CSV. Não há nenhum metadado legível por máquinas que poderia acrescentar significado
(semântico) aos dados. Sem os metadados, soluções automáticas são impossíveis, sendo
necessário para visualização e integração dos dados, a utilização de abordagens semi-manuais.
Para reduzir esse problema, apresentamos uma arquitetura chamada STBioData. Com ela é
possível vincular automaticamente dados de biodiversidade, com informações espaço-temporais
provenientes de fontes heterogêneas, tornando mais fácil a pesquisa, visualização e download
dos dados relevantes. Ele suporta a geração de mapas interativos e o mapeamento entre dados de
biodiversidade e ontologias que os descrevem (como Darwin Core, DBpedia, GeoSPARQL, Time
e PROV-O). Foi proposto um novo modelo de proveniência para biodiversidade (BioProv), que
estende o modelo de dados PROV W3C. BioProv permite que aplicativos que lidam com dados
de biodiversidade incorporem os dados de proveniência em suas informações. Foi implementado
um protótipo Web baseado nesta arquitetura. Ele oferece suporte aos especialistas do domínio
de biodiversidade em tarefas como, identificação do status de conservação da espécie, além de
automatizar a maioria das tarefas necessária. Foi utilizado coleções de dados de importantes
pesquisas brasileiras sobre biodiversidade, juntamente com dados de distribuição geográfica das
espécies e seu estado de conservação, provenientes da lista de espécies ameaçadas da IUCN
(Red List). Esses dados são convertidos em dados conectados, enriquecidos e salvados como
triplas RDF. Os usuários podem acessar o sistema, usando uma interface web que permite
procurar, utilizando os nomes das espécies, intervalos de tempo e localização geográfica. Os
dados recuperados podem ser visualizados no mapa interativo. O conteúdo de registros também
é mostrado (incluindo dados de proveniência), juntamente com links para os registros originais
no GBIF e IUCN. Os usuários podem exportar o conjunto de dados, como um arquivo CSV
ou RDF, ou salvar em PDF (incluindo as visualizações). Escolhendo diferentes intervalos de
tempo, os usuários podem por exemplo, verificar a evolução da distribuição das espécies. O
protótipo STBioData foi testado usando casos de uso. Para esses testes, 46.211 registros de
coleção do SpeciesLink e 38.589 registros de estado de conservação da IUCN (incluindo mapas),
sobre mamíferos marinhos, foram convertidos em 2.233.782 triplas RDF. Essas triplas reutilizam ontologias representativas da área . 90% dos especialistas em biodiversidade, usaram a ferramenta
para determinar o estado de conservação, eles foram capaz de encontrar as informações
sobre determinada espécie de golfinho, com um tempo de recuperação satisfatório e também
foram capaz de entender o mapa interativo gerado. Em um experimento sobre recuperação de
informações, quando comparado com o sistema de busca por palavra-chave utilizado pela base
SpeciesLink, a busca semântica realizada pelo protótipo STBioData, em média, é 24% melhor
em testes de precisão e 22% melhor em testes de revocação. Não são considerados os casos onde
o protótipo somente retornou o resultado da busca. Esses resultados demonstram o valor de ter
dados conectados sobre biodiversidade disponíveis publicamente em um formato semântico.",TESE,Usando um modelo de proveniência e informações espaço-temporais para integrar dados semânticos heterogêneos sobre biodiversidade,5377993,1
"With the growing volume of opinion information on the web, extracting and synthesizing subjective and
relevant content from the web has to be shown a priority task that passes through different society
domains, such as political, social, economical, etc. The semantic organization of this type of content is
very important nowadays since it allows a better use of those data, as well as it benefits customers and
both private and governmental organizations. The area responsible for extracting, processing and
presenting the subjective content is opinion mining, also known as sentiment analysis. Opinion mining
is divided into granularity levels: document, sentence and aspect levels. In this research, the deepest
level of granularity was studied, the opinion mining based on aspects, which consists of three main
tasks: aspect recognition and clustering, polarity extracting, and summarization. Aspects are the
properties and parts of the evaluated object and it may be implicit or explicit. Recognizing and
clustering aspects are critical tasks for opinion mining; nonetheless, they are also challenging. For
example, in reviews, users use distinct terms to refer to the same object property. Therefore, in this
work, the aspect clustering task was the focus. To solve this problem, a linguistic approach was chosen.
The main intrinsic and extrinsic phenomena in reviews were investigated in order to find linguistic
standards and actionable inputs, so it was possible to propose automatic methods of aspect clustering
for opinion mining. In addition, six automatic linguistic-based methods for explicit and implicit aspect
clustering were proposed, implemented and compared. Besides that, a new method was suggested for
this task, which surpassed the other implemented methods, specially the synonym lexicon-based
method (baseline) and a word embeddings approach. This suggested method is also language and
domain independent and, in this work, was tailored for Brazilian Portuguese and products domain.",Versão de defesa - Francielle Alves Vargas.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,FRANCIELLE ALVES VARGAS,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),29/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Aspect-based opinion mining;Aspect clustering;Natural language processing',INTELIGÊNCIA COMPUTACIONAL,THIAGO ALEXANDRE SALGUEIRO PARDO,126,Mineração de opinião baseada em aspectos;Agrupamento de aspectos;Processamento de linguagem natural,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Sumarização Automática,"Com o rápido crescimento do volume de informações opinativas na web, extrair e sintetizar conteúdo
subjetivo e relevante da rede é uma tarefa prioritária e que perpassa vários domínios da sociedade:
político, social, econômico, etc. A organização semântica desse tipo de conteúdo, é uma tarefa
importante no contexto atual, pois possibilita um melhor aproveitamento desses dados, além de
benefícios diretos tanto para consumidores quanto para organizações privadas e governamentais. A
área responsável pela extração, processamento e apresentação de conteúdo subjetivo é a mineração
de opinião, também chamada de análise de sentimentos. A mineração de opinião é dividida em níveis
de granularidade de análise: o nível do documento, o nível da sentença e o nível de aspectos. Neste
trabalho, atuou-se no nível mais fino de granularidade, a mineração de opinião baseada em aspectos,
que consiste de três principais tarefas: o reconhecimento e agrupamento de aspectos, a extração de
polaridade e a sumarização. Aspectos são propriedades do objeto avaliado e podem ser implícitos e
explícitos. Reconhecer e agrupar aspectos são tarefas críticas para mineração de opinião, no entanto,
também são desafiadoras. Por exemplo, em textos opinativos, usuários utilizam termos distintos para
se referir a uma mesma propriedade do objeto. Portanto, neste trabalho, focamos no problema de
agrupamento de aspectos para mineração de opinião. Para resolução deste problema, optamos por
uma abordagem linguística. Investigou-se os principais fenômenos intrínsecos e extrínsecos em textos
opinativos a fim de encontrar padrões linguísticos e insumos acionáveis para proposição de métodos
automáticos de agrupamento de aspectos correlatos para mineração de opinião. Nós propomos,
implementamos e comparamos seis métodos automáticos baseados em conhecimento linguístico para
a tarefa de agrupamento de aspectos explícitos e implícitos. Um método inédito foi proposto para essa
tarefa que superou os demais métodos implementados, especialmente o método baseado em léxico
de sinônimos (baseline) e o modelo estatístico com base em \textit{word embeddings}. O método
proposto também não é dependente de uma língua ou de um domínio, no entanto, focou-se no
português do brasil e no domínio de produtos da web.",DISSERTAÇÃO,Agrupamento semântico de aspectos para mineração de opinião,5379181,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5399117,
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5399423,
"Research in the biodiversity field is generally interdisciplinar. It tries to solve complex problems
that need transdisciplinar knowledge and require the cooperation of researchers from different
fields. However, it is rare that two or more distinct disciplines have observations, data and
methods in formats that enabled immediate collaboration on complex and interdisciplinary
hypotheses. The speed with which any discipline obtains scientific advances depends on how
well its researchers collaborate with each other and with e-Science technologists from areas
such as databases, workflow management, visualization, and cloud computing technologies.
In this situation, cyberinfrastruture proposals are being discussed and implemented. Among
them, two are noteworthy for their high level of innovation: (a) the Conceptual Landscape
of Science Technology Enabled (CLTES) and (b) the GBIO framework. The Semantic Web
appears here not only as a new generation of tools for information representation, but also for
automation, integration, interoperability and reuse of resources. Very similar to the context
of scientific studies, in particularly in biodiversity research, the data available on the web are
only used for the presentation of content, leaving the interpretation of the information contained
therein only to human understanding. In this work, a semantic infrastructure is proposed for the
integration of scientific data on biodiversity. Its architecture is based on concepts from the GBIO
Framework integrated with the CLTES, which applies Semantic Web technologies. We sought to
develop an efficient, robust and scalable infrastructure using Semantic Web technologies applied
to biodiversity. Therefore, we can achieve the goal of Biodiversity Informatics and applies it
in Amazonian research institutions to show through the semantic integration of scientific data,
information contained in these transdisciplinary research in a clear way. We have the support of
the Laboratório de Interoperabilidade Semântica at the Instituto de Pesquisas da Amazônia and
the Núcleo de Biogeoinformática at Museu Paraense Emílio Goeldi.",Versão de defesa - Kleberson Junio do Amaral Serique.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,KLEBERSON JUNIO DO AMARAL SERIQUE,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),21/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Semantic Technologies;Biodiversity Data;SemanticWeb;RDF;DSL',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,DILVAN DE ABREU MOREIRA,152,"Tecnologias Semânticas;Dados de Biodiversidade;Web Semântica;RDF, DSL",CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Web Semântica na Criação de Anotações Inteligentes em Bioinformática,"Pesquisas na área de biodiversidade são geralmente interdisciplinares. Essas pesquisas tentam
responder problemas complexos que necessitam de conhecimento transdisciplinar e requerem
a cooperação entre pesquisadores de diversas disciplinas. No entanto, é raro que duas ou
mais disciplinas distintas tenham observações, dados e métodos em formatos que permitam
a colaboração imediata sobre hipóteses complexas e interdisciplinares. A velocidade com
que qualquer disciplina obtêm avanços científicos depende de quão bem seus pesquisadores
colaboram entre si e com tecnologistas das áreas de bancos de dados, gerenciamento de workflow,
visualização e tecnologias de computação em nuvem. Dentro desse cenário, propostas de
ciberinfraestruturas em biodiversidade vêm sendo discutidas e implementadas. Dentre elas, duas
merecem destaque pelo seu alto grau de inovação: (a) Conceptual Landscape of Technology
Enabled Science (CLTES) e (b) GBIO framework. A Web Semântica surge nesse contexto
não só como uma nova geração de ferramentas para a representação de informações, mais
também para a automação, integração, interoperabilidade e reutilização de recursos. Muito
similar ao contexto de estudos científicos, e em particular nas pesquisas de biodiversidade,
os dados, hoje, disponíveis na Web são utilizados apenas para a apresentação de conteúdo,
deixando a interpretação das informações lá contidas apenas para o entendimento humano.
Neste trabalho, uma infraestrutura semântica é proposta para a integração de dados científicos
sobre biodiversidade. Sua uma arquitetura é baseada nos conceitos do CLTES integrado ao
GBIO Framework, e aplica tecnologias da Web Semântica. Dessa forma, buscamos desenvolver
uma infraestrutura eficiente, robusta e escalável com o uso de tecnologias da Web Semântica
aplicada à Biodiversidade. Por conseguinte, poderemos alcançar o objetivo da Informática
para Biodiversidade e aplicá-lo nas instituições de pesquisa da Amazônia para demostrarmos,
através da integração semântica dos dados científicos, a transdisciplinaridade contidas nas
informações dessas pesquisas de uma maneira mais clara possível. Contamos com o apoio do
Laboratório de Interoperabilidade Semântica do Instituto de Pesquisas da Amazônia e do Núcleo
de Biogeoinformática do Museu Paraense Emílio Goeldi.",TESE,Uma infraestrutura semântica para integração de dados científicos sobre biodiversidade,5399676,1
"High school timetabling problems consist in assigning meetings between classes and teachers,
with the goal of minimizing the violation of specific soft requisites. This category of problems
has been extensively studied since the 1950s, mostly via mixed-integer programming and
metaheuristic techniques. However, the computation of optimal or near-optimal solutions using
mixed-integer programs or metaheuristics is still a challenge for most practical problems. In
this thesis, we investigate new mixed-integer programming formulations, column generation
approaches and parallel metaheuristic based algorithms to compute lower bounds and solutions
for high school timetabling problems. Extensive computational experiments conducted with
real-world instances demonstrate that our best formulations are competitive with best-known
formulations, while our parallel algorithms present superior performance than the state-of-the-art
methods.",Versão revisada - Landir Saviniec.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,LANDIR SAVINIEC,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),18/12/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'High school timetabling problem;Mixed-integer programming;Column generation;Parallel metaheuristics',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,MARISTELA OLIVEIRA DOS SANTOS,157,Problema de horários escolares;Programação linear inteira mista;Geração de colunas;Metaheurísticas paralelas,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Problemas de horários escolares consistem em alocar encontros entre turmas e professores, com
objetivo de minimizar violações a requisitos qualitativos específicos. Esta categoria de problemas
tem sido largamente estudada desde 1950, particularmente via técnicas de programação linear
inteira mista e metaheurísticas. Entretanto, a computação de soluções ótimas ou quase ótimas
usando programas inteiro-mistos ou metaheurísticas ainda é um desafio na maioria dos problemas
práticos. Nesta tese, nós investigamos novas formulações inteiro-mistas, decomposições por
geração de colunas e algoritmos baseados em metaheurísticas paralelas para computar limitantes
inferiores e soluções para problemas de horários escolares. Extensivos experimentos
computacionais conduzidos com instâncias reais demonstram que nossas melhores formulações
são competitivas com as melhores formulações existentes, enquanto nossos algoritmos paralelos
são superiores em performance computacional quando comparados com métodos que são
estado-da-arte.",TESE,Modelos e algoritmos para problemas de horários escolares,5402237,1
"The increasing availability of user-generated content in community Q&A sites has led to
the advancement of Question Answering (QA) models that relies on reuse. Such approach
can be implemented by the task of Answer Selection (AS), which consists in finding the
best answer for a given question in a pre-selected pool candidate answers. Recently, good
results have been achieved by AS models based on distributed word vectors and deep
neural networks that are used to rank answers for a given question. Convolutinal Neural
Networks (CNNs) are particularly succesful in this task. Most of the AS models are built
over datasets that contains only short and objective questions expressed as interrogative
sentences containing few words. Complex text structures are rarely considered. However,
consumer questions may be really complex. This kind of question is the main form of
seeking information in community Q&A sites, forums and customer services. Consumer
questions have characteristics that increase the difficulty of the answer selection task. In
general, they are composed of multiple interrelated sentences that are usually subjective,
and contains layman’s terms and excess of details that may be not particulary relevant.
In this work, we propose an answer selection model for consumer questions. Specifically
the contributions of this work are: (i) a definition for the “consumer questions”
research object; (ii) a new dataset of this kind of question, which we call MilkQA; and
(iii) an answer selection model, named SlimRank. MilkQA was created from an archive
of questions and answers collected by the customer service of a well-known public agricultural
research institution (Embrapa). It contains 2.6 thousand question-answer pairs
selected and anonymized by human annotators guided by the definition proposed in this
work. The analysis of questions in MilkQA led to the development of SlimRank, which
combines semantic textual graphs with CNN architectures. SlimRank was evaluated on
MilkQA and compared to baselines and two state-of-the-art answer selection models. The
results achieved by our model were much higher than the baselines and comparable to
the state of the art, but with significant reduction of computational time. Our results
suggest that combining semantic text graphs with convolutional neural networks are a
promising approach for dealing with the challenges imposed by consumer questions unique
characteristics.",Versão revisada - Marcelo Criscuolo.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,MARCELO CRISCUOLO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),16/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'answer selection;convolutional neural networks;distributed word vectors;semantic graphs',INTELIGÊNCIA COMPUTACIONAL,SANDRA MARIA ALUISIO,143,seleção de respostas;redes neurais convolutivas;vetores distribucionais;grafos semânticos,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Recursos, Métodos e Sistemas Baseados em Córpus para a Área de Processamento de Línguas Naturais (Pln) e Inteligência Artificial na Educação","A disponibilidade de conteúdo gerado por usuários em sites colaborativos de perguntas e
respostas tem impulsionado o avanço de modelos de Question Answering (QA) baseados
em reúso. Essa abordagem pode ser implementada por meio da tarefa de seleção de respostas
(Answer Selection, AS), que consiste em encontrar a melhor resposta para uma dada
pergunta em um conjunto pré-selecionado de respostas candidatas. Nos últimos anos,
abordagens baseadas em vetores distribucionais e em redes neurais profundas, em particular
em redes neurais convolutivas (CNNs), têm apresentado bons resultados na tarefa
de AS. Contudo, a maioria dos modelos é avaliada sobre córpus de perguntas objetivas
e bem formadas, contendo poucas palavras. Raramente estruturas textuais complexas
são consideradas. Perguntas de consumidores, comuns em sites colaborativos, podem ser
bastante complexas. Em geral, são representadas por múltiplas frases inter-relacionadas,
que apresentam pouca objetividade, vocabulário leigo e, frequentemente, contêm informações
em excesso. Essas características aumentam a dificuldade da tarefa de AS. Neste
trabalho, propomos um modelo de seleção de respostas para perguntas de consumidores.
São contribuições deste trabalho: (i) uma definição para o objeto de pesquisa “perguntas
de consumidores”; (ii) um novo dataset desse tipo de pergunta, chamado MilkQA; e (iii)
um modelo de seleção de respostas, chamado SlimRank. O MilkQA foi criado a partir
de um arquivo de perguntas e respostas coletadas pelo serviço de atendimento de uma
renomada instituição pública de pesquisa agropecuária (Embrapa). Anotadores guiados
pela definição de perguntas de consumidores proposta neste trabalho selecionaram 2,6 mil
pares de perguntas e respostas contidas nesse arquivo. A análise dessas perguntas levou ao
desenvolvimento do modelo SlimRank, que combina representação de textos na forma de
grafos semânticos com arquiteturas de CNNs. O SlimRank foi avaliado no dataset MilkQA
e comparado com baselines e dois modelos do estado da arte. Os resultados alcançados
pelo SlimRank foram bastante superiores aos resultados dos baselines, e compatíveis com
resultados de modelos do estado da arte; porém, com uma significativa redução do tempo
computacional. Acreditamos que a representação de textos na forma de grafos semânticos
combinada com CNNs seja uma abordagem promissora para o tratamento dos desafios
impostos pelas características singulares das perguntas de consumidores.",TESE,SlimRank: um modelo de seleção de respostas para perguntas de consumidores,5402288,1
"Similarity searching is a foundational paradigm for many modern computer applications, such as
clustering, classification and information retrieval. Within this context, the meaning of “similarity”
is related to the distance between objects, which can be formally expressed by the Metric
Spaces Theory. Many studies have focused on the inclusion of similarity search into Database
Management Systems (DBMSs) for (i) enabling similarity comparisons to be combined with the
DBMSs’ identity and order comparisons and (ii) providing scalability for very large databases.
As a step further, we propose the extension of the DBMS Query Optimizer and, particularly,
the extension of two modules of the Query Optimizer, namely Data Distribution Space and
Cost Model modules. Although the Data Distribution Space enables representations of stored
data, such representations are unsuitable for modeling the behavior of similarity comparisons,
which requires the extension of the module to support distance distributions. Likewise, the Cost
Model module must be extended to support cost models that depend on distance distributions.
Our study is based on five contributions. A new synopsis for distance distributions, called
Compact-Distance Histogram (CDH), is proposed and enables radius and selectivity estimation for
similarity searching. An experimental comparison showed the gains of the estimates drawn from
CDH in comparison to several competitors. A cost model based on the CDH synopsis and with
accurate estimates, called Stockpile, is also proposed. Omni-Histograms are presented as the
third contribution of the thesis. Such indexing structures are constructed according to histogram
partition constraints and enable the optimization of queries that combine similarity, identity
and order comparisons. The fourth contribution refers to the model RVRM, which indicates the
possible use of the estimates obtained from distance-based synopses for the query optimization
of high-dimensional datasets and identifies intervals of dimensions where similarity searching
can be efficiently executed. Finally, the thesis proposes the integration of the reviewed synopses
and cost models into a single system with a high-level language that can be coupled to a DBMS
Query Optimizer.",Versão revisada - Marcos Vinicius Naves Bedo.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,MARCOS VINICIUS NAVES BEDO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),10/10/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Similarity Searching;Query optimization;Distance concentration',BANCO DE DADOS/COMPUTAÇÃO GRÁFICA E PROCESSAMENTO DE IMAGENS,CAETANO TRAINA JUNIOR,196,Consultas por similaridade;Otimização;Concentração de distâncias,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Indexing and Data Mining in Multimedia Databases,"Consultas por similaridade constituem um paradigma de busca que fornece suporte à diversas
tarefas computacionais, tais como agrupamento, classificação e recuperação de informação.
Neste contexto, medir a similaridade entre objetos requer comparar a distância entre eles, o que
pode ser formalmente modelado pela teoria de espaços métricos. Recentemente, um grande
esforço de pesquisa tem sido dedicado à inclusão de consultas por similaridade em Sistemas
Gerenciadores de Bases de Dados (SGBDs), com o objetivo de (i) permitir a combinação de
comparações por similaridade com as comparações por identidade e ordem já existentes em
SGBDs e (ii) obter escalabilidade para grandes bases de dados. Nesta tese, procuramos dar um
próximo passo ao estendermos também o otimizador de consultas de um SGBD. Em particular,
propomos a ampliação de dois módulos do otimizador: o módulo de Espaço de Distribuição
de Dados e o módulo de Modelo de Custo. Ainda que o módulo de Espaço de Distribuição de
Dados permita representar os dados armazenados, essas representações são insuficientes para
modelar o comportamento das comparações em espaços métricos, sendo necessário estender este
módulo para contemplar distribuições de distância. De forma semelhante, o módulo Modelo
de Custo precisa ser ampliado para dar suporte à modelos de custo que utilizem estimativas
sobre distribuições de distância. Toda a investigação aqui conduzida se concentra em cinco
contribuições. Primeiro, foi criada uma nova sinopse para distribuições de distância, o Histograma
Compactado de Distância (CDH), de onde é possível inferir valores de seletividade e raios
para consultas por similaridade. Uma comparação experimental permitiu mostrar os ganhos
das estimativas da sinopse CDH com relação à diversos competidores. Também foi proposto um
modelo de custo baseado na sinopse CDH, o modelo Stockpile, cujas estimativas se mostraram
mais precisas na comparação com outros modelos. Os Histogramas-Omni são apresentados
como a terceira contribuição desta tese. Estas estruturas de indexação, construídas a partir de
restrições de particionamento de histogramas, permitem a execução otimizada de consultas que
mesclam comparações por similaridade, identidade e ordem. A quarta contribuição de nossa
investigação se refere ao modelo RVRM, que é capaz de indicar quanto é possível empregar as
estimativas das sinopses de distância para otimizar consultas por similaridade em conjuntos de
dados de alta dimensionalidade. O modelo RVRM se mostrou capaz de identificar intervalos de
dimensões para os quais essas consultas podem ser executadas eficientes. Finalmente, a última
contribuição desta tese propõe a integração das sinopses e modelos revisados em um sistema
com sintaxe de alto nível que pode ser acoplado em um otimizador de consultas.",TESE,Modelos de custo e estatísticas para consultas por similaridade,5402451,1
"Background: In recent years, mild cognitive impairment (MCI) has received great attention
because it may represent a pre-clinical stage of Alzheimer’s Disease (AD). In terms of distinction
between healthy elderly (CTL) and MCI patients, several studies have shown that speech
production is a sensitive task to detect aging effects and to differentiate individuals with MCI
from healthy ones. Natural language procesing tools have been applied to transcripts of narratives
in English and also in Brazilian Portuguese, for example, Coh-Metrix-Dementia. Gaps: However,
the absence of sentence boundary information and the presence of disfluencies in transcripts
prevent the direct application of tools that depend on well-formed texts, such as taggers and
parsers. Objectives: The main objective of this work is to develop methods to segment the
transcripts into sentences and to detect the disfluencies present in them (independently and
jointly), to serve as a preprocessing step for the application of subsequent Natural Language
Processing (NLP) tools. Methods and Evaluation: We proposed a method based on recurrent
convolutional neural networks (RCNNs) with prosodic, morphosyntactic and word embeddings
features for the sentence segmentation (SS) task. For the disfluency detection (DD) task, we
divided the method and the evaluation according to the categories of disfluencies: (i) for fillers
(filled pauses and discourse marks), we proposed the same RCNN with the same SS features
along with a predetermined list of words; (ii) for edit disfluencies (repetitions, revisions and
restarts), we added features traditionally employed in related works and introduced a CRF
model after the RCNN output layer. We evaluated all the tasks intrinsically, analyzing the
most important features, comparing the proposed methods to simpler ones, and identifying the
main hits and misses. In addition, a final method, called DeepBonDD, was created combining
all tasks and was evaluated extrinsically using 9 syntactic metrics of Coh-Metrix-Dementia.
Conclusion: For SS, we obtained F1 = 0:77 in CTL transcripts and F1 = 0:74 in MCI, achieving
the state of the art for this task on impaired speech. For the filler detection, we obtained, on
average, F1 = 0:90 for CTL and F1 = 0:92 for MCI, results that are similar to related works of
the English language. When restarts were ignored in the detection of edit disfluencies, F1 = 0:70
was obtained for CTL and F1 = 0:75 for MCI. In the extrinsic evaluation, only 3 metrics showed
a significant difference between the manual MCI transcripts and those generated by DeepBonDD,
suggesting that, despite result differences in sentence boundaries and disfluencies, DeepBonDD
is able to generate transcriptions to be properly processed by NLP tools.",Versão revisada - Marcos Vinícius Treviso.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,MARCOS VINICIUS TREVISO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),20/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Sentence Segmentation;Sentence Boundary Detection;Disfluecy Detection;Impaired Speech',INTELIGÊNCIA COMPUTACIONAL,SANDRA MARIA ALUISIO,208,Segmentação de Sentenças;Detecção de Limites de Sentença;Detecção de Disfluências;Fala Comprometida,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),"Recursos, Métodos e Sistemas Baseados em Córpus para a Área de Processamento de Línguas Naturais (Pln) e Inteligência Artificial na Educação","Contexto: Nos últimos anos, o Comprometimento Cognitivo Leve (CCL) tem recebido uma
grande atenção, pois pode representar um estágio pré-clínico da Doença de Alzheimer (DA).
Em termos de distinção entre idosos saudáveis (CTL) e pacientes com CCL, vários estudos têm
mostrado que a produção de discurso é uma tarefa sensível para detectar efeitos de envelhecimento
e para diferenciar indivíduos com CCL dos saudáveis. Ferramentas de Processamento de
Língua Natural (PLN) têm sido aplicadas em transcrições de narrativas em inglês e também em
português brasileiro, por exemplo, o ambiente Coh-Metrix-Dementia. Lacunas: No entanto, a
ausência de informações de limites de sentenças e a presença de disfluências em transcrições
impedem a aplicação direta de ferramentas que dependem de um texto bem formado, como
taggers e parsers. Objetivos: O objetivo principal deste trabalho é desenvolver métodos para
segmentar as transcrições em sentenças e detectar/remover as disfluências presentes nelas, de
modo que sirvam como uma etapa de pré-processamento para ferramentas subsequentes de
PLN. Métodos e Avaliação: Propusemos um método baseado em redes neurais recorrentes
convolucionais (RCNNs) com informações prosódicas, morfossintáticas e word embeddings para
a tarefa de segmentação de sentenças (SS). Já para a detecção de disfluências (DD), dividimos
o método e a avaliação de acordo com as categorias de disfluências: (i) para preenchimentos
(pausas preenchidas e marcadores discursivos), propusemos a mesma RCNN com as mesmas
features de SS em conjunto com uma lista pré-determinada de palavras; (ii) para disfluências de
edição (repetições, revisões e recomeços), adicionamos features tradicionalmente empregadas
em trabalhos relacionados e introduzimos um modelo de CRF na camada de saída da RCNN.
Avaliamos todas as tarefas intrinsecamente, analisando as features mais importantes, comparando
os métodos propostos com métodos mais simples, e identificando os principais acertos e erros.
Além disso, um método final, chamado DeepBonDD, foi criado combinando todas as tarefas, e
foi avaliado extrinsecamente com 9 métricas sintáticas do Coh-Metrix-Dementia. Conclusão:
Para SS, obteve-se F1 = 0:77 em transcrições de CTL e F1 = 0:74 de CCL, caracterizando o
estado-da-arte para esta tarefa em fala comprometida. Para detecção de preenchimentos, obtevese
em média F1 = 0:90 para CTL e F1 = 0:92 para CCL, resultados que estão dentro da margem
de trabalhos relacionados da língua inglesa. Ao serem ignorados os recomeços na detecção
de disfluências de edição, obteve-se em média F1 = 0:70 para CTL e F1 = 0:75 para CCL. Na
avaliação extrínseca, apenas 3 métricas mostraram diferença significativa entre as transcrições de
CCL manuais e as geradas pelo DeepBonDD, sugerindo que, apesar das variações de limites de
sentença e de disfluências, o DeepBonDD é capaz de gerar transcrições para serem processadas por ferramentas de PLN.",DISSERTAÇÃO,Segmentação de sentenças e detecção de disfluências em narrativas transcritas de testes neuropsicológicos,5402747,1
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5402881,
,,,,,,,USP_SC_CIENCIAS_DA_COMPUTACAO_E_MATEMATICA_COMPUTACIONAL_33002045004P1_7_2017_2.html,,,,,,,,,,,5403521,
"Aspect-based sentiment analysis is the field of study which extracts and interpret the sentiment,
usually classified as positive or negative, towards some target or aspect in an opinionated text.
This doctoral dissertation details an empirical study of techniques and methods for aspect
extraction in aspect-based sentiment analysis with the focus on Portuguese. Three different
approaches were explored: frequency-based, relation-based and machine learning. In each one,
this work shows a comparative study between a Portuguese and an English corpora and the
differences found in applying the approaches. In addition, richer linguistic knowledge is also
explored by using syntatic dependencies and semantic roles, leading to better results. This work
lead to the establishment of new benchmarks for the aspect extraction in Portuguese.",Versão revisada - Pedro Paulo Balage Filho.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,PEDRO PAULO BALAGE FILHO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),29/08/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Aspect-based sentiment analysis;Sentiment analysis;Opinion mining',INTELIGÊNCIA COMPUTACIONAL,THIAGO ALEXANDRE SALGUEIRO PARDO,74,Análise de sentimentos orientada a aspectos;Análise de Sentimentos;Mineração de Opiniões,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Sumarização Automática,"A análise do sentimento orientada a aspectos é o campo de estudo que extrai e interpreta o
sentimento, geralmente classificado como positivo ou negativo, em direção a algum alvo ou
aspecto em um texto de opinião. Esta tese de doutorado detalha um estudo empírico de técnicas
e métodos para extração de aspectos em análises de sentimentos baseadas em aspectos com
foco na língua Portuguesa. Foram exploradas três diferentes abordagens: métodos baseados na
frequências, métodos baseados na relação e métodos de aprendizagem de máquina. Em cada
abordagem, este trabalho mostra um estudo comparativo entre um córpus para o Português e
outro para o Inglês e as diferenças encontradas na aplicação destas abordagens. Além disso, o
conhecimento linguístico mais rico também é explorado pelo uso de dependências sintáticas e
papéis semânticos, levando a melhores resultados. Este trabalho resultou no estabelecimento de
novos padrões de avaliação para a extração de aspectos em Português.",TESE,"""Extração de aspectos em análise de sentimentos para língua portuguesa",5403872,1
"The automation of software testing involves high costs in large-scale systems, once it requires
complex test scenarios and extremely long run times. Moreover, each of its steps demands
computational resources and considerable time for running many test cases, which makes it a
bottleneck for IT companies. The benefits and opportunities offered by the combination of cloud
computing and Testing as a Service (TaaS), considered a new business and service model, can
reduce the execution time of tests in a cost-effective way and improve Return on Investment
(ROI). However, the lock-in problem, i.e., the imprisonment of the user in the platform of a
specific vendor or test service caused by the difficult migration from one TaaS provider to
another limits the effective use of such new technologies and prevents the widespread adoption
of TaaS. As studies conducted are neither rigorous, nor conclusive and mainly due to the lack of
empirical evidence, many issues must be investigated from the perspective of migration among
TaaS providers. This doctoral thesis addresses a comparative evaluation of the impact caused
by the vendor lock-in problem on the writing, configuration, execution and management of
automated tests in cloud computing. A comparative evaluation of Multi-TaaS and conventional
migration approaches regarding the difficulty, efficiency, effectiveness and effort of migration
among TaaS providers was conducted through a controlled experiment. According to the results,
the Multi-TaaS approach facilitates the exchange of test service, increases efficiency, i.e., reduces
the time spent, the effort of migration among TaaS providers and maintenance costs and improves
the final quality of the web software developed. It contributes to the portability of automated
tests in the cloud and summarization of the results of tests and, consequently, can assist software
engineers regarding quality control and decision-making. We can conclude the portability
and interoperability are two important quality attributes that influence and contribute to the
widespread adoption of the TaaS service model in cloud computing.",Versão de defesa - Ricardo Ramos de Oliveira.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,RICARDO RAMOS DE OLIVEIRA,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),14/12/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Cloud Computing;Testing as a Service (TaaS);Vendor Lock-in;Test Automation;Controlled Experiment',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ADENILSO DA SILVA SIMAO,227,Computação em Nuvem;Teste como Serviço (TaaS);Vendor Lock-in;Automatização de Testes;Experimento Controlado,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Estudo Teórico e Aplicado de Critérios de Teste e Validação na  Produção de Software,"O processo de automatização de teste de software possui alto custo envolvido em sistemas de
larga escala, pois exigem cenários de teste complexos e tempos de execução extremamente
longos. Além disso, cada etapa do processo de teste requer recursos computacionais e um tempo
considerável para a execução de muitos casos de teste, tornando-se um gargalo para as empresas
de TI. Neste contexto, os benefícios e oportunidades oferecidos pela combinação da computação
em nuvem com o Teste como Serviço (Testing as a Service, TaaS) que é considerado um novo
modelo de negócio e de serviço atraente e promissor, podem proporcionar um impacto positivo
na redução do tempo de execução dos testes de maneira custo-efetiva e aumentar o retorno sobre
o investimento (ROI). Todavia, existe o problema de vendor lock-in que é o aprisionamento do
usuário à plataforma de um fornecedor específico ou serviço de teste, ocasionado pela dificuldade
de migrar de um fornecedor TaaS para outro, limitando a utilização dessas novas tecnologias
de maneira efetiva e eficiente, impedindo assim, a ampla adoção do TaaS. Devido aos estudos
existentes não serem rigorosos ou conclusivos e principalmente devido à falta de evidência
empírica na área de serviço de teste, muitas questões devem ser investigadas na perspectiva da
migração entre os provedores de TaaS. O objetivo desse trabalho é reduzir o impacto ocasionado
pelo problema de vendor lock-in no processo de automatização de testes na computação em
nuvem, ou seja, na escrita, configuração, execução e gerenciamento dos resultados de testes
automatizados por meio de experimentos controlados. Foram realizadas avaliações da eficiência,
efetividade, dificuldade e do esforço de migração entre fornecedores de TaaS comparando a
abordagem Multi-TaaS com a abordagem convencional por meio de um experimento controlado.
Os resultados deste trabalho indicam que a nova abordagem permite facilitar a troca do serviço de
teste, melhorar a eficiência e principalmente reduzir o esforço de migração entre os fornecedores
de TaaS. Os estudos realizados no experimento controlado são promissores e podem auxiliar os
engenheiros de software no controle de qualidade e na tomada de decisão de forma eficiente e ao
mesmo tempo reduzir os custos de manutenção por meio da redução do esforço de migração entre
fornecedores de TaaS, contribuindo principalmente para a portabilidade dos testes automatizados
na nuvem e da sumarização dos resultados dos testes e que consequentemente, possibilite ampliar
também a adoção do modelo de serviço TaaS na computação em nuvem.",TESE,Avaliação da portabilidade entre fornecedores de teste como serviço na computação em nuvem,5404312,1
"This thesis describes the application of KIII, a biologically more plausible neural network
model, for forecasting economic time series. K-sets are connectionist models based on neural
populations and have been used in many machine learning applications, including time series
prediction. In this thesis, this method was applied to IPCA, a Brazilian consumer price index
surveyed by IBGE in 13 metropolitan areas. The values ranged from August 1994 to June 2017.
Experiments were performed using four non-parametric models (KIII, continuous kNN, classical
ANN, and SVM) and four parametric methods: ARIMA, SARIMA, Moving Average, SES, Holt,
Additive Holt–Winters, and Multiplicative Holt–Winters. The statistical metric RMSE was used
to compare methods performance. Freeman’s KIII sets worked well as filter, improving method
performance, but it was not a good prediction method, and was overcome in most experiments
by other time series prediction methods. This thesis contributes with the use of non-parametrics
models for forecasting inflation in a developing country.",Versão revisada - Victor Henrique Gonçalves.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,VICTOR HENRIQUE GONCALVES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),24/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),"b'Time Series;K-Sets, Forecasting;Non-Parametric Methods;Parametric Methods'",INTELIGÊNCIA COMPUTACIONAL,JOAO LUIS GARCIA ROSA,79,Séries Temporais;Conjuntos K;Previsão;Métodos Não Paramétricos;Métodos Paramétricos,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Desenvolvimento de Algoritmos e Técnicas Computacionais para Aplicação em  Interfaces Cérebro-Computador,"Esta dissertação descreve a aplicação do KIII, um modelo de rede neural biologicamente mais
plausível, para a previsão de séries temporais econômicas. Os conjuntos K são modelos conexionistas
baseados em populações de neurônios e foram usados em muitas aplicações de
aprendizado de máquina, incluindo previsões de séries temporais. Nesta dissertação, este método
foi aplicado ao IPCA, um índice de preços ao consumidor brasileiro pesquisado pelo IBGE
em 13 regiões metropolitanas. Os valores abrangem o período de agosto de 1994 a junho de
2017. Os experimentos foram realizados utilizando quatro modelos não-paramétricos (KIII, kNN
contínuo, RNAs clássicas e SVM) e seis métodos paramétricos: ARIMA, SARIMA, Médias
Móveis, SES, Holt, Holt-Winters Aditivo e Holt-Winters Multiplicativo. A médida estatística
RMSE foi utilizada para comparar o desempenho dos métodos. Os conjuntos KIII de Freeman
funcionaram bem como um filtro, melhorando o desempenho do método, mas não foram um bom
método de previsão, sendo superado, na maior parte dos experimentos, por outros métodos de
prévisão de séries temporais. Esta dissertação contribui com o uso de modelos não paramétricos
para prever a inflação em um país em desenvolvimento.",DISSERTAÇÃO,Previsão de séries temporais econômicas usando redes neurais caóticas,5409025,1
"This doctoral dissertation addresses the simultaneous lot sizing and scheduling problem in a real
world production environment where production lines share scarce production resources. Due to
the lack of resources, the production lines cannot operate all simultaneously and they need to be
assembled in each period respecting the capacity constraints of the resources. This dissertation
presents mixed integer programming models to deal with the problem as well as various heuristic
approaches: constructive and improvement procedures based on the mathematical formulation
of the problem and lagrangian heuristics. Relax-and-fix heuristics exploring some partitions
of the set of binary variables of a model and a decomposition based heuristic are proposed
to construct solutions. Fix-and-optimize heuristics and iterative MIP-based neighbourhood
search matheuristics are proposed to improvement solutions obtained by constructive procedures.
Computational tests are performed with randomly instances and show that the proposed methods
can find better solutions than the Branch-and-Cut algorithm of a commercial solver for medium
and large size instances.",Versão revisada - Willy Alves de Oliveira Soler.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,WILLY ALVES DE OLIVEIRA SOLER,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),21/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Lot sizing and scheduling problem;Mixed integer programming;Heuristics;Lagrangian relaxation',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,MARISTELA OLIVEIRA DOS SANTOS,131,Dimensionamento e sequenciamento de lotes;Programação matemática inteira mista;Heurísticas;Relaxação lagrangiana,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Otimização e Pesquisa Operacional,"Esta tese aborda um problema de dimensionamento e sequenciamento de lotes de produção
baseado em uma indústria alimentícia brasileira que opera por meio de diversas linhas de
produção heterogêneas. Nesse ambiente produtivo, as linhas de produção compartilham recursos
escassos, tais como, trabalhadores e máquinas e devem ser montadas (ativadas) em cada período
produtivo, respeitando-se a capacidade disponível de cada recurso necessário para ativação das
mesmas. Modelos de programação matemática inteira mista são propostos para representação do
problema, bem como diversos métodos heurísticos de solução, compreendendo procedimentos
construtivos e de melhoramento baseados na formulação matemática do problema e heurísticas
lagrangianas. São propostas heurísticas do tipo relax-and-fix explorando diversas partições
das variáveis binárias dos modelos e uma heurística baseada na decomposição do modelo para
construção de soluções. Procedimentos do tipo fix-and-optimize e matheuristics do tipo iterative
MIP-based neighbourhood search são propostas para o melhoramento das soluções iniciais
obtidas pelos procedimentos construtivos. Testes computacionais são realizados com instâncias
geradas aleatoriamente e mostram que os métodos propostos são capazes de oferecer melhores
soluções do que o algoritmo Branch-and-Cut de um resolvedor comercial para instâncias de
médio e grande porte.",TESE,"Análise, proposição e solução de modelos para o problema integrado de dimensionamento de lotes e sequenciamento da produção",5409028,1
"Engenharia de Linha de Produto de Software (SPLE) é uma abordagem utilizada no
desenvolvimento de produtos similares, que explora a reutilização sistemática de artefatos
de software. O processo da SPLE executa várias atividades para garantir a qualidade do
software. Atividades de garantia de qualidade são fundamentais para alcançar e manter
altos níveis de qualidade em todos os tipos de artefatos de software, tais como produtos e
processos. Atividades de teste são amplamente utilizadas na indústria para o gerenciamento
de qualidade. No entanto, o esforço para a aplicação de testes geralmente é alto e melhorar
a eficiência dos testes é um desafio relacionado a todas as atividades da engenharia de
sistemas. Uma maneira de melhorar a eficiência da atividade de teste é automatizar a
geração e execução dos testes. A geração automática de testes pode ser realizada por
abordagens tais como o Teste Baseado em Modelos (TBM), em que o comportamento real
do sistema de software é comparado a um modelo de teste abstrato.
Várias técnicas, processos e estratégias foram desenvolvidas para o teste de SPLE, contudo,
existem diversos desafios nessa área de pesquisa. O desafio em foco é a redução do esforço
geral de teste necessário para testar produtos da SPLE. O esforço de teste pode ser
reduzido maximizando o reuso de teste usando modelos que representam variabilidades
entre os produtos.
O objetivo da tese é automatizar a geração de pequenos conjuntos de testes com alta
capacidade de detecção de falhas e baixa redundância de teste entre produtos. Para
alcançar tal objetivo, testes equivalentes são identificados e reutilizados para um conjunto
de produtos usando conjuntos de teste completos e configuráveis. Duas direções de pesquisa
são exploradas, uma centrada no produto e a outra baseada na arquitetura reutilizável.
Foram gerados conjuntos de teste que tenham cobertura de falhas completa a partir
de máquinas de estado com e sem restrições de características. A implementação de
uma ferramenta foi desenvolvida para automatizar a geração de teste. Além disso, a
abordagem proposta foi avaliada usando exemplos, estudos experimentais e um estudo de
caso industrial.",,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,VANDERSON HAFEMANN FRAGAL,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),28/11/2017,INGLES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Linha de Produto de Software;Teste Baseado em Modelos;Gerao de Teste;Cobertura de Falhas Completa',ENGENHARIA DE SOFTWARE E SISTEMAS DE INFORMAÇÃO/SISTEMAS WEB E MULTIMÍDIA INTERATIVOS,ADENILSO DA SILVA SIMAO,163,Software Product Line;Model-Based Testing;Test Generation;Full Fault Coverage,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Estudo Teórico e Aplicado de Critérios de Teste e Validação na  Produção de Software,"Software Product Line Engineering (SPLE) is an approach used in the development of
similar products, which explores the systematic reuse of software artifacts. The SPLE
process has several activities that are executed to ensure software quality. Quality assurance
is of vital importance for achieving and maintaining a high quality of all kinds of artifacts,
such as products and processes. Testing activities are widely used in the industry for
quality management. However, the effort for applying testing is usually high and increasing
the testing efficiency is a major concern of all system engineering activities. A common
means of increasing efficiency is automation of the test execution and the test design.
Automated test design can be performed using approaches such as Model-Based Testing
(MBT) where the real behavior of a software system is compared to an abstract test model.
Several techniques, processes, and strategies were developed for SPLE testing, but still
many problems are open in this area of research. The challenge in focus is the reduction
of the overall test effort required to test SPLE products. Test effort can be reduced by
maximizing test reuse using models that take advantage of the similarity between products.
The thesis goal is to automate the generation of small test-suites with high fault detection
and low test redundancy between products. To achieve the goal, equivalent tests are
identified and reused for a set of products using complete and configurable test-suites.
Two research directions are explored, one is product-based centered, and the other is based
on the reusable architecture. For test design, test-suites that have full fault coverage
were generated from state machines with and without feature constraints. A prototype
implementation tool was developed for test design automation. Moreover, the proposed
approach was evaluated using standard examples, experimental studies, and an industrial
case study.",TESE,Geração automática de conjuntos de teste configuráveis para linhas de produto de software,5409039,1
"Elliptic equations with interface problems are often encountered in fluid dynamics and material
sciences. In particular, the Immersed Interface Method (IIM) figures among the most effective
approaches, where traditionally it is used to simulate the flow behaviour over complex bodies
immersed in Cartesian mesh. This thesis introduces a new immersed interface to solve
Poisson equations with discontinuous coefficients and singular sources on Cartesian grids. The
immersed interface method (IIM) is regenerated for a comparison study of the proposed method.
The robustness of this method is verified against the large magnitude of the jump discontinuity
across the interface. The nature of high efficiency of this method is extensively validated
via solving some elliptic problems with discontinuous coefficients and singular sources in
two-dimensions.",Versão de defesa - Marilaine Colnago.pdf,CIÊNCIAS  DE COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL,MARILAINE COLNAGO,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),23/11/2017,PORTUGUES,UNIVERSIDADE DE SÃO PAULO ( SÃO CARLOS ),b'Immersed Interface Method;Eliptic Equation;Poisson Equation',MECÂNICA DOS FLUIDOS COMPUTACIONAL/OTIMIZAÇÃO/MODELOS ESTOCÁSTICOS,LEANDRO FRANCO DE SOUZA,107,Método de Interface Imersa;Equações Elípticas;Equação de Poisson,CIÊNCIAS DA COMPUTAÇÃO E MATEMÁTICA COMPUTACIONAL (33002045004P1),Solução Numérica das Equações de Navier-Stokes - Escoamentos Tridimensionais,"As equações elípticas com presença de interface ou descontinuidades são frequentemente
encontradas na dinâmica de fluidos e nas ciências dos materiais. Em particular, o Método de
Interface Imersa (IIM) figura entre as abordagens mais eficazes para solução desses problemas,
onde tradicionalmente é usado para simular o comportamento do fluxo sobre corpos complexos
imersos em malha cartesiana. Esta tese apresenta um novo método de interface imersa para
resolver equações de Poisson com coeficientes descontínuos e fontes singulares em malhas
cartesianas. O método de interface imersa (IIM) é regenerado para um estudo comparativo
do método proposto. A robustez deste método é verificada em relação à grande magnitude
da descontinuidade do salto em toda a interface. A natureza da alta eficiência deste método
é amplamente validada através da resolução de alguns problemas elípticos com coeficientes
descontínuos e fontes singulares em duas dimensões.",TESE,Um método de interface imersa de alta ordem para a resolução de equações elípticas com coeficientes descontínuos,5409040,1
